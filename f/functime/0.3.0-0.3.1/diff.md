# Comparing `tmp/functime-0.3.0-py3-none-any.whl.zip` & `tmp/functime-0.3.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,44 +1,43 @@
-Zip file size: 57432 bytes, number of entries: 42
--rw-r--r--  2.0 unx       66 b- defN 23-Jul-24 08:42 functime/__init__.py
--rw-r--r--  2.0 unx      117 b- defN 23-Jul-24 08:42 functime/__main__.py
--rw-r--r--  2.0 unx     5893 b- defN 23-Jul-24 08:42 functime/backtesting.py
--rw-r--r--  2.0 unx     1838 b- defN 23-Jul-24 08:42 functime/conformal.py
--rw-r--r--  2.0 unx     1379 b- defN 23-Jul-24 08:42 functime/conversion.py
--rw-r--r--  2.0 unx     5549 b- defN 23-Jul-24 08:42 functime/cross_validation.py
--rw-r--r--  2.0 unx      123 b- defN 23-Jul-24 08:42 functime/embeddings.py
--rw-r--r--  2.0 unx     1495 b- defN 23-Jul-24 08:42 functime/offsets.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 08:42 functime/plotting.py
--rw-r--r--  2.0 unx    20397 b- defN 23-Jul-24 08:42 functime/preprocessing.py
--rw-r--r--  2.0 unx     1976 b- defN 23-Jul-24 08:42 functime/ranges.py
--rw-r--r--  2.0 unx     1481 b- defN 23-Jul-24 08:42 functime/stats.py
--rw-r--r--  2.0 unx      217 b- defN 23-Jul-24 08:42 functime/base/__init__.py
--rw-r--r--  2.0 unx     8253 b- defN 23-Jul-24 08:42 functime/base/forecaster.py
--rw-r--r--  2.0 unx     1630 b- defN 23-Jul-24 08:42 functime/base/metric.py
--rw-r--r--  2.0 unx     2771 b- defN 23-Jul-24 08:42 functime/base/model.py
--rw-r--r--  2.0 unx     2237 b- defN 23-Jul-24 08:42 functime/base/transformer.py
--rw-r--r--  2.0 unx      285 b- defN 23-Jul-24 08:42 functime/feature_extraction/__init__.py
--rw-r--r--  2.0 unx     4064 b- defN 23-Jul-24 08:42 functime/feature_extraction/calendar.py
--rw-r--r--  2.0 unx      745 b- defN 23-Jul-24 08:42 functime/forecasting/__init__.py
--rw-r--r--  2.0 unx    12624 b- defN 23-Jul-24 08:42 functime/forecasting/_ar.py
--rw-r--r--  2.0 unx     4909 b- defN 23-Jul-24 08:42 functime/forecasting/_evaluate.py
--rw-r--r--  2.0 unx     2231 b- defN 23-Jul-24 08:42 functime/forecasting/_reduction.py
--rw-r--r--  2.0 unx     7569 b- defN 23-Jul-24 08:42 functime/forecasting/_regressors.py
--rw-r--r--  2.0 unx     8515 b- defN 23-Jul-24 08:42 functime/forecasting/automl.py
--rw-r--r--  2.0 unx     2175 b- defN 23-Jul-24 08:42 functime/forecasting/catboost.py
--rw-r--r--  2.0 unx     3714 b- defN 23-Jul-24 08:42 functime/forecasting/censored.py
--rw-r--r--  2.0 unx     1010 b- defN 23-Jul-24 08:42 functime/forecasting/knn.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Jul-24 08:42 functime/forecasting/lance.py
--rw-r--r--  2.0 unx     4215 b- defN 23-Jul-24 08:42 functime/forecasting/lightgbm.py
--rw-r--r--  2.0 unx     3958 b- defN 23-Jul-24 08:42 functime/forecasting/linear.py
--rw-r--r--  2.0 unx     2412 b- defN 23-Jul-24 08:42 functime/forecasting/xgboost.py
--rw-r--r--  2.0 unx      229 b- defN 23-Jul-24 08:42 functime/metrics/__init__.py
--rw-r--r--  2.0 unx     3826 b- defN 23-Jul-24 08:42 functime/metrics/multi_objective.py
--rw-r--r--  2.0 unx     6810 b- defN 23-Jul-24 08:42 functime/metrics/point.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-24 08:42 functime/metrics/probabilistic.py
--rw-r--r--  2.0 unx    34523 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     6366 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       52 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3510 b- defN 23-Jul-24 08:43 functime-0.3.0.dist-info/RECORD
-42 files, 172968 bytes uncompressed, 51842 bytes compressed:  70.0%
+Zip file size: 57211 bytes, number of entries: 41
+-rw-r--r--  2.0 unx       66 b- defN 23-Jul-26 23:58 functime/__init__.py
+-rw-r--r--  2.0 unx      117 b- defN 23-Jul-26 23:58 functime/__main__.py
+-rw-r--r--  2.0 unx     5908 b- defN 23-Jul-26 23:58 functime/backtesting.py
+-rw-r--r--  2.0 unx     1838 b- defN 23-Jul-26 23:58 functime/conformal.py
+-rw-r--r--  2.0 unx     1379 b- defN 23-Jul-26 23:58 functime/conversion.py
+-rw-r--r--  2.0 unx     5549 b- defN 23-Jul-26 23:58 functime/cross_validation.py
+-rw-r--r--  2.0 unx      123 b- defN 23-Jul-26 23:58 functime/embeddings.py
+-rw-r--r--  2.0 unx     1495 b- defN 23-Jul-26 23:58 functime/offsets.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 23:58 functime/plotting.py
+-rw-r--r--  2.0 unx    20125 b- defN 23-Jul-26 23:58 functime/preprocessing.py
+-rw-r--r--  2.0 unx     1976 b- defN 23-Jul-26 23:58 functime/ranges.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-26 23:58 functime/base/__init__.py
+-rw-r--r--  2.0 unx     8253 b- defN 23-Jul-26 23:58 functime/base/forecaster.py
+-rw-r--r--  2.0 unx     1630 b- defN 23-Jul-26 23:58 functime/base/metric.py
+-rw-r--r--  2.0 unx     2771 b- defN 23-Jul-26 23:58 functime/base/model.py
+-rw-r--r--  2.0 unx     2155 b- defN 23-Jul-26 23:58 functime/base/transformer.py
+-rw-r--r--  2.0 unx      285 b- defN 23-Jul-26 23:58 functime/feature_extraction/__init__.py
+-rw-r--r--  2.0 unx     4294 b- defN 23-Jul-26 23:58 functime/feature_extraction/calendar.py
+-rw-r--r--  2.0 unx      745 b- defN 23-Jul-26 23:58 functime/forecasting/__init__.py
+-rw-r--r--  2.0 unx    12667 b- defN 23-Jul-26 23:58 functime/forecasting/_ar.py
+-rw-r--r--  2.0 unx     4909 b- defN 23-Jul-26 23:58 functime/forecasting/_evaluate.py
+-rw-r--r--  2.0 unx     2231 b- defN 23-Jul-26 23:58 functime/forecasting/_reduction.py
+-rw-r--r--  2.0 unx     7569 b- defN 23-Jul-26 23:58 functime/forecasting/_regressors.py
+-rw-r--r--  2.0 unx     8513 b- defN 23-Jul-26 23:58 functime/forecasting/automl.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Jul-26 23:58 functime/forecasting/catboost.py
+-rw-r--r--  2.0 unx     3717 b- defN 23-Jul-26 23:58 functime/forecasting/censored.py
+-rw-r--r--  2.0 unx     1010 b- defN 23-Jul-26 23:58 functime/forecasting/knn.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Jul-26 23:58 functime/forecasting/lance.py
+-rw-r--r--  2.0 unx     4208 b- defN 23-Jul-26 23:58 functime/forecasting/lightgbm.py
+-rw-r--r--  2.0 unx     3958 b- defN 23-Jul-26 23:58 functime/forecasting/linear.py
+-rw-r--r--  2.0 unx     2412 b- defN 23-Jul-26 23:58 functime/forecasting/xgboost.py
+-rw-r--r--  2.0 unx      229 b- defN 23-Jul-26 23:58 functime/metrics/__init__.py
+-rw-r--r--  2.0 unx     3826 b- defN 23-Jul-26 23:58 functime/metrics/multi_objective.py
+-rw-r--r--  2.0 unx     6810 b- defN 23-Jul-26 23:58 functime/metrics/point.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-26 23:58 functime/metrics/probabilistic.py
+-rw-r--r--  2.0 unx    34523 b- defN 23-Jul-26 23:59 functime-0.3.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7412 b- defN 23-Jul-26 23:59 functime-0.3.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-26 23:59 functime-0.3.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       52 b- defN 23-Jul-26 23:59 functime-0.3.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-26 23:59 functime-0.3.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3436 b- defN 23-Jul-26 23:59 functime-0.3.1.dist-info/RECORD
+41 files, 172387 bytes uncompressed, 51731 bytes compressed:  70.0%
```

## zipnote {}

```diff
@@ -27,17 +27,14 @@
 
 Filename: functime/preprocessing.py
 Comment: 
 
 Filename: functime/ranges.py
 Comment: 
 
-Filename: functime/stats.py
-Comment: 
-
 Filename: functime/base/__init__.py
 Comment: 
 
 Filename: functime/base/forecaster.py
 Comment: 
 
 Filename: functime/base/metric.py
@@ -102,26 +99,26 @@
 
 Filename: functime/metrics/point.py
 Comment: 
 
 Filename: functime/metrics/probabilistic.py
 Comment: 
 
-Filename: functime-0.3.0.dist-info/LICENSE
+Filename: functime-0.3.1.dist-info/LICENSE
 Comment: 
 
-Filename: functime-0.3.0.dist-info/METADATA
+Filename: functime-0.3.1.dist-info/METADATA
 Comment: 
 
-Filename: functime-0.3.0.dist-info/WHEEL
+Filename: functime-0.3.1.dist-info/WHEEL
 Comment: 
 
-Filename: functime-0.3.0.dist-info/entry_points.txt
+Filename: functime-0.3.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: functime-0.3.0.dist-info/top_level.txt
+Filename: functime-0.3.1.dist-info/top_level.txt
 Comment: 
 
-Filename: functime-0.3.0.dist-info/RECORD
+Filename: functime-0.3.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## functime/backtesting.py

```diff
@@ -143,22 +143,22 @@
                 max_horizons=forecaster.max_horizons,
                 artifacts=forecaster.state["artifacts"],
             )
             y_resid = y_resid.with_columns(pl.lit(i).alias("split"))
             y_resids.append(y_resid)
 
     y_preds = pl.concat(y_preds)
-    full_model = forecaster.fit(y=y, X=X)
+    full_forecaster = forecaster.fit(y=y, X=X)
     if residualize:
         y_resids = _merge_autoreg_residuals(
             y=y,
             X=X,
             y_resids=pl.concat(y_resids),
-            strategy=full_model.state.strategy,
+            strategy=full_forecaster.state.strategy,
             lags=forecaster.lags,
             max_horizons=forecaster.max_horizons,
-            artifacts=full_model.state.artifacts,
+            artifacts=full_forecaster.state.artifacts,
         )
         pl.enable_string_cache(False)
         return y_preds, y_resids
     pl.enable_string_cache(False)
     return y_preds
```

## functime/preprocessing.py

```diff
@@ -297,36 +297,26 @@
         idx_cols = X.columns[:2]
         entity_col, time_col = idx_cols
         numeric_cols = X.select(PL_NUMERIC_COLS(entity_col, time_col)).columns
         boolean_cols = None
         _mean = None
         _std = None
         if use_mean:
-            X = X.with_columns(
-                PL_NUMERIC_COLS(entity_col, time_col)
-                .mean()
-                .over(entity_col)
-                .suffix("_mean")
-            )
-            mean_cols = [col for col in X.columns if col.endswith("_mean")]
-            _mean = X.select([*idx_cols, *mean_cols])
-            X = X.select(
+            _mean = X.groupby(entity_col).agg(
+                PL_NUMERIC_COLS(entity_col, time_col).mean().suffix("_mean")
+            )
+            X = X.join(_mean, on=entity_col).select(
                 idx_cols + [pl.col(col) - pl.col(f"{col}_mean") for col in numeric_cols]
             )
         if use_std:
-            X = X.with_columns(
-                PL_NUMERIC_COLS(entity_col, time_col)
-                .std()
-                .over(entity_col)
-                .suffix("_std")
-            )
-            std_cols = [col for col in X.columns if col.endswith("_std")]
-            _std = X.select([*idx_cols, *std_cols])
-            X = X.select(
-                idx_cols + [pl.col(col) / pl.col(f"{col}_std") for col in numeric_cols]
+            _std = X.groupby(entity_col).agg(
+                PL_NUMERIC_COLS(entity_col, time_col).mean().suffix("_std")
+            )
+            X = X.join(_std, on=entity_col).select(
+                idx_cols + [pl.col(col) - pl.col(f"{col}_std") for col in numeric_cols]
             )
         if rescale_bool:
             boolean_cols = X.select(pl.col(pl.Boolean)).columns
             X = X.with_columns(pl.col(pl.Boolean).cast(pl.Int8) * 2 - 1)
         artifacts = {
             "X_new": X,
             "numeric_cols": numeric_cols,
@@ -334,24 +324,25 @@
             "_mean": _mean,
             "_std": _std,
         }
         return artifacts
 
     def invert(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
         idx_cols = X.columns[:2]
+        entity_col = idx_cols[0]
         artifacts = state.artifacts
         numeric_cols = artifacts["numeric_cols"]
         if use_std:
             _std = artifacts["_std"]
-            X = X.join(_std, on=idx_cols, how="left").select(
+            X = X.join(_std, on=entity_col, how="left").select(
                 idx_cols + [pl.col(col) * pl.col(f"{col}_std") for col in numeric_cols]
             )
         if use_mean:
             _mean = artifacts["_mean"]
-            X = X.join(_mean, on=idx_cols, how="left").select(
+            X = X.join(_mean, on=entity_col, how="left").select(
                 idx_cols + [pl.col(col) + pl.col(f"{col}_mean") for col in numeric_cols]
             )
         if rescale_bool:
             X = X.with_columns(pl.col(artifacts["boolean_cols"]).cast(pl.Int8))
         return X
 
     def transform_new(state: ModelState, X: pl.LazyFrame) -> pl.LazyFrame:
```

## functime/base/forecaster.py

```diff
@@ -39,15 +39,15 @@
         Only applied if `strategy` equals "direct" or "ensemble".
     strategy : Optional[str]
         Forecasting strategy. Currently supports "recursive", "direct",
         and "ensemble" of both recursive and direct strategies.
     target_transform : Optional[Transformer]
         functime transformer to apply to `y` before fit. The transform is inverted at predict time.
     **kwargs : Mapping[str, Any]
-        Additional keyword arguments passed into underlying sklearn-compatible estimator.
+        Additional keyword arguments passed into underlying sklearn-compatible regressor.
     """
 
     def __init__(
         self,
         freq: Union[str, None],
         lags: int,
         max_horizons: Optional[int] = None,
```

## functime/base/transformer.py

```diff
@@ -12,38 +12,34 @@
 
 DF_TYPE = Union[pl.LazyFrame, pl.DataFrame]
 
 
 class Transformer:
     """A transformer."""
 
-    def __init__(self, model: Callable, *args, **kwargs):
-        self.model = model
+    def __init__(self, transf: Callable, *args, **kwargs):
+        self.transf = transf
         self.args = args
         self.kwargs = kwargs
         self.state = None
 
     @property
     def func(self):
-        return self.model(*self.args, **self.kwargs)
+        return self.transf(*self.args, **self.kwargs)
 
     @property
     def params(self):
-        model = self.model
+        transf = self.transf
         kwargs = self.kwargs
-        sig = inspect.signature(model)
-        model_params = sig.parameters
-        model_args = list(model_params.keys())
+        sig = inspect.signature(transf)
+        params = sig.parameters
+        args = list(params.keys())
         params = {
-            **{
-                k: kwargs.get(k, v.default)
-                for k, v in model_params.items()
-                if k != "kwargs"
-            },
-            **{model_args[i]: p for i, p in enumerate(self.args)},
+            **{k: kwargs.get(k, v.default) for k, v in params.items() if k != "kwargs"},
+            **{args[i]: p for i, p in enumerate(self.args)},
         }
         return params
 
     def __call__(self, X: DF_TYPE):
         return self.transform(X)
 
     @cached_property
@@ -66,13 +62,13 @@
 
     def transform_new(self, X: DF_TYPE) -> pl.LazyFrame:
         transform = self.func[2]
         X_new = transform(state=self.state, X=X.lazy())
         return X_new
 
 
-def transformer(model: Callable[P, R]):
-    @wraps(model)
+def transformer(transf: Callable[P, R]):
+    @wraps(transf)
     def _transformer(*args: P.args, **kwargs: P.kwargs) -> Transformer:
-        return Transformer(model, *args, **kwargs)
+        return Transformer(transf, *args, **kwargs)
 
     return _transformer
```

## functime/feature_extraction/calendar.py

```diff
@@ -9,14 +9,15 @@
 
 
 @transformer
 def add_calendar_effects(
     attrs: List[
         Literal["minute", "hour", "day", "weekday", "week", "month", "quarter", "year"]
     ],
+    as_dummies: bool = False,
 ):
     """Extract calendar effects from time column, returns calendar effects as categorical columns.
 
     Parameters
     ----------
     attrs : list of str
         List of calendar effects to be applied to the time column:\n
@@ -24,27 +25,31 @@
         - "hour"
         - "day"
         - "weekday"
         - "week"
         - "month"
         - "quarter"
         - "year"
+    as_dummies : bool
+        Returns calendar effects as columns of one-hot-encoded dummies.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
         time_col = pl.col(X.columns[1])
         X_new = X.with_columns(
             [
                 getattr(time_col.dt, attr)()
                 .alias(attr)
                 .cast(pl.Utf8)
                 .cast(pl.Categorical)
                 for attr in attrs
             ]
         )
+        if as_dummies:
+            X_new = X_new.collect(streaming=True).to_dummies(columns=attrs).lazy()
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
```

## functime/forecasting/_ar.py

```diff
@@ -32,19 +32,19 @@
     X_final, y_final = pl.collect_all(
         [
             X_y_final.select(pl.all().exclude(target_col)),
             X_y_final.select([*X_y_final.columns[:2], target_col]),
         ]
     )
     # 2. Fit
-    fitted_model = regress(X=X_final, y=y_final)
+    fitted_regressor = regress(X=X_final, y=y_final)
     # 3. Collect artifacts
     y_lag = make_y_lag(X_y_final, target_col=y.columns[-1], lags=lags)
     artifacts = {
-        "regressor": fitted_model,
+        "regressor": fitted_regressor,
         "y_lag": y_lag.collect(streaming=True),
     }
     return artifacts
 
 
 def fit_direct(
     regress: Callable[[pl.LazyFrame, pl.LazyFrame], Any],
@@ -55,26 +55,26 @@
 ) -> Mapping[str, Any]:
     idx_cols = y.columns[:2]
     target_col = y.columns[-1]
     feature_cols = X.columns[2:] if X is not None else []
     # 1. Impose AR structure
     X_y_final = make_direct_reduction(lags=lags, max_horizons=max_horizons, y=y, X=X)
     # 2. Fit
-    fitted_models = []
+    fitted_regressors = []
     for i in trange(1, max_horizons + 1, desc="Fitting direct forecasters:"):
         selected_lags = range(i, lags + i)
         lag_cols = [f"{target_col}__lag_{j}" for j in selected_lags]
         X_final = X_y_final.select([*idx_cols, *lag_cols, *feature_cols])
         y_final = X_y_final.select([*idx_cols, target_col])
-        fitted_model = regress(X=X_final, y=y_final)
-        fitted_models.append(fitted_model)
+        fitted_regressor = regress(X=X_final, y=y_final)
+        fitted_regressors.append(fitted_regressor)
     # 3. Collect artifacts
     y_lag = make_y_lag(X_y_final, target_col=y.columns[-1], lags=lags + max_horizons)
     artifacts = {
-        "regressors": fitted_models,
+        "regressors": fitted_regressors,
         "y_lag": y_lag.collect(streaming=True),
     }
     return artifacts
 
 
 def fit_autoreg(
     regress: Callable[[pl.LazyFrame, pl.LazyFrame], Any],
@@ -192,21 +192,21 @@
         **best_params,
         "max_horizons": max_horizons,
         "strategy": strategy,
         **kwargs,
     }
     best_params["lags"] = best_lags
     logging.info("✅ Found `best_params` %s", best_params)
-    best_model = forecaster_cls(**best_params)
-    best_model.fit(y=y, X=X)
+    best_forecaster = forecaster_cls(**best_params)
+    best_forecaster.fit(y=y, X=X)
     # Prepare artifacts
     # TODO: Investigate ensembling across hyperparameter sets
     # Ref: https://arxiv.org/abs/2006.13570
     artifacts = {
-        **best_model.state.artifacts,
+        **best_forecaster.state.artifacts,
         "best_score": best_score,
         "best_params": best_params,
         "lags_path": lags_path,
         "scores_path": scores_path,
     }
 
     return artifacts
```

## functime/forecasting/_regressors.py

```diff
@@ -104,16 +104,16 @@
         elif isinstance(self.predict_dtype, Callable):
             X_coerced = self.predict_dtype(X)
         y_pred = self.regressor.predict(X_coerced)
         return y_pred
 
 
 class SklearnRegressor:
-    def __init__(self, estimator):
-        self.estimator = estimator
+    def __init__(self, regressor):
+        self.regressor = regressor
 
     def _preproc_X(self, X: pl.DataFrame):
         entity_col, time_col = X.columns[:2]
         X_new = X.select(
             [
                 entity_col,
                 time_col,
@@ -125,22 +125,22 @@
         return X_new
 
     def fit(self, X: pl.DataFrame, y: pl.DataFrame):
         X_new = self._preproc_X(X).lazy()
         # Regress
         with sklearn.config_context(assume_finite=True):
             # NOTE: We can assume finite due to preproc
-            self.estimator = self.estimator.fit(X=_X_to_numpy(X_new), y=_y_to_numpy(y))
+            self.regressor = self.regressor.fit(X=_X_to_numpy(X_new), y=_y_to_numpy(y))
         return self
 
     def predict(self, X: pl.DataFrame) -> np.ndarray:
         X_new = self._preproc_X(X).lazy()
         with sklearn.config_context(assume_finite=True):
             # NOTE: We can assume finite due to preproc
-            y_pred = self.estimator.predict(_X_to_numpy(X_new))
+            y_pred = self.regressor.predict(_X_to_numpy(X_new))
         return y_pred
 
 
 class CensoredRegressor:
     def __init__(
         self,
         threshold: Union[int, float],
```

## functime/forecasting/automl.py

```diff
@@ -10,15 +10,15 @@
 from functime.base.transformer import Transformer
 from functime.forecasting.knn import knn
 from functime.forecasting.lightgbm import lightgbm
 from functime.forecasting.linear import elastic_net, lasso, linear_model, ridge
 
 
 class AutoForecaster(Forecaster):
-    """AutoML forecaster with automated hyperparameter tuning and lags selection.
+    """Forecaster with automated hyperparameter tuning and lags selection.
 
     Parameters
     ----------
     freq : str
         Offset alias as dictated.
     min_lags : int
         Minimum number of lagged target values.
@@ -43,15 +43,15 @@
     points_to_evaluate : Optional[dict]
         Equivalent to `points_to_evaluate` in [FLAML](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#warm-start)
     num_samples : int
         Number of hyper-parameter sets to test. -1 means unlimited (until `time_budget` is exhausted.)
     target_transform : Optional[Transformer]
         functime transformer to apply to `y` before fit. The transform is inverted at predict time.
     **kwargs : Mapping[str, Any]
-        Additional keyword arguments passed into underlying sklearn-compatible estimator.
+        Additional keyword arguments passed into underlying sklearn-compatible regressor.
     """
 
     def __init__(
         self,
         # NOTE: MUST EXPLICITLY SPECIFIC FREQ IN ORDER FOR
         # CROSS-VALIDATION y_pred and y_test time index to match up
         freq: Union[str, None],
@@ -202,15 +202,15 @@
     @property
     def low_cost_partial_config(self):
         return {"n_estimators": 50, "num_leaves": 2}
 
 
 class auto_knn(AutoForecaster):
     @property
-    def model(self):
+    def forecaster(self):
         return knn
 
     @property
     def default_search_space(self):
         return {"leaf_size": tune.choice([30, 60, 120, 400])}
 
     @property
```

## functime/forecasting/censored.py

```diff
@@ -9,25 +9,25 @@
 from functime.forecasting._reduction import make_reduction
 from functime.forecasting._regressors import CensoredRegressor, _X_to_numpy, _y_to_numpy
 
 
 def default_regress(X: np.ndarray, y: np.ndarray):
     from sklearn.ensemble import HistGradientBoostingRegressor
 
-    estimator = HistGradientBoostingRegressor()
-    estimator.fit(X=X, y=y)
-    return estimator
+    regressor = HistGradientBoostingRegressor()
+    regressor.fit(X=X, y=y)
+    return regressor
 
 
 def default_classify(X: np.ndarray, y: np.ndarray):
     from sklearn.ensemble import HistGradientBoostingClassifier
 
-    estimator = HistGradientBoostingClassifier()
-    estimator.fit(X=X, y=y)
-    return estimator
+    classifier = HistGradientBoostingClassifier()
+    classifier.fit(X=X, y=y)
+    return classifier
 
 
 class censored_model(Forecaster):
     """Censored forecaster given `threshold` parameter (defaults to 0.0).
 
     Two separate forecasters are fit above and below a certain threshold.
     The forecasts are then combined using a binary classifier where: {0: "below_threshold", 1: "above_threshold"}.
```

## functime/forecasting/knn.py

```diff
@@ -8,15 +8,15 @@
 
 
 def _knn(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.neighbors import KNeighborsRegressor
 
         regressor = SklearnRegressor(
-            estimator=KNeighborsRegressor(**kwargs, n_jobs=-1),
+            regressor=KNeighborsRegressor(**kwargs, n_jobs=-1),
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 class knn(Forecaster):
```

## functime/forecasting/lightgbm.py

```diff
@@ -71,15 +71,14 @@
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _flaml_lightgbm(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
-        # Fix estimator list to just lightgbm
         custom_hp = kwargs.pop("custom_hp", {})
         custom_lgbm_kwargs = (
             {
                 param: value["domain"]
                 for param, value in custom_hp.get("lgbm", {}).items()
             }
             if "lgbm" in custom_hp
@@ -116,15 +115,15 @@
             lags=self.lags,
             max_horizons=self.max_horizons,
             strategy=self.strategy,
         )
 
 
 class flaml_lightgbm(Forecaster):
-    """Autoregressive FLAML AutoML LightGBM forecaster.
+    """Autoregressive FLAML LightGBM forecaster with automated lags and hyperparameter tuning.
 
     Reference:
     https://microsoft.github.io/FLAML/docs/Examples/AutoML-for-LightGBM/
     """
 
     def _fit(self, y: pl.LazyFrame, X: Optional[pl.LazyFrame] = None):
         y_new = y.pipe(
```

## functime/forecasting/linear.py

```diff
@@ -8,51 +8,51 @@
 
 
 def _linear_model(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import LinearRegression
 
         regressor = SklearnRegressor(
-            estimator=LinearRegression(**kwargs, copy_X=False),
+            regressor=LinearRegression(**kwargs, copy_X=False),
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _lasso(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import Lasso
 
         regressor = SklearnRegressor(
-            estimator=Lasso(**kwargs, tol=0.001, copy_X=False, max_iter=10000),
+            regressor=Lasso(**kwargs, tol=0.001, copy_X=False, max_iter=10000),
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _ridge(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import Ridge
 
         regressor = SklearnRegressor(
-            estimator=Ridge(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
+            regressor=Ridge(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 def _elastic_net(**kwargs):
     def regress(X: pl.DataFrame, y: pl.DataFrame):
         from sklearn.linear_model import ElasticNet
 
         regressor = SklearnRegressor(
-            estimator=ElasticNet(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
+            regressor=ElasticNet(**kwargs, tol=0.001, copy_X=False, max_iter=10000)
         )
         return regressor.fit(X=X, y=y)
 
     return regress
 
 
 class linear_model(Forecaster):
```

## Comparing `functime-0.3.0.dist-info/LICENSE` & `functime-0.3.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `functime-0.3.0.dist-info/METADATA` & `functime-0.3.1.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: functime
-Version: 0.3.0
+Version: 0.3.1
 Summary: The easiest way to run and scale time-series machine learning in the Cloud.
 Author-email: functime Team <team@functime.ai>, Chris Lo <chris@functime.ai>, Daryl Lim <daryl@functime.ai>
 Project-URL: Homepage, https://github.com/descendant-ai/functime
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Science/Research
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
@@ -64,30 +64,30 @@
 [![GitHub Publish to PyPI](https://github.com/descendant-ai/functime/actions/workflows/publish.yml/badge.svg)](https://github.com/descendant-ai/functime/actions/workflows/publish.yml)
 [![GitHub Build Docs](https://github.com/descendant-ai/functime/actions/workflows/docs.yml/badge.svg)](https://docs.functime.ai/)
 [![GitHub Run Quickstart](https://github.com/descendant-ai/functime/actions/workflows/quickstart.yml/badge.svg)](https://github.com/descendant-ai/functime/actions/workflows/quickstart.yml)
 
 </div>
 
 ---
-**functime** is a powerful [Python library]((https://pypi.org/project/functime/)) for production-ready AutoML forecasting and temporal embeddings.
+**functime** is a powerful [Python library]((https://pypi.org/project/functime/)) for production-ready forecasting and temporal embeddings.
 
 **functime** also comes with time-series [preprocessing](https://docs.functime.ai/ref/preprocessing/) (box-cox, differencing etc), cross-validation [splitters](https://docs.functime.ai/ref/cross-validation/) (expanding and sliding window), and forecast [metrics](https://docs.functime.ai/ref/metrics/) (MASE, SMAPE etc). All optimized as [lazy Polars](https://pola-rs.github.io/polars-book/user-guide/lazy/using/) transforms.
 
 Want to use **functime** for seamless time-series predictive analytics across your data team?
-Looking for production-grade time-series AutoML in a [serverless](#serverless-deployment) Cloud deployment?
+Looking for production-grade time-series machine learning in a [serverless](#serverless-deployment) Cloud deployment?
 Shoot Chris a message on [LinkedIn](https://www.linkedin.com/in/chrislohy/) to learn more about `functime` Cloud.
 
 ## Highlights
 - **Fast:** Forecast 100,000 time series in seconds *on your laptop*
 - **Efficient:** Embarrassingly parallel [feature engineering](https://docs.functime.ai/ref/preprocessing/) for time-series using [`Polars`](https://www.pola.rs/)
 - **Battle-tested:** Machine learning algorithms that deliver real business impact and win competitions
 - **Exogenous features:** supported by every forecaster
 - **Backtesting** with expanding window and sliding window splitters
-- **AutoML**: Automated lags and hyperparameter tuning using [`FLAML`](https://github.com/microsoft/FLAML)
-- **Censored model:** for zero-inflated and thresholding forecasts
+- **Automated lags and hyperparameter tuning** using [`FLAML`](https://github.com/microsoft/FLAML)
+- **Censored forecaster:** for zero-inflated and thresholding forecasts
 
 ## Getting Started
 Install `functime` via the [pip](https://pypi.org/project/functime) package manager.
 ```bash
 pip install functime
 ```
 
@@ -103,29 +103,48 @@
 y = pl.read_parquet("https://github.com/descendant-ai/functime/raw/main/data/commodities.parquet")
 entity_col, time_col = y.columns[:2]
 
 # Time series split
 y_train, y_test = y.pipe(train_test_split(test_size=3))
 
 # Fit-predict
-model = lightgbm(freq="1mo", lags=24, max_horizons=3, strategy="ensemble")
-model.fit(y=y_train)
-y_pred = model.predict(fh=3)
+forecaster = lightgbm(freq="1mo", lags=24, max_horizons=3, strategy="ensemble")
+forecaster.fit(y=y_train)
+y_pred = forecaster.predict(fh=3)
 
 # functime ❤️ functional design
 # fit-predict in a single line
 y_pred = lightgbm(freq="1mo", lags=24)(y=y_train, fh=3)
 
 # Score forecasts in parallel
 scores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)
 ```
 
+View the [full walkthrough](https://docs.functime.ai/forecasting.md) on forecasting with `functime`.
+
+## Embeddings
+
+Currently in closed-beta for `functime` Cloud.
+Have an interesting use-case? Contact us at [Calendly](https://calendly.com/functime).
+
+Temporal embeddings measure the relatedness of time-series.
+Embeddings are more accurate and efficient compared to statistical methods (e.g. Catch22) for characteristing time-series.[^1]
+Embeddings have applications across many domains from finance to IoT monitoring.
+They are commonly used for the following tasks:
+
+- **Matching:** Where time-series are ranked by similarity to a given time-series
+- **Classification:** Where time-series are grouped together by matching patterns
+- **Clustering:** Where time-series are assigned labels (e.g. normal vs irregular heart rate)
+- **Anomaly detection:** Where outliers with unexpected regime / trend changes are identified
+
+View the [full walkthrough](https://docs.functime.ai/embeddings.md) on temporal embeddings with `functime`.
+
 ## Serverless Deployment
 
-Currently in closed-beta for `functime` Teams.
+Currently in closed-beta for `functime` Cloud.
 Contact us for a demo via [Calendly](https://calendly.com/functime).
 
 Deploy and train forecasters the moment you call any `.fit` method.
 Run the `functime list` CLI command to list all deployed models.
 Finally, track data and forecasts usage using `functime usage` CLI command.
 
 ![Example CLI usage](static/gifs/functime_cli_usage.gif)
```

## Comparing `functime-0.3.0.dist-info/RECORD` & `functime-0.3.1.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 functime/__init__.py,sha256=IpTQKBIY-7DJ1VnnWAQcziICkAKKoLMM-rJkzlO_x2o,66
 functime/__main__.py,sha256=HpYJUAIeN5HXQhaM5_kDwo441jDPe9GJ8dLjF9HE9VQ,117
-functime/backtesting.py,sha256=zuFuaqM_qWyf-3-xpnlGgo7s24ngV9akp1UNPpmgo9s,5893
+functime/backtesting.py,sha256=S85juPZ_X3x52rIcXAxmalXGWQeRw3KCVXqVcR6aKns,5908
 functime/conformal.py,sha256=SLLPOEEzOdzVdFzCpNfmETmr4fHKqIhtJC-Xce6mHyo,1838
 functime/conversion.py,sha256=ulS0QWDUWgA3KgJ_2Gn0R-eMRWADN9sVfSOghtzjUIE,1379
 functime/cross_validation.py,sha256=uTkcotiLATM_qA0zfgWAIPaD4fJW9LSqiBABfAbeqvY,5549
 functime/embeddings.py,sha256=rupMM8Fs6-nExoWYHS-PWFHiEHXsrNBvP4o-PPzACos,123
 functime/offsets.py,sha256=GUCVLFs0xBSj_gsSaBz19ibBr_nquJVUxtuosObBucU,1495
 functime/plotting.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime/preprocessing.py,sha256=l7j_cELjhMMK9zupXNGfNpUrVD4Uer9krLGDqnB9enI,20397
+functime/preprocessing.py,sha256=zd_oQWZSyef4m_YgVp7c3Ur5wi_NjYY7rpxucRjVEVM,20125
 functime/ranges.py,sha256=b3biGP7s2OpoYopEGExELes79D_LdloDW4QH9j1slxI,1976
-functime/stats.py,sha256=fDy8OpsQl0TCYyD98r9D9Y5IVo8m2u9DoY8aOsYA7CA,1481
 functime/base/__init__.py,sha256=YKzO_7RyxYbn3-jTa4gipQxrwnzUQN2_d2edjDehLmw,217
-functime/base/forecaster.py,sha256=Ve7Ba5a0sJwkUtGrx7Mwqv3P_-xug4TrbVsRRqBLqCc,8253
+functime/base/forecaster.py,sha256=kzK7x_VzS0t-YIBZEpuQkdZs4CCVGBfp9NXxmFQXSfw,8253
 functime/base/metric.py,sha256=Ous83EZ4ADpcyvHnTGNaxekqKEkwQB7qbYWuzKiGAAI,1630
 functime/base/model.py,sha256=mL8GFL7qNNb4rufm3ZbLzxyDPoDuoHgQhpZ0t20AHv4,2771
-functime/base/transformer.py,sha256=mL0ljSUtdJ6ZX-nEck553pbHk6odZdkqxnGkfYDqWJs,2237
+functime/base/transformer.py,sha256=qSuKYisdWL1qFufJlr50zNg-NryMX5q7MP61I1DgPHw,2155
 functime/feature_extraction/__init__.py,sha256=GS4FtQUUuzJZAJJ4g7fgamtbZhAecz8iwaQsejPTVhs,285
-functime/feature_extraction/calendar.py,sha256=yQwbo_27M2lyCqr9FxUKwEHC5_Kp75XB003HhEsnAmE,4064
+functime/feature_extraction/calendar.py,sha256=z6WsFPma0KV0zAq93sZ3vlofaG_LMht47ihry8y4GEg,4294
 functime/forecasting/__init__.py,sha256=XWBiKZpNADvDUe-aaMtdppZt_8LvEYth7xzELJuy6Do,745
-functime/forecasting/_ar.py,sha256=n612ZuK-cnCmEN3rn7wkINw64Lq-naZgidmRjs9Bnl0,12624
+functime/forecasting/_ar.py,sha256=MIV_JBQ5HlgL6uV7W4GKGDl6jx8gwZqmSaPcwJ-Xadw,12667
 functime/forecasting/_evaluate.py,sha256=mVXNCX-JBXq2FNJFAGn_rF22zVidlQFG6cIA0dtyxaQ,4909
 functime/forecasting/_reduction.py,sha256=FLYt6FXJCIcg5j60BiJNX8HvE8LHnkBdoc0BvYyDEtA,2231
-functime/forecasting/_regressors.py,sha256=9z5yWKAWP2aHpk5q6__yWevPUc-9VtDpT1cgHnnAuH8,7569
-functime/forecasting/automl.py,sha256=LmCXrETtn39LbX_H3jVg7-Xs8TB6BfNCVf8qvRrNXwI,8515
+functime/forecasting/_regressors.py,sha256=Zw3THK02QvgnP0VTyIEJ0elFlbz6ic7uRS9rymXR2bs,7569
+functime/forecasting/automl.py,sha256=Wan9SoRzPdi7M8RkkniuHclxw1PO34XbC2EOPiwESW8,8513
 functime/forecasting/catboost.py,sha256=nF8I4AxJH7qDIu2BUGE_l7w-guNt8iYuNKH9oy-N3tU,2175
-functime/forecasting/censored.py,sha256=rZjxId4AZlz3bkf0YnKp5fqR5S3kI9Q42Swl0gGFtaU,3714
-functime/forecasting/knn.py,sha256=nu4XqAiJfMbNdCH0g9dzERSKvmrbGIuk9U3ypMqkX3E,1010
+functime/forecasting/censored.py,sha256=-iOSbD1byLF_ixsDKCLlqgY4YttRDyFbvRfoNGkXgok,3717
+functime/forecasting/knn.py,sha256=NA2-dCTHtVMUonuQMCU8i-bZuScB2H6mRVCXB_uUytQ,1010
 functime/forecasting/lance.py,sha256=LiEhbLrd6DpMoFhYjbjyPXWhHxNEdM_v_XdpPcgpwiU,3703
-functime/forecasting/lightgbm.py,sha256=b0nDwFQLFtyAB-0vbeL8JRehO_sszjJFm_PAOT5MFJY,4215
-functime/forecasting/linear.py,sha256=u7uwt4n_sCP3SfZClP0m_CGx1U0ZhLQq8XWi_V0HXrM,3958
+functime/forecasting/lightgbm.py,sha256=1TPdFXhtXfp6cl7fQGzmHg1G0OUpAoSXZkvbiP1amKE,4208
+functime/forecasting/linear.py,sha256=adMbCOpqNZZDLICTULY8Ya6dDTCJacmeY0-76SC7odI,3958
 functime/forecasting/xgboost.py,sha256=kguM3IvqtJKSD3tysxgCiOe-NwApBew5tOO9_zxt4-I,2412
 functime/metrics/__init__.py,sha256=Bx-vU-jxkAg-zy2mVWYDtNOkluvgKJnL9uf0IhKIapw,229
 functime/metrics/multi_objective.py,sha256=EP9sGQ4HSCRN1SQazoJfntcovMP6RAVf7hZMXgcA2gE,3826
 functime/metrics/point.py,sha256=HlkcYvGAtFPr8kiTHQnoXZujysg2DK-BuUJVmLwiNKM,6810
 functime/metrics/probabilistic.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime-0.3.0.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
-functime-0.3.0.dist-info/METADATA,sha256=NijULCgMeRLG_PeeYnTqyy1wbcXVsN447LUx0TJAaDY,6366
-functime-0.3.0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
-functime-0.3.0.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
-functime-0.3.0.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
-functime-0.3.0.dist-info/RECORD,,
+functime-0.3.1.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
+functime-0.3.1.dist-info/METADATA,sha256=-4WZtO2X6q7yALRUWUpoRsxloMYVQMReqBtBV0y_Vj0,7412
+functime-0.3.1.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+functime-0.3.1.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
+functime-0.3.1.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
+functime-0.3.1.dist-info/RECORD,,
```


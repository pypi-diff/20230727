# Comparing `tmp/QuNet-0.0.98-py3-none-any.whl.zip` & `tmp/QuNet-0.0.99-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,32 @@
-Zip file size: 68158 bytes, number of entries: 28
--rw-rw-rw-  2.0 fat      519 b- defN 23-Jun-12 07:32 qunet/__init__.py
+Zip file size: 70813 bytes, number of entries: 30
+-rw-rw-rw-  2.0 fat      591 b- defN 23-Jun-24 08:18 qunet/__init__.py
 -rw-rw-rw-  2.0 fat      616 b- defN 23-Apr-27 07:47 qunet/batch.py
--rw-rw-rw-  2.0 fat     5703 b- defN 23-May-05 12:11 qunet/callback.py
+-rw-rw-rw-  2.0 fat     5672 b- defN 23-Jun-24 08:14 qunet/callback.py
+-rw-rw-rw-  2.0 fat     4966 b- defN 23-Jun-24 15:05 qunet/config.py
 -rw-rw-rw-  2.0 fat     8843 b- defN 23-May-25 11:08 qunet/data.py
+-rw-rw-rw-  2.0 fat     1049 b- defN 23-Jun-24 08:10 qunet/ema.py
 -rw-rw-rw-  2.0 fat     2955 b- defN 23-May-28 11:12 qunet/losses.py
--rw-rw-rw-  2.0 fat    23518 b- defN 23-Jun-21 08:44 qunet/modelstate.py
+-rw-rw-rw-  2.0 fat    26465 b- defN 23-Jun-24 18:12 qunet/modelstate.py
 -rw-rw-rw-  2.0 fat    16977 b- defN 23-Apr-20 12:40 qunet/old.py
 -rw-rw-rw-  2.0 fat    10969 b- defN 23-Jun-22 11:54 qunet/plots.py
--rw-rw-rw-  2.0 fat    53971 b- defN 23-Jun-22 07:47 qunet/trainer.py
--rw-rw-rw-  2.0 fat     6545 b- defN 23-Jun-06 08:41 qunet/utils.py
+-rw-rw-rw-  2.0 fat    53940 b- defN 23-Jun-24 08:18 qunet/trainer.py
+-rw-rw-rw-  2.0 fat      812 b- defN 23-Jun-24 08:13 qunet/utils.py
 -rw-rw-rw-  2.0 fat        3 b- defN 23-Apr-26 10:17 qunet/modules/__init__.py
--rw-rw-rw-  2.0 fat    25497 b- defN 23-Jun-23 12:14 qunet/modules/cnn.py
--rw-rw-rw-  2.0 fat     7964 b- defN 23-May-20 06:54 qunet/modules/cnn3D.py
--rw-rw-rw-  2.0 fat     6302 b- defN 23-Apr-27 12:45 qunet/modules/gnn.py
--rw-rw-rw-  2.0 fat     6861 b- defN 23-Jun-23 12:39 qunet/modules/mlp.py
--rw-rw-rw-  2.0 fat     2897 b- defN 23-Apr-28 13:35 qunet/modules/points.py
+-rw-rw-rw-  2.0 fat    25498 b- defN 23-Jun-24 08:13 qunet/modules/cnn.py
+-rw-rw-rw-  2.0 fat     7965 b- defN 23-Jun-24 08:13 qunet/modules/cnn3D.py
+-rw-rw-rw-  2.0 fat     6517 b- defN 23-Jun-24 08:13 qunet/modules/gnn.py
+-rw-rw-rw-  2.0 fat    14054 b- defN 23-Jun-24 18:28 qunet/modules/mlp.py
+-rw-rw-rw-  2.0 fat     2898 b- defN 23-Jun-24 08:13 qunet/modules/points.py
 -rw-rw-rw-  2.0 fat     1594 b- defN 23-Jun-13 17:44 qunet/modules/total.py
--rw-rw-rw-  2.0 fat    20312 b- defN 23-May-12 12:52 qunet/modules/transformer.py
+-rw-rw-rw-  2.0 fat    20312 b- defN 23-Jun-24 08:12 qunet/modules/transformer.py
 -rw-rw-rw-  2.0 fat        3 b- defN 23-Apr-26 10:17 qunet/optim/__init__.py
 -rw-rw-rw-  2.0 fat     8517 b- defN 23-Jun-07 15:24 qunet/optim/adams.py
 -rw-rw-rw-  2.0 fat    14092 b- defN 23-Apr-25 10:33 qunet/optim/scheduler.py
 -rw-rw-rw-  2.0 fat        3 b- defN 23-Apr-26 10:17 qunet/rl/__init__.py
 -rw-rw-rw-  2.0 fat    15969 b- defN 23-Jun-05 12:31 qunet/rl/dqn.py
--rw-rw-rw-  2.0 fat     1068 b- defN 23-Jun-23 12:39 QuNet-0.0.98.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    16131 b- defN 23-Jun-23 12:39 QuNet-0.0.98.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-23 12:39 QuNet-0.0.98.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Jun-23 12:39 QuNet-0.0.98.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2122 b- defN 23-Jun-23 12:39 QuNet-0.0.98.dist-info/RECORD
-28 files, 260049 bytes uncompressed, 64834 bytes compressed:  75.1%
+-rw-rw-rw-  2.0 fat     1068 b- defN 23-Jun-24 18:30 QuNet-0.0.99.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    16131 b- defN 23-Jun-24 18:30 QuNet-0.0.99.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-24 18:30 QuNet-0.0.99.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 23-Jun-24 18:30 QuNet-0.0.99.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2263 b- defN 23-Jun-24 18:30 QuNet-0.0.99.dist-info/RECORD
+30 files, 270840 bytes uncompressed, 67283 bytes compressed:  75.2%
```

## zipnote {}

```diff
@@ -3,17 +3,23 @@
 
 Filename: qunet/batch.py
 Comment: 
 
 Filename: qunet/callback.py
 Comment: 
 
+Filename: qunet/config.py
+Comment: 
+
 Filename: qunet/data.py
 Comment: 
 
+Filename: qunet/ema.py
+Comment: 
+
 Filename: qunet/losses.py
 Comment: 
 
 Filename: qunet/modelstate.py
 Comment: 
 
 Filename: qunet/old.py
@@ -63,23 +69,23 @@
 
 Filename: qunet/rl/__init__.py
 Comment: 
 
 Filename: qunet/rl/dqn.py
 Comment: 
 
-Filename: QuNet-0.0.98.dist-info/LICENSE
+Filename: QuNet-0.0.99.dist-info/LICENSE
 Comment: 
 
-Filename: QuNet-0.0.98.dist-info/METADATA
+Filename: QuNet-0.0.99.dist-info/METADATA
 Comment: 
 
-Filename: QuNet-0.0.98.dist-info/WHEEL
+Filename: QuNet-0.0.99.dist-info/WHEEL
 Comment: 
 
-Filename: QuNet-0.0.98.dist-info/top_level.txt
+Filename: QuNet-0.0.99.dist-info/top_level.txt
 Comment: 
 
-Filename: QuNet-0.0.98.dist-info/RECORD
+Filename: QuNet-0.0.99.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qunet/__init__.py

```diff
@@ -1,13 +1,15 @@
 ﻿from .utils               import *
+from .config              import *
 from .data                import *
 from .trainer             import *
 from .plots               import *
 from .callback            import *
 from .losses              import *
+from .ema                 import *
 from .modelstate          import *
 
 from .modules.mlp         import *
 from .modules.cnn         import *
 from .modules.cnn3D       import *
 from .modules.transformer import *
 from .modules.points      import *
```

## qunet/callback.py

```diff
@@ -1,9 +1,7 @@
-import copy, time, gc, psutil
-
 class Callback:
     def __init__(self):
         """
         Abstract base class used to build new callbacks.
         Subclass this class and override any of the relevant hooks.
 
         Example:
```

## qunet/modelstate.py

```diff
@@ -1,12 +1,28 @@
 import numpy as np, matplotlib.pyplot as plt
 import torch, torch.nn as nn
 
 class ModelState:
     def __init__(self, model, beta=0.8):
+        """
+        Example
+        ------------
+        ```
+            state = ModelState(model)
+
+            state.num_params() # number of model parameters
+            
+            state.layers()     # model layers
+            state.params()     # model parameters 
+            state.state()      # params and register_buffer
+
+            state.plot()       # draw params and grads
+            ModelState.hist_params([m.weight, m.bias], ["w","b"])
+        ```
+        """
         self.model = model
         self.beta  = min(0.999, max(0.001, beta))
         self.__params   = {}
         self.__layers   = []
         self.__layersID = {}
         self.agg        = 0
 
@@ -206,21 +222,21 @@
                     elif layer in ['RNN','GRU','LSTM']: descr += f"({mo.input_size},{mo.hidden_size}, l:{mo.num_layers}, bi:{'T' if mo.bidirectional else 'F'})"
 
                 skip = f"│{' '*max(0,3*depth-1)}{'├─ ' if i+1 < count else '└─ '}" if depth > 0 else "├─ "
                 lr['st'] = f"{skip}{descr}"
 
     #---------------------------------------------------------------------------
     #
-    def layers(self, info=0, is_names=True, input_size=None, input_data=None):
+    def layers(self, info=1, is_names=True, input_size=None, input_data=None):
         """
         Display information about model layers
 
         Args:
         ------------
-            info (int=0):
+            info (int=1):
                 (-1): name of layer; (0,1,2) - name of layer class with parameters of varying degrees of detail
             is_name (bool=True):
                 show path to given layer (for -1,0,1)
         """
         if len(self.__layers) == 0:
             self.get_layers(self.model, layers=self.__layers)
         self.get_layers_descr(info = info)                        # info can change
@@ -274,14 +290,26 @@
         print("="*(ma+12))
         n1, n2, n3 = self.num_params(True),  self.num_params(False),  self.num_params(None)
         print(f"{'trainable:'+' '*(ma-15)}     {ModelState.i2s(n1,12)}")
         if n2 or n3 != n1:
             print(f"{'other:'+' '*(ma-11)}     {ModelState.i2s(n2,12)}")
             print(f"{'total:'+' '*(ma-11)}     {ModelState.i2s(n3,12)}")
 
+    #---------------------------------------------------------------------------
+
+    def state(self):
+        """
+        Display information about all parameters (including register buffers)
+        """
+        ma = max([len(k)  for k in self.model.state_dict().keys()])
+        descr = "param" + " "*(ma-5) + "   value            num  shape"
+        print(descr)
+        for k,v in self.model.state_dict().items():
+            val = v if v.numel() < 2 else torch.sqrt(torch.square(v).mean())
+            print(f"{k+' '*(ma-len(k))} {val.item():8.4f}  {ModelState.i2s(v.numel(),12)}  {tuple(v.shape)}")
 
     #---------------------------------------------------------------------------
 
     def forward_pre_hook(self, m, inputs):
         """
         Allows for examination and modification of the input before the forward pass.
         Note that inputs are always wrapped in a tuple.
@@ -486,14 +514,51 @@
             elif p.dim()==2:
                 y = p.data.cpu().numpy()
                 if sorted:
                     y = np.sort(y)
                 ax.imshow(y)
         plt.show()
 
+    #---------------------------------------------------------------------------
+
+    @staticmethod
+    def hist_params(params, titles=None, bins=50, w=12, h=3,  digits=2):        
+        if type(params) not in [list, tuple]: params = [params]
+        if type(titles) is str:               titles = [titles]
+
+        fig, axs = plt.subplots(1,len(params), figsize=(w, h))
+        for i, p in enumerate(params):
+            ax = axs[i] if len(params) > 1 else axs                   
+            values = p.data.flatten().cpu().numpy()
+            #values = p.data.abs().amax(dim=0).flatten().cpu().numpy()
+            #print(np.min(values), np.max(values))
+            title = titles[i] if titles is not None and type(titles) in [list, tuple] and len(titles) > i else ""
+
+            ModelState.hist_param(ax, values, title, bins=bins, digits=digits) 
+
+        plt.show()
+    
+    #---------------------------------------------------------------------------
+
+    @staticmethod
+    def hist_param(ax, v, title, bins=50, digits=2):
+        r = lambda x: '{x:.{digits}f}'.format(x=round(x,digits), digits=digits)                       
+
+        y, x = np.histogram(np.abs(v), bins=bins*10, density=True)
+        y = np.array([y[i]*(x[i+1]-x[i]) for i in range(len(y))])
+        y = np.cumsum(y)
+        x = np.array([0.5*(x[i]+x[i+1]) for i in range(len(x)-1)])
+        ax.plot( x, y, "-g")
+        ax.grid(ls=":"); ax.set_ylabel("sum prob |p|", color='g'); ax.set_xlabel("|p|", color='g')
+        ax.set_title(f"{title} mean={r(v.mean())} ± {r(v.std())} [{r(v.min())}, {r(v.max())}]; cnt={len(v)}", fontsize=10)                
+            
+        ax2  = ax.twinx().twiny(); 
+        ax2.hist( v, bins=bins, color="lightblue", ec="black", alpha=0.5)
+        ax2.set_ylabel("N(p)", color="lightblue")
+
 
 """
 Полный набор параметров, зарегистрированных модулем, можно просмотреть с помощью вызова parameters()
 или named_parameters(), где последний включает имя каждого параметра.
 Вызовы parameters()и named_parameters()будут рекурсивно включать все дочерние параметры.
 
 Метод parameters() - это генератор только по обучаемым параметрам (его мы передаём оптимизатору).
```

## qunet/trainer.py

```diff
@@ -1,16 +1,15 @@
 ﻿import os, glob, math, copy, time, datetime
 from pathlib import Path
 from   tqdm.auto import tqdm
 import numpy as np, matplotlib.pyplot as plt
 import torch, torch.nn as nn
 
-from .utils    import Config
-from .utils    import ModelEma
-from .callback import Callback
+from .config   import Config
+from .ema      import ModelEma
 from .batch    import Batch
 from .optim.scheduler    import Scheduler
 from .plots    import plot_history
 
 class Trainer:
     """
     Generic model training class.
```

## qunet/utils.py

```diff
@@ -1,10 +1,8 @@
-import copy, time, gc, psutil
-import torch
-import copy
+import time, gc, psutil
 
 class Info:
     def __init__(self) -> None:
         self.beg  = time.time()
         self.last = time.time()
 
     def info(self, text, pref="", end="\n"):
@@ -22,176 +20,7 @@
         self.last = time.time()
         return self
 
     def __call__(self, text, pref="", end="\n"):
         self.info(text, pref=pref, end=end)
 
 #===============================================================================
-
-class Config:
-    def __init__(self, *args, **kvargs):
-        """
-        Universal parameter store
-        Example:
-        ```
-        cfg = Config(x=2, y=5, z=Config(v=0))
-        print(cfg)         # x:2, y:5, z: {v:0, }, 
-        cfg(x=5, y=2)      # x:5, y:2, z: {v:0, }, 
-        cfg2 = Config(x=0)
-        cfg(cfg2)          # x:0, y:2, z: {v:0, }, 
-        ```
-        """
-
-        self.check_variable_existence = False
-        self.set(*args, **kvargs)
-        self.check_variable_existence = True
-
-    def __call__(self, *args,  **kvargs):
-        return self.set(*args, **kvargs)
-
-    def __str__(self):
-        return self.get_str()
-
-    def copy(self):
-        return copy.deepcopy(self)
-
-    def protect(self, check=True):
-        """ check=True - protected version (checks for the presence of a variable), otherwise no """
-        self.check_variable_existence = check
-
-    def has(self, param):
-        """ check if param exists in config """
-        return param in self.__dict__
-
-    def set(self, *args, **kvargs):
-        for a in args:            
-            assert isinstance(a, Config), "Config args can  be only another Config!"            
-            self.set_cfg(a)
-
-        for k, v in kvargs.items():            
-            if self.check_variable_existence and k not in self.__dict__:
-                print(f'! Config warning: no property "{k}" set({k}={v})')            
-            if isinstance(v, Config):
-                if k not in self.__dict__:
-                    self.__dict__[k] = Config()
-                self.__dict__[k].set_cfg(v)
-            else:
-                self.__dict__[k] = v
-           
-        return self
-        
-    def set_cfg(self, cfg):
-        """ set values from another Config """
-        for k,v in cfg.__dict__.items():            
-            if isinstance(v, Config): 
-                if k not in self.__dict__:
-                    self.__dict__[k] = Config()
-                self.__dict__[k].set_cfg(v)
-            elif not k.startswith("__") and k not in ['check_variable_existence', 'set']:
-                self.__dict__[k] = v
-
-    def get_str(self, end=", ", exclude=[]):
-        """ output of config parameters """
-        res = ""
-        for k,v in self.__dict__.items():
-            if not k.startswith("__") and k not in ["check_variable_existence", "set"] + exclude:
-                if isinstance(v, Config):
-                    res += f"{k}:"+" {"+v.get_str(exclude=exclude)+"}"+end
-                else:
-                    if type(v) == str:
-                        res +=  f"{k}:'{v}'{end}"
-                    else:
-                        res +=  f"{k}:{v}{end}"
-        return res
-    
-    def get_jaml(self, exclude=[], level=0):
-        """ output of config parameters """
-        res = ""
-        for k,v in self.__dict__.items():
-            if not k.startswith("__") and k not in ["check_variable_existence", "set"] + exclude:
-                if isinstance(v, Config):
-                    res += " "*(4*level) + f"{k}:\n" +v.get_jaml(exclude=exclude, level=level+1)+"\n"
-                else:
-                    if type(v) == str:
-                        res += " "*(4*level) + f"{k}: '{v}'\n"
-                    else:
-                        res +=   " "*(4*level) + f"{k}: {v}\n"
-        return res    
-
-    def get_dict(self, exclude=[]):
-        """ output of config parameters """
-        res = {}
-        for k,v in self.__dict__.items():
-            if not k.startswith("__") and k not in ["check_variable_existence"] + exclude:
-                if isinstance(v, Config):
-                    res[k] = v.get_dict(exclude=exclude)
-                else:
-                    res[k] = v
-        return res
-
-class ModelEma(torch.nn.Module):
-    def __init__(self, model, decay=0.9999, device=None):
-        super(ModelEma, self).__init__()
-        # make a copy of the model for accumulating moving average of weights
-        self.module = copy.deepcopy(model)
-        self.module.eval()
-        self.decay = decay
-        self.device = device  # perform ema on different device from model if set
-        if self.device is not None:
-            self.module.to(device=device)
-
-    def _update(self, model, update_fn):
-        with torch.no_grad():
-            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):
-                if self.device is not None:
-                    model_v = model_v.to(device=self.device)
-                ema_v.copy_(update_fn(ema_v, model_v))
-
-    def update(self, model):
-        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)
-
-    def set(self, model):
-        self._update(model, update_fn=lambda e, m: m)
-
-if __name__ == '__main__':    
-    info = Info()
-    info("begin")
-
-    cfg1 = Config(x=1, y=2)
-    cfg2 = Config(cfg1, z=3)
-    print(cfg2)
-
-    cfg1 = cfg2.copy()
-    cfg1.z=8
-    print(cfg2)
-
-    cfg2.cfg = Config(a=1, b=Config(z=4, t=9))
-    print(cfg2.get_dict())
-
-    print(cfg2.get_jaml())
-
-    if False:
-        cfg = Config(x=2, y=5, z=Config(v=0))
-        print(cfg)         # x:2, y:5, z: {v:0, }, 
-        cfg(x=5, y=2)      # x:5, y:2, z: {v:0, }, 
-        print(cfg) 
-        cfg2 = Config(x=0)
-        cfg(cfg2)          # x:0, y:2, z: {v:0, }, 
-        print(cfg) 
-
-    if False:
-        print("---------")
-        cfg = Config(
-            x=3, 
-            z=4, 
-            loss=Config(z=5)
-        )
-        print('cfg: ', cfg)
-        cfg.loss(x=8)
-        print('cfg: ', cfg)
-        cfg.loss.protect(False)
-        cfg.loss(a='a')
-        print('cfg: ', cfg)
-        print('-----------------')
-        cfg2 = Config( x=5 )
-        cfg.set_cfg(cfg2)
-        print(cfg)
```

## qunet/modules/cnn.py

```diff
@@ -1,11 +1,11 @@
 ﻿import copy
 import torch, torch.nn as nn
 
-from ..utils import Config
+from ..config import Config
 from .total  import get_activation
 #========================================================================================
 
 class CNN(nn.Module):
     def __init__(self,  *args, **kvargs) -> None:
         """
         Simple convolutional network: (B,C,H,W) ->  (B,C',H',W')
```

## qunet/modules/cnn3D.py

```diff
@@ -1,11 +1,11 @@
 import copy
 import torch, torch.nn as nn
 
-from ..utils   import Config
+from ..config   import Config
 
 class CNN3DBlock(nn.Module):
     def __init__(self, in_channels,  out_channels, kernel = 3, stride = 1, layers=2,  mode = 'zeros',  batchnorm=True, bias=False, residual=0):
         super(CNN3DBlock, self).__init__()
 
         layers, channels, padding = [],  [in_channels] + [out_channels]*layers,   kernel // 2
         for i in range(len(channels)-1):
```

## qunet/modules/gnn.py

```diff
@@ -1,8 +1,8 @@
-from ..utils   import Config
+from ..config   import Config
 from typing import Any, Callable, Optional, Sequence, Union
 from torch_geometric.nn import EdgeConv
 from torch_geometric.nn.pool import knn_graph
 from torch_geometric.typing import Adj
 from torch import Tensor, LongTensor
 from torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum
 import torch, torch.nn as nn
@@ -57,19 +57,19 @@
         return x, edge_index
 
 class GraphNet(nn.Module):
     def __init__(self, *arg, **kvargs):        
         super().__init__()  
         self.cfg = Config(
             name = 'GraphNet',
-            nb_inputs = 3, # ÷èñëî ôè÷ âåðøèí èñõîäíîãî ãðàôà
-            layers = [     # óðîâíè DynEdgeConv                       
-                       Config(mlp=[16,16],                  # ñëîè MLP â DynEdgeConv
-                              features_subset=slice(0, 3),  # äèàïàçîí ôè÷ ïî êîòîðûì ñ÷èòàþòñß ñîñåäè
-                              nb_neighbors=4),              # ÷èñëî ñîñåäåé äëß êàæäîé âåðøèíû ãðàôà
+            nb_inputs = 3, # ����� ��� ������ ��������� �����
+            layers = [     # ������ DynEdgeConv                       
+                       Config(mlp=[16,16],                  # ���� MLP � DynEdgeConv
+                              features_subset=slice(0, 3),  # �������� ��� �� ������� ��������� ������
+                              nb_neighbors=4),              # ����� ������� ��� ������ ������� �����
                        Config(mlp=[16,16], 
                               features_subset=slice(0, 3), 
                               nb_neighbors=4),
             ],
             post_processing = [16,16],
             global_pooling = ["min","max","mean"],
             readout = [128,10]
```

### encoding

```diff
@@ -1 +1 @@
-iso-8859-1
+utf-8
```

## qunet/modules/mlp.py

```diff
@@ -1,12 +1,13 @@
 ﻿import math, copy
+import numpy as np, matplotlib.pyplot as plt
 import torch, torch.nn as nn
 
-from ..utils   import Config
-from .total  import get_activation
+from ..config import Config
+from .total   import get_activation
 
 #========================================================================================
 
 class UnitTensor(nn.Module):
     def forward(self, x):
         return torch.nn.functional.normalize(x, p=2, dim=-1)
 
@@ -129,63 +130,246 @@
 
         mlp = MLP(cfg, hidden=[128, 512])
 
         mlp = MLP(cfg, hidden=128, norm=True)
 
         print("ok MLP")
         return True
+    
+#========================================================================================    
+#                             MLP with skip connections                  
+#========================================================================================
+
+class ResBlockMLP(nn.Module):
+    def __init__(self,  *args, **kvargs) -> None:
+        super().__init__()
+        self.cfg = ResBlockMLP.default()
+        cfg = self.cfg.set(*args, **kvargs)
+        
+        assert cfg.mlp.input == cfg.mlp.output, f"In mlp should be input ({cfg.mlp.input}) == output ({cfg.mlp.output})"
+
+        self.mlp  = MLP(cfg.mlp)
+        self.norm = nn.LayerNorm(cfg.mlp.input)
+        self.register_buffer("mult", torch.tensor(float(1.0)))
+        self.register_buffer("std",  torch.tensor(float(0.0)))        
+
+        self.debug  = False
+        self.avr_x    = None
+        self.avr_dx   = None
+        self.grads    = []
+        self.__hook   = None 
+
+    #---------------------------------------------------------------------------
+
+    @staticmethod
+    def default():
+        return copy.deepcopy(Config(
+            mlp = MLP.default(),             
+            beta  = 0.9,                        
+        ))
+
+    #---------------------------------------------------------------------------
+
+    def forward(self, x):
+        
+        dx = self.mlp(self.norm(x))                # B,...,F
+
+        if self.debug:                             # ema модуля параметров модели
+            v_x  = torch.sqrt(torch.square(x. detach()).sum(-1).mean())
+            v_dx = torch.sqrt(torch.square(dx.detach()).sum(-1).mean())
+            b, b1 = self.cfg.beta, 1-self.cfg.beta
+            if self.avr_x  is None: self.avr_x  = v_x
+            else:                   self.avr_x  = b * self.avr_x  + b1 * v_x
+            if self.avr_dx is None: self.avr_dx = v_dx            
+            else:                   self.avr_dx = b * self.avr_dx + b1 * v_dx
+        
+        if self.training and self.std > 0:
+            return self.mult * x  + dx * (1+torch.randn(1, device=x.device)*self.std)
+        else:
+            return self.mult * x  + dx
+    
+    #---------------------------------------------------------------------------
+
+    def backward_hook(self, module, grad_input, grad_output):
+        i = 0
+        for layer in module:
+            if type(layer) == nn.Linear:
+                g = torch.sqrt(torch.square(layer.weight.grad).mean()).cpu().item()
+                if len(self.grads) == i:
+                    self.grads.append(g)
+                    i += 1
+                else:
+                    self.grads[i] = self.cfg.beta * self.grads[i]  + (1-self.cfg.beta) * g
+
+    #---------------------------------------------------------------------------
+
+    def add_hook(self):
+        if self.__hook is None:
+            self.__hook = self.mlp.layers.register_full_backward_hook(self.backward_hook)
+
+    #---------------------------------------------------------------------------
+
+    def remove_hook(self):
+        if self.__hook is not None:
+            self.__hook.remove()
+            self.__hook  = None
+            self.grads = []
 
 #========================================================================================    
 
-class SkipMLP(nn.Module):
+class ResMLP(nn.Module):
     def __init__(self,  *args, **kvargs) -> None:
         """
         Args
         ------------
             n_blocks (int=2):
                 number of transformer blocks
             mlp ( Config=MLP.default() ):
                 should be: input == output
 
         Example
         ------------
         ```
-            mlp = SkipMLP(n_blocks=5, mlp=Config(input=32, hidden=[128,128], output=32))
+            mlp = ResMLP(n_blocks=5, mlp=Config(input=32, hidden=[128,128], output=32))
             y = mlp( torch.randn(1, 32) )
         ```
         """
         super().__init__()
-        self.cfg = SkipMLP.default()
+        self.cfg = ResMLP.default()
         cfg = self.cfg.set(*args, **kvargs)
         
         assert cfg.mlp.input == cfg.mlp.output, f"In mlp should be input ({cfg.mlp.input}) == output ({cfg.mlp.output})"
 
-        self.blocks = nn.ModuleList([  MLP(cfg.mlp)                 for _ in range(cfg.n_blocks) ])
-        self.norms  = nn.ModuleList([  nn.LayerNorm (cfg.mlp.input) for _ in range(cfg.n_blocks) ] )
+        self.blocks = nn.ModuleList([  ResBlockMLP(mlp=cfg.mlp) for _ in range(cfg.n_blocks) ]) 
+        self.mult(cfg.mults)   
+        self.std(cfg.stds)       
 
     #---------------------------------------------------------------------------
 
     @staticmethod
     def default():
         return copy.deepcopy(Config(
             n_blocks = 1,
-            mlp = MLP.default(), 
+            mlp = MLP.default(),             
+            beta  = 0.9,
+            mults = 1.0,     # can be list
+            stds  = 0.0,     # can be list
         ))
 
     #---------------------------------------------------------------------------
 
     def forward(self, x):
-        for block, norm in zip(self.blocks, self.norms):            
-            x = block(norm(x)) + x                        
+        for block in self.blocks:            
+            x = block(x)
         return x
     
     #---------------------------------------------------------------------------
 
+    def debug(self, value=True, beta=None):
+        for block in self.blocks:
+            block.debug = value
+            if value:
+                block.add_hook()    
+            else:
+                block.avr_x = block.avr_dx  = None        
+                block.grads  = []
+
+            if beta is not None:
+                block.cfg.beta = beta
+
+    #---------------------------------------------------------------------------
+
+    def std(self, stds, i=None):
+        if i is not None:
+            assert type(stds) in [float, int] and i >=0 and i < self.cfg.n_blocks, f"Wrong stds={stds} for i={i}"    
+            self.blocks[i].std.fill_(float(stds))
+            return
+
+        if type(stds) in [float, int]:
+            stds = [stds] * self.cfg.n_blocks
+
+        assert type(stds) in [list, tuple] and len(stds) == self.cfg.n_blocks, f"Wrong stds={stds}"
+        for block,std in zip(self.blocks, stds):
+            block.std.fill_(float(std))          
+
+    #---------------------------------------------------------------------------
+
+    def mult(self, mults, i=None):
+        if i is not None:
+            assert type(mults) in [float, int] and i >=0 and i < self.cfg.n_blocks, f"Wrong mults={mults} for i={i}"    
+            self.blocks[i].mult.fill_(float(mults))
+            return
+
+        if type(mults) in [float, int]:
+            mults = [mults] * self.cfg.n_blocks
+
+        assert type(mults) in [list, tuple] and len(mults) == self.cfg.n_blocks, f"Wrong mults={mults}"
+        for block, mult in zip(self.blocks, mults):
+            block.mult.fill_(float(mult))
+
+    #---------------------------------------------------------------------------
+
+    def add_hook(self):
+        for block in self.blocks:
+            block.add_hook()    
+
+    #---------------------------------------------------------------------------
+
+    def remove_hook(self):
+        for block in self.blocks:
+            block.remove_hook()    
+
+    #---------------------------------------------------------------------------
+
+    def plot(self, w=12,h=3,eps=1e-8):
+        fig, ax = plt.subplots(1,1, figsize=(w, h))
+
+        plt.text(0,0,f" mult\n std\n", ha='left', transform = ax.transAxes, fontsize=8)
+        weights, dx = [], []
+        for i,block in enumerate(self.blocks):
+            ww = [ ]
+            for layer in block.mlp.layers:
+                if type(layer) == nn.Linear:
+                    ww.append(torch.sqrt((layer.weight.data ** 2).mean()).cpu().item())
+            weights.append(ww)
+            dx.append( block.avr_dx/(block.avr_x+eps) )
+
+            plt.text(i,0,f"{block.mult.item():.2f}\n{block.std.item():.2f}\n", ha='center', fontsize=8)
+
+        idxs = np.arange(self.cfg.n_blocks)        
+        ax.set_xticks(idxs)
+        ax.bar(idxs, dx, alpha=0.8, color="lightgray", ec="black")
+        ax.set_ylim(0, np.max(np.array(dx).flatten())*1.1) 
+        ax.set_ylabel("dx/x");  ax.set_xlabel("blocks");
+        ax.grid(ls=":")
+
+        ax2 = ax.twinx() 
+        weights = np.array(weights).transpose()
+        for i,w in enumerate(weights):            
+            ax2.plot(idxs, w, marker=".", label=f'{i}')
+        ax2.set_ylabel("|weight|")            
+        ax2.set_ylim(0, weights.flatten().max()*1.1) 
+        ax2.legend(loc='upper right')
+
+        grads = [ block.grads for block in self.blocks if len(block.grads) ]
+        if len (grads):
+            grads = np.array(grads).transpose()            
+            ax3 = ax.twinx() 
+            for i,g in enumerate(grads):            
+                ax3.plot(idxs, g, ls=":", marker=".")
+            ax3.spines["right"].set_position(("outward", 50))
+            ax3.set_ylim(0, grads.flatten().max()*1.1) 
+            ax3.set_ylabel("--- |grad|")            
+            
+
+        plt.show()
+    #---------------------------------------------------------------------------
+
     @staticmethod
     def unit_test():
-        mlp = SkipMLP(n_blocks=5, mlp=Config(input=32, hidden=[128,128], output=32))
+        mlp = ResMLP(n_blocks=5, mlp=Config(input=32, hidden=[128,128], output=32))
         y = mlp( torch.randn(1, 32) )
-        print(f"ok SkipMLP: {y.shape}")
+        print(f"ok ResMLP: {y.shape}")
         return True
 
-#========================================================================================
+
```

## qunet/modules/points.py

```diff
@@ -1,11 +1,11 @@
 ﻿import copy
 import torch, torch.nn as nn
 
-from ..utils   import Config
+from ..config   import Config
 from .mlp      import MLP
 #===============================================================================
 
 class  PointsBlock(nn.Module):
     def __init__(self,  *args, **kvargs) -> None:
         super().__init__()
         self.cfg = PointsBlock.default()
```

## qunet/modules/transformer.py

```diff
@@ -1,11 +1,11 @@
 ﻿import math, copy
 import torch, torch.nn as nn
 
-from ..utils   import Config
+from ..config  import Config
 from .mlp      import MLP
 
 #===============================================================================
 
 class SelfAttention(nn.Module):
     """
     Self Attention: (B,T,E) -> (B,T,E)
```

## Comparing `QuNet-0.0.98.dist-info/LICENSE` & `QuNet-0.0.99.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `QuNet-0.0.98.dist-info/METADATA` & `QuNet-0.0.99.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: QuNet
-Version: 0.0.98
+Version: 0.0.99
 Summary: Working with deep learning models
 Home-page: https://github.com/step137/qunet
 Author: synset
 Author-email: steps137ai@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

## Comparing `QuNet-0.0.98.dist-info/RECORD` & `QuNet-0.0.99.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-qunet/__init__.py,sha256=NSnUKrhrcHjtYOtYjqQE0qxv6NbhpRwUe2uQNlj6K9w,519
+qunet/__init__.py,sha256=xJmNkuSqk-fprusIACJ2rHRfjrPOTkYAOG2xGoZUqLg,591
 qunet/batch.py,sha256=bBlUctBato37BkDMo69dH1icFvsCpOxH94ScYjKkwfc,616
-qunet/callback.py,sha256=Jic-Ehcg1uxRLX1Qr9H55gAfdIuX2ZdeNbGZEVw9Aq8,5703
+qunet/callback.py,sha256=LyWjZJ-95B4eUyc-YoxXdYQC6UNnrrt6J_zRif5GJFU,5672
+qunet/config.py,sha256=D-2VK7yFt5GTgmX08OKQjfk4xRZQRMiyrR-lLfNpZgA,4966
 qunet/data.py,sha256=2oOEhaPyqXyuQd_KhaA8ZER_9c3AFzSaZnVG4JqFMAQ,8843
+qunet/ema.py,sha256=RmHuFr8cPd7K_K4Sg2GwEFvLX0HGsui8WwTPfDKShWY,1049
 qunet/losses.py,sha256=-euuc242iM5I8CXZgXTiG7R_hTOea0qUhlNug0D3Ijg,2955
-qunet/modelstate.py,sha256=vaZIJzanCrWCAdQBGLF4RWYGQW9PV-tL06P-rSdKBy0,23518
+qunet/modelstate.py,sha256=dCPhrEpTgktLgTyoErhO5i6EXSE9NpKf9QaJP_TN95o,26465
 qunet/old.py,sha256=wfsu9d4vcptlBT1lgPKHCfqDm8lirDvTTvEEzKGjTig,16977
 qunet/plots.py,sha256=-i0h3qgERN7RlrlcyogE3XiQI8INP7MVgGWkxEEBJTo,10969
-qunet/trainer.py,sha256=zw_g-XZOPW7j4I-ayW33mm6TB4owWalraLlTzHnuELc,53971
-qunet/utils.py,sha256=85GOgOkATjgrditptjfBpVK174o5hLO-M7lyNm90lWA,6545
+qunet/trainer.py,sha256=VOyrtQA8l81Qfcs5L7fLrDno4xcsl1_5HWamUjU8-0M,53940
+qunet/utils.py,sha256=wiLbKddufWSlAZeZgAKVo06PH-braa7Meq2uvq1Rgw0,812
 qunet/modules/__init__.py,sha256=8ZRc1sGeVrPBx4lD717BgRaQekyh78QKV9SKsdt638U,3
-qunet/modules/cnn.py,sha256=kpIJfFm83HIfgzg-Wl8GJB4FQEUYjcPsCty0A7cz-Rc,25497
-qunet/modules/cnn3D.py,sha256=Q_htR1sNQccF6BbEjU8QPgJkuTnA910Fa-VfqaTGPgU,7964
-qunet/modules/gnn.py,sha256=FY3rc4hL6HtMYh_Yn9tGYNZqqvBn-Qdwx0O_HJcbuzk,6302
-qunet/modules/mlp.py,sha256=TkBVEKfy4hyQ912jRguqMgD4XkUfRe8pXu1MI4H-1NM,6861
-qunet/modules/points.py,sha256=3XxfdSdOMgGte4NJWO9Lf_4zXIIHiPpCUBbsoAsVONg,2897
+qunet/modules/cnn.py,sha256=MxBdR1kzerGuGqxloiX4n3kg1KW-RvjFCA6BgJBK8js,25498
+qunet/modules/cnn3D.py,sha256=hHIGlogn_P9qctneoKrumVE0oMQ1gM2g4jD31tzLlZk,7965
+qunet/modules/gnn.py,sha256=zgZO08lMPB0OOhIiPR7ZfDRMO9YTsq9DgGqQCwpIaBI,6517
+qunet/modules/mlp.py,sha256=TQ8XU_qq4FxmipyNzOIf665hwVd0FOyfRlnFxntRTRI,14054
+qunet/modules/points.py,sha256=JpeywTF3ZxTdJp5zyapsnNj7fiojHc8xIj8y8GQNzRA,2898
 qunet/modules/total.py,sha256=wjjrX4g5gxkH0GSRsyYAMtTjr9KqRKi2zdO-T07Tvxo,1594
-qunet/modules/transformer.py,sha256=Q48tMEiDYR94h1CwRVwCp2p36rTMPu3UMvlCoYFjSDs,20312
+qunet/modules/transformer.py,sha256=uH4CIRFCZwpPrfcf6dUkSznoRzM5mTMY_lacie1IsSE,20312
 qunet/optim/__init__.py,sha256=8ZRc1sGeVrPBx4lD717BgRaQekyh78QKV9SKsdt638U,3
 qunet/optim/adams.py,sha256=UbaKMSaK3HHjuZKuyPxfLw2m2yrR8mD6CS3BKmCQ3K0,8517
 qunet/optim/scheduler.py,sha256=tAAgUS0LaAOPxVqxDsYAitF3ExX7Pt_dNK4XwzbTYdY,14092
 qunet/rl/__init__.py,sha256=8ZRc1sGeVrPBx4lD717BgRaQekyh78QKV9SKsdt638U,3
 qunet/rl/dqn.py,sha256=Ex18lo17luQuRUhaqaeLhO_IrT_s6ObJexiznSgxoSY,15969
-QuNet-0.0.98.dist-info/LICENSE,sha256=TSOuIu1obl3tk6okEX3AbHzQjUhXvTKU3gY_yDhpZM0,1068
-QuNet-0.0.98.dist-info/METADATA,sha256=5D8G-UU4S46uWfJS95f-IX9Gqr9h91kG2l6qU7GTHFs,16131
-QuNet-0.0.98.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-QuNet-0.0.98.dist-info/top_level.txt,sha256=fZTlCNIA7T6KO6-qMX35d8uK01uhYTGRKO117oubma8,6
-QuNet-0.0.98.dist-info/RECORD,,
+QuNet-0.0.99.dist-info/LICENSE,sha256=TSOuIu1obl3tk6okEX3AbHzQjUhXvTKU3gY_yDhpZM0,1068
+QuNet-0.0.99.dist-info/METADATA,sha256=xWn8B0PYgyabGjdUdMg3qgzY6iDqIev77yUVqf6qIJ4,16131
+QuNet-0.0.99.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+QuNet-0.0.99.dist-info/top_level.txt,sha256=fZTlCNIA7T6KO6-qMX35d8uK01uhYTGRKO117oubma8,6
+QuNet-0.0.99.dist-info/RECORD,,
```


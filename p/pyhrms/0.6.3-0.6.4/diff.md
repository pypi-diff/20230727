# Comparing `tmp/pyhrms-0.6.3.zip` & `tmp/pyhrms-0.6.4.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 77241 bytes, number of entries: 15
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-26 16:42 pyhrms-0.6.3/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms.egg-info/
--rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.6.3/License.txt
--rw-rw-rw-  2.0 fat    23697 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/PKG-INFO
--rw-rw-rw-  2.0 fat    22773 b- defN 23-Jul-26 16:39 pyhrms-0.6.3/README.rst
--rw-rw-rw-  2.0 fat       42 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/setup.cfg
--rw-rw-rw-  2.0 fat      919 b- defN 23-Jul-26 16:41 pyhrms-0.6.3/setup.py
--rw-rw-rw-  2.0 fat   221826 b- defN 23-Jul-13 08:04 pyhrms-0.6.3/pyhrms/pyhrms.py
--rw-rw-rw-  2.0 fat     1438 b- defN 23-Jul-13 16:53 pyhrms-0.6.3/pyhrms/__init__.py
--rw-rw-rw-  2.0 fat        1 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms.egg-info/dependency_links.txt
--rw-rw-rw-  2.0 fat    23697 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms.egg-info/PKG-INFO
--rw-rw-rw-  2.0 fat      173 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms.egg-info/requires.txt
--rw-rw-rw-  2.0 fat      216 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms.egg-info/SOURCES.txt
--rw-rw-rw-  2.0 fat        7 b- defN 23-Jul-26 16:42 pyhrms-0.6.3/pyhrms.egg-info/top_level.txt
-15 files, 295877 bytes uncompressed, 75195 bytes compressed:  74.6%
+Zip file size: 81392 bytes, number of entries: 15
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-27 07:28 pyhrms-0.6.4/
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms/
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms.egg-info/
+-rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.6.4/License.txt
+-rw-rw-rw-  2.0 fat    23697 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/PKG-INFO
+-rw-rw-rw-  2.0 fat    22773 b- defN 23-Jul-27 07:25 pyhrms-0.6.4/README.rst
+-rw-rw-rw-  2.0 fat       42 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/setup.cfg
+-rw-rw-rw-  2.0 fat      919 b- defN 23-Jul-27 07:26 pyhrms-0.6.4/setup.py
+-rw-rw-rw-  2.0 fat   239439 b- defN 23-Jul-26 14:55 pyhrms-0.6.4/pyhrms/pyhrms.py
+-rw-rw-rw-  2.0 fat     1526 b- defN 23-Jul-27 07:24 pyhrms-0.6.4/pyhrms/__init__.py
+-rw-rw-rw-  2.0 fat        1 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms.egg-info/dependency_links.txt
+-rw-rw-rw-  2.0 fat    23697 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms.egg-info/PKG-INFO
+-rw-rw-rw-  2.0 fat      173 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms.egg-info/requires.txt
+-rw-rw-rw-  2.0 fat      216 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms.egg-info/SOURCES.txt
+-rw-rw-rw-  2.0 fat        7 b- defN 23-Jul-27 07:28 pyhrms-0.6.4/pyhrms.egg-info/top_level.txt
+15 files, 313578 bytes uncompressed, 79346 bytes compressed:  74.7%
```

## zipnote {}

```diff
@@ -1,46 +1,46 @@
-Filename: pyhrms-0.6.3/
+Filename: pyhrms-0.6.4/
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms/
+Filename: pyhrms-0.6.4/pyhrms/
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms.egg-info/
+Filename: pyhrms-0.6.4/pyhrms.egg-info/
 Comment: 
 
-Filename: pyhrms-0.6.3/License.txt
+Filename: pyhrms-0.6.4/License.txt
 Comment: 
 
-Filename: pyhrms-0.6.3/PKG-INFO
+Filename: pyhrms-0.6.4/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.6.3/README.rst
+Filename: pyhrms-0.6.4/README.rst
 Comment: 
 
-Filename: pyhrms-0.6.3/setup.cfg
+Filename: pyhrms-0.6.4/setup.cfg
 Comment: 
 
-Filename: pyhrms-0.6.3/setup.py
+Filename: pyhrms-0.6.4/setup.py
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms/pyhrms.py
+Filename: pyhrms-0.6.4/pyhrms/pyhrms.py
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms/__init__.py
+Filename: pyhrms-0.6.4/pyhrms/__init__.py
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms.egg-info/dependency_links.txt
+Filename: pyhrms-0.6.4/pyhrms.egg-info/dependency_links.txt
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms.egg-info/PKG-INFO
+Filename: pyhrms-0.6.4/pyhrms.egg-info/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms.egg-info/requires.txt
+Filename: pyhrms-0.6.4/pyhrms.egg-info/requires.txt
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms.egg-info/SOURCES.txt
+Filename: pyhrms-0.6.4/pyhrms.egg-info/SOURCES.txt
 Comment: 
 
-Filename: pyhrms-0.6.3/pyhrms.egg-info/top_level.txt
+Filename: pyhrms-0.6.4/pyhrms.egg-info/top_level.txt
 Comment: 
 
 Zip file comment:
```

## Comparing `pyhrms-0.6.3/License.txt` & `pyhrms-0.6.4/License.txt`

 * *Files identical despite different names*

## Comparing `pyhrms-0.6.3/PKG-INFO` & `pyhrms-0.6.4/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.6.3
+Version: 0.6.4
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,15 +22,15 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-July.13.2023: pyhrms 0.6.3 new features:
+July.27.2023: pyhrms 0.6.4 new features:
 
     * Added ion mobility data processing functions
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
```

## Comparing `pyhrms-0.6.3/README.rst` & `pyhrms-0.6.4/README.rst`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-July.13.2023: pyhrms 0.6.3 new features:
+July.27.2023: pyhrms 0.6.4 new features:
 
     * Added ion mobility data processing functions
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
```

## Comparing `pyhrms-0.6.3/setup.py` & `pyhrms-0.6.4/setup.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 def readme_file():
     with open('README.rst') as rf:
         return rf.read()
 
 setuptools.setup(
     name = 'pyhrms',
-    version = '0.6.3',
+    version = '0.6.4',
     author = 'Wang Rui',
     author_email = 'wtrt7009@gmail.com',
     url = 'https://github.com/WangRui5/PyHRMS.git',
     description = 'A powerful GC/LC-HRMS data analysis tool',
     long_description = readme_file(),
     packages = setuptools.find_packages(),
     install_requires = ['numpy>=1.19.2','pandas>=1.3.3'
```

## Comparing `pyhrms-0.6.3/pyhrms/pyhrms.py` & `pyhrms-0.6.4/pyhrms/pyhrms.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,14 +18,17 @@
 from sklearn.decomposition import PCA
 from sklearn import preprocessing
 import scipy.stats as st
 from scipy import integrate
 import itertools
 import bisect
 import re
+from pyopenms import MSExperiment, MzMLFile
+from collections import defaultdict
+
 
 """
 ========================================================================================================
 1. basic function
 ========================================================================================================
 """
 
@@ -35,15 +38,15 @@
           'Cliso': 36.965903, 'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770,
           'Si': 27.976928, 'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
           'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486
           })
 
 
 def one_step_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], ms2_analysis=True,
-                     filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1):
+                     filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1,orbi = False):
     """
     This function using one processor to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - control_group (List[str]): A list of labels representing the control group.These labels are used in the search for relevant file names.
@@ -54,15 +57,15 @@
                            and fold change is computed as the ratio of the mean sample area
                            to the mean control area.
        - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
        - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
        - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
        - split_n (int): The number of pieces to split the large dataframe.
        - sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
-
+       - orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     returns:
         None.Generate Excel files that summarizes the differences between the control sets and sample sets.
 
     """
     print('                                                                            ')
     print('============================================================================')
     print('First process started...')
@@ -70,15 +73,15 @@
     print('                                                                            ')
 
     files_mzml = glob(os.path.join(path, '*.mzML'))
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
     for file in files_mzml:
         first_process(file, company=company, profile=profile, control_group=control_group, ms2_analysis=ms2_analysis,
-                      split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
+                      split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n,orbi = orbi)
 
     # 检查是否有遗漏的
     files_excel_temp = glob(os.path.join(path, '*.xlsx'))
     files_excel_names = [os.path.basename(i)[:-5] for i in files_excel_temp]
     path_omitted = []
     if len(files_mzml) > len(files_excel_names):
         # 检查是哪个文件漏掉了
@@ -89,29 +92,29 @@
                 path_omitted.append(path1)
     if len(path_omitted) == 0:
         pass
     else:
         for file in path_omitted:
             first_process(file, company=company, profile=profile, control_group=control_group,
                           ms2_analysis=ms2_analysis,
-                          split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
+                          split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n,orbi = orbi)
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
     # 第二个过程
     print('                                                                            ')
     print('============================================================================')
     print('Second process started...')
     print('============================================================================')
     print('                                                                            ')
     for file in files_mzml:
-        second_process(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n)
+        second_process(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n, orbi = orbi)
 
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process started...')
     print('============================================================================')
     print('                                                                            ')
@@ -141,15 +144,15 @@
                         df1.loc[i, 'frag_DDA'] = str(list(s_ms2.index))
                         df1.loc[i, 'MS2_spec_DDA'] = str(s_ms2.astype(int))
                 df1.to_excel(file_excel)
 
 
 def ultimate_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500,
                           SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
-                          mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, long_rt_split_n=1, rt_overlap=1):
+                          mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, long_rt_split_n=1, rt_overlap=1,orbi = False):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
     Args:
         ms1 (scan list): generated from sep_scans(file.mzML).
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
@@ -159,26 +162,27 @@
         SN_threshold (float): Signal-to-noise threshold.
         rt_error_alignment (float, optional): Retention time error alignment threshold.
         mz_error_alignment (float, optional): m/z error alignment threshold.
         mz_overlap (float,optional): The m/z overlap (Da) between adjacent sections of data when splitting it.
         sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
         long_rt_split_n: The number of pieces to split the ms1.
         rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
+        orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False).
     Returns:
         pandas.DataFrame: A dataframe with information on the peaks including retention time, m/z value,
         intensity, and area.
     """
 
     if long_rt_split_n == 1:
         peak_all = split_peak_picking(ms1, profile=profile, split_n=split_n, threshold=threshold,
                                       i_threshold=i_threshold,
                                       SN_threshold=SN_threshold, noise_threshold=noise_threshold,
                                       rt_error_alignment=rt_error_alignment,
                                       mz_error_alignment=mz_error_alignment, mz_overlap=mz_overlap,
-                                      sat_intensity=sat_intensity)
+                                      sat_intensity=sat_intensity,orbi = orbi)
     else:
         # Calculate the length of each part
         total_spectra = len(ms1)
         part_length = total_spectra // long_rt_split_n
         overlap_spectra = int(rt_overlap / (ms1[1].scan_time[0] - ms1[0].scan_time[
             0]))  # calculate the number of spectra in 1 minute of retention time
 
@@ -209,49 +213,49 @@
         my_list = ranges
         ranges = [[my_list[i][0], my_list[i + 1][0]] for i in range(len(my_list) - 1)] + [my_list[-1]]
 
         # start to do peak picking for each part
         peak_list_all = []
         for n, part in enumerate(parts):
             peak_all = split_peak_picking(part, profile=profile, i_threshold=i_threshold,
-                                          SN_threshold=SN_threshold, split_n=split_n, sat_intensity=sat_intensity)
+                                          SN_threshold=SN_threshold, split_n=split_n, sat_intensity=sat_intensity,orbi = orbi)
             peak_all = peak_all[(peak_all['rt'] > ranges[n][0]) & (peak_all['rt'] <= ranges[n][1])]
             peak_list_all.append(peak_all)
 
         peak_all = pd.concat(peak_list_all).reset_index(drop=True)
     return peak_all
 
 
 def first_process(file, company, profile=True, control_group=['methanol_blank', 'control', 'lab_blank'],
                   i_threshold=200, SN_threshold=3,
-                  ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1):
+                  ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1,orbi = False):
     """
     Processes HRMS data by performing peak picking and generating a result file.
 
     Args:
         file (str): Path to the input file to be processed.
         company (str): The manufacturer of the instrument used to generate the data (e.g., 'Waters', 'Agilent', etc.).
         profile (bool): A flag indicating whether or not to perform profiling of chromatographic peaks.
         SN_threshold (int): The signal-to-noise threshold for peak detection.
         frag_rt_error (float): The retention time error to use for fragment MS2 analysis.
         i_threshold (int): The intensity threshold for peak detection.
         ms2_analysis (bool): A flag indicating whether or not to perform MS2 analysis on fragment peaks.
         split_n (int): The number of pieces to split the large dataframe.
         sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
         long_rt_split_n: The number of pieces to split the ms1.
-
+        orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
 
     Returns:
         None. Instead, the function exports an Excel file with the result information.
     """
     mz_round = 4
     ms1, ms2 = sep_scans(file, company)
     peak_all = ultimate_peak_picking(ms1, profile=profile, split_n=split_n, i_threshold=i_threshold,
                                      SN_threshold=SN_threshold, sat_intensity=sat_intensity,
-                                     long_rt_split_n=long_rt_split_n)
+                                     long_rt_split_n=long_rt_split_n,orbi = orbi)
 
     # 是否分析ms2
     if len(ms2) == 0:
         pass
     else:
         if ms2_analysis is True:
             basename_file = os.path.basename(file)
@@ -259,15 +263,15 @@
                 pass
             else:
                 print('----------------------------')
                 print('Starting DIA ms2 analysis...')
                 print('----------------------------')
                 peak_all2 = ultimate_peak_picking(ms2, profile=profile, split_n=split_n, i_threshold=i_threshold,
                                                   SN_threshold=SN_threshold, sat_intensity=sat_intensity,
-                                                  long_rt_split_n=long_rt_split_n)
+                                                  long_rt_split_n=long_rt_split_n, orbi = orbi)
 
                 frag_all = []
                 spec_all = []
                 for i in tqdm(range(len(peak_all)), desc='Assign DIA MS2 spectrum'):
                     rt = peak_all.loc[i, 'rt']
                     df_DIA = peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
                                        & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
@@ -319,15 +323,15 @@
     print(f'Result generated! File name:{file_name}')
     print('--------------------------------------------------------------')
     print('                                                              ')
 
 
 def multi_process(path, company, profile=True, control_group=['lab_blank', 'methanol'], processors=1, i_threshold=200,
                   SN_threshold=3, ms2_analysis=True,
-                  filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1):
+                  filter_type=1, split_n=20, sat_intensity=False, long_rt_split_n=1,orbi = False):
     """
     This function is to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
@@ -339,15 +343,15 @@
                            and fold change is computed as the ratio of the mean sample area
                            to the mean control area.
        - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
        - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
        - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
        - split_n (int): The number of pieces to split the large dataframe.
        - sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
-
+       - orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     returns:
         None.Generate Excel files that summarizes the differences between the control sets and sample sets.
 
     """
     files_mzml = glob(os.path.join(path, '*.mzML'))
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
@@ -355,15 +359,15 @@
     i_threshold = 200
     SN_threshold = 3,
     frag_rt_error = 0.02
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
         pool.apply_async(first_process, args=(file, company, profile, control_group, i_threshold, SN_threshold,
-                                              ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n))
+                                              ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n,orbi))
 
     print('                                                                            ')
     print('============================================================================')
     print('First process started...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
@@ -384,28 +388,28 @@
         pass
     else:
         pool = Pool(processes=processors)
         for file in path_omitted:
             print('Omitted files')
             print(file)
             pool.apply_async(first_process, args=(file, company, profile, control_group, i_threshold, SN_threshold,
-                                                  ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n))
+                                                  ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n,orbi))
         pool.close()
         pool.join()
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
     # 第二个过程
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
-        pool.apply_async(second_process, args=(file, ref_all, company, profile, long_rt_split_n))
+        pool.apply_async(second_process, args=(file, ref_all, company, profile, long_rt_split_n,orbi))
     print('                                                                            ')
     print('============================================================================')
     print('Second process started...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
     pool.join()
@@ -439,47 +443,79 @@
                         s_ms2_info = df_frag.iloc[np.argmin(abs(df_frag['rt'].values - rt))]
                         ms2_rt = df_frag['rt'].values[np.argmin(abs(df_frag['rt'].values - rt))]
                         s_ms2 = pd.Series(data=s_ms2_info['intensity'], index=s_ms2_info['frag'], name=ms2_rt)
                         df1.loc[i, 'frag_DDA'] = str(list(s_ms2.index))
                         df1.loc[i, 'MS2_spec_DDA'] = str(s_ms2.astype(int))
                 df1.to_excel(file_excel)
 
-
-def sep_scans(path, company):
+        
+def sep_scans(path, company, tool = 'pymzml'):
     """
     Separates scans for MS1 and MS2 in mzML files using pymzml package.
     Args:
        path (str): The path of the mzML file.
        company (str): The instrument company name. Currently, supports 'Waters', 'Agilent', 'Thermo' or 'AB'.
-
+       tool(str): pymzml or pyopenms
     Returns:
         Tuple: A tuple of two lists containing MS1 and MS2 scans respectively.
     """
-    if company.lower() == 'waters':
-        # create a pymzml Reader object
-        run = pymzml.run.Reader(path)
-        ms1, ms2 = [], []
-        # iterate over each scan in the mzML file
-        for scan in tqdm(run, desc='Separating MS1 and MS2'):
-            # extract function value from the scan's id_dict attribute
-            if scan.id_dict['function'] == 1:
-                ms1.append(scan)
-            if scan.ms_level == 2:
-                ms2.append(scan)
-        return ms1, ms2
-    else:
-        run = pymzml.run.Reader(path)
-        ms1, ms2 = [], []
-        for scan in tqdm(run, desc='Separating MS1 and MS2'):
-            if scan.ms_level == 1:
-                ms1.append(scan)
-            else:
-                ms2.append(scan)
-        return ms1, ms2
-
+    if tool == 'pymzml':
+        if company.lower() == 'waters':
+            # create a pymzml Reader object
+            run = pymzml.run.Reader(path)
+            ms1, ms2 = [], []
+            # iterate over each scan in the mzML file
+            for scan in tqdm(run, desc='Separating MS1 and MS2'):
+                # extract function value from the scan's id_dict attribute
+                if scan.id_dict['function'] == 1:
+                    ms1.append(scan)
+                if scan.ms_level == 2:
+                    ms2.append(scan)       
+            return ms1, ms2
+        else:
+            run = pymzml.run.Reader(path)
+            ms1, ms2 = [], []
+            for scan in tqdm(run, desc='Separating MS1 and MS2'):
+                if scan.ms_level == 1:
+                    ms1.append(scan)
+                else:
+                    ms2.append(scan)
+            return ms1, ms2
+    elif tool == 'pyopenms':
+        # Load the data
+        print(f'Loading data..')
+        exp = MSExperiment()
+        file = MzMLFile()
+        t1 = time.time()
+        file.load(path, exp)
+        t2 = time.time()
+        print(f'Loading data finished, total time: {round(t2-t1,0)} s')
+        if company.lower() == 'waters':
+            ms1,ms2, lockspray = [],[],[]
+            for i in tqdm(range(exp.size())):
+                scan = exp[i]
+                native_id = scan.getNativeID()
+                if eval(native_id.split('function=')[1][0])==1:
+                    ms1.append(scan) 
+                elif scan.getMSLevel() == 2:
+                    ms2.append(scan)
+                elif eval(native_id.split('function=')[1][0])==3:
+                    lockspray.append(scan)
+            return ms1,ms2,lockspray
+        else:
+            ms1,ms2 = [],[]
+            for i in tqdm(range(exp.size())):
+                scan = exp[i]
+                if scan.getMSLevel()==1:
+                    ms1.append(scan) 
+                elif scan.getMSLevel() == 2:
+                    ms2.append(scan)  
+            return ms1,ms2
+    else:
+        ValueError("Invalid tool. Expected 'pymzml' or 'pyopenms'.")   
 
 def gen_df(ms1, ms_round=4, profile=True, raw_info=True):
     """
     Loads all raw data files and generates a big DataFrame. If the data is in profile mode,it will be transformed into centroid data.
 
     Args:
         ms1: A list of all MS1 scans generated from pymzml.
@@ -932,17 +968,19 @@
     """
     f = interp1d(x, y)
     x_new = np.linspace(x.min(), x.max(), num=num_points, endpoint=True)
     y_new = f(x_new)
     return x_new, y_new
 
 
+
+
 def split_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500,
                        SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
-                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False):
+                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, orbi = False):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
     Args:
         ms1 (scan list): generated from sep_scans(file.mzML).
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
@@ -950,14 +988,15 @@
         threshold (int): Threshold for finding peaks.
         i_threshold (int): Threshold for peak intensity.
         SN_threshold (float): Signal-to-noise threshold.
         rt_error_alignment (float, optional): Retention time error alignment threshold.
         mz_error_alignment (float, optional): m/z error alignment threshold.
         mz_overlap (float,optional): The overlap between adjacent sections of data when splitting it.
         sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+        orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False).
     Returns:
         pandas.DataFrame: A dataframe with information on the peaks including retention time, m/z value,
         intensity, and area.
     """
 
     def target_spec1(spec, target_mz, width=0.04):
         """
@@ -980,22 +1019,42 @@
         peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]] for i in range(len(ms1))]
         raw_info_centroid = {
             round(ms1[i].scan_time[0], 3): pd.Series(data=ms1[i].i[peaks], index=ms1[i].mz[peaks].round(4),
                                                      name=round(ms1[i].scan_time[0], 3)) for i, peaks in peaks_index}
         raw_info_profile = {round(ms1[i].scan_time[0], 3):
                                 pd.Series(data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3))
                             for i in range(len(ms1))}
-        data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                raw_info_centroid.items()]
+        if orbi is True:
+            data = []
+            for k,v in tqdm(raw_info_centroid.items(),desc = 'Checking orbitrap data'):
+                df = v.to_frame().reset_index()
+                df = df.sort_values(['index', k])
+                df['index'] = np.round(df['index'].values,3)
+                df = df.drop_duplicates('index', keep='last')
+                s = pd.Series(df[k].values, index=df['index'],name = k)
+                data.append(s)
+        else:
+            data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
+                    raw_info_centroid.items()]
     else:
         raw_info_centroid = {round(ms1[i].scan_time[0], 3): pd.Series(
             data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3)) for i in
             range(len(ms1))}
-        data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                raw_info_centroid.items()]
+        if orbi is True:
+            data = []
+            for k,v in tqdm(raw_info_centroid.items(),desc = 'Checking orbitrap data'):
+                df = v.to_frame().reset_index()
+                df = df.sort_values(['index', k])
+                df['index'] = np.round(df['index'].values,3)
+                df = df.drop_duplicates('index', keep='last')
+                s = pd.Series(df[k].values, index=df['index'],name = k)
+                data.append(s)
+        else:
+            data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
+                    raw_info_centroid.items()]
 
     t2 = time.time()
     time1 = round(t2 - t1, 0)
     print(f'\r *** Loading data successfully: {time1} s')
 
     # 开始分割
     # 定义变量名称
@@ -1122,14 +1181,15 @@
                             update_mz = new_spec1.idxmax()
                             if max_intensity < sat_intensity:
                                 peak_all.loc[j, 'mz'] = update_mz
                                 break
     return peak_all
 
 
+
 def remove_unnamed_columns(df):
     """
     Remove any columns in the input DataFrame that are named 'Unnamed:*'.
 
     Args:
         df (pandas.DataFrame): The input DataFrame to process.
 
@@ -1371,60 +1431,62 @@
         final_reference_pairs = np.vstack([reference_pairs, omitted_reference_pairs])
     else:
         final_reference_pairs = reference_pairs
 
     return final_reference_pairs
 
 
-def second_process(file, ref_all, company, profile=True, long_rt_split_n=1):
+def second_process(file, ref_all, company, profile=True, long_rt_split_n=1,orbi = False):
     """
     This function will use the reference rt&mz pair, and obtain the peak area at specific rt & mz
     Args:
         profile: True or False
         file: single file to process
         ref_all: all reference peaks
         company: e.g., 'Waters', 'Agilent',etc,
+        orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     returns:
         export to files
 
     """
     ms_round = 4
     ms1, ms2 = sep_scans(file, company)
 
     name1 = os.path.basename(file).split('.')[0]
     final_result = ultimate_checking_area(ref_all, ms1, name1, profile=profile,
-                                          rt_overlap=1, long_rt_split_n=long_rt_split_n)
+                                          rt_overlap=1, long_rt_split_n=long_rt_split_n, orbi = orbi)
     final_result.to_excel(file.replace('.mzML', '_final_area.xlsx'))
 
     print('                                                              ')
     print('--------------------------------------------------------------')
     print(f'Final area files have been created: {os.path.basename(file)}')
     print('--------------------------------------------------------------')
     print('                                                              ')
 
 
 def ultimate_checking_area(ref_all, ms1, name1, profile=True,
-                           split_n=20, rt_overlap=1, long_rt_split_n=4):
+                           split_n=20, rt_overlap=1, long_rt_split_n=4, orbi = False):
     """
     Based on peak reference, intergrate peak are for each reference m/z and retention time pair.
 
     Args:
         ref_all: reference m/z and retention time pair
         ms1: generated from sep_scans(file.mzML).
         name1: file name.
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
         split_n: The number of pieces to split the large dataframe.
         long_rt_split_n: The number of pieces to split the ms1.
         rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
+        orbi: A boolean indicating whether the data is in orbitrap data (True) or TOF-MS data (False)
     return:
         The final areas for reference m/z and retention time pair.
     """
 
     if long_rt_split_n == 1:
-        final_area = peak_checking_area_split(ref_all, ms1, name1, profile=profile, split_n=split_n, noise_threshold=0)
+        final_area = peak_checking_area_split(ref_all, ms1, name1, profile=profile, split_n=split_n, noise_threshold=0,orbi = orbi)
 
     else:
         # Calculate the length of each part
         total_spectra = len(ms1)
         part_length = total_spectra // long_rt_split_n
         overlap_spectra = int(rt_overlap / (ms1[1].scan_time[0] - ms1[0].scan_time[
             0]))  # calculate the number of spectra in 1 minute of retention time
@@ -1450,15 +1512,15 @@
         # start to split ref_all
         ref_all = remove_unnamed_columns(ref_all)
         ref_all_parts = [ref_all[(ref_all['rt'] >= ranges1[0]) & (ref_all['rt'] < ranges1[1])] for ranges1 in ranges]
 
         # start to collect each peak_all
         peak_area_all = []
         for i in range(len(parts)):
-            each_peak_area = peak_checking_area_split(ref_all_parts[i], parts[i], '', profile=profile, split_n=split_n)
+            each_peak_area = peak_checking_area_split(ref_all_parts[i], parts[i], '', profile=profile, split_n=split_n,orbi = orbi)
             peak_area_all.append(each_peak_area)
         final_area = pd.concat(peak_area_all)
         final_area.columns = [name1]
     return final_area
 
 
 def peak_checking_area(ref_all, df1, name):
@@ -1494,32 +1556,54 @@
     # Uses the locators found in stepas 2 and 3 to calculate the peak areas for each rt&mz pair in `df1`, using the `scipy.integrate.simps` function.
     # The result is multiplied by 4, then rounded to 0 decimal places.
 
     sample_area = pd.DataFrame(area_all, index=peak_index, columns=[name])
     return sample_area + 1  # Adds 1 to each value in the resulting data frame and returns it to avoid zero value in result.
 
 
-def peak_checking_area_split(ref_all, ms1, name1, profile=True, split_n=20, noise_threshold=0):
+
+def peak_checking_area_split(ref_all, ms1, name1, profile=True, split_n=20, noise_threshold=0,orbi = False):
     # 需要给ref_all排序
     print('Loading data for 2nd process...')
     ref_all1 = ref_all.sort_values(by='mz')
 
     if profile is True:
         peaks_index = [[i, scipy.signal.find_peaks(ms1[i].i.copy())[0]] for i in range(len(ms1))]
         raw_info_centroid = {
             round(ms1[i].scan_time[0], 3): pd.Series(data=ms1[i].i[peaks], index=ms1[i].mz[peaks].round(4),
                                                      name=round(ms1[i].scan_time[0], 3)) for i, peaks in peaks_index}
-        data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                raw_info_centroid.items()]
+        if orbi is True:
+            data = []
+            for k,v in tqdm(raw_info_centroid.items(),desc = 'Checking orbitrap data'):
+                df = v.to_frame().reset_index()
+                df = df.sort_values(['index', k])
+                df['index'] = np.round(df['index'].values,3)
+                df = df.drop_duplicates('index', keep='last')
+                s = pd.Series(df[k].values, index=df['index'],name = k)
+                data.append(s)
+        else:
+            data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
+                    raw_info_centroid.items()]
     else:
         raw_info_centroid = {round(ms1[i].scan_time[0], 3): pd.Series(
             data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3)) for i in
             range(len(ms1))}
-        data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
-                raw_info_centroid.items()]
+        
+        if orbi is True:
+            data = []
+            for k,v in tqdm(raw_info_centroid.items(),desc = 'Checking orbitrap data'):
+                df = v.to_frame().reset_index()
+                df = df.sort_values(['index', k])
+                df['index'] = np.round(df['index'].values,3)
+                df = df.drop_duplicates('index', keep='last')
+                s = pd.Series(df[k].values, index=df['index'],name = k)
+                data.append(s)
+        else:
+            data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
+                    raw_info_centroid.items()]
 
     # 开始分割 series数据
     # 定义变量名称
     all_data = []
     for j in range(split_n):
         name = 'a' + str(j + 1)
         locals()[name] = []
@@ -1563,14 +1647,18 @@
 
     # 合成所有的area
     final_df = pd.concat(area_all)
     final_df.columns = [name1]
     return final_df
 
 
+
+
+
+
 def concat_alignment(files_excel):
     """
     Concatenate all dataframes containing 'area' in their name
     and return the final dataframe.
 
     Args:
         files_excel: list of excel file paths.
@@ -3871,17 +3959,306 @@
     AI_denominator = final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - final_result['N']
     AI_numerator = 1 + final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - 0.5 * (final_result['H'] - x)
     AI = AI_numerator / (AI_denominator.sort_values() + 1 * 1e-6)
     final_result['AI'] = AI
     return final_result
 
 
+
+"""
+========================================================================================================
+5. Ion mobility data processing
+========================================================================================================
+"""
+def first_step_for_IMS(path,mz_range = [50,2000],company = 'Waters',profile = True,lock_mass = 556.2771,ms2_analysis = True,frag_rt_error = 0.05,dft_error = 0.5,
+                       split_n = 20,long_rt_split_n = 5,rt_overlap = 1, mz_overlap = 1,noise_threshold = 0, threshold=15, i_threshold=200,
+                       SN_threshold=3, rt_error_alignment=0.1,mz_error_alignment=0.015, sat_intensity = False):
+    
+    files = glob(os.path.join(path,'*.mzML'))
+    for file in files:
+        # 1. 先分离，获得ms1，ms2和lockspray
+        ms1,ms2,lockspray = sep_scans(file,company,tool = 'pyopenms')
+        # 2. 获得矫正数据factor
+        mz_corr_factors = []
+        for scan in lockspray:
+            mz,intensity = scan.get_peaks()
+            s = pd.Series(data = intensity, index = mz)
+            s1 = s.loc[(s.index>lock_mass-0.2)&(s.index<lock_mass+0.2)]
+            lockmass_obs = s1.idxmax()
+            factor = lock_mass/lockmass_obs
+            mz_corr_factors.append(factor)
+        factor = np.median(mz_corr_factors)
+
+        # 3. 开始处理ms1
+        peak_all1 = peak_picking_ion_mobility_DIA1(ms1,mz_range = mz_range,profile = profile,split_n = split_n,long_rt_split_n = long_rt_split_n,
+                                 rt_overlap = rt_overlap, mz_overlap = mz_overlap,noise_threshold = noise_threshold, threshold=threshold, i_threshold=i_threshold, SN_threshold=SN_threshold,
+                                 rt_error_alignment=rt_error_alignment,mz_error_alignment=mz_error_alignment, factor = factor)
+
+        # 4. 开始处理ms2
+        if (len(ms2) ==0)|(ms2_analysis==ms2_analysis) :
+            pass
+        else:
+            peak_all2 = peak_picking_ion_mobility_DIA1(ms2,mz_range = mz_range,profile = profile,split_n = split_n,long_rt_split_n = long_rt_split_n,
+                                     rt_overlap = rt_overlap, mz_overlap = mz_overlap,noise_threshold = noise_threshold, threshold=threshold, i_threshold=i_threshold, SN_threshold=SN_threshold,
+                                     rt_error_alignment=rt_error_alignment,mz_error_alignment=mz_error_alignment, factor = factor)
+
+
+            for i in tqdm(range(len(peak_all1))):
+                rt, mz, dft = peak_all1.loc[i,['rt','mz','Drift Time']]
+                frag_df = peak_all2[(peak_all2['rt']>rt-frag_rt_error)&(peak_all2['rt']<rt+frag_rt_error)&(peak_all2['mz']<mz+1)]
+                mz, intensities = frag_df['mz'].values, frag_df['intensity'].values
+                s_frag = pd.Series(data = intensities, index = mz, name = dft).sort_index(ascending = False)
+                d_frag = s_frag.to_dict()
+                peak_all1.loc[i,'ms2_spec'] = str(s_frag)
+                peak_all1.loc[i,'ms2_spec_dict'] = str(d_frag)
+        peak_all1.to_excel(file.replace('.mzML','.xlsx'))
+
+
+
+def peak_picking_ion_mobility_DIA1(ms1, mz_range = [50,2000],profile = True,split_n = 20,long_rt_split_n = 5,
+                             rt_overlap = 1, mz_overlap = 1,noise_threshold = 0, threshold=15, i_threshold=200, SN_threshold=3,
+                             rt_error_alignment=0.1,mz_error_alignment=0.015, factor = 1, sat_intensity = False):
+    
+    # using the same factor
+    
+    # 1. put the information of mz, rt, intensity and ion mobility in one dict
+
+    # Initialize an empty defaultdict of lists
+    dict_name = defaultdict(list)
+
+    # Iterate over each scan in ms1
+    for scan in tqdm(ms1,desc = 'Reading each scan'):
+        # Create a pandas Series for the current scan
+        mzs, intensities = scan.get_peaks()
+        mzs = mzs*factor
+        drift_time = round(scan.getDriftTime(), 3)  # Assuming drift time is available
+        s = pd.Series(index=mzs, data=intensities, name=drift_time)
+        # Append the series to the list associated with its retention time in the dictionary
+        rt = round(scan.getRT()/60, 3)
+        if len(s)!=0:
+            dict_name[rt].append(s)
+
+    # 2. combine all data if their retention is the same
+
+    result = []
+    for rt, series_list in tqdm(dict_name.items(),desc = 'Combine all drift data'):
+
+        combined_series = pd.concat(series_list).groupby(level=0).sum()
+        combined_series.name = rt
+        result.append(combined_series)
+
+
+    # 3. convert profile to centroid
+    if profile is True:
+        centroid_data = []
+        for scan in tqdm(result,desc = 'Convert to centroid'):
+            scan1 = ms_to_centroid(scan)
+            centroid_data.append(scan1)
+    else:
+        centroid_data = result
+
+
+    # 4. 横向分割
+
+    # Calculate the length of each part
+    total_spectra = len(centroid_data)
+    part_length = total_spectra // long_rt_split_n
+    overlap_spectra = int(rt_overlap/ (centroid_data[1].name - centroid_data[0].name))  # calculate the number of spectra in 1 minute of retention time
+
+    # Split the list into parts
+    parts = []
+    for i in range(long_rt_split_n):
+        start_index = i * part_length - overlap_spectra
+        start_index = max(start_index, 0)  # set start index to 0 if it is less than 0
+        end_index = (i+1) * part_length + overlap_spectra
+        part = centroid_data[start_index:end_index]
+        parts.append(part)
+
+    # Add any remaining spectra to the last part
+    if end_index < total_spectra:
+        last_part = ms1[end_index:]
+        parts[-1] += last_part
+
+    # 5. 众向分割
+
+    if long_rt_split_n == 1:
+        peak_all = split_peak_picking2(centroid_data, mz_range=mz_range, i_threshold=i_threshold,
+                                          SN_threshold=SN_threshold, split_n=split_n)
+    else:
+        # Calculate the length of each part
+        total_spectra = len(centroid_data)
+        part_length = total_spectra // long_rt_split_n
+        overlap_spectra = int(rt_overlap/ (centroid_data[1].name - centroid_data[0].name))  # calculate the number of spectra in 1 minute of retention time
+        # Split the list into parts
+        parts = []
+        for i in range(long_rt_split_n):
+            start_index = i * part_length - overlap_spectra
+            start_index = max(start_index, 0)  # set start index to 0 if it is less than 0
+            end_index = (i+1) * part_length + overlap_spectra
+            part = centroid_data[start_index:end_index]
+            parts.append(part)
+
+        # Add any remaining spectra to the last part
+        if end_index < total_spectra:
+            last_part = centroid_data[end_index:]
+            parts[-1] += last_part
+
+
+    parts1 = [centroid_data[i*part_length:(i+1)*part_length] for i in range(long_rt_split_n)] # find the cut point of the retention time.
+    ranges = []
+    for i, part in enumerate(parts1):
+        rt_start = part[0].name
+        rt_end = part[-1].name 
+        range1 = [rt_start,rt_end]
+        ranges.append(range1)
+    # to make sure there is no gap between each list.
+    my_list = ranges
+    ranges = [[my_list[i][0], my_list[i+1][0]] for i in range(len(my_list) - 1)] + [my_list[-1]]
+
+    # start to do peak picking for each part
+    peak_list_all = []
+    for n,part in enumerate(parts):
+        peak_all = split_peak_picking2(part,mz_range=mz_range, i_threshold=i_threshold,
+                                          SN_threshold=SN_threshold, split_n=split_n)
+        peak_all = peak_all[(peak_all['rt']>ranges[n][0])&(peak_all['rt']<=ranges[n][1])]
+        peak_list_all.append(peak_all)
+
+    peak_all = pd.concat(peak_list_all).reset_index(drop = True)
+    
+    # 对peak all进行排序
+    peak_all = peak_all.sort_values(by = 'intensity',ascending = False).reset_index(drop = True)
+    
+    # 开始处理淌度数据
+    # 1. 找到所有的RTs
+    scan_times = []
+    for k,v in dict_name.items():
+        scan_times.append(k)
+        
+    # 2找到特定的rt，mz，并找到最高点作为漂移时间
+    for i in tqdm(range(len(peak_all)),desc = 'Collecting drift time'):
+        rt,mz = peak_all.loc[i,['rt','mz']]
+        rt_index = np.argmin(abs(scan_times-rt))
+        # 2. 找到特定的scans，去看drift time
+        target_rt = scan_times[rt_index]
+        target_rt_scans = dict_name[target_rt]
+        # 3. 获得eic和time
+        drift_eic = np.array([scan[(scan.index>mz*(1-50e-6))&(scan.index<mz*(1+50e-6))].sum() for scan in target_rt_scans])
+        drift_times = np.array([scan.name for scan in target_rt_scans])
+        peak_all.loc[i,'Drift Time'] = drift_times[np.argmax(drift_eic)]
+    return peak_all
+        
+        
+  
+        
+
+def split_peak_picking2(data,mz_range = [50,2000], split_n = 20, mz_overlap = 1, 
+                        noise_threshold = 0, threshold=15, i_threshold=200, 
+                        SN_threshold=3, rt_error_alignment=0.1, mz_error_alignment=0.015):
+    
+    """
+    Find peaks in the orginal Series list, analyze isotope and adduct information, and return a dataframe with
+    information on the peaks including retention time, m/z value, intensity, and area. Note this is used for the data processed by pyopenms.
+
+    Args:
+        ms1 (scan list): generated from sep_scans(file.mzML).
+        profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
+        split_n (int): The number of pieces to split the large dataframe.
+        threshold (int): Threshold for finding peaks.
+        i_threshold (int): Threshold for peak intensity.
+        SN_threshold (float): Signal-to-noise threshold.
+        rt_error_alignment (float, optional): Retention time error alignment threshold.
+        mz_error_alignment (float, optional): m/z error alignment threshold.
+        mz_overlap (float,optional): The overlap between adjacent sections of data when splitting it.
+        sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+    Returns:
+        pandas.DataFrame: A dataframe with information on the peaks including retention time, m/z value,
+        intensity, and area.
+    """
+
+    # 定义变量名称
+    all_data = []
+    for j in range(split_n):
+        name = 'a' + str(j + 1)
+        locals()[name] = []
+        
+    # 对series进行切割
+    ms_increase = int(mz_range[1] / split_n)
+    for i in tqdm(range(len(data)), desc='Split series:'):
+        s1 = data[i]
+        low, high = 50, 50 + ms_increase
+        for j in range(split_n):
+            name = 'a' + str(j + 1)
+            locals()[name].append(
+                s1[(s1.index < high + mz_overlap) & (s1.index >= low - mz_overlap) & (s1.index > noise_threshold)])
+            low += ms_increase
+            high += ms_increase
+    for j in range(split_n):
+        name = 'a' + str(j + 1)
+        all_data.append(locals()[name])
+        
+    # 开始分段提取
+    all_peak_all = []
+    for data in tqdm(all_data, desc='Split peak picking process:'):
+        df1 = pd.concat(data, axis=1)
+        df1 = df1.fillna(0)
+        if len(df1) == 0:
+            pass
+        else:
+            peak_all = peak_picking(df1, isotope_analysis=False, threshold=threshold,
+                                    i_threshold=i_threshold, SN_threshold=SN_threshold,
+                                    rt_error_alignment=rt_error_alignment,
+                                    mz_error_alignment=mz_error_alignment, enable_progress_bar=False, alignment=False)
+            all_peak_all.append(peak_all)
+
+    peak_all = pd.concat(all_peak_all).sort_values(by='intensity', ascending=False).reset_index(drop=True)
+    
+
+    # 做alignment
+    print('\r Single file alignment...', end='')
+    t1 = time.time()
+    peak_p = np.array([peak_all.rt.values, peak_all.mz.values]).T
+    indice = [
+        peak_all[
+            (peak_all.mz > peak_p[i][1] - mz_error_alignment) & (peak_all.mz < peak_p[i][1] + mz_error_alignment) &
+            (peak_all.rt > peak_p[i][0] - rt_error_alignment) & (
+                    peak_all.rt < peak_p[i][0] + rt_error_alignment)].index[-1] for
+        i in range(len(peak_p))]
+    indice1 = np.array(list(set(indice)))
+    peak_all = peak_all.loc[indice1, :].sort_values(by='intensity', ascending=False).reset_index(drop=True)
+    t2 = time.time()
+    t_ = round(t2 - t1, 1)
+    print(f'\r *** Single file alignment finished: {t_} s')
+
+    # 对同位素丰度进行记录
+    print('\r Recording isotope information...', end='')
+    raw_info_rts = [data1.name for data1 in data]
+    rts = peak_all.rt.values
+    mzs = peak_all.mz.values
+
+
+    rt_keys = [argmin(abs(np.array(raw_info_rts) - i)) for i in rts]  # 基于上述rt找到ms的时间索引
+
+    iso_info = [str(isotope_distribution(data[rt_keys[i]], mzs[i])) for i in range(len(mzs))]
+    peak_all['iso_distribution'] = iso_info
+
+    t3 = time.time()
+    t_ = round(t3 - t2, 1)
+    print(f'\r *** Recording isotope information finished: {t_} s')
+    return peak_all
+
+
+
+
+
+
+
+
 """
 ========================================================================================================
-5. other functions
+6. other functions
 ========================================================================================================
 """
 
 
 def rename_files(rename_info, files):
     """
     Rename files based on a mapping of old names to new names.
@@ -4594,15 +4971,15 @@
                 df.loc[i, 'Smile'] = new_smi
                 df.loc[i, 'CAS'] = cas_number
     return df
 
 
 def AIF_multi_ce(path, company='Agilent', profile=False, control_group=['Methanol'], collision_energies=[10, 20, 40],
                  filter_type=1, frag_rt_error=0.02, split_n=20,
-                 sat_intensity=False, long_rt_split_n=1, threshold=15, i_threshold=500, SN_threshold=3):
+                 sat_intensity=False, long_rt_split_n=1, threshold=15, i_threshold=500, SN_threshold=3, orbi = False):
     """
     Processes AIF data when multiple collision energies are present.
 
     Args:
         path (str): The file path for the mzML files to be processed. Example, 'C:/Users/Desktop/my_HRMS_files'.
         company (str): The brand of the mass spectrometer used to gather the data. Acceptable options are 'Waters', 'Thermo', 'Sciex', 'Agilent'.
         profile (bool): Specifies whether the data is in profile or centroid mode. Set as True for profile mode, False for centroid mode.
@@ -4613,28 +4990,28 @@
         frag_rt_error (float): The retention time (RT) error for matching fragment peak to the precursor's peak.
         split_n (int): The mass range will be divided into n parts for easier processing.
         sat_intensity (bool): Specifies if the mass spectrometer has a saturated intensity. Default is False.
         long_rt_split_n (int): The retention time will be divided into n parts for easier processing.
         threshold (int): The noise level threshold for a peak. Default is 15.
         i_threshold (int): The intensity threshold for the peak picking step. Default is 500.
         SN_threshold (int): The signal to noise ratio threshold for the peak picking step. Default is 3.
-
+        
     Returns:
         None. The processed data is saved to an output file.
     """
 
     collision_energies = sorted(collision_energies)
 
     files = glob(os.path.join(path, '*.mzML'))
     # 1. first step peak picking
     for file in files:
         if any(control.lower() in file.lower() for control in control_group):
             first_process(file, company, profile=profile, i_threshold=200,
                           SN_threshold=3, ms2_analysis=False, frag_rt_error=frag_rt_error, split_n=split_n,
-                          sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
+                          sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n,orbi = orbi)
         else:
             ms_scans = [[] for _ in range(len(collision_energies) + 1)]
             run = pymzml.run.Reader(file)
             for scan in tqdm(run, desc='Separating Scans'):
                 if scan.ms_level == 1:
                     ms_scans[0].append(scan)
                 elif 'collision energy' in scan:
@@ -4644,15 +5021,15 @@
                         ms_scans[index].append(scan)
             peak_alls = []
             for ms in ms_scans:
                 peak_all = ultimate_peak_picking(ms, profile=profile, split_n=split_n, threshold=threshold,
                                                  i_threshold=i_threshold, SN_threshold=SN_threshold,
                                                  rt_error_alignment=0.05, mz_error_alignment=0.015, mz_overlap=1,
                                                  sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n,
-                                                 rt_overlap=1)
+                                                 rt_overlap=1,orbi = orbi)
                 peak_alls.append(peak_all)
             peak_all_ms1 = peak_alls[0]
             peak_alls_ms2 = peak_alls[1:]
             for j, each_ms2_peak_all in enumerate(peak_alls_ms2):
                 frag_all, spec_all = get_frag_DIA(peak_all_ms1, each_ms2_peak_all, frag_rt_error=frag_rt_error)
                 column_name1 = 'frag_DIA_' + str(collision_energies[j]) + 'V'
                 column_name2 = 'Spec_DIA' + str(collision_energies[j]) + 'V'
@@ -4663,15 +5040,15 @@
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
     # 第二部分
     for file in files:
-        second_process(file, ref_all, company=company, profile=profile, long_rt_split_n=long_rt_split_n)
+        second_process(file, ref_all, company=company, profile=profile, long_rt_split_n=long_rt_split_n,orbi = orbi)
 
     # 第三部份，差异性分析
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process started...')
     print('============================================================================')
@@ -4738,10 +5115,11 @@
         result = result.sort_values(ascending=False).iloc[:20]
 
         spec_all.append(str(result))
 
     return frag_all, spec_all
 
 
+
 if __name__ == '__main__':
     pass  # %config InlineBackendlineBackend.figure_format ='retina'
 #  plt.rcParams['font.sans-serif'] = 'Times New Roman'  # 设置全局字体
```

## Comparing `pyhrms-0.6.3/pyhrms/__init__.py` & `pyhrms-0.6.4/pyhrms/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from . import pyhrms
 
-__version__ = "0.6.2"
+__version__ = "0.6.4"
 
 __all__ = [
     "sep_scans",
     "gen_df",
     "peak_picking",
     "split_peak_picking",
     "remove_unnamed_columns",
@@ -55,11 +55,13 @@
     "Calibration",
     "final_result_filter",
     "update_category",
     "fold_change_filter",
     "get_frag_DIA",
     "AIF_multi_ce",
     "pubchem_search",
-    "draw_pie_chart"
-    
+    "draw_pie_chart",
+    "first_step_for_IMS",
+    "peak_picking_ion_mobility_DIA1",
+    "split_peak_picking2"
     
 ]
```

## Comparing `pyhrms-0.6.3/pyhrms.egg-info/PKG-INFO` & `pyhrms-0.6.4/pyhrms.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.6.3
+Version: 0.6.4
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,15 +22,15 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-July.13.2023: pyhrms 0.6.3 new features:
+July.27.2023: pyhrms 0.6.4 new features:
 
     * Added ion mobility data processing functions
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
```


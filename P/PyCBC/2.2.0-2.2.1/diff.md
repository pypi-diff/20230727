# Comparing `tmp/PyCBC-2.2.0.tar.gz` & `tmp/PyCBC-2.2.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/PyCBC-2.2.0.tar", last modified: Thu Mar  9 21:19:37 2023, max compression
+gzip compressed data, was "dist/PyCBC-2.2.1.tar", last modified: Thu Jul 27 17:58:17 2023, max compression
```

## Comparing `PyCBC-2.2.0.tar` & `PyCBC-2.2.1.tar`

### file list

```diff
@@ -1,825 +1,834 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/
--rw-r--r--   0 runner    (1001) docker     (122)    35141 2023-03-09 21:19:31.000000 PyCBC-2.2.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (122)      197 2023-03-09 21:19:31.000000 PyCBC-2.2.0/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (122)     4018 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PKG-INFO
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PyCBC.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)     4018 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PyCBC.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)    23744 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PyCBC.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PyCBC.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)      372 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PyCBC.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)        6 2023-03-09 21:19:37.000000 PyCBC-2.2.0/PyCBC.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (122)     2492 2023-03-09 21:19:31.000000 PyCBC-2.2.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/all_sky_search/
--rwxr-xr-x   0 runner    (1001) docker     (122)    29737 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_add_statmap
--rw-r--r--   0 runner    (1001) docker     (122)     4231 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_apply_rerank
--rw-r--r--   0 runner    (1001) docker     (122)     3897 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_average_psd
--rw-r--r--   0 runner    (1001) docker     (122)     6355 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_bin_trigger_rates_dq
--rw-r--r--   0 runner    (1001) docker     (122)     3472 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_calculate_dq
--rw-r--r--   0 runner    (1001) docker     (122)     2483 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_calculate_dqflag
--rwxr-xr-x   0 runner    (1001) docker     (122)     4844 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_calculate_psd
--rw-r--r--   0 runner    (1001) docker     (122)    22979 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_findtrigs
--rwxr-xr-x   0 runner    (1001) docker     (122)    14108 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_hdfinjfind
--rwxr-xr-x   0 runner    (1001) docker     (122)     6098 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_mergetrigs
--rwxr-xr-x   0 runner    (1001) docker     (122)    21534 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_statmap
--rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_statmap_inj
--rw-r--r--   0 runner    (1001) docker     (122)     3973 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_combine_coincident_events
--rwxr-xr-x   0 runner    (1001) docker     (122)     8387 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_combine_statmap
--rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_cut_merge_triggers_to_tmpltbank
--rw-r--r--   0 runner    (1001) docker     (122)     2453 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_distribute_background_bins
--rw-r--r--   0 runner    (1001) docker     (122)    11088 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_dtphase
--rw-r--r--   0 runner    (1001) docker     (122)     4732 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_exclude_zerolag
--rw-r--r--   0 runner    (1001) docker     (122)    19787 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_binned
--rwxr-xr-x   0 runner    (1001) docker     (122)    16852 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_by_template
--rwxr-xr-x   0 runner    (1001) docker     (122)    13721 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_over_multiparam
--rw-r--r--   0 runner    (1001) docker     (122)     9377 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_over_param
--rw-r--r--   0 runner    (1001) docker     (122)    21126 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_split_binned
--rw-r--r--   0 runner    (1001) docker     (122)     3445 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_followup_file
--rwxr-xr-x   0 runner    (1001) docker     (122)     2121 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_foreground_censor
--rw-r--r--   0 runner    (1001) docker     (122)     3232 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_get_loudest_params
--rwxr-xr-x   0 runner    (1001) docker     (122)     2490 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_merge_psds
--rw-r--r--   0 runner    (1001) docker     (122)     4181 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_plot_kde_vals
--rw-r--r--   0 runner    (1001) docker     (122)     2471 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_reduce_template_bank
--rw-r--r--   0 runner    (1001) docker     (122)     5084 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_rerank_dq
--rw-r--r--   0 runner    (1001) docker     (122)     1386 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_rerank_passthrough
--rw-r--r--   0 runner    (1001) docker     (122)     7429 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_findtrigs
--rw-r--r--   0 runner    (1001) docker     (122)    16771 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_pastro
--rwxr-xr-x   0 runner    (1001) docker     (122)    16627 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_statmap
--rw-r--r--   0 runner    (1001) docker     (122)     5725 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_statmap_inj
--rw-r--r--   0 runner    (1001) docker     (122)     2580 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_strip_injections
--rw-r--r--   0 runner    (1001) docker     (122)     8682 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_template_kde_calc
--rw-r--r--   0 runner    (1001) docker     (122)     4798 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/all_sky_search/pycbc_template_recovery_hist
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/bank/
--rw-r--r--   0 runner    (1001) docker     (122)     7665 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_aligned_bank_cat
--rw-r--r--   0 runner    (1001) docker     (122)    12145 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_aligned_stoch_bank
--rw-r--r--   0 runner    (1001) docker     (122)    12618 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_bank_verification
--rw-r--r--   0 runner    (1001) docker     (122)    17070 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_brute_bank
--rw-r--r--   0 runner    (1001) docker     (122)     4926 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_coinc_bank2hdf
--rw-r--r--   0 runner    (1001) docker     (122)    17389 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_geom_aligned_2dstack
--rw-r--r--   0 runner    (1001) docker     (122)    23518 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_geom_aligned_bank
--rw-r--r--   0 runner    (1001) docker     (122)    13737 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_geom_nonspinbank
--rw-r--r--   0 runner    (1001) docker     (122)     7079 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/bank/pycbc_tmpltbank_to_chi_params
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/hwinj/
--rw-r--r--   0 runner    (1001) docker     (122)    22443 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/hwinj/pycbc_generate_hwinj
--rw-r--r--   0 runner    (1001) docker     (122)     5115 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/hwinj/pycbc_generate_hwinj_from_xml
--rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/hwinj/pycbc_insert_frame_hwinj
--rw-r--r--   0 runner    (1001) docker     (122)     2872 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/hwinj/pycbc_plot_hwinj
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/inference/
--rw-r--r--   0 runner    (1001) docker     (122)     6405 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference
--rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_create_fits
--rw-r--r--   0 runner    (1001) docker     (122)     7268 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_extract_samples
--rw-r--r--   0 runner    (1001) docker     (122)     5736 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_model_stats
--rw-r--r--   0 runner    (1001) docker     (122)     2912 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_monitor
--rw-r--r--   0 runner    (1001) docker     (122)     3942 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_acceptance_rate
--rw-r--r--   0 runner    (1001) docker     (122)     5628 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_acf
--rw-r--r--   0 runner    (1001) docker     (122)     3575 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_acl
--rw-r--r--   0 runner    (1001) docker     (122)     2603 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_dynesty_run
--rw-r--r--   0 runner    (1001) docker     (122)     2682 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_dynesty_traceplot
--rw-r--r--   0 runner    (1001) docker     (122)     4461 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_gelman_rubin
--rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_geweke
--rw-r--r--   0 runner    (1001) docker     (122)     4300 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_inj_recovery
--rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_mcmc_history
--rw-r--r--   0 runner    (1001) docker     (122)    15034 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_movie
--rw-r--r--   0 runner    (1001) docker     (122)    13533 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_posterior
--rw-r--r--   0 runner    (1001) docker     (122)     5830 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_pp
--rw-r--r--   0 runner    (1001) docker     (122)     5256 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_prior
--rw-r--r--   0 runner    (1001) docker     (122)     4922 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_samples
--rw-r--r--   0 runner    (1001) docker     (122)     2241 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_skymap
--rw-r--r--   0 runner    (1001) docker     (122)     3283 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_plot_thermodynamic_integrand
--rw-r--r--   0 runner    (1001) docker     (122)     5039 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_pp_table_summary
--rw-r--r--   0 runner    (1001) docker     (122)     1407 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_start_from_samples
--rw-r--r--   0 runner    (1001) docker     (122)     6460 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_inference_table_summary
--rw-r--r--   0 runner    (1001) docker     (122)     2360 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/pycbc_validate_test_posterior
--rw-r--r--   0 runner    (1001) docker     (122)       80 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/inference/run_pycbc_inference
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/live/
--rw-r--r--   0 runner    (1001) docker     (122)     7349 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/live/pycbc_live_combine_single_fits
--rw-r--r--   0 runner    (1001) docker     (122)     7531 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/live/pycbc_live_plot_combined_single_fits
--rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/live/pycbc_live_plot_single_trigger_fits
--rw-r--r--   0 runner    (1001) docker     (122)    17164 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/live/pycbc_live_single_trigger_fits
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/minifollowups/
--rw-r--r--   0 runner    (1001) docker     (122)    10521 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_foreground_minifollowup
--rw-r--r--   0 runner    (1001) docker     (122)    13661 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_injection_minifollowup
--rw-r--r--   0 runner    (1001) docker     (122)     8520 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_page_coincinfo
--rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_page_injinfo
--rw-r--r--   0 runner    (1001) docker     (122)     8423 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_page_snglinfo
--rw-r--r--   0 runner    (1001) docker     (122)     2389 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_plot_chigram
--rw-r--r--   0 runner    (1001) docker     (122)     4714 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_plot_trigger_timeseries
--rw-r--r--   0 runner    (1001) docker     (122)     3928 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_single_template_plot
--rw-r--r--   0 runner    (1001) docker     (122)     9897 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/minifollowups/pycbc_sngl_minifollowup
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/plotting/
--rw-r--r--   0 runner    (1001) docker     (122)     5543 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_banksim_plot_eff_fitting_factor
--rw-r--r--   0 runner    (1001) docker     (122)     5769 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_banksim_plot_fitting_factors
--rw-r--r--   0 runner    (1001) docker     (122)     4824 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_banksim_table_point_injs
--rw-r--r--   0 runner    (1001) docker     (122)     1668 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_create_html_snippet
--rwxr-xr-x   0 runner    (1001) docker     (122)     5359 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_faithsim_plots
--rw-r--r--   0 runner    (1001) docker     (122)    10855 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_ifar_catalog
--rw-r--r--   0 runner    (1001) docker     (122)     3604 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_mass_area_plot
--rw-r--r--   0 runner    (1001) docker     (122)     2638 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_mchirp_plots
--rw-r--r--   0 runner    (1001) docker     (122)     1598 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_banktriggerrate
--rw-r--r--   0 runner    (1001) docker     (122)     7937 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_coinc_snrchi
--rwxr-xr-x   0 runner    (1001) docker     (122)     8513 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_foreground
--rw-r--r--   0 runner    (1001) docker     (122)    10669 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_foundmissed
--rw-r--r--   0 runner    (1001) docker     (122)    13695 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_ifar
--rw-r--r--   0 runner    (1001) docker     (122)     5291 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_injtable
--rw-r--r--   0 runner    (1001) docker     (122)     9894 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_recovery
--rw-r--r--   0 runner    (1001) docker     (122)     3872 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_segments
--rw-r--r--   0 runner    (1001) docker     (122)     7449 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_segplot
--rw-r--r--   0 runner    (1001) docker     (122)     7188 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_segtable
--rwxr-xr-x   0 runner    (1001) docker     (122)    16396 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_sensitivity
--rw-r--r--   0 runner    (1001) docker     (122)     3263 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_snrchi
--rw-r--r--   0 runner    (1001) docker     (122)    15425 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_snrifar
--rwxr-xr-x   0 runner    (1001) docker     (122)    10377 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_snrratehist
--rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_page_vetotable
--rw-r--r--   0 runner    (1001) docker     (122)     2147 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_background_coincs
--rw-r--r--   0 runner    (1001) docker     (122)     5207 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_bank_bins
--rw-r--r--   0 runner    (1001) docker     (122)     2393 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_dq_likelihood_vs_time
--rw-r--r--   0 runner    (1001) docker     (122)     2768 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_dq_percentiles
--rwxr-xr-x   0 runner    (1001) docker     (122)     8101 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_gate_triggers
--rw-r--r--   0 runner    (1001) docker     (122)     3683 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_gating
--rw-r--r--   0 runner    (1001) docker     (122)     5504 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_hist
--rwxr-xr-x   0 runner    (1001) docker     (122)    11670 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_multiifo_dtphase
--rw-r--r--   0 runner    (1001) docker     (122)     5972 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_psd_file
--rw-r--r--   0 runner    (1001) docker     (122)     5428 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_psd_timefreq
--rw-r--r--   0 runner    (1001) docker     (122)    11089 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_qscan
--rw-r--r--   0 runner    (1001) docker     (122)     3339 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_range
--rw-r--r--   0 runner    (1001) docker     (122)     3544 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_range_vs_mtot
--rw-r--r--   0 runner    (1001) docker     (122)    10339 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_singles_timefreq
--rw-r--r--   0 runner    (1001) docker     (122)     6713 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_singles_vs_params
--rwxr-xr-x   0 runner    (1001) docker     (122)     2937 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_throughput
--rw-r--r--   0 runner    (1001) docker     (122)    12162 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_trigrate
--rw-r--r--   0 runner    (1001) docker     (122)     5888 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_vt_ratio
--rw-r--r--   0 runner    (1001) docker     (122)     8450 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/plotting/pycbc_plot_waveform
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/population/
--rw-r--r--   0 runner    (1001) docker     (122)    16341 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/population/pycbc_multiifo_pastro
--rw-r--r--   0 runner    (1001) docker     (122)     4752 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/population/pycbc_population_plots
--rw-r--r--   0 runner    (1001) docker     (122)     7417 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/population/pycbc_population_rates
--rw-r--r--   0 runner    (1001) docker     (122)    19520 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_banksim
--rw-r--r--   0 runner    (1001) docker     (122)     2569 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_banksim_combine_banks
--rw-r--r--   0 runner    (1001) docker     (122)     7343 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_banksim_match_combine
--rw-r--r--   0 runner    (1001) docker     (122)    22157 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_banksim_skymax
--rw-r--r--   0 runner    (1001) docker     (122)     7820 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_coinc_time
--rwxr-xr-x   0 runner    (1001) docker     (122)     9082 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_compress_bank
--rw-r--r--   0 runner    (1001) docker     (122)     5294 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_condition_strain
--rw-r--r--   0 runner    (1001) docker     (122)     3104 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_copy_output_map
--rw-r--r--   0 runner    (1001) docker     (122)    11438 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_create_injections
--rwxr-xr-x   0 runner    (1001) docker     (122)     1962 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_data_store
--rw-r--r--   0 runner    (1001) docker     (122)    11094 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_faithsim
--rwxr-xr-x   0 runner    (1001) docker     (122)     2395 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_faithsim_collect_results
--rw-r--r--   0 runner    (1001) docker     (122)    17380 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_fit_sngl_trigs
--rw-r--r--   0 runner    (1001) docker     (122)     9008 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_get_ffinal
--rwxr-xr-x   0 runner    (1001) docker     (122)     5952 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_hdf5_splitbank
--rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_hdf_splitinj
--rw-r--r--   0 runner    (1001) docker     (122)     4905 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_inj_cut
--rw-r--r--   0 runner    (1001) docker     (122)     2637 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_inspinj2hdf
--rw-r--r--   0 runner    (1001) docker     (122)    24465 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_inspiral
--rwxr-xr-x   0 runner    (1001) docker     (122)    18972 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_inspiral_skymax
--rwxr-xr-x   0 runner    (1001) docker     (122)    54408 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_live
--rw-r--r--   0 runner    (1001) docker     (122)     2209 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_live_nagios_monitor
--rw-r--r--   0 runner    (1001) docker     (122)     4998 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_losc_segment_query
--rw-r--r--   0 runner    (1001) docker     (122)    17345 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_make_banksim
--rw-r--r--   0 runner    (1001) docker     (122)    12546 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_make_faithsim
--rw-r--r--   0 runner    (1001) docker     (122)    11717 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_make_html_page
--rwxr-xr-x   0 runner    (1001) docker     (122)    25365 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_make_skymap
--rwxr-xr-x   0 runner    (1001) docker     (122)     3782 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_merge_inj_hdf
--rwxr-xr-x   0 runner    (1001) docker     (122)    30868 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_multi_inspiral
--rw-r--r--   0 runner    (1001) docker     (122)    15327 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_optimal_snr
--rwxr-xr-x   0 runner    (1001) docker     (122)    19360 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_optimize_snr
--rw-r--r--   0 runner    (1001) docker     (122)     5790 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_process_sngls
--rw-r--r--   0 runner    (1001) docker     (122)     9080 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_randomize_inj_dist_by_optsnr
--rwxr-xr-x   0 runner    (1001) docker     (122)    20946 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_single_template
--rwxr-xr-x   0 runner    (1001) docker     (122)     3992 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_source_probability_offline
--rw-r--r--   0 runner    (1001) docker     (122)     2149 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_split_inspinj
--rw-r--r--   0 runner    (1001) docker     (122)     6922 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_splitbank
--rw-r--r--   0 runner    (1001) docker     (122)     2740 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_stageout_failed_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    10902 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_submit_dax
--rwxr-xr-x   0 runner    (1001) docker     (122)    11868 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pycbc_upload_xml_to_gracedb
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/pygrb/
--rw-r--r--   0 runner    (1001) docker     (122)     7925 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_grb_inj_combiner
--rw-r--r--   0 runner    (1001) docker     (122)     9065 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_grb_inj_finder
--rw-r--r--   0 runner    (1001) docker     (122)     7661 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_grb_trig_cluster
--rw-r--r--   0 runner    (1001) docker     (122)    15754 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_grb_trig_combiner
--rw-r--r--   0 runner    (1001) docker     (122)    20351 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_make_offline_grb_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    27308 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_efficiency
--rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_grb_info_table
--rw-r--r--   0 runner    (1001) docker     (122)     8657 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_minifollowups
--rw-r--r--   0 runner    (1001) docker     (122)    37060 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_page_tables
--rw-r--r--   0 runner    (1001) docker     (122)    10830 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_chisq_veto
--rw-r--r--   0 runner    (1001) docker     (122)    10435 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_coh_ifosnr
--rw-r--r--   0 runner    (1001) docker     (122)    21948 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_injs_results
--rw-r--r--   0 runner    (1001) docker     (122)     8687 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_null_stats
--rw-r--r--   0 runner    (1001) docker     (122)     3831 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_skygrid
--rw-r--r--   0 runner    (1001) docker     (122)     6795 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_snr_timeseries
--rw-r--r--   0 runner    (1001) docker     (122)     8174 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_stats_distribution
--rw-r--r--   0 runner    (1001) docker     (122)    30510 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_pp_workflow
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/workflow_comparisons/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/workflow_comparisons/offline_search/
--rwxr-xr-x   0 runner    (1001) docker     (122)    12151 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_combine_injection_comparisons
--rwxr-xr-x   0 runner    (1001) docker     (122)    21916 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_injection_set_comparison
--rwxr-xr-x   0 runner    (1001) docker     (122)     2768 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_plot_injections_found_both_workflows
--rwxr-xr-x   0 runner    (1001) docker     (122)     3072 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_plot_injections_missed_one_workflow
--rwxr-xr-x   0 runner    (1001) docker     (122)     4595 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_plot_vt_ratio_vs_ifar
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/bin/workflows/
--rw-r--r--   0 runner    (1001) docker     (122)    16320 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_bank_verifier_workflow
--rwxr-xr-x   0 runner    (1001) docker     (122)    36669 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_coinc_search_workflow
--rwxr-xr-x   0 runner    (1001) docker     (122)     4599 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_faithsim_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    15794 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_inference_inj_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    10717 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_inference_plots_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    12772 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_inference_workflow
--rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_psd_estimation_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    16316 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_sbank_workflow
--rw-r--r--   0 runner    (1001) docker     (122)    12259 2023-03-09 21:19:31.000000 PyCBC-2.2.0/bin/workflows/pycbc_make_uberbank_workflow
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/banksim/
--rw-r--r--   0 runner    (1001) docker     (122)    21402 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/banksim/injection0.xml
--rw-r--r--   0 runner    (1001) docker     (122)      461 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/banksim/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/cal/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/
--rw-r--r--   0 runner    (1001) docker     (122)     6703 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/pycbc_check_esd_saturation.sh
--rw-r--r--   0 runner    (1001) docker     (122)     4460 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/pycbc_check_pcal_saturation.sh
--rw-r--r--   0 runner    (1001) docker     (122)     2582 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/run_pcal_saturation_example.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/catalog/
--rw-r--r--   0 runner    (1001) docker     (122)      716 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/catalog/data.py
--rw-r--r--   0 runner    (1001) docker     (122)      339 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/catalog/stat.py
--rw-r--r--   0 runner    (1001) docker     (122)      184 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/catalog/what.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/dataquality/
--rw-r--r--   0 runner    (1001) docker     (122)      504 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/dataquality/hwinj.py
--rw-r--r--   0 runner    (1001) docker     (122)      709 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/dataquality/on.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/detector/
--rw-r--r--   0 runner    (1001) docker     (122)      997 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/detector/ant.py
--rw-r--r--   0 runner    (1001) docker     (122)      785 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/detector/delay.py
--rw-r--r--   0 runner    (1001) docker     (122)      575 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/detector/loc.py
--rw-r--r--   0 runner    (1001) docker     (122)      259 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/detector/travel.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/distributions/
--rw-r--r--   0 runner    (1001) docker     (122)      100 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/distributions/list_distributions.py
--rw-r--r--   0 runner    (1001) docker     (122)     1651 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/distributions/mass_examples.py
--rw-r--r--   0 runner    (1001) docker     (122)     1986 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/distributions/mchirp_q_from_uniform_m1m2_example.py
--rw-r--r--   0 runner    (1001) docker     (122)     1236 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/distributions/sampling_from_config_example.py
--rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/distributions/spin_examples.py
--rw-r--r--   0 runner    (1001) docker     (122)     1933 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/distributions/spin_spatial_distr_example.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/faith/
--rw-r--r--   0 runner    (1001) docker     (122)    21402 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/faith/injection0.xml
--rw-r--r--   0 runner    (1001) docker     (122)      182 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/faith/pegasus_workflow_create.sh
--rw-r--r--   0 runner    (1001) docker     (122)       28 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/faith/pegasus_workflow_submit.sh
--rw-r--r--   0 runner    (1001) docker     (122)      336 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/faith/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/filter/
--rw-r--r--   0 runner    (1001) docker     (122)     1198 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/filter/chisq.py
--rw-r--r--   0 runner    (1001) docker     (122)      624 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/filter/fir.py
--rw-r--r--   0 runner    (1001) docker     (122)      795 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/filter/pass.py
--rw-r--r--   0 runner    (1001) docker     (122)      949 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/filter/snr.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/gw150914/
--rw-r--r--   0 runner    (1001) docker     (122)     1159 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/gw150914/audio.py
--rw-r--r--   0 runner    (1001) docker     (122)     1106 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/gw150914/gw150914_h1_snr.py
--rw-r--r--   0 runner    (1001) docker     (122)      934 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/gw150914/gw150914_shape.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/analytic-normal2d/
--rwxr-xr-x   0 runner    (1001) docker     (122)      379 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/analytic-normal2d/make_movie.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/analytic-normal2d/plot.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      174 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/analytic-normal2d/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/bbh-injection/
--rwxr-xr-x   0 runner    (1001) docker     (122)      316 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/bbh-injection/make_injection.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      649 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/bbh-injection/run.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      967 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/bbh-injection/run_test.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/gw150914/
--rwxr-xr-x   0 runner    (1001) docker     (122)      582 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/gw150914/plot.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      656 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/gw150914/run.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)     1149 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/gw150914/run_test.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/hierarchical/
--rwxr-xr-x   0 runner    (1001) docker     (122)      269 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/hierarchical/make_injections.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      230 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/hierarchical/run.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      565 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/hierarchical/run_test.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/lisa_smbhb/
--rw-r--r--   0 runner    (1001) docker     (122)     2421 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/lisa_smbhb/advanced_plot.py
--rw-r--r--   0 runner    (1001) docker     (122)      522 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/lisa_smbhb/get.sh
--rw-r--r--   0 runner    (1001) docker     (122)      501 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/lisa_smbhb/plot.sh
--rw-r--r--   0 runner    (1001) docker     (122)      133 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/lisa_smbhb/run.sh
--rw-r--r--   0 runner    (1001) docker     (122)      183 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/list_parameters.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/margtime/
--rw-r--r--   0 runner    (1001) docker     (122)      268 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/margtime/get.sh
--rw-r--r--   0 runner    (1001) docker     (122)      735 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/margtime/run.sh
--rw-r--r--   0 runner    (1001) docker     (122)      934 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/margtime/run_inj.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/multisignal/
--rw-r--r--   0 runner    (1001) docker     (122)      212 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/multisignal/get.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      369 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/multisignal/make_injections.sh
--rw-r--r--   0 runner    (1001) docker     (122)      129 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/multisignal/rel.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      151 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/multisignal/run.sh
--rw-r--r--   0 runner    (1001) docker     (122)      135 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/multisignal/single.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/relative/
--rw-r--r--   0 runner    (1001) docker     (122)      212 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/relative/get.sh
--rw-r--r--   0 runner    (1001) docker     (122)      102 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/relative/plot.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      139 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/relative/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/samplers/
--rwxr-xr-x   0 runner    (1001) docker     (122)      572 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/samplers/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inference/single/
--rw-r--r--   0 runner    (1001) docker     (122)      212 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/single/get.sh
--rw-r--r--   0 runner    (1001) docker     (122)       98 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/single/plot.sh
--rw-r--r--   0 runner    (1001) docker     (122)      142 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/single/run.sh
--rw-r--r--   0 runner    (1001) docker     (122)      364 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/single/run_instant.sh
--rw-r--r--   0 runner    (1001) docker     (122)      805 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inference/single/run_marg.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/inspiral/
--rw-r--r--   0 runner    (1001) docker     (122)     1207 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inspiral/check_GW150914_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)      118 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inspiral/clean.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)     3586 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/inspiral/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/live/
--rwxr-xr-x   0 runner    (1001) docker     (122)     7946 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/live/check_results.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1508 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/live/generate_injections.py
--rw-r--r--   0 runner    (1001) docker     (122)     2238 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/live/make_singles_fits_file.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6584 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/live/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/make_skymap/
--rw-r--r--   0 runner    (1001) docker     (122)      255 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/make_skymap/GW150914.sh
--rw-r--r--   0 runner    (1001) docker     (122)      553 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/make_skymap/GW170817.sh
--rw-r--r--   0 runner    (1001) docker     (122)      855 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/make_skymap/simulated_data.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/multi_inspiral/
--rw-r--r--   0 runner    (1001) docker     (122)     1158 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/multi_inspiral/check_faceon_faceaway_trigs.py
--rw-r--r--   0 runner    (1001) docker     (122)      957 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/multi_inspiral/check_gw170817_trigs.py
--rw-r--r--   0 runner    (1001) docker     (122)       37 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/multi_inspiral/clean.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)     2736 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/multi_inspiral/faceon_faceaway.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)     2530 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/multi_inspiral/run.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/noise/
--rw-r--r--   0 runner    (1001) docker     (122)      668 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/noise/frequency.py
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/noise/timeseries.py
--rw-r--r--   0 runner    (1001) docker     (122)     1520 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/overlap.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/psd/
--rw-r--r--   0 runner    (1001) docker     (122)      576 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/psd/analytic.py
--rw-r--r--   0 runner    (1001) docker     (122)      884 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/psd/estimate.py
--rw-r--r--   0 runner    (1001) docker     (122)      742 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/psd/read.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/search/
--rw-r--r--   0 runner    (1001) docker     (122)      408 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/bank.sh
--rw-r--r--   0 runner    (1001) docker     (122)     1136 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/check_job.py
--rw-r--r--   0 runner    (1001) docker     (122)      236 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/gen.sh
--rw-r--r--   0 runner    (1001) docker     (122)      302 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/get.sh
--rw-r--r--   0 runner    (1001) docker     (122)      154 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/master.sh
--rw-r--r--   0 runner    (1001) docker     (122)      759 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/stats.sh
--rw-r--r--   0 runner    (1001) docker     (122)       80 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/search/submit.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/tmpltbank/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/tmpltbank/bank_workflow_test/
--rw-r--r--   0 runner    (1001) docker     (122)      114 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/bank_workflow_test/gen.sh
--rw-r--r--   0 runner    (1001) docker     (122)      190 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/cleanOutput.sh
--rw-r--r--   0 runner    (1001) docker     (122)      273 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/make_cache.sh
--rw-r--r--   0 runner    (1001) docker     (122)      527 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testAligned.sh
--rw-r--r--   0 runner    (1001) docker     (122)      888 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testAligned2.sh
--rw-r--r--   0 runner    (1001) docker     (122)      547 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testAligned3.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      649 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testNonspin.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      495 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testNonspin2.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      327 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testNonspin3.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      675 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testStoch.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      449 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testStoch2.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      355 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testStoch3.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      531 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/tmpltbank/testStoch4.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/waveform/
--rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/add_waveform.py
--rw-r--r--   0 runner    (1001) docker     (122)     1045 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/match_waveform.py
--rw-r--r--   0 runner    (1001) docker     (122)     1500 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/plot_detwaveform.py
--rw-r--r--   0 runner    (1001) docker     (122)      997 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/plot_fd_td.py
--rw-r--r--   0 runner    (1001) docker     (122)      700 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/plot_freq.py
--rw-r--r--   0 runner    (1001) docker     (122)      657 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/plot_phase.py
--rw-r--r--   0 runner    (1001) docker     (122)      498 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/plot_waveform.py
--rw-r--r--   0 runner    (1001) docker     (122)      426 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/waveform/what_waveform.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/data_checker/
--rwxr-xr-x   0 runner    (1001) docker     (122)     6801 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/data_checker/daily_test.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2162 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/data_checker/get_data_example.py
--rw-r--r--   0 runner    (1001) docker     (122)      130 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/data_checker/run_daily_test.sh
--rw-r--r--   0 runner    (1001) docker     (122)      130 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/data_checker/run_get_data_example.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/dayhopecheck/
--rwxr-xr-x   0 runner    (1001) docker     (122)     7198 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/dayhopecheck/dayhopecheck.py
--rw-r--r--   0 runner    (1001) docker     (122)      146 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/dayhopecheck/run_dayhopecheck.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/generic/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/generic/multilevel_subworkflow_data/
--rw-r--r--   0 runner    (1001) docker     (122)       90 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/generic/multilevel_subworkflow_data/run.sh
--rw-r--r--   0 runner    (1001) docker     (122)     1914 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/generic/multilevel_subworkflow_data/simple.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/generic/simple_subworkflow_data/
--rw-r--r--   0 runner    (1001) docker     (122)       77 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/generic/simple_subworkflow_data/run.sh
--rw-r--r--   0 runner    (1001) docker     (122)     1914 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/generic/simple_subworkflow_data/simple.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/inference/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/inference/bbh_inj-dynesty/
--rw-r--r--   0 runner    (1001) docker     (122)      610 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/inference/bbh_inj-dynesty/create_inj_workflow.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/inference/gw150914_gw170814-dynesty/
--rwxr-xr-x   0 runner    (1001) docker     (122)      486 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/inference/gw150914_gw170814-dynesty/create_workflow.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/inference/gw150914_gw170814-emcee_pt/
--rwxr-xr-x   0 runner    (1001) docker     (122)      480 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/inference/gw150914_gw170814-emcee_pt/create_workflow.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/inference/small_test/
--rw-r--r--   0 runner    (1001) docker     (122)      496 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/inference/small_test/gen.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/pygrb/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/pygrb/ER7/
--rwxr-xr-x   0 runner    (1001) docker     (122)      739 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/pygrb/ER7/run_er7_pygrb_offline.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/pygrb/ER8/
--rwxr-xr-x   0 runner    (1001) docker     (122)      825 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/pygrb/ER8/run_er8_pygrb_offline.sh
--rwxr-xr-x   0 runner    (1001) docker     (122)      822 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/pygrb/ER8/run_er8_pygrb_online.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/examples/workflow/pygrb/S6/
--rwxr-xr-x   0 runner    (1001) docker     (122)      866 2023-03-09 21:19:31.000000 PyCBC-2.2.0/examples/workflow/pygrb/S6/run_s6_pygrb.sh
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/
--rw-r--r--   0 runner    (1001) docker     (122)     6207 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3219 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/_version.py
--rw-r--r--   0 runner    (1001) docker     (122)     6380 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/_version_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)    21555 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/bin_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    15026 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/boundaries.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/catalog/
--rw-r--r--   0 runner    (1001) docker     (122)     6843 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/catalog/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2488 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/catalog/catalog.py
--rw-r--r--   0 runner    (1001) docker     (122)    56098 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/conversions.py
--rw-r--r--   0 runner    (1001) docker     (122)     4141 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/coordinates.py
--rw-r--r--   0 runner    (1001) docker     (122)    23038 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/cosmology.py
--rw-r--r--   0 runner    (1001) docker     (122)    28467 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/detector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/distributions/
--rw-r--r--   0 runner    (1001) docker     (122)     9279 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    19645 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/angular.py
--rw-r--r--   0 runner    (1001) docker     (122)    13126 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/arbitrary.py
--rw-r--r--   0 runner    (1001) docker     (122)    13606 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/bounded.py
--rw-r--r--   0 runner    (1001) docker     (122)     5056 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/constraints.py
--rw-r--r--   0 runner    (1001) docker     (122)     8901 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/external.py
--rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/fixedsamples.py
--rw-r--r--   0 runner    (1001) docker     (122)     9342 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (122)    14330 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/joint.py
--rw-r--r--   0 runner    (1001) docker     (122)    10257 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/mass.py
--rw-r--r--   0 runner    (1001) docker     (122)     7732 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/power_law.py
--rw-r--r--   0 runner    (1001) docker     (122)    10564 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/qnm.py
--rw-r--r--   0 runner    (1001) docker     (122)     1464 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/sky_location.py
--rw-r--r--   0 runner    (1001) docker     (122)    12002 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/spins.py
--rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/uniform.py
--rw-r--r--   0 runner    (1001) docker     (122)     2814 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/uniform_log.py
--rw-r--r--   0 runner    (1001) docker     (122)     4617 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/distributions/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    16663 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/dq.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/events/
--rw-r--r--   0 runner    (1001) docker     (122)      128 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13726 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/coherent.py
--rw-r--r--   0 runner    (1001) docker     (122)    52673 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/coinc.py
--rw-r--r--   0 runner    (1001) docker     (122)     6808 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/coinc_rate.py
--rw-r--r--   0 runner    (1001) docker     (122)    15820 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/cuts.py
--rw-r--r--   0 runner    (1001) docker     (122)    49942 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/eventmgr.py
--rw-r--r--   0 runner    (1001) docker     (122)     7374 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/eventmgr_cython.pyx
--rw-r--r--   0 runner    (1001) docker     (122)    10863 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)    13205 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/significance.py
--rw-r--r--   0 runner    (1001) docker     (122)    13294 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/simd_threshold_ccode.cpp
--rw-r--r--   0 runner    (1001) docker     (122)     3258 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/simd_threshold_cython.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     7717 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/single.py
--rw-r--r--   0 runner    (1001) docker     (122)    84395 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/stat.py
--rw-r--r--   0 runner    (1001) docker     (122)     3511 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/threshold_cpu.py
--rw-r--r--   0 runner    (1001) docker     (122)     8580 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/threshold_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)     9465 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/trigger_fits.py
--rw-r--r--   0 runner    (1001) docker     (122)     8750 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/triggers.py
--rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/events/veto.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/fft/
--rw-r--r--   0 runner    (1001) docker     (122)      952 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1283 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/backend_cpu.py
--rw-r--r--   0 runner    (1001) docker     (122)     1315 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/backend_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)     1197 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/backend_mkl.py
--rw-r--r--   0 runner    (1001) docker     (122)     3126 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/backend_support.py
--rw-r--r--   0 runner    (1001) docker     (122)     3391 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/class_api.py
--rw-r--r--   0 runner    (1001) docker     (122)    11855 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/core.py
--rw-r--r--   0 runner    (1001) docker     (122)     2283 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/cuda_pyfft.py
--rw-r--r--   0 runner    (1001) docker     (122)     3710 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/cufft.py
--rw-r--r--   0 runner    (1001) docker     (122)     8478 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/fft_callback.py
--rw-r--r--   0 runner    (1001) docker     (122)    22514 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/fftw.py
--rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/fftw_pruned.py
--rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/fftw_pruned_cython.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     3470 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/func_api.py
--rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/mkl.py
--rw-r--r--   0 runner    (1001) docker     (122)     3765 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/npfft.py
--rw-r--r--   0 runner    (1001) docker     (122)     4879 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/fft/parser_support.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/filter/
--rw-r--r--   0 runner    (1001) docker     (122)       53 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5472 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/autocorrelation.py
--rw-r--r--   0 runner    (1001) docker     (122)     3609 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/fotonfilter.py
--rw-r--r--   0 runner    (1001) docker     (122)    82568 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/matchedfilter.py
--rw-r--r--   0 runner    (1001) docker     (122)     2810 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/matchedfilter_cpu.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     2147 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/matchedfilter_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)      830 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/matchedfilter_numpy.py
--rw-r--r--   0 runner    (1001) docker     (122)     7048 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/qtransform.py
--rw-r--r--   0 runner    (1001) docker     (122)    15735 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/resample.py
--rw-r--r--   0 runner    (1001) docker     (122)     3116 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/simd_correlate.py
--rw-r--r--   0 runner    (1001) docker     (122)    15271 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/simd_correlate_ccode.cpp
--rw-r--r--   0 runner    (1001) docker     (122)     2347 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/simd_correlate_cython.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     3947 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/filter/zpk.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/frame/
--rw-r--r--   0 runner    (1001) docker     (122)     1386 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/frame/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    34138 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/frame/frame.py
--rw-r--r--   0 runner    (1001) docker     (122)     5736 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/frame/losc.py
--rw-r--r--   0 runner    (1001) docker     (122)     2303 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/frame/store.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/inference/
--rw-r--r--   0 runner    (1001) docker     (122)      137 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    30844 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/burn_in.py
--rw-r--r--   0 runner    (1001) docker     (122)     8796 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/entropy.py
--rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/evidence.py
--rw-r--r--   0 runner    (1001) docker     (122)     6652 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/gelman_rubin.py
--rw-r--r--   0 runner    (1001) docker     (122)     2802 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/geweke.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/inference/io/
--rw-r--r--   0 runner    (1001) docker     (122)    32158 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    38576 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/base_hdf.py
--rw-r--r--   0 runner    (1001) docker     (122)    36040 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/base_mcmc.py
--rw-r--r--   0 runner    (1001) docker     (122)    16944 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/base_multitemper.py
--rw-r--r--   0 runner    (1001) docker     (122)     2918 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/base_nested_sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     5298 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/base_sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     1205 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/cpnest.py
--rw-r--r--   0 runner    (1001) docker     (122)     7473 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/dynesty.py
--rw-r--r--   0 runner    (1001) docker     (122)     3848 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/emcee.py
--rw-r--r--   0 runner    (1001) docker     (122)     4859 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/emcee_pt.py
--rw-r--r--   0 runner    (1001) docker     (122)    11226 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/epsie.py
--rw-r--r--   0 runner    (1001) docker     (122)     5382 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/multinest.py
--rw-r--r--   0 runner    (1001) docker     (122)     3653 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/posterior.py
--rw-r--r--   0 runner    (1001) docker     (122)     5947 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/ptemcee.py
--rw-r--r--   0 runner    (1001) docker     (122)     2421 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/txt.py
--rw-r--r--   0 runner    (1001) docker     (122)     1226 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/io/ultranest.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/inference/jump/
--rw-r--r--   0 runner    (1001) docker     (122)     3211 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/jump/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4775 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/jump/angular.py
--rw-r--r--   0 runner    (1001) docker     (122)     4652 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/jump/bounded_normal.py
--rw-r--r--   0 runner    (1001) docker     (122)     5794 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/jump/discrete.py
--rw-r--r--   0 runner    (1001) docker     (122)    20231 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/jump/normal.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/inference/models/
--rw-r--r--   0 runner    (1001) docker     (122)    11258 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8703 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/analytic.py
--rw-r--r--   0 runner    (1001) docker     (122)    33142 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5697 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/base_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     7614 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/brute_marg.py
--rw-r--r--   0 runner    (1001) docker     (122)    23677 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    31219 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/gated_gaussian_noise.py
--rw-r--r--   0 runner    (1001) docker     (122)    49252 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/gaussian_noise.py
--rw-r--r--   0 runner    (1001) docker     (122)    24411 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/hierarchical.py
--rw-r--r--   0 runner    (1001) docker     (122)    32990 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/marginalized_gaussian_noise.py
--rw-r--r--   0 runner    (1001) docker     (122)    22631 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/relbin.py
--rw-r--r--   0 runner    (1001) docker     (122)     8733 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/relbin_cpu.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     8435 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/single_template.py
--rw-r--r--   0 runner    (1001) docker     (122)    33243 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/models/tools.py
--rw-r--r--   0 runner    (1001) docker     (122)    16324 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/option_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/inference/sampler/
--rw-r--r--   0 runner    (1001) docker     (122)     2813 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8201 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     3361 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/base_cube.py
--rw-r--r--   0 runner    (1001) docker     (122)    36376 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/base_mcmc.py
--rw-r--r--   0 runner    (1001) docker     (122)    16327 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/base_multitemper.py
--rw-r--r--   0 runner    (1001) docker     (122)     8274 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/cpnest.py
--rw-r--r--   0 runner    (1001) docker     (122)     2594 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/dummy.py
--rw-r--r--   0 runner    (1001) docker     (122)    23999 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/dynesty.py
--rw-r--r--   0 runner    (1001) docker     (122)     9506 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/emcee.py
--rw-r--r--   0 runner    (1001) docker     (122)    18909 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/emcee_pt.py
--rw-r--r--   0 runner    (1001) docker     (122)    19831 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/epsie.py
--rw-r--r--   0 runner    (1001) docker     (122)    15427 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/multinest.py
--rw-r--r--   0 runner    (1001) docker     (122)    27259 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/ptemcee.py
--rw-r--r--   0 runner    (1001) docker     (122)     7359 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inference/sampler/ultranest.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/inject/
--rw-r--r--   0 runner    (1001) docker     (122)       79 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inject/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    48285 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inject/inject.py
--rw-r--r--   0 runner    (1001) docker     (122)    19409 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/inject/injfilterrejector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/io/
--rw-r--r--   0 runner    (1001) docker     (122)      621 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    55604 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/io/hdf.py
--rw-r--r--   0 runner    (1001) docker     (122)    12494 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/io/ligolw.py
--rw-r--r--   0 runner    (1001) docker     (122)    24724 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/io/live.py
--rw-r--r--   0 runner    (1001) docker     (122)    77607 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/io/record.py
--rw-r--r--   0 runner    (1001) docker     (122)     9494 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/libutils.py
--rw-r--r--   0 runner    (1001) docker     (122)    13070 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/mchirp_area.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/neutron_stars/
--rw-r--r--   0 runner    (1001) docker     (122)      373 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/neutron_stars/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6861 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/neutron_stars/eos_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/neutron_stars/ns_data/
--rw-r--r--   0 runner    (1001) docker     (122)    53263 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/neutron_stars/ns_data/equil_2H.dat
--rw-r--r--   0 runner    (1001) docker     (122)    11046 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/neutron_stars/pg_isso_solver.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/noise/
--rw-r--r--   0 runner    (1001) docker     (122)       89 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/noise/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5192 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/noise/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (122)     8316 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/noise/reproduceable.py
--rw-r--r--   0 runner    (1001) docker     (122)     5385 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/opt.py
--rw-r--r--   0 runner    (1001) docker     (122)    39910 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/pnutils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5462 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/pool.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/population/
--rw-r--r--   0 runner    (1001) docker     (122)      327 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    30477 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/fgmc_functions.py
--rw-r--r--   0 runner    (1001) docker     (122)     6246 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/fgmc_laguerre.py
--rw-r--r--   0 runner    (1001) docker     (122)    10012 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/fgmc_plots.py
--rw-r--r--   0 runner    (1001) docker     (122)     9061 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/live_pastro.py
--rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/live_pastro_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    16559 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/population_models.py
--rw-r--r--   0 runner    (1001) docker     (122)    13826 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/rates_functions.py
--rw-r--r--   0 runner    (1001) docker     (122)    16956 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/population/scale_injections.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/psd/
--rw-r--r--   0 runner    (1001) docker     (122)    28796 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/psd/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6263 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/psd/analytical.py
--rw-r--r--   0 runner    (1001) docker     (122)    22380 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/psd/analytical_space.py
--rw-r--r--   0 runner    (1001) docker     (122)    12318 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/psd/estimate.py
--rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/psd/read.py
--rw-r--r--   0 runner    (1001) docker     (122)     8554 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/psd/variation.py
--rw-r--r--   0 runner    (1001) docker     (122)    12540 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/rate.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/
--rw-r--r--   0 runner    (1001) docker     (122)      423 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      703 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/color.py
--rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/dq.py
--rw-r--r--   0 runner    (1001) docker     (122)     5023 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/followup.py
--rw-r--r--   0 runner    (1001) docker     (122)     3937 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/layout.py
--rw-r--r--   0 runner    (1001) docker     (122)     4692 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/legacy_grb.py
--rw-r--r--   0 runner    (1001) docker     (122)     4769 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/metadata.py
--rw-r--r--   0 runner    (1001) docker     (122)     4810 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/mpld3_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1509 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/plot.py
--rw-r--r--   0 runner    (1001) docker     (122)     2081 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/psd.py
--rw-r--r--   0 runner    (1001) docker     (122)     5193 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/pygrb_plotting_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    38592 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/pygrb_postprocessing_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     7996 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/render.py
--rw-r--r--   0 runner    (1001) docker     (122)    33143 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/scatter_histograms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/css/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/css/bootstrap/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/css/bootstrap/3.3.2/
--rw-r--r--   0 runner    (1001) docker     (122)   117150 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/css/bootstrap/3.3.2/bootstrap.min.css
--rw-r--r--   0 runner    (1001) docker     (122)   117150 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/css/bootstrap.min.css
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/css/fancybox/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/css/fancybox/2.1.5/
--rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/css/fancybox/2.1.5/jquery.fancybox.css
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/css/pycbc/
--rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/css/pycbc/orange.css
--rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/css/pycbc/red.css
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/fonts/
--rw-r--r--   0 runner    (1001) docker     (122)      418 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.css
--rw-r--r--   0 runner    (1001) docker     (122)   258264 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.eot
--rw-r--r--   0 runner    (1001) docker     (122)      657 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.html
--rw-r--r--   0 runner    (1001) docker     (122)   658924 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.ttf
--rw-r--r--   0 runner    (1001) docker     (122)   312760 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.woff
--rw-r--r--   0 runner    (1001) docker     (122)      436 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.css
--rw-r--r--   0 runner    (1001) docker     (122)   270560 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.eot
--rw-r--r--   0 runner    (1001) docker     (122)      666 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.html
--rw-r--r--   0 runner    (1001) docker     (122)   702616 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.ttf
--rw-r--r--   0 runner    (1001) docker     (122)   326132 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.woff
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/bootstrap/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/bootstrap/3.3.2/
--rw-r--r--   0 runner    (1001) docker     (122)    35452 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/bootstrap/3.3.2/bootstrap.min.js
--rw-r--r--   0 runner    (1001) docker     (122)    35452 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/bootstrap.min.js
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/fancybox/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/
--rw-r--r--   0 runner    (1001) docker     (122)     3187 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/fancybox-orange.js
--rwxr-xr-x   0 runner    (1001) docker     (122)    48706 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.js
--rwxr-xr-x   0 runner    (1001) docker     (122)    23135 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.pack.js
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/jquery/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/static/js/jquery/1.10.2/
--rw-r--r--   0 runner    (1001) docker     (122)    93107 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/jquery/1.10.2/jquery-1.10.2.min.js
--rw-r--r--   0 runner    (1001) docker     (122)     1384 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/jquery/1.10.2/jquery.mousewheel-3.0.6.pack.js
--rw-r--r--   0 runner    (1001) docker     (122)    95786 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/static/js/jquery.min.js
--rw-r--r--   0 runner    (1001) docker     (122)     9027 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/str_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4698 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/table_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/templates/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/templates/files/
--rw-r--r--   0 runner    (1001) docker     (122)     1991 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/files/file_default.html
--rw-r--r--   0 runner    (1001) docker     (122)      855 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/files/file_glitchgram.html
--rw-r--r--   0 runner    (1001) docker     (122)      153 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/files/file_img.html
--rw-r--r--   0 runner    (1001) docker     (122)      629 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/files/file_pre.html
--rw-r--r--   0 runner    (1001) docker     (122)     1041 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/files/file_tmplt.html
--rw-r--r--   0 runner    (1001) docker     (122)    11740 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/orange.html
--rw-r--r--   0 runner    (1001) docker     (122)    11734 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/red.html
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/results/templates/wells/
--rw-r--r--   0 runner    (1001) docker     (122)      290 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/wells/sitemap.html
--rw-r--r--   0 runner    (1001) docker     (122)     3968 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/templates/wells/two_column.html
--rw-r--r--   0 runner    (1001) docker     (122)     7878 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/results/versioning.py
--rw-r--r--   0 runner    (1001) docker     (122)    10948 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/scheme.py
--rw-r--r--   0 runner    (1001) docker     (122)    11609 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/sensitivity.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/strain/
--rw-r--r--   0 runner    (1001) docker     (122)     1299 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/strain/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5742 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/strain/calibration.py
--rw-r--r--   0 runner    (1001) docker     (122)     6383 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/strain/gate.py
--rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/strain/lines.py
--rw-r--r--   0 runner    (1001) docker     (122)    18783 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/strain/recalibrate.py
--rw-r--r--   0 runner    (1001) docker     (122)    91091 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/strain/strain.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/tmpltbank/
--rw-r--r--   0 runner    (1001) docker     (122)      361 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/bank_conversions.py
--rw-r--r--   0 runner    (1001) docker     (122)    12504 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/bank_output_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    23515 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/brute_force_methods.py
--rw-r--r--   0 runner    (1001) docker     (122)    23569 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/calc_moments.py
--rw-r--r--   0 runner    (1001) docker     (122)    33431 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/coord_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    10726 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/lambda_mapping.py
--rw-r--r--   0 runner    (1001) docker     (122)     5472 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/lattice_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    57132 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/option_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    28137 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/tmpltbank/partitioned_bank.py
--rw-r--r--   0 runner    (1001) docker     (122)    84293 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/types/
--rw-r--r--   0 runner    (1001) docker     (122)      137 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2021 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/aligned.py
--rw-r--r--   0 runner    (1001) docker     (122)    39845 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/array.py
--rw-r--r--   0 runner    (1001) docker     (122)     5421 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/array_cpu.pyx
--rw-r--r--   0 runner    (1001) docker     (122)    11741 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/array_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)    26102 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/config.py
--rw-r--r--   0 runner    (1001) docker     (122)    25727 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/frequencyseries.py
--rw-r--r--   0 runner    (1001) docker     (122)    21934 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/optparse.py
--rw-r--r--   0 runner    (1001) docker     (122)    45520 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/types/timeseries.py
--rw-r--r--   0 runner    (1001) docker     (122)      743 2023-03-09 21:19:35.000000 PyCBC-2.2.0/pycbc/version.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/vetoes/
--rw-r--r--   0 runner    (1001) docker     (122)       72 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12524 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/autochisq.py
--rw-r--r--   0 runner    (1001) docker     (122)     9693 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/bank_chisq.py
--rw-r--r--   0 runner    (1001) docker     (122)    19668 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/chisq.py
--rw-r--r--   0 runner    (1001) docker     (122)     4964 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/chisq_cpu.pyx
--rw-r--r--   0 runner    (1001) docker     (122)    12354 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/chisq_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)     7224 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/vetoes/sgchisq.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/waveform/
--rw-r--r--   0 runner    (1001) docker     (122)    18361 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/SpinTaylorF2.py
--rw-r--r--   0 runner    (1001) docker     (122)      520 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    39507 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/bank.py
--rw-r--r--   0 runner    (1001) docker     (122)    30086 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/compress.py
--rw-r--r--   0 runner    (1001) docker     (122)     2155 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/decompress_cpu.py
--rw-r--r--   0 runner    (1001) docker     (122)     9811 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/decompress_cpu_ccode.cpp
--rw-r--r--   0 runner    (1001) docker     (122)     3580 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/decompress_cpu_cython.pyx
--rw-r--r--   0 runner    (1001) docker     (122)    11138 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/decompress_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)    51496 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/generator.py
--rw-r--r--   0 runner    (1001) docker     (122)     3724 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/multiband.py
--rw-r--r--   0 runner    (1001) docker     (122)     2683 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/nltides.py
--rw-r--r--   0 runner    (1001) docker     (122)    29135 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/parameters.py
--rw-r--r--   0 runner    (1001) docker     (122)     4171 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/plugin.py
--rw-r--r--   0 runner    (1001) docker     (122)     1282 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/premerger.py
--rw-r--r--   0 runner    (1001) docker     (122)    15018 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/pycbc_phenomC_tmplt.py
--rw-r--r--   0 runner    (1001) docker     (122)    54746 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/ringdown.py
--rw-r--r--   0 runner    (1001) docker     (122)     1285 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/sinegauss.py
--rw-r--r--   0 runner    (1001) docker     (122)     8829 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/spa_tmplt.py
--rw-r--r--   0 runner    (1001) docker     (122)     6584 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/spa_tmplt_cpu.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     3243 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/spa_tmplt_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)     1893 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/supernovae.py
--rw-r--r--   0 runner    (1001) docker     (122)    19760 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/utils_cpu.pyx
--rw-r--r--   0 runner    (1001) docker     (122)     3136 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/utils_cuda.py
--rw-r--r--   0 runner    (1001) docker     (122)    57072 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/waveform.py
--rw-r--r--   0 runner    (1001) docker     (122)    11590 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/waveform/waveform_modes.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/workflow/
--rw-r--r--   0 runner    (1001) docker     (122)     2023 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    26619 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/coincidence.py
--rw-r--r--   0 runner    (1001) docker     (122)    10939 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/configparser_test.py
--rw-r--r--   0 runner    (1001) docker     (122)    15824 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    88854 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/core.py
--rw-r--r--   0 runner    (1001) docker     (122)    50017 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/datafind.py
--rw-r--r--   0 runner    (1001) docker     (122)     9556 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/dq.py
--rw-r--r--   0 runner    (1001) docker     (122)    21408 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/grb_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    42899 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/inference_followups.py
--rw-r--r--   0 runner    (1001) docker     (122)    13587 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/injection.py
--rw-r--r--   0 runner    (1001) docker     (122)    53624 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/jobsetup.py
--rw-r--r--   0 runner    (1001) docker     (122)    13905 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/matched_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)    36429 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/minifollowups.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/pycbc/workflow/pegasus_files/
--rw-r--r--   0 runner    (1001) docker     (122)     2574 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/pegasus_files/pegasus-properties.conf
--rw-r--r--   0 runner    (1001) docker     (122)    11840 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/pegasus_sites.py
--rw-r--r--   0 runner    (1001) docker     (122)    32699 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/pegasus_workflow.py
--rw-r--r--   0 runner    (1001) docker     (122)    23393 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/plotting.py
--rw-r--r--   0 runner    (1001) docker     (122)     4824 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/psd.py
--rw-r--r--   0 runner    (1001) docker     (122)     5160 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/psdfiles.py
--rw-r--r--   0 runner    (1001) docker     (122)    19177 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/segment.py
--rw-r--r--   0 runner    (1001) docker     (122)     6964 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/splittable.py
--rw-r--r--   0 runner    (1001) docker     (122)    14333 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pycbc/workflow/tmpltbank.py
--rw-r--r--   0 runner    (1001) docker     (122)      618 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (122)       85 2023-03-09 21:19:31.000000 PyCBC-2.2.0/pytest.ini
--rw-r--r--   0 runner    (1001) docker     (122)       38 2023-03-09 21:19:37.000000 PyCBC-2.2.0/setup.cfg
--rwxr-xr-x   0 runner    (1001) docker     (122)    12377 2023-03-09 21:19:31.000000 PyCBC-2.2.0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/test/
--rw-r--r--   0 runner    (1001) docker     (122)     3650 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/bankvetotest.py
--rw-r--r--   0 runner    (1001) docker     (122)    37722 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/fft_base.py
--rw-r--r--   0 runner    (1001) docker     (122)    16620 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/lalsim.py
--rw-r--r--   0 runner    (1001) docker     (122)    21211 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_array.py
--rw-r--r--   0 runner    (1001) docker     (122)     5083 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_array_lal.py
--rw-r--r--   0 runner    (1001) docker     (122)     6519 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_autochisq.py
--rw-r--r--   0 runner    (1001) docker     (122)     2989 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_calibration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_chisq.py
--rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_coinc_stat.py
--rw-r--r--   0 runner    (1001) docker     (122)     9400 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_conversions.py
--rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_correlate.py
--rw-r--r--   0 runner    (1001) docker     (122)     8904 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_cuts.py
--rw-r--r--   0 runner    (1001) docker     (122)     4062 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)    11134 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_distributions.py
--rw-r--r--   0 runner    (1001) docker     (122)     5132 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_dq.py
--rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_fft_mkl_threaded.py
--rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_fft_unthreaded.py
--rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_fftw_openmp.py
--rw-r--r--   0 runner    (1001) docker     (122)     2442 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_fftw_pthreads.py
--rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_frame.py
--rw-r--r--   0 runner    (1001) docker     (122)    24440 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_frequencyseries.py
--rw-r--r--   0 runner    (1001) docker     (122)     7338 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_infmodel.py
--rw-r--r--   0 runner    (1001) docker     (122)     7052 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_injection.py
--rw-r--r--   0 runner    (1001) docker     (122)     6282 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_io_live.py
--rw-r--r--   0 runner    (1001) docker     (122)     6818 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_live_coinc_compare.py
--rw-r--r--   0 runner    (1001) docker     (122)     9715 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_matchedfilter.py
--rw-r--r--   0 runner    (1001) docker     (122)     2721 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_noise.py
--rw-r--r--   0 runner    (1001) docker     (122)     4562 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_pnutils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5934 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_psd.py
--rw-r--r--   0 runner    (1001) docker     (122)     3646 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_resample.py
--rw-r--r--   0 runner    (1001) docker     (122)     7707 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_schemes.py
--rw-r--r--   0 runner    (1001) docker     (122)     7904 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_significance_module.py
--rw-r--r--   0 runner    (1001) docker     (122)    19047 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_skymax.py
--rw-r--r--   0 runner    (1001) docker     (122)     3434 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_spatmplt.py
--rw-r--r--   0 runner    (1001) docker     (122)     2341 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_threshold.py
--rw-r--r--   0 runner    (1001) docker     (122)    25115 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_timeseries.py
--rw-r--r--   0 runner    (1001) docker     (122)    26649 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_tmpltbank.py
--rw-r--r--   0 runner    (1001) docker     (122)     3458 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     7464 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_waveform.py
--rw-r--r--   0 runner    (1001) docker     (122)     4561 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/test_waveform_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    38174 2023-03-09 21:19:31.000000 PyCBC-2.2.0/test/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/benchmarking/
--rwxr-xr-x   0 runner    (1001) docker     (122)      822 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/benchmarking/absolute_times.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/einsteinathome/
--rw-r--r--   0 runner    (1001) docker     (122)     1290 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/einsteinathome/check_GW150914_detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/static/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/static/hooks/
--rw-r--r--   0 runner    (1001) docker     (122)     3570 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/static/hooks/hook-pycbc.py
--rw-r--r--   0 runner    (1001) docker     (122)      734 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/static/runtime-scipy.py
--rw-r--r--   0 runner    (1001) docker     (122)      180 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/static/runtime-tkinter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/timing/
--rwxr-xr-x   0 runner    (1001) docker     (122)     2286 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/timing/arr_perf.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 21:19:37.000000 PyCBC-2.2.0/tools/timing/banksim/
--rw-r--r--   0 runner    (1001) docker     (122)    12160 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/timing/banksim/banksim.py
--rw-r--r--   0 runner    (1001) docker     (122)     1489 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/timing/correlate_perf.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2935 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/timing/fft_perf.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3154 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/timing/match_perf.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2472 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tools/timing/wav_perf.py
--rw-r--r--   0 runner    (1001) docker     (122)     1444 2023-03-09 21:19:31.000000 PyCBC-2.2.0/tox.ini
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/
+-rw-r--r--   0 runner    (1001) docker     (122)    35141 2023-07-27 17:58:07.000000 PyCBC-2.2.1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (122)      197 2023-07-27 17:58:07.000000 PyCBC-2.2.1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (122)     4018 2023-07-27 17:58:17.000000 PyCBC-2.2.1/PKG-INFO
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/PyCBC.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)     4018 2023-07-27 17:58:16.000000 PyCBC-2.2.1/PyCBC.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    24048 2023-07-27 17:58:16.000000 PyCBC-2.2.1/PyCBC.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-07-27 17:58:16.000000 PyCBC-2.2.1/PyCBC.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      379 2023-07-27 17:58:16.000000 PyCBC-2.2.1/PyCBC.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        6 2023-07-27 17:58:16.000000 PyCBC-2.2.1/PyCBC.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)     2492 2023-07-27 17:58:07.000000 PyCBC-2.2.1/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/all_sky_search/
+-rwxr-xr-x   0 runner    (1001) docker     (122)    29623 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_add_statmap
+-rw-r--r--   0 runner    (1001) docker     (122)     4231 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_apply_rerank
+-rw-r--r--   0 runner    (1001) docker     (122)     3897 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_average_psd
+-rw-r--r--   0 runner    (1001) docker     (122)     6355 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_bin_trigger_rates_dq
+-rw-r--r--   0 runner    (1001) docker     (122)     3472 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_calculate_dq
+-rw-r--r--   0 runner    (1001) docker     (122)     2483 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_calculate_dqflag
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4844 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_calculate_psd
+-rw-r--r--   0 runner    (1001) docker     (122)    23095 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_findtrigs
+-rwxr-xr-x   0 runner    (1001) docker     (122)    14222 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_hdfinjfind
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6098 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_mergetrigs
+-rwxr-xr-x   0 runner    (1001) docker     (122)    21534 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_statmap
+-rw-r--r--   0 runner    (1001) docker     (122)     3803 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_statmap_inj
+-rw-r--r--   0 runner    (1001) docker     (122)     3973 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_combine_coincident_events
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8387 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_combine_statmap
+-rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_cut_merge_triggers_to_tmpltbank
+-rw-r--r--   0 runner    (1001) docker     (122)     2453 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_distribute_background_bins
+-rw-r--r--   0 runner    (1001) docker     (122)    11088 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_dtphase
+-rw-r--r--   0 runner    (1001) docker     (122)     4732 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_exclude_zerolag
+-rw-r--r--   0 runner    (1001) docker     (122)    19787 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_binned
+-rwxr-xr-x   0 runner    (1001) docker     (122)    16852 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_by_template
+-rwxr-xr-x   0 runner    (1001) docker     (122)    16797 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_over_multiparam
+-rw-r--r--   0 runner    (1001) docker     (122)     9377 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_over_param
+-rw-r--r--   0 runner    (1001) docker     (122)    21126 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_split_binned
+-rw-r--r--   0 runner    (1001) docker     (122)     3445 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_followup_file
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2121 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_foreground_censor
+-rw-r--r--   0 runner    (1001) docker     (122)     3232 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_get_loudest_params
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2490 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_merge_psds
+-rw-r--r--   0 runner    (1001) docker     (122)     4175 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_plot_kde_vals
+-rw-r--r--   0 runner    (1001) docker     (122)     2471 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_reduce_template_bank
+-rw-r--r--   0 runner    (1001) docker     (122)     5084 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_rerank_dq
+-rw-r--r--   0 runner    (1001) docker     (122)     1386 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_rerank_passthrough
+-rw-r--r--   0 runner    (1001) docker     (122)     7502 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_findtrigs
+-rw-r--r--   0 runner    (1001) docker     (122)    16771 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_pastro
+-rwxr-xr-x   0 runner    (1001) docker     (122)    16627 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_statmap
+-rw-r--r--   0 runner    (1001) docker     (122)     4780 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_statmap_inj
+-rw-r--r--   0 runner    (1001) docker     (122)     2580 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_strip_injections
+-rw-r--r--   0 runner    (1001) docker     (122)    15370 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_template_kde_calc
+-rw-r--r--   0 runner    (1001) docker     (122)     4798 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/all_sky_search/pycbc_template_recovery_hist
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/bank/
+-rw-r--r--   0 runner    (1001) docker     (122)     6331 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_aligned_bank_cat
+-rw-r--r--   0 runner    (1001) docker     (122)    12115 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_aligned_stoch_bank
+-rw-r--r--   0 runner    (1001) docker     (122)    12624 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_bank_verification
+-rw-r--r--   0 runner    (1001) docker     (122)    16986 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_brute_bank
+-rw-r--r--   0 runner    (1001) docker     (122)     4926 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_coinc_bank2hdf
+-rw-r--r--   0 runner    (1001) docker     (122)    17389 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_geom_aligned_2dstack
+-rw-r--r--   0 runner    (1001) docker     (122)    23518 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_geom_aligned_bank
+-rw-r--r--   0 runner    (1001) docker     (122)    13629 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_geom_nonspinbank
+-rw-r--r--   0 runner    (1001) docker     (122)     7079 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/bank/pycbc_tmpltbank_to_chi_params
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/hwinj/
+-rw-r--r--   0 runner    (1001) docker     (122)    22443 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/hwinj/pycbc_generate_hwinj
+-rw-r--r--   0 runner    (1001) docker     (122)     5100 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/hwinj/pycbc_generate_hwinj_from_xml
+-rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/hwinj/pycbc_insert_frame_hwinj
+-rw-r--r--   0 runner    (1001) docker     (122)     2872 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/hwinj/pycbc_plot_hwinj
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/inference/
+-rw-r--r--   0 runner    (1001) docker     (122)     6405 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference
+-rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_create_fits
+-rw-r--r--   0 runner    (1001) docker     (122)     7268 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_extract_samples
+-rw-r--r--   0 runner    (1001) docker     (122)     5727 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_model_stats
+-rw-r--r--   0 runner    (1001) docker     (122)     2912 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_monitor
+-rw-r--r--   0 runner    (1001) docker     (122)     3942 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_acceptance_rate
+-rw-r--r--   0 runner    (1001) docker     (122)     5628 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_acf
+-rw-r--r--   0 runner    (1001) docker     (122)     3575 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_acl
+-rw-r--r--   0 runner    (1001) docker     (122)     2603 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_dynesty_run
+-rw-r--r--   0 runner    (1001) docker     (122)     2682 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_dynesty_traceplot
+-rw-r--r--   0 runner    (1001) docker     (122)     4461 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_gelman_rubin
+-rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_geweke
+-rw-r--r--   0 runner    (1001) docker     (122)     4300 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_inj_recovery
+-rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_mcmc_history
+-rw-r--r--   0 runner    (1001) docker     (122)    15034 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_movie
+-rw-r--r--   0 runner    (1001) docker     (122)    13533 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_posterior
+-rw-r--r--   0 runner    (1001) docker     (122)     5830 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_pp
+-rw-r--r--   0 runner    (1001) docker     (122)     5256 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_prior
+-rw-r--r--   0 runner    (1001) docker     (122)     4922 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_samples
+-rw-r--r--   0 runner    (1001) docker     (122)     2241 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_skymap
+-rw-r--r--   0 runner    (1001) docker     (122)     3283 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_plot_thermodynamic_integrand
+-rw-r--r--   0 runner    (1001) docker     (122)     5039 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_pp_table_summary
+-rw-r--r--   0 runner    (1001) docker     (122)     1407 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_start_from_samples
+-rw-r--r--   0 runner    (1001) docker     (122)     6460 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_inference_table_summary
+-rw-r--r--   0 runner    (1001) docker     (122)     2360 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/pycbc_validate_test_posterior
+-rw-r--r--   0 runner    (1001) docker     (122)       80 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/inference/run_pycbc_inference
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/live/
+-rw-r--r--   0 runner    (1001) docker     (122)     7602 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/live/pycbc_live_combine_single_fits
+-rw-r--r--   0 runner    (1001) docker     (122)     8106 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/live/pycbc_live_plot_combined_single_fits
+-rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/live/pycbc_live_plot_single_trigger_fits
+-rw-r--r--   0 runner    (1001) docker     (122)    17303 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/live/pycbc_live_single_trigger_fits
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/minifollowups/
+-rw-r--r--   0 runner    (1001) docker     (122)    10521 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_foreground_minifollowup
+-rw-r--r--   0 runner    (1001) docker     (122)    13661 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_injection_minifollowup
+-rw-r--r--   0 runner    (1001) docker     (122)     8436 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_page_coincinfo
+-rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_page_injinfo
+-rw-r--r--   0 runner    (1001) docker     (122)     8423 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_page_snglinfo
+-rw-r--r--   0 runner    (1001) docker     (122)     2389 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_plot_chigram
+-rw-r--r--   0 runner    (1001) docker     (122)     4714 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_plot_trigger_timeseries
+-rw-r--r--   0 runner    (1001) docker     (122)     3928 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_single_template_plot
+-rw-r--r--   0 runner    (1001) docker     (122)     9897 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/minifollowups/pycbc_sngl_minifollowup
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/plotting/
+-rw-r--r--   0 runner    (1001) docker     (122)     5543 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_banksim_plot_eff_fitting_factor
+-rw-r--r--   0 runner    (1001) docker     (122)     5769 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_banksim_plot_fitting_factors
+-rw-r--r--   0 runner    (1001) docker     (122)     4824 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_banksim_table_point_injs
+-rw-r--r--   0 runner    (1001) docker     (122)     1668 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_create_html_snippet
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5359 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_faithsim_plots
+-rw-r--r--   0 runner    (1001) docker     (122)    10855 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_ifar_catalog
+-rw-r--r--   0 runner    (1001) docker     (122)     3604 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_mass_area_plot
+-rw-r--r--   0 runner    (1001) docker     (122)     2638 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_mchirp_plots
+-rw-r--r--   0 runner    (1001) docker     (122)     1598 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_banktriggerrate
+-rw-r--r--   0 runner    (1001) docker     (122)     7937 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_coinc_snrchi
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8502 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_foreground
+-rw-r--r--   0 runner    (1001) docker     (122)    10669 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_foundmissed
+-rw-r--r--   0 runner    (1001) docker     (122)    13645 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_ifar
+-rw-r--r--   0 runner    (1001) docker     (122)     5291 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_injtable
+-rw-r--r--   0 runner    (1001) docker     (122)     9894 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_recovery
+-rw-r--r--   0 runner    (1001) docker     (122)     3872 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_segments
+-rw-r--r--   0 runner    (1001) docker     (122)     7449 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_segplot
+-rw-r--r--   0 runner    (1001) docker     (122)     7188 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_segtable
+-rwxr-xr-x   0 runner    (1001) docker     (122)    16396 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_sensitivity
+-rw-r--r--   0 runner    (1001) docker     (122)     3263 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_snrchi
+-rw-r--r--   0 runner    (1001) docker     (122)    15414 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_snrifar
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10366 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_snrratehist
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2226 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_versioning
+-rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_page_vetotable
+-rw-r--r--   0 runner    (1001) docker     (122)     2147 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_background_coincs
+-rw-r--r--   0 runner    (1001) docker     (122)     5207 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_bank_bins
+-rw-r--r--   0 runner    (1001) docker     (122)    10068 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_bank_corner
+-rw-r--r--   0 runner    (1001) docker     (122)     2393 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_dq_likelihood_vs_time
+-rw-r--r--   0 runner    (1001) docker     (122)     2768 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_dq_percentiles
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8101 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_gate_triggers
+-rw-r--r--   0 runner    (1001) docker     (122)     3683 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_gating
+-rw-r--r--   0 runner    (1001) docker     (122)     5504 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_hist
+-rwxr-xr-x   0 runner    (1001) docker     (122)    11670 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_multiifo_dtphase
+-rw-r--r--   0 runner    (1001) docker     (122)     5972 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_psd_file
+-rw-r--r--   0 runner    (1001) docker     (122)     5428 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_psd_timefreq
+-rw-r--r--   0 runner    (1001) docker     (122)    11089 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_qscan
+-rw-r--r--   0 runner    (1001) docker     (122)     3339 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_range
+-rw-r--r--   0 runner    (1001) docker     (122)     3544 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_range_vs_mtot
+-rw-r--r--   0 runner    (1001) docker     (122)    10339 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_singles_timefreq
+-rw-r--r--   0 runner    (1001) docker     (122)     6613 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_singles_vs_params
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2937 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_throughput
+-rw-r--r--   0 runner    (1001) docker     (122)    12162 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_trigrate
+-rw-r--r--   0 runner    (1001) docker     (122)     7158 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_vt_ratio
+-rw-r--r--   0 runner    (1001) docker     (122)     8450 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/plotting/pycbc_plot_waveform
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/population/
+-rw-r--r--   0 runner    (1001) docker     (122)    16341 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/population/pycbc_multiifo_pastro
+-rw-r--r--   0 runner    (1001) docker     (122)     4752 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/population/pycbc_population_plots
+-rw-r--r--   0 runner    (1001) docker     (122)     7417 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/population/pycbc_population_rates
+-rw-r--r--   0 runner    (1001) docker     (122)    19520 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_banksim
+-rw-r--r--   0 runner    (1001) docker     (122)     2569 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_banksim_combine_banks
+-rw-r--r--   0 runner    (1001) docker     (122)     7343 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_banksim_match_combine
+-rw-r--r--   0 runner    (1001) docker     (122)    22157 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_banksim_skymax
+-rw-r--r--   0 runner    (1001) docker     (122)     7820 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_coinc_time
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9082 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_compress_bank
+-rw-r--r--   0 runner    (1001) docker     (122)     5460 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_condition_strain
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6634 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_convertinjfiletohdf
+-rw-r--r--   0 runner    (1001) docker     (122)     3110 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_copy_output_map
+-rw-r--r--   0 runner    (1001) docker     (122)    11438 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_create_injections
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1962 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_data_store
+-rw-r--r--   0 runner    (1001) docker     (122)    11094 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_faithsim
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2395 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_faithsim_collect_results
+-rw-r--r--   0 runner    (1001) docker     (122)    17380 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_fit_sngl_trigs
+-rw-r--r--   0 runner    (1001) docker     (122)     9008 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_get_ffinal
+-rw-r--r--   0 runner    (1001) docker     (122)     4971 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_gwosc_segment_query
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5952 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_hdf5_splitbank
+-rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_hdf_splitinj
+-rw-r--r--   0 runner    (1001) docker     (122)     4905 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_inj_cut
+-rw-r--r--   0 runner    (1001) docker     (122)    24588 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_inspiral
+-rwxr-xr-x   0 runner    (1001) docker     (122)    18972 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_inspiral_skymax
+-rwxr-xr-x   0 runner    (1001) docker     (122)    57539 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_live
+-rw-r--r--   0 runner    (1001) docker     (122)     2209 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_live_nagios_monitor
+-rw-r--r--   0 runner    (1001) docker     (122)    17345 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_make_banksim
+-rw-r--r--   0 runner    (1001) docker     (122)    12546 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_make_faithsim
+-rw-r--r--   0 runner    (1001) docker     (122)    11717 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_make_html_page
+-rwxr-xr-x   0 runner    (1001) docker     (122)    25365 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_make_skymap
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3782 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_merge_inj_hdf
+-rwxr-xr-x   0 runner    (1001) docker     (122)    33869 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_multi_inspiral
+-rw-r--r--   0 runner    (1001) docker     (122)    15332 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_optimal_snr
+-rwxr-xr-x   0 runner    (1001) docker     (122)    23603 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_optimize_snr
+-rw-r--r--   0 runner    (1001) docker     (122)     5790 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_process_sngls
+-rw-r--r--   0 runner    (1001) docker     (122)     9080 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_randomize_inj_dist_by_optsnr
+-rwxr-xr-x   0 runner    (1001) docker     (122)    20946 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_single_template
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3992 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_source_probability_offline
+-rw-r--r--   0 runner    (1001) docker     (122)     2149 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_split_inspinj
+-rw-r--r--   0 runner    (1001) docker     (122)     6708 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_splitbank
+-rw-r--r--   0 runner    (1001) docker     (122)     2741 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_stageout_failed_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)     8423 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_submit_dax
+-rwxr-xr-x   0 runner    (1001) docker     (122)    11868 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pycbc_upload_xml_to_gracedb
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/pygrb/
+-rw-r--r--   0 runner    (1001) docker     (122)     7925 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pygrb/pycbc_grb_inj_combiner
+-rw-r--r--   0 runner    (1001) docker     (122)     9709 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pygrb/pycbc_grb_inj_finder
+-rw-r--r--   0 runner    (1001) docker     (122)     7661 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pygrb/pycbc_grb_trig_cluster
+-rw-r--r--   0 runner    (1001) docker     (122)    15754 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pygrb/pycbc_grb_trig_combiner
+-rw-r--r--   0 runner    (1001) docker     (122)    18903 2023-07-27 17:58:07.000000 PyCBC-2.2.1/bin/pygrb/pycbc_make_offline_grb_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)    27170 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_efficiency
+-rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_grb_info_table
+-rw-r--r--   0 runner    (1001) docker     (122)     8732 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_minifollowups
+-rw-r--r--   0 runner    (1001) docker     (122)    38056 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_page_tables
+-rw-r--r--   0 runner    (1001) docker     (122)    11936 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_chisq_veto
+-rw-r--r--   0 runner    (1001) docker     (122)    10712 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_coh_ifosnr
+-rw-r--r--   0 runner    (1001) docker     (122)    20941 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_injs_results
+-rw-r--r--   0 runner    (1001) docker     (122)     9025 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_null_stats
+-rw-r--r--   0 runner    (1001) docker     (122)     3831 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_skygrid
+-rw-r--r--   0 runner    (1001) docker     (122)     6819 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_snr_timeseries
+-rw-r--r--   0 runner    (1001) docker     (122)     8174 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_stats_distribution
+-rw-r--r--   0 runner    (1001) docker     (122)    18850 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_pp_workflow
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/workflow_comparisons/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/workflow_comparisons/offline_search/
+-rwxr-xr-x   0 runner    (1001) docker     (122)    12151 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_combine_injection_comparisons
+-rwxr-xr-x   0 runner    (1001) docker     (122)    21916 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_injection_set_comparison
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2768 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_plot_injections_found_both_workflows
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3072 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_plot_injections_missed_one_workflow
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4595 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_plot_vt_ratio_vs_ifar
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/bin/workflows/
+-rw-r--r--   0 runner    (1001) docker     (122)    16323 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_bank_verifier_workflow
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4599 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_faithsim_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)    15813 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_inference_inj_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)    10737 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_inference_plots_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)    12837 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_inference_workflow
+-rwxr-xr-x   0 runner    (1001) docker     (122)    36679 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_offline_search_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_psd_estimation_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)    16316 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_sbank_workflow
+-rw-r--r--   0 runner    (1001) docker     (122)    12259 2023-07-27 17:58:08.000000 PyCBC-2.2.1/bin/workflows/pycbc_make_uberbank_workflow
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/banksim/
+-rw-r--r--   0 runner    (1001) docker     (122)    21402 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/banksim/injection0.xml
+-rw-r--r--   0 runner    (1001) docker     (122)      461 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/banksim/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/cal/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/
+-rw-r--r--   0 runner    (1001) docker     (122)     6703 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/pycbc_check_esd_saturation.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     4460 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/pycbc_check_pcal_saturation.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     2582 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/run_pcal_saturation_example.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/catalog/
+-rw-r--r--   0 runner    (1001) docker     (122)      716 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/catalog/data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      339 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/catalog/stat.py
+-rw-r--r--   0 runner    (1001) docker     (122)      184 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/catalog/what.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/dataquality/
+-rw-r--r--   0 runner    (1001) docker     (122)      504 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/dataquality/hwinj.py
+-rw-r--r--   0 runner    (1001) docker     (122)      709 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/dataquality/on.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/detector/
+-rw-r--r--   0 runner    (1001) docker     (122)      997 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/detector/ant.py
+-rw-r--r--   0 runner    (1001) docker     (122)      785 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/detector/delay.py
+-rw-r--r--   0 runner    (1001) docker     (122)      550 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/detector/loc.py
+-rw-r--r--   0 runner    (1001) docker     (122)      259 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/detector/travel.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/distributions/
+-rw-r--r--   0 runner    (1001) docker     (122)      100 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/distributions/list_distributions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1651 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/distributions/mass_examples.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1986 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/distributions/mchirp_q_from_uniform_m1m2_example.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1236 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/distributions/sampling_from_config_example.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/distributions/spin_examples.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1933 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/distributions/spin_spatial_distr_example.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/faith/
+-rw-r--r--   0 runner    (1001) docker     (122)    21402 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/faith/injection0.xml
+-rw-r--r--   0 runner    (1001) docker     (122)      182 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/faith/pegasus_workflow_create.sh
+-rw-r--r--   0 runner    (1001) docker     (122)       28 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/faith/pegasus_workflow_submit.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      336 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/faith/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/filter/
+-rw-r--r--   0 runner    (1001) docker     (122)     1198 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/filter/chisq.py
+-rw-r--r--   0 runner    (1001) docker     (122)      624 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/filter/fir.py
+-rw-r--r--   0 runner    (1001) docker     (122)      795 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/filter/pass.py
+-rw-r--r--   0 runner    (1001) docker     (122)      949 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/filter/snr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/gw150914/
+-rw-r--r--   0 runner    (1001) docker     (122)     1150 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/gw150914/audio.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1097 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/gw150914/gw150914_h1_snr.py
+-rw-r--r--   0 runner    (1001) docker     (122)      934 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/gw150914/gw150914_shape.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/analytic-normal2d/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      379 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/analytic-normal2d/make_movie.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/analytic-normal2d/plot.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      174 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/analytic-normal2d/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/bbh-injection/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      316 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/bbh-injection/make_injection.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      649 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/bbh-injection/run.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      967 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/bbh-injection/run_test.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/gw150914/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      582 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/gw150914/plot.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      656 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/gw150914/run.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1149 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/gw150914/run_test.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/hierarchical/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      269 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/hierarchical/make_injections.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      230 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/hierarchical/run.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      565 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/hierarchical/run_test.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_inj/
+-rw-r--r--   0 runner    (1001) docker     (122)      328 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_inj/injection_smbhb.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      516 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_inj/plot.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      431 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_inj/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/
+-rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/advanced_plot.py
+-rw-r--r--   0 runner    (1001) docker     (122)      522 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/get.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      508 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/plot.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      182 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/run.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      183 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/list_parameters.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/margtime/
+-rw-r--r--   0 runner    (1001) docker     (122)      272 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/margtime/get.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      735 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/margtime/run.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      934 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/margtime/run_inj.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/multisignal/
+-rw-r--r--   0 runner    (1001) docker     (122)      212 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/multisignal/get.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      369 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/multisignal/make_injections.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      129 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/multisignal/rel.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      151 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/multisignal/run.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      135 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/multisignal/single.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/relative/
+-rw-r--r--   0 runner    (1001) docker     (122)      212 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/relative/get.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/relative/plot.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      139 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/relative/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/relmarg/
+-rw-r--r--   0 runner    (1001) docker     (122)      191 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/relmarg/get.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      743 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/relmarg/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/samplers/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      572 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/samplers/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inference/single/
+-rw-r--r--   0 runner    (1001) docker     (122)      212 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/single/get.sh
+-rw-r--r--   0 runner    (1001) docker     (122)       98 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/single/plot.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      142 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/single/run.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      364 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/single/run_instant.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      805 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inference/single/run_marg.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/inspiral/
+-rw-r--r--   0 runner    (1001) docker     (122)     1207 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inspiral/check_GW150914_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)      118 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inspiral/clean.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3683 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/inspiral/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/live/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8211 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/live/check_results.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1508 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/live/generate_injections.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2238 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/live/make_singles_fits_file.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6546 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/live/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/make_skymap/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/make_skymap/GW150914.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/make_skymap/GW170817.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      855 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/make_skymap/simulated_data.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/multi_inspiral/
+-rw-r--r--   0 runner    (1001) docker     (122)     1158 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/multi_inspiral/check_faceon_faceaway_trigs.py
+-rw-r--r--   0 runner    (1001) docker     (122)      957 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/multi_inspiral/check_gw170817_trigs.py
+-rw-r--r--   0 runner    (1001) docker     (122)       37 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/multi_inspiral/clean.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2736 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/multi_inspiral/faceon_faceaway.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2482 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/multi_inspiral/run.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/noise/
+-rw-r--r--   0 runner    (1001) docker     (122)      668 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/noise/frequency.py
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/noise/timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1520 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/overlap.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/psd/
+-rw-r--r--   0 runner    (1001) docker     (122)      576 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/psd/analytic.py
+-rw-r--r--   0 runner    (1001) docker     (122)      884 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/psd/estimate.py
+-rw-r--r--   0 runner    (1001) docker     (122)      742 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/psd/read.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/search/
+-rw-r--r--   0 runner    (1001) docker     (122)      408 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/bank.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     1136 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/check_job.py
+-rw-r--r--   0 runner    (1001) docker     (122)      238 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/gen.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      302 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/get.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      154 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/master.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      759 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/stats.sh
+-rw-r--r--   0 runner    (1001) docker     (122)       52 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/search/submit.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/tmpltbank/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/tmpltbank/bank_workflow_test/
+-rw-r--r--   0 runner    (1001) docker     (122)      114 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/bank_workflow_test/gen.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      190 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/cleanOutput.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      273 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/make_cache.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      527 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testAligned.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      888 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testAligned2.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      547 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testAligned3.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      649 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testNonspin.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      495 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testNonspin2.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      327 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testNonspin3.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      675 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testStoch.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      449 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testStoch2.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      355 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testStoch3.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      531 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/tmpltbank/testStoch4.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/waveform/
+-rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/add_waveform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1045 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/match_waveform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1500 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/plot_detwaveform.py
+-rw-r--r--   0 runner    (1001) docker     (122)      997 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/plot_fd_td.py
+-rw-r--r--   0 runner    (1001) docker     (122)      700 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/plot_freq.py
+-rw-r--r--   0 runner    (1001) docker     (122)      657 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/plot_phase.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/plot_waveform.py
+-rw-r--r--   0 runner    (1001) docker     (122)      426 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/waveform/what_waveform.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/data_checker/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6801 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/data_checker/daily_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2162 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/data_checker/get_data_example.py
+-rw-r--r--   0 runner    (1001) docker     (122)      130 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/data_checker/run_daily_test.sh
+-rw-r--r--   0 runner    (1001) docker     (122)      130 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/data_checker/run_get_data_example.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/dayhopecheck/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7198 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/dayhopecheck/dayhopecheck.py
+-rw-r--r--   0 runner    (1001) docker     (122)      146 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/dayhopecheck/run_dayhopecheck.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/generic/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/generic/multilevel_subworkflow_data/
+-rw-r--r--   0 runner    (1001) docker     (122)       90 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/generic/multilevel_subworkflow_data/run.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     1914 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/generic/multilevel_subworkflow_data/simple.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/generic/simple_subworkflow_data/
+-rw-r--r--   0 runner    (1001) docker     (122)       77 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/generic/simple_subworkflow_data/run.sh
+-rw-r--r--   0 runner    (1001) docker     (122)     1914 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/generic/simple_subworkflow_data/simple.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/inference/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/inference/bbh_inj-dynesty/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/inference/bbh_inj-dynesty/create_inj_workflow.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/inference/gw150914_gw170814-dynesty/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      486 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/inference/gw150914_gw170814-dynesty/create_workflow.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/inference/gw150914_gw170814-emcee_pt/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      480 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/inference/gw150914_gw170814-emcee_pt/create_workflow.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/inference/small_test/
+-rw-r--r--   0 runner    (1001) docker     (122)      496 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/inference/small_test/gen.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/pygrb/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/pygrb/ER7/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      739 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/pygrb/ER7/run_er7_pygrb_offline.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/pygrb/ER8/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      825 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/pygrb/ER8/run_er8_pygrb_offline.sh
+-rwxr-xr-x   0 runner    (1001) docker     (122)      822 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/pygrb/ER8/run_er8_pygrb_online.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/examples/workflow/pygrb/S6/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      866 2023-07-27 17:58:08.000000 PyCBC-2.2.1/examples/workflow/pygrb/S6/run_s6_pygrb.sh
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/
+-rw-r--r--   0 runner    (1001) docker     (122)     6218 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3219 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/_version.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6380 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/_version_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21555 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/bin_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15026 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/boundaries.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/catalog/
+-rw-r--r--   0 runner    (1001) docker     (122)     6843 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/catalog/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2479 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/catalog/catalog.py
+-rw-r--r--   0 runner    (1001) docker     (122)    62478 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4141 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/coordinates.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23038 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/cosmology.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30530 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/distributions/
+-rw-r--r--   0 runner    (1001) docker     (122)     9321 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19645 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/angular.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13126 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/arbitrary.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13606 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/bounded.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5056 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/constraints.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8901 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/external.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/fixedsamples.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9342 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/gaussian.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14281 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/joint.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10257 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/mass.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7732 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/power_law.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10564 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/qnm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6716 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/sky_location.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12002 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/spins.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/uniform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2814 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/uniform_log.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4617 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/distributions/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16660 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/dq.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/events/
+-rw-r--r--   0 runner    (1001) docker     (122)      128 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13726 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/coherent.py
+-rw-r--r--   0 runner    (1001) docker     (122)    53283 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/coinc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6808 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/coinc_rate.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18453 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/cuts.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49942 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/eventmgr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7374 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/eventmgr_cython.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)    10863 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13225 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/significance.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13294 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/simd_threshold_ccode.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)     3258 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/simd_threshold_cython.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)    10618 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/single.py
+-rw-r--r--   0 runner    (1001) docker     (122)    84871 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/stat.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3511 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/threshold_cpu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8580 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/threshold_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9465 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/trigger_fits.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8750 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/triggers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/events/veto.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/fft/
+-rw-r--r--   0 runner    (1001) docker     (122)      952 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1285 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/backend_cpu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1315 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/backend_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1197 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/backend_mkl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3126 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/backend_support.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3391 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/class_api.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11855 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/core.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2283 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/cuda_pyfft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3710 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/cufft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8478 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/fft_callback.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22788 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/fftw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/fftw_pruned.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/fftw_pruned_cython.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)     3470 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/func_api.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6206 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/mkl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3765 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/npfft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4879 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/fft/parser_support.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/filter/
+-rw-r--r--   0 runner    (1001) docker     (122)       53 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5472 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/autocorrelation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3609 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/fotonfilter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    83046 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/matchedfilter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2810 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/matchedfilter_cpu.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)     2147 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/matchedfilter_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)      830 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/matchedfilter_numpy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7048 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/qtransform.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15735 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/resample.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3116 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/simd_correlate.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15271 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/simd_correlate_ccode.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)     2347 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/simd_correlate_cython.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)     3947 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/filter/zpk.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/frame/
+-rw-r--r--   0 runner    (1001) docker     (122)     1365 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/frame/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33651 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/frame/frame.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5783 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/frame/gwosc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2303 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/frame/store.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/inference/
+-rw-r--r--   0 runner    (1001) docker     (122)      137 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30844 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/burn_in.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8796 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/entropy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/evidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6652 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/gelman_rubin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2802 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/geweke.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/inference/io/
+-rw-r--r--   0 runner    (1001) docker     (122)    32158 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38576 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/base_hdf.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36040 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/base_mcmc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16944 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/base_multitemper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2918 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/base_nested_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5298 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/base_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1205 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/cpnest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7473 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/dynesty.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3848 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/emcee.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4859 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/emcee_pt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11226 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/epsie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5382 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/multinest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3653 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/posterior.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5947 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/ptemcee.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2421 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/txt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1226 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/io/ultranest.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/inference/jump/
+-rw-r--r--   0 runner    (1001) docker     (122)     3211 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/jump/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4775 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/jump/angular.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4652 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/jump/bounded_normal.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5794 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/jump/discrete.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20231 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/jump/normal.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/inference/models/
+-rw-r--r--   0 runner    (1001) docker     (122)    11328 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8703 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/analytic.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33142 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5697 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/base_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7614 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/brute_marg.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23677 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31219 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/gated_gaussian_noise.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49252 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/gaussian_noise.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25771 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/hierarchical.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32995 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/marginalized_gaussian_noise.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32610 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/relbin.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20731 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/relbin_cpu.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)     8440 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/single_template.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36070 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/models/tools.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16324 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/option_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/inference/sampler/
+-rw-r--r--   0 runner    (1001) docker     (122)     2813 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8201 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3361 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/base_cube.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36376 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/base_mcmc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16327 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/base_multitemper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8274 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/cpnest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2594 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/dummy.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23999 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/dynesty.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9506 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/emcee.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18909 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/emcee_pt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19831 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/epsie.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15427 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/multinest.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27259 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/ptemcee.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7359 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inference/sampler/ultranest.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/inject/
+-rw-r--r--   0 runner    (1001) docker     (122)       79 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inject/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48285 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inject/inject.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19409 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/inject/injfilterrejector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/io/
+-rw-r--r--   0 runner    (1001) docker     (122)      621 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55538 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/io/hdf.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12522 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/io/ligolw.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24753 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/io/live.py
+-rw-r--r--   0 runner    (1001) docker     (122)    77702 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/io/record.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9494 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/libutils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13070 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/mchirp_area.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/neutron_stars/
+-rw-r--r--   0 runner    (1001) docker     (122)      373 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/neutron_stars/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7931 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/neutron_stars/eos_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/neutron_stars/ns_data/
+-rw-r--r--   0 runner    (1001) docker     (122)    53263 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/neutron_stars/ns_data/equil_2H.dat
+-rw-r--r--   0 runner    (1001) docker     (122)    11046 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/neutron_stars/pg_isso_solver.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/noise/
+-rw-r--r--   0 runner    (1001) docker     (122)       89 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/noise/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5192 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/noise/gaussian.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8316 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/noise/reproduceable.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5385 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/opt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    40237 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/pnutils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5462 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/pool.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/population/
+-rw-r--r--   0 runner    (1001) docker     (122)      327 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30477 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/fgmc_functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6246 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/fgmc_laguerre.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10012 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/fgmc_plots.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10265 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/live_pastro.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3907 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/live_pastro_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16559 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/population_models.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13826 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/rates_functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16956 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/population/scale_injections.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/psd/
+-rw-r--r--   0 runner    (1001) docker     (122)    28796 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/psd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6384 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/psd/analytical.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26286 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/psd/analytical_space.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12318 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/psd/estimate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/psd/read.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8554 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/psd/variation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12540 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/rate.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/
+-rw-r--r--   0 runner    (1001) docker     (122)      423 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      703 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/color.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/dq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5023 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/followup.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3937 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/layout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4769 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4810 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/mpld3_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1509 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/plot.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2081 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/psd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9071 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/pygrb_plotting_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33850 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/pygrb_postprocessing_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7996 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/render.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33143 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/scatter_histograms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/results/static/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/css/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/results/static/css/bootstrap/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/css/bootstrap/3.3.2/
+-rw-r--r--   0 runner    (1001) docker     (122)   117150 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/css/bootstrap/3.3.2/bootstrap.min.css
+-rw-r--r--   0 runner    (1001) docker     (122)   117150 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/css/bootstrap.min.css
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/results/static/css/fancybox/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/css/fancybox/2.1.5/
+-rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/css/fancybox/2.1.5/jquery.fancybox.css
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/css/pycbc/
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/css/pycbc/orange.css
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/css/pycbc/red.css
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/fonts/
+-rw-r--r--   0 runner    (1001) docker     (122)      418 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.css
+-rw-r--r--   0 runner    (1001) docker     (122)   258264 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.eot
+-rw-r--r--   0 runner    (1001) docker     (122)      657 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.html
+-rw-r--r--   0 runner    (1001) docker     (122)   658924 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.ttf
+-rw-r--r--   0 runner    (1001) docker     (122)   312760 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.woff
+-rw-r--r--   0 runner    (1001) docker     (122)      436 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.css
+-rw-r--r--   0 runner    (1001) docker     (122)   270560 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.eot
+-rw-r--r--   0 runner    (1001) docker     (122)      666 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.html
+-rw-r--r--   0 runner    (1001) docker     (122)   702616 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.ttf
+-rw-r--r--   0 runner    (1001) docker     (122)   326132 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.woff
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/js/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/results/static/js/bootstrap/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/js/bootstrap/3.3.2/
+-rw-r--r--   0 runner    (1001) docker     (122)    35452 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/bootstrap/3.3.2/bootstrap.min.js
+-rw-r--r--   0 runner    (1001) docker     (122)    35452 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/bootstrap.min.js
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/results/static/js/fancybox/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/
+-rw-r--r--   0 runner    (1001) docker     (122)     3187 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/fancybox-orange.js
+-rwxr-xr-x   0 runner    (1001) docker     (122)    48706 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.js
+-rwxr-xr-x   0 runner    (1001) docker     (122)    23135 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.pack.js
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/pycbc/results/static/js/jquery/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/static/js/jquery/1.10.2/
+-rw-r--r--   0 runner    (1001) docker     (122)    93107 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/jquery/1.10.2/jquery-1.10.2.min.js
+-rw-r--r--   0 runner    (1001) docker     (122)     1384 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/jquery/1.10.2/jquery.mousewheel-3.0.6.pack.js
+-rw-r--r--   0 runner    (1001) docker     (122)    95786 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/static/js/jquery.min.js
+-rw-r--r--   0 runner    (1001) docker     (122)     9027 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/str_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4726 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/table_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/templates/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/templates/files/
+-rw-r--r--   0 runner    (1001) docker     (122)     1991 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/files/file_default.html
+-rw-r--r--   0 runner    (1001) docker     (122)      855 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/files/file_glitchgram.html
+-rw-r--r--   0 runner    (1001) docker     (122)      153 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/files/file_img.html
+-rw-r--r--   0 runner    (1001) docker     (122)      629 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/files/file_pre.html
+-rw-r--r--   0 runner    (1001) docker     (122)     1041 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/files/file_tmplt.html
+-rw-r--r--   0 runner    (1001) docker     (122)    11740 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/orange.html
+-rw-r--r--   0 runner    (1001) docker     (122)    11734 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/red.html
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/results/templates/wells/
+-rw-r--r--   0 runner    (1001) docker     (122)      290 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/wells/sitemap.html
+-rw-r--r--   0 runner    (1001) docker     (122)     3968 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/templates/wells/two_column.html
+-rw-r--r--   0 runner    (1001) docker     (122)     6293 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/results/versioning.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11120 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/scheme.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11609 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/sensitivity.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/strain/
+-rw-r--r--   0 runner    (1001) docker     (122)     1299 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/strain/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5742 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/strain/calibration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6383 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/strain/gate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/strain/lines.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18783 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/strain/recalibrate.py
+-rw-r--r--   0 runner    (1001) docker     (122)    93225 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/strain/strain.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/tmpltbank/
+-rw-r--r--   0 runner    (1001) docker     (122)      361 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5828 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/bank_conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14135 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/bank_output_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23515 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/brute_force_methods.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23569 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/calc_moments.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33431 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/coord_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10726 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/lambda_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5472 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/lattice_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    57353 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/option_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28141 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/tmpltbank/partitioned_bank.py
+-rw-r--r--   0 runner    (1001) docker     (122)    84293 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/types/
+-rw-r--r--   0 runner    (1001) docker     (122)      137 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2021 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/aligned.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39845 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/array.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5421 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/array_cpu.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)    11741 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/array_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28298 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25727 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/frequencyseries.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21934 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/optparse.py
+-rw-r--r--   0 runner    (1001) docker     (122)    46009 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/types/timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (122)      771 2023-07-27 17:58:13.000000 PyCBC-2.2.1/pycbc/version.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/vetoes/
+-rw-r--r--   0 runner    (1001) docker     (122)       72 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12524 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/autochisq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9693 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/bank_chisq.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19668 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/chisq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4964 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/chisq_cpu.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)    12354 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/chisq_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7224 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/vetoes/sgchisq.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/waveform/
+-rw-r--r--   0 runner    (1001) docker     (122)    18361 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/SpinTaylorF2.py
+-rw-r--r--   0 runner    (1001) docker     (122)      520 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39507 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/bank.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30086 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/compress.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2155 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/decompress_cpu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9811 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/decompress_cpu_ccode.cpp
+-rw-r--r--   0 runner    (1001) docker     (122)     3580 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/decompress_cpu_cython.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)    11138 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/decompress_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)    51496 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3724 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/multiband.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2683 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/nltides.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29135 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/parameters.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4171 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/plugin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1282 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/premerger.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15018 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/pycbc_phenomC_tmplt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    54746 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/ringdown.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1285 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/sinegauss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9385 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/spa_tmplt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6584 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/spa_tmplt_cpu.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)     3243 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/spa_tmplt_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1893 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/supernovae.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19760 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/utils_cpu.pyx
+-rw-r--r--   0 runner    (1001) docker     (122)     3136 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/utils_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (122)    57449 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/waveform.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11042 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/waveform/waveform_modes.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/workflow/
+-rw-r--r--   0 runner    (1001) docker     (122)     2063 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32083 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/coincidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10939 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/configparser_test.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15775 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    90638 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/core.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48147 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/datafind.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9556 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/dq.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33027 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/grb_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42899 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/inference_followups.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10485 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/injection.py
+-rw-r--r--   0 runner    (1001) docker     (122)    51006 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/jobsetup.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13867 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/matched_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36594 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/minifollowups.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/pycbc/workflow/pegasus_files/
+-rw-r--r--   0 runner    (1001) docker     (122)     2440 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/pegasus_files/pegasus-properties.conf
+-rw-r--r--   0 runner    (1001) docker     (122)    11815 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/pegasus_sites.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32296 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/pegasus_workflow.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23833 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/plotting.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4824 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/psd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5160 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/psdfiles.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19177 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/segment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6964 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/splittable.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14333 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/tmpltbank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2384 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pycbc/workflow/versioning.py
+-rw-r--r--   0 runner    (1001) docker     (122)      618 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (122)       85 2023-07-27 17:58:08.000000 PyCBC-2.2.1/pytest.ini
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-07-27 17:58:17.000000 PyCBC-2.2.1/setup.cfg
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10711 2023-07-27 17:58:08.000000 PyCBC-2.2.1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/test/
+-rw-r--r--   0 runner    (1001) docker     (122)     3650 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/bankvetotest.py
+-rw-r--r--   0 runner    (1001) docker     (122)    37722 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/fft_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16620 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/lalsim.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21211 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_array.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5083 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_array_lal.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6519 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_autochisq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2989 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_calibration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_chisq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_coinc_stat.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9562 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_correlate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8904 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_cuts.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4019 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11170 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_distributions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5132 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_dq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_fft_mkl_threaded.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_fft_unthreaded.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_fftw_openmp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2442 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_fftw_pthreads.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_frame.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24440 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_frequencyseries.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7338 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_infmodel.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6986 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_injection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6282 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_io_live.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6818 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_live_coinc_compare.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9715 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_matchedfilter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2721 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_noise.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4562 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_pnutils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5913 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_psd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3646 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_resample.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7707 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_schemes.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7904 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_significance_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19047 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_skymax.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3434 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_spatmplt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2341 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_threshold.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26161 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26607 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_tmpltbank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3458 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7464 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_waveform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4561 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/test_waveform_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38174 2023-07-27 17:58:08.000000 PyCBC-2.2.1/test/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:16.000000 PyCBC-2.2.1/tools/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/tools/benchmarking/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      822 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/benchmarking/absolute_times.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/tools/einsteinathome/
+-rw-r--r--   0 runner    (1001) docker     (122)     1290 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/einsteinathome/check_GW150914_detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/tools/static/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/tools/static/hooks/
+-rw-r--r--   0 runner    (1001) docker     (122)     3570 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/static/hooks/hook-pycbc.py
+-rw-r--r--   0 runner    (1001) docker     (122)      734 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/static/runtime-scipy.py
+-rw-r--r--   0 runner    (1001) docker     (122)      180 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/static/runtime-tkinter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/tools/timing/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2286 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/timing/arr_perf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-07-27 17:58:17.000000 PyCBC-2.2.1/tools/timing/banksim/
+-rw-r--r--   0 runner    (1001) docker     (122)    12160 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/timing/banksim/banksim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1489 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/timing/correlate_perf.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2935 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/timing/fft_perf.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3154 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/timing/match_perf.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2472 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tools/timing/wav_perf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1500 2023-07-27 17:58:08.000000 PyCBC-2.2.1/tox.ini
```

### Comparing `PyCBC-2.2.0/LICENSE` & `PyCBC-2.2.1/LICENSE`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/PKG-INFO` & `PyCBC-2.2.1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: PyCBC
-Version: 2.2.0
+Version: 2.2.1
 Summary: Core library to analyze gravitational-wave data, find signals, and study their parameters.
 Home-page: http://www.pycbc.org/
 Author: The PyCBC team
 Author-email: alex.nitz@gmail.org
 License: UNKNOWN
-Download-URL: https://github.com/gwastro/pycbc/tarball/v2.2.0
+Download-URL: https://github.com/gwastro/pycbc/tarball/v2.2.1
 Description: ![GW150914](https://raw.githubusercontent.com/gwastro/pycbc-logo/master/pycbc_logo_name.png)
         
         [PyCBC](http://pycbc.org) is a software package used to explore astrophysical sources of gravitational waves.
         It contains algorithms to analyze gravitational-wave data,
         detect coalescing compact binaries, and make bayesian inferences from gravitational-wave data.
         PyCBC was used in the [first direct detection of gravitational waves](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.116.061102) and
         is used in flagship analyses of LIGO and Virgo data.
```

### Comparing `PyCBC-2.2.0/PyCBC.egg-info/PKG-INFO` & `PyCBC-2.2.1/PyCBC.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 Metadata-Version: 2.1
 Name: PyCBC
-Version: 2.2.0
+Version: 2.2.1
 Summary: Core library to analyze gravitational-wave data, find signals, and study their parameters.
 Home-page: http://www.pycbc.org/
 Author: The PyCBC team
 Author-email: alex.nitz@gmail.org
 License: UNKNOWN
-Download-URL: https://github.com/gwastro/pycbc/tarball/v2.2.0
+Download-URL: https://github.com/gwastro/pycbc/tarball/v2.2.1
 Description: ![GW150914](https://raw.githubusercontent.com/gwastro/pycbc-logo/master/pycbc_logo_name.png)
         
         [PyCBC](http://pycbc.org) is a software package used to explore astrophysical sources of gravitational waves.
         It contains algorithms to analyze gravitational-wave data,
         detect coalescing compact binaries, and make bayesian inferences from gravitational-wave data.
         PyCBC was used in the [first direct detection of gravitational waves](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.116.061102) and
         is used in flagship analyses of LIGO and Virgo data.
```

### Comparing `PyCBC-2.2.0/PyCBC.egg-info/SOURCES.txt` & `PyCBC-2.2.1/PyCBC.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -13,30 +13,30 @@
 bin/pycbc_banksim
 bin/pycbc_banksim_combine_banks
 bin/pycbc_banksim_match_combine
 bin/pycbc_banksim_skymax
 bin/pycbc_coinc_time
 bin/pycbc_compress_bank
 bin/pycbc_condition_strain
+bin/pycbc_convertinjfiletohdf
 bin/pycbc_copy_output_map
 bin/pycbc_create_injections
 bin/pycbc_data_store
 bin/pycbc_faithsim
 bin/pycbc_faithsim_collect_results
 bin/pycbc_fit_sngl_trigs
 bin/pycbc_get_ffinal
+bin/pycbc_gwosc_segment_query
 bin/pycbc_hdf5_splitbank
 bin/pycbc_hdf_splitinj
 bin/pycbc_inj_cut
-bin/pycbc_inspinj2hdf
 bin/pycbc_inspiral
 bin/pycbc_inspiral_skymax
 bin/pycbc_live
 bin/pycbc_live_nagios_monitor
-bin/pycbc_losc_segment_query
 bin/pycbc_make_banksim
 bin/pycbc_make_faithsim
 bin/pycbc_make_html_page
 bin/pycbc_make_skymap
 bin/pycbc_merge_inj_hdf
 bin/pycbc_multi_inspiral
 bin/pycbc_optimal_snr
@@ -158,17 +158,19 @@
 bin/plotting/pycbc_page_segments
 bin/plotting/pycbc_page_segplot
 bin/plotting/pycbc_page_segtable
 bin/plotting/pycbc_page_sensitivity
 bin/plotting/pycbc_page_snrchi
 bin/plotting/pycbc_page_snrifar
 bin/plotting/pycbc_page_snrratehist
+bin/plotting/pycbc_page_versioning
 bin/plotting/pycbc_page_vetotable
 bin/plotting/pycbc_plot_background_coincs
 bin/plotting/pycbc_plot_bank_bins
+bin/plotting/pycbc_plot_bank_corner
 bin/plotting/pycbc_plot_dq_likelihood_vs_time
 bin/plotting/pycbc_plot_dq_percentiles
 bin/plotting/pycbc_plot_gate_triggers
 bin/plotting/pycbc_plot_gating
 bin/plotting/pycbc_plot_hist
 bin/plotting/pycbc_plot_multiifo_dtphase
 bin/plotting/pycbc_plot_psd_file
@@ -204,19 +206,19 @@
 bin/pygrb/pycbc_pygrb_pp_workflow
 bin/workflow_comparisons/offline_search/pycbc_combine_injection_comparisons
 bin/workflow_comparisons/offline_search/pycbc_injection_set_comparison
 bin/workflow_comparisons/offline_search/pycbc_plot_injections_found_both_workflows
 bin/workflow_comparisons/offline_search/pycbc_plot_injections_missed_one_workflow
 bin/workflow_comparisons/offline_search/pycbc_plot_vt_ratio_vs_ifar
 bin/workflows/pycbc_make_bank_verifier_workflow
-bin/workflows/pycbc_make_coinc_search_workflow
 bin/workflows/pycbc_make_faithsim_workflow
 bin/workflows/pycbc_make_inference_inj_workflow
 bin/workflows/pycbc_make_inference_plots_workflow
 bin/workflows/pycbc_make_inference_workflow
+bin/workflows/pycbc_make_offline_search_workflow
 bin/workflows/pycbc_make_psd_estimation_workflow
 bin/workflows/pycbc_make_sbank_workflow
 bin/workflows/pycbc_make_uberbank_workflow
 examples/overlap.py
 examples/banksim/injection0.xml
 examples/banksim/run.sh
 examples/cal/foton_filter_esd_saturation/pycbc_check_esd_saturation.sh
@@ -257,29 +259,34 @@
 examples/inference/bbh-injection/run_test.sh
 examples/inference/gw150914/plot.sh
 examples/inference/gw150914/run.sh
 examples/inference/gw150914/run_test.sh
 examples/inference/hierarchical/make_injections.sh
 examples/inference/hierarchical/run.sh
 examples/inference/hierarchical/run_test.sh
-examples/inference/lisa_smbhb/advanced_plot.py
-examples/inference/lisa_smbhb/get.sh
-examples/inference/lisa_smbhb/plot.sh
-examples/inference/lisa_smbhb/run.sh
+examples/inference/lisa_smbhb_inj/injection_smbhb.sh
+examples/inference/lisa_smbhb_inj/plot.sh
+examples/inference/lisa_smbhb_inj/run.sh
+examples/inference/lisa_smbhb_ldc/advanced_plot.py
+examples/inference/lisa_smbhb_ldc/get.sh
+examples/inference/lisa_smbhb_ldc/plot.sh
+examples/inference/lisa_smbhb_ldc/run.sh
 examples/inference/margtime/get.sh
 examples/inference/margtime/run.sh
 examples/inference/margtime/run_inj.sh
 examples/inference/multisignal/get.sh
 examples/inference/multisignal/make_injections.sh
 examples/inference/multisignal/rel.sh
 examples/inference/multisignal/run.sh
 examples/inference/multisignal/single.sh
 examples/inference/relative/get.sh
 examples/inference/relative/plot.sh
 examples/inference/relative/run.sh
+examples/inference/relmarg/get.sh
+examples/inference/relmarg/run.sh
 examples/inference/samplers/run.sh
 examples/inference/single/get.sh
 examples/inference/single/plot.sh
 examples/inference/single/run.sh
 examples/inference/single/run_instant.sh
 examples/inference/single/run_marg.sh
 examples/inspiral/check_GW150914_detection.py
@@ -433,15 +440,15 @@
 pycbc/filter/resample.py
 pycbc/filter/simd_correlate.py
 pycbc/filter/simd_correlate_ccode.cpp
 pycbc/filter/simd_correlate_cython.pyx
 pycbc/filter/zpk.py
 pycbc/frame/__init__.py
 pycbc/frame/frame.py
-pycbc/frame/losc.py
+pycbc/frame/gwosc.py
 pycbc/frame/store.py
 pycbc/inference/__init__.py
 pycbc/inference/burn_in.py
 pycbc/inference/entropy.py
 pycbc/inference/evidence.py
 pycbc/inference/gelman_rubin.py
 pycbc/inference/geweke.py
@@ -526,15 +533,14 @@
 pycbc/psd/read.py
 pycbc/psd/variation.py
 pycbc/results/__init__.py
 pycbc/results/color.py
 pycbc/results/dq.py
 pycbc/results/followup.py
 pycbc/results/layout.py
-pycbc/results/legacy_grb.py
 pycbc/results/metadata.py
 pycbc/results/mpld3_utils.py
 pycbc/results/plot.py
 pycbc/results/psd.py
 pycbc/results/pygrb_plotting_utils.py
 pycbc/results/pygrb_postprocessing_utils.py
 pycbc/results/render.py
@@ -649,14 +655,15 @@
 pycbc/workflow/pegasus_workflow.py
 pycbc/workflow/plotting.py
 pycbc/workflow/psd.py
 pycbc/workflow/psdfiles.py
 pycbc/workflow/segment.py
 pycbc/workflow/splittable.py
 pycbc/workflow/tmpltbank.py
+pycbc/workflow/versioning.py
 pycbc/workflow/pegasus_files/pegasus-properties.conf
 test/bankvetotest.py
 test/fft_base.py
 test/lalsim.py
 test/test_array.py
 test/test_array_lal.py
 test/test_autochisq.py
```

### Comparing `PyCBC-2.2.0/README.md` & `PyCBC-2.2.1/README.md`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_add_statmap` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_add_statmap`

 * *Files 2% similar despite different names*

```diff
@@ -222,15 +222,15 @@
                     dtype=int)])
                 bg_exc_trig_times[ifo] = np.concatenate([bg_exc_trig_times[ifo],
                     -1 * np.ones_like(f_in['background_exc/stat'][:],
                     dtype=float)])
                 bg_exc_trig_ids[ifo] = np.concatenate([bg_exc_trig_ids[ifo],
                     -1 * np.ones_like(f_in['background_exc/stat'][:],
                     dtype=int)])
-n_triggers = f['foreground/ifar'].size
+n_triggers = f['foreground/stat'].size
 logging.info('{} foreground events before clustering'.format(n_triggers))
 
 for ifo in all_ifos:
     f.create_dataset('foreground/{}/time'.format(ifo),
                      data=fg_trig_times[ifo])
     f.create_dataset('foreground/{}/trigger_id'.format(ifo),
                      data=fg_trig_ids[ifo])
@@ -267,18 +267,15 @@
 for key in f['foreground'].keys():
     if key not in all_ifos:
         filter_dataset(f, 'foreground/%s' % key, cidx)
     else:  # key is an ifo
         for k in f['foreground/%s' % key].keys():
             filter_dataset(f, 'foreground/{}/{}'.format(key, k), cidx)
 
-fg_ifar_init = f['foreground/ifar'][:]
-fg_ifar_exc_init = f['foreground/ifar_exc'][:]
-
-n_triggers = fg_ifar_init.size
+n_triggers = f['foreground/stat'].size
 
 logging.info('Calculating event times to determine which types of coinc '
              'are available')
 times_tuple = (f['foreground/{}/time'.format(ifo)][:] for ifo in all_ifos)
 test_times = np.array([pycbc.events.mean_if_greater_than_zero(tc)[0]
                        for tc in zip(*times_tuple)])
 
@@ -377,16 +374,14 @@
     logging.info('Censoring %.2f seconds', abs(vtime))
     f.attrs['foreground_time_exc'] -= abs(vtime)
     f['segments/foreground_veto/start'] = vstart
     f['segments/foreground_veto/end'] = vend
     # Only output non-exclusive ifar/fap if it is _not_ an injection case
     f['foreground/ifar'][:] = fg_ifar
     f['foreground/fap'] = 1 - np.exp(-conv.sec_to_year(fg_time) / fg_ifar)
-else:
-    del f['foreground/ifar']
 
 del test_times
 
 f['foreground/ifar_exc'][:] = fg_ifar_exc
 fg_time_exc = f.attrs['foreground_time_exc']
 f['foreground/fap_exc'] = 1 - np.exp(-conv.sec_to_year(fg_time_exc) /
                                      fg_ifar_exc)
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_apply_rerank` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_apply_rerank`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_average_psd` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_average_psd`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_bin_trigger_rates_dq` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_bin_trigger_rates_dq`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_calculate_dq` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_calculate_dq`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_calculate_dqflag` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_calculate_dqflag`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_calculate_psd` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_calculate_psd`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_findtrigs` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_findtrigs`

 * *Files 0% similar despite different names*

```diff
@@ -177,30 +177,32 @@
 
 if args.randomize_template_order:
     seed(0)
     template_ids = numpy.arange(0, num_templates)
     shuffle(template_ids)
     template_ids = template_ids[tmin:tmax]
 else:
-    template_ids = range(tmin, tmax)
+    template_ids = np.array([range(tmin, tmax)])
+
+original_bank_len = len(template_ids)
 
 # Only analyze templates which might have coincs
 if tids_with_trigs is not None:
     template_ids = numpy.intersect1d(tids_with_trigs, template_ids)
 
 # Apply cuts to templates
 template_ids = cuts.apply_template_cuts(
     trigs.singles[0].bank,
     template_cut_dict,
     statistic=rank_method,
     ifos=trigs.ifos,
     template_ids=template_ids)
 
 logging.info("%d out of %d templates kept after applying template cuts",
-             len(template_ids), len(trigs.singles[0].bank))
+             len(template_ids), original_bank_len)
 
 # 'data' will store output of coinc finding
 # in addition to these lists of coinc info, will also store trigger times and
 # ids in each ifo
 data = {'stat': [], 'decimation_factor': [], 'timeslide_id': [], 'template_id': []}
 for ifo in trigs.ifos:
     data['%s/time' % ifo] = []
@@ -236,15 +238,16 @@
     times_full = {}
     sds_full = {}
     tids_full = {}
     logging.info('Obtaining trigs for template %i ..' % (tnum))
     for i, sngl in zip(trigs.ifos, trigs.singles):
         # Apply cuts to triggers
         tids_uncut = sngl.set_template(tnum)
-        trigger_keep_ids = cuts.apply_trigger_cuts(sngl, trigger_cut_dict)
+        trigger_keep_ids = cuts.apply_trigger_cuts(sngl, trigger_cut_dict,
+                                                   statistic=rank_method)
 
         tids_full[i] = tids_uncut[trigger_keep_ids]
         times_full[i] = sngl['end_time'][trigger_keep_ids]
         logging.info('%s:%s', i, len(tids_uncut))
         if len(tids_full[i]) < len(tids_uncut):
             logging.info("%s triggers cut",
                          len(tids_uncut) - len(tids_full[i]))
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_hdfinjfind` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_hdfinjfind`

 * *Files 2% similar despite different names*

```diff
@@ -273,16 +273,18 @@
             fkey = f[key]
         else:
             fkey = f
         if 'pivot' in fo[key].attrs:
             # This is a coincident statmap file
             fo[key].attrs['pivot'] = fkey.attrs['pivot']
             fo[key].attrs['fixed'] = fkey.attrs['fixed']
-        fo[key].attrs['foreground_time'] = fkey.attrs['foreground_time']
-        fo[key].attrs['foreground_time_exc'] = fkey.attrs['foreground_time_exc']
+        if 'foreground_time' in fkey.attrs.keys():
+            fo[key].attrs['foreground_time'] = fkey.attrs['foreground_time']
+        if 'foreground_time_exc' in fkey.attrs.keys():
+            fo[key].attrs['foreground_time_exc'] = fkey.attrs['foreground_time_exc']
 
     hdf_append(fo, 'missed/all', missed + injection_index)
     hdf_append(fo, 'missed/within_analysis', missed_within_time + injection_index)
     hdf_append(fo, 'missed/after_vetoes', missed_after_vetoes + injection_index)
     hdf_append(fo, 'found/template_id', template_id[time_sorting][found_fore])
     hdf_append(fo, 'found/injection_index', found + injection_index)
     hdf_append(fo, 'found/stat', stat[time_sorting][found_fore])
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_mergetrigs` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_mergetrigs`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_coinc_statmap` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_coinc_statmap`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_combine_coincident_events` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_combine_coincident_events`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_combine_statmap` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_combine_statmap`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_cut_merge_triggers_to_tmpltbank` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_cut_merge_triggers_to_tmpltbank`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_distribute_background_bins` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_distribute_background_bins`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_dtphase` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_dtphase`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_exclude_zerolag` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_exclude_zerolag`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_binned` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_binned`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_by_template` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_by_template`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_over_multiparam` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_over_multiparam`

 * *Files 18% similar despite different names*

```diff
@@ -134,14 +134,41 @@
 
     dists is an array of the distances of the templates from the
     template of interest
     """
     return _smooth_dist_func[smoothing_method](nabove, invalphan,
                                                ntotal, dists, **kwargs)
 
+# Number of smoothing lengths around the current template where
+# distances will be calculated
+# n_closest has no limit as it needs to contain enough
+# templates to contain n triggers, which we cannot know beforehand
+
+_smooth_cut = {
+    'smooth_tophat': 1,
+    'n_closest': numpy.inf,
+    'distance_weighted': 3,
+}
+
+
+def report_percentage(i, length):
+    """
+    Convenience function - report how long through the loop we are.
+    Every ten percent
+    Parameters
+    ----------
+    i: integer
+        index being looped through
+    length : integer
+        number of loops we will go through in total
+    """
+    pc = int(numpy.floor(i / length * 100))
+    pc_last = int(numpy.floor((i - 1) / length * 100))
+    if not pc % 10 and pc_last % 10:
+        logging.info(f"Template {i} out of {length} ({pc:.0f}%)")
 
 parser = argparse.ArgumentParser(usage="",
     description="Smooth (regress) the dependence of coefficients describing "
                 "single-ifo background trigger distributions on a template "
                 "parameter, to suppress random noise in the resulting "
                 "background model.")
 
@@ -234,40 +261,41 @@
 tid = fits['template_id'][:]
 
 logging.info('Calculating template parameter values')
 bank = h5py.File(args.bank_file, 'r')
 m1, m2, s1z, s2z = triggers.get_mass_spin(bank, tid)
 
 parvals = []
+parnames = []
 
 for param, slog in zip(args.fit_param, args.log_param):
     data = triggers.get_param(param, args, m1, m2, s1z, s2z)
     if slog in ['false', 'False', 'FALSE']:
         logging.info('Using param: %s', param)
         parvals.append(data)
+        parnames.append(param)
     elif slog in ['true', 'True', 'TRUE']:
         logging.info('Using log param: %s', param)
         parvals.append(numpy.log(data))
+        parnames.append(f"log({param})")
     else:
         raise ValueError("invalid log param argument, use 'true', or 'false'")
 
 nabove = fits['count_above_thresh'][:]
 ntotal = fits['count_in_template'][:]
 # For an exponential fit 1/alpha is linear in the trigger statistic values
 # so calculating weighted sums or averages of 1/alpha is appropriate
 invalpha = 1. / fits['fit_coeff'][:]
 invalphan = invalpha * nabove
 
 nabove_smoothed = []
-ntotal_smoothed = []
 alpha_smoothed = []
+ntotal_smoothed = []
 rang = numpy.arange(0, len(nabove))
 
-logging.info("Smoothing ...")
-
 # Handle the one-dimensional case of tophat smoothing separately
 # as it is easier to optimize computational performance.
 if len(parvals) == 1 and args.smoothing_method == 'smooth_tophat':
     logging.info("Using efficient 1D tophat smoothing")
     sort = parvals[0].argsort()
     parvals_0 = parvals[0][sort]
 
@@ -275,26 +303,78 @@
     # the chosen window.
     left = numpy.searchsorted(parvals_0, parvals[0] - args.smoothing_width[0])
     right = numpy.searchsorted(parvals_0, parvals[0] + args.smoothing_width[0]) - 1
 
     del parvals_0
     # Precompute the sums so we can quickly look up differences between
     # templates
-    ntsum = ntotal.cumsum()
     nasum = nabove.cumsum()
     invsum = invalphan.cumsum()
+    ntsum = ntotal.cumsum()
     num = right - left
 
-    ntotal_smoothed = (ntsum[right] - ntsum[left]) / num
+    logging.info("Smoothing ...")
     nabove_smoothed = (nasum[right] - nasum[left]) / num
     invmean = (invsum[right] - invsum[left]) / num
     alpha_smoothed = nabove_smoothed / invmean
+    ntotal_smoothed = (ntsum[right] - ntsum[left]) / num
+
+elif numpy.isfinite(_smooth_cut[args.smoothing_method]):
+    c = _smooth_cut[args.smoothing_method]
+    cut_lengths = [s * c for s in args.smoothing_width]
+    # Find the "longest" dimension in cut lengths
+    sort_dim = numpy.argmax([(v.max() - v.min()) / c
+                              for v, c in zip(parvals, cut_lengths)])
+    logging.info("Sorting / Cutting on dimension %s", parnames[sort_dim])
+
+    # Sort parvals by the sort dimension
+    par_sort = numpy.argsort(parvals[sort_dim])
+    parvals = [p[par_sort] for p in parvals]
+
+    # For each template, find the range of nearby templates which fall within
+    # the chosen window.
+    lefts = numpy.searchsorted(parvals[sort_dim],
+            parvals[sort_dim] - cut_lengths[sort_dim])
+    rights = numpy.searchsorted(parvals[sort_dim],
+            parvals[sort_dim] + cut_lengths[sort_dim])
+    n_removed = len(parvals[0]) - rights + lefts
+    logging.info("Cutting between %d and %d templates for each smoothing",
+                 n_removed.min(), n_removed.max())
+    # Sort the values to be smoothed by parameter value
+    nabove = nabove[par_sort]
+    invalphan = invalphan[par_sort]
+    ntotal = ntotal[par_sort]
+    logging.info("Smoothing ...")
+    slices = [slice(l,r) for l, r in zip(lefts, rights)]
+    for i in rang:
+        report_percentage(i, rang.max())
+        slc = slices[i]
+        d = dist(i, slc, parvals, args.smoothing_width)
+
+        smoothed_tuple = smooth(nabove[slc],
+                                invalphan[slc],
+                                ntotal[slc],
+                                d,
+                                args.smoothing_method,
+                                **kwarg_dict)
+        nabove_smoothed.append(smoothed_tuple[0])
+        alpha_smoothed.append(smoothed_tuple[1])
+        ntotal_smoothed.append(smoothed_tuple[2])
+
+    # Undo the sorts
+    unsort = numpy.argsort(par_sort)
+    parvals = [p[unsort] for p in parvals]
+    nabove_smoothed = numpy.array(nabove_smoothed)[unsort]
+    alpha_smoothed = numpy.array(alpha_smoothed)[unsort]
+    ntotal_smoothed = numpy.array(ntotal_smoothed)[unsort]
 
 else:
-    for i in range(len(nabove)):
+    logging.info("Smoothing ...")
+    for i in rang:
+        report_percentage(i, rang.max())
         d = dist(i, rang, parvals, args.smoothing_width)
         smoothed_tuple = smooth(nabove, invalphan, ntotal, d,
                                 args.smoothing_method, **kwarg_dict)
         nabove_smoothed.append(smoothed_tuple[0])
         alpha_smoothed.append(smoothed_tuple[1])
         ntotal_smoothed.append(smoothed_tuple[2])
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_over_param` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_over_param`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_fit_sngls_split_binned` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_fit_sngls_split_binned`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_followup_file` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_followup_file`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_foreground_censor` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_foreground_censor`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_get_loudest_params` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_get_loudest_params`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_merge_psds` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_merge_psds`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_plot_kde_vals` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_plot_kde_vals`

 * *Files 3% similar despite different names*

```diff
@@ -27,17 +27,17 @@
     if len(args.param) != 2:
         parser.error('For param_vs_param, give exactly two parameter names')
 if len(args.param) != len(args.log_axis):
     parser.error('Must specify either log (True) or non-log (False) for each parameter')
 
 if args.signal_file:
     signal_data = h5py.File(args.signal_file, 'r')
-    signal_kde = signal_data['signal_kde'][:]
+    signal_kde = signal_data['data_kde'][:]
 template_data = h5py.File(args.template_file, 'r')
-template_kde = template_data['template_kde'][:]
+template_kde = template_data['data_kde'][:]
 param0 = template_data[args.param[0]][:]
 if len(args.param) > 1:
     param1 = template_data[args.param[1]][:]
 
 if args.plot_type == 'kde_vs_param':
     fig, ax = plt.subplots(1, figsize=(12,7), constrained_layout=True)
     if args.which_kde == 'signal_kde':
@@ -59,18 +59,18 @@
         ax.set_yscale('linear')
     plot_loc = args.plot_dir + args.which_kde + '_vs_' + args.param[0] + '.png'
     plt.savefig(plot_loc)
 
 elif args.plot_type == 'param_vs_param':
     fig, ax = plt.subplots(1, figsize=(12,7), constrained_layout=True)
     if args.which_kde == 'signal_kde':
-        im = ax.scatter(param0, param1, marker=".", c=signal_kde, cmap='RdBu_r',
+        im = ax.scatter(param0, param1, marker=".", c=signal_kde, cmap='plasma',
                         s=5, norm=LogNorm())
     elif args.which_kde == 'template_kde':
-        im = ax.scatter(param0, param1, marker=".", c=template_kde, cmap='RdBu_r',
+        im = ax.scatter(param0, param1, marker=".", c=template_kde, cmap='plasma',
                         s=5, norm=LogNorm())
     elif args.which_kde == 'ratio_kde':
         im = ax.scatter(param0, param1, marker=".", c=signal_kde / template_kde,
                         cmap='RdBu_r', s=5, norm=LogNorm())
     else:
         raise RuntimeError('Unknown value of which_kde', args.which_kde)
     cbar = fig.colorbar(im, ax=ax)
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_reduce_template_bank` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_reduce_template_bank`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_rerank_dq` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_rerank_dq`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_rerank_passthrough` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_rerank_passthrough`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_findtrigs` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_findtrigs`

 * *Files 3% similar despite different names*

```diff
@@ -86,14 +86,16 @@
     seed(0)
     template_ids = np.arange(0, num_templates)
     shuffle(template_ids)
     template_ids = template_ids[tmin:tmax]
 else:
     template_ids = np.array(range(tmin, tmax))
 
+original_bank_len = len(template_ids)
+
 from pycbc.io.hdf import ReadByTemplate
 trigs = ReadByTemplate(trigger_file,
                        args.template_bank,
                        args.segment_name,
                        args.veto_files,
                        args.gating_veto_windows)
 logging.info("%d triggers in file", trigf[ifo + '/snr'].size)
@@ -110,43 +112,46 @@
     trigs.bank,
     template_cut_dict,
     statistic=rank_method,
     ifos=[ifo],
     template_ids=template_ids)
 
 logging.info("%d out of %d templates kept after applying template cuts",
-             len(template_ids), len(trigs.bank))
+             len(template_ids), original_bank_len)
 
 logging.info('Clustering events over %s s window within each template',
              args.cluster_window)
+
+extra_kwargs = {}
+for inputstr in args.statistic_keywords:
+    try:
+        key, value = inputstr.split(':')
+        extra_kwargs[key] = value
+    except ValueError:
+        err_txt = "--statistic-keywords must take input in the " \
+                  "form KWARG1:VALUE1 KWARG2:VALUE2 KWARG3:VALUE3 ... " \
+                  "Received {}".format(args.statistic_keywords)
+        raise ValueError(err_txt)
+
+
 for tnum in template_ids:
     tids_uncut = trigs.set_template(tnum)
 
-    trigger_keep_ids = cuts.apply_trigger_cuts(trigs, trigger_cut_dict)
+    trigger_keep_ids = cuts.apply_trigger_cuts(trigs, trigger_cut_dict,
+                                               statistic=rank_method)
     tids_full = tids_uncut[trigger_keep_ids]
     logging.info('%s:%s', tnum, len(tids_uncut))
     if len(tids_full) < len(tids_uncut):
         logging.info("%s triggers cut",
                      len(tids_uncut) - len(tids_full))
 
     n_tot_trigs = tids_full.size
     if not n_tot_trigs: continue
 
     # Stat class instance to calculate the ranking statistic
-    extra_kwargs = {}
-    for inputstr in args.statistic_keywords:
-        try:
-            key, value = inputstr.split(':')
-            extra_kwargs[key] = value
-        except ValueError:
-            err_txt = "--statistic-keywords must take input in the " \
-                      "form KWARG1:VALUE1 KWARG2:VALUE2 KWARG3:VALUE3 ... " \
-                      "Received {}".format(args.statistic_keywords)
-            raise ValueError(err_txt)
-
     sds = rank_method.single(trigs)[trigger_keep_ids]
     stat_t = rank_method.rank_stat_single((ifo, sds),
                                           **extra_kwargs)
     trigger_times = sds['end_time']
     if args.cluster_window:
         cid = coinc.cluster_over_time(stat_t, trigger_times,
                                       args.cluster_window)
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_pastro` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_pastro`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_statmap` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_statmap`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_sngls_statmap_inj` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_sngls_statmap_inj`

 * *Files 10% similar despite different names*

```diff
@@ -47,19 +47,14 @@
 parser.add_argument('--verbose', action='count')
 parser.add_argument('--cluster-window', type=float, default=10,
                     help='Length of time window in seconds to cluster coinc '
                          'events [default=10s]')
 parser.add_argument('--veto-window', type=float, default=.1,
                     help='Time around each zerolag trigger to window out '
                          '[default=.1s]')
-parser.add_argument('--background-removal-statistic', type=float,
-                    default=numpy.inf,
-                    help="Remove any triggers with statistic higher "
-                         "than this from the background. "
-                          "Default=None removed")
 significance.insert_significance_option_group(parser)
 parser.add_argument('--output-file')
 args = parser.parse_args()
 
 pycbc.init_logging(args.verbose)
 
 significance.check_significance_options(args, parser)
@@ -67,15 +62,15 @@
 logging.info("Loading triggers")
 logging.info("IFO input: %s" % args.ifos[0])
 all_trigs = pycbc.io.MultiifoStatmapData(files=args.sngls_files, ifos=args.ifos)
 ifo = args.ifos[0]
 assert ifo + '/time' in all_trigs.data
 
 logging.info("We have %s triggers" % len(all_trigs.stat))
-logging.info("Clustering coinc triggers (inclusive of zerolag)")
+logging.info("Clustering triggers")
 all_trigs = all_trigs.cluster(args.cluster_window)
 
 logging.info('getting background statistics')
 
 fb = h5py.File(args.full_data_background,'r')
 back_stat = fb['background/stat'][:]
 back_stat_exc = fb['background_exc/stat'][:]
@@ -109,47 +104,29 @@
 
 logging.info("Estimating FAN from background statistic values")
 # Ranking statistic of foreground and background
 fore_stat = all_trigs.stat[fore_locs]
 
 significance_dict = significance.digest_significance_options([ifo], args)
 
-# Cumulative array of inclusive background triggers and the number of
-# inclusive background triggers louder than each foreground trigger
-back_cnum, fnlouder = significance.get_n_louder(
-    back_stat,
-    fore_stat,
-    bkg_dec_facs,
-    **significance_dict[ifo])
-
 # Cumulative array of exclusive background triggers and the number
 # of exclusive background triggers louder than each foreground trigger
 back_cnum_exc, fnlouder_exc = significance.get_n_louder(
     back_stat_exc,
     fore_stat,
     bkg_exc_dec_facs,
     **significance_dict[ifo])
 
-fg_ifar = fg_time / (fnlouder + 1)
-bg_ifar = fg_time / (back_cnum + 1)
 fg_ifar_exc = fg_time_exc / (fnlouder_exc + 1)
 bg_ifar_exc = fg_time_exc / (back_cnum_exc + 1)
 
-f['background/ifar'] = conv.sec_to_year(bg_ifar)
 f['background_exc/ifar'] = conv.sec_to_year(bg_ifar_exc)
-f.attrs['background_time'] = fg_time
-f.attrs['foreground_time'] = fg_time
 f.attrs['background_time_exc'] = fg_time_exc
 f.attrs['foreground_time_exc'] = fg_time_exc
 
-logging.info("calculating foreground ifar/fap values")
-
-fap = 1 - numpy.exp(- fg_time / fg_ifar)
-f['foreground/ifar'] = conv.sec_to_year(fg_ifar)
-f['foreground/fap'] = fap
 fap_exc = 1 - numpy.exp(- fg_time_exc / fg_ifar_exc)
 f['foreground/ifar_exc'] = conv.sec_to_year(fg_ifar_exc)
 f['foreground/fap_exc'] = fap_exc
 
 if 'name' in all_trigs.attrs:
     f.attrs['name'] = all_trigs.attrs['name']
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_strip_injections` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_strip_injections`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_template_kde_calc` & `PyCBC-2.2.1/bin/minifollowups/pycbc_sngl_minifollowup`

 * *Files 26% similar despite different names*

```diff
@@ -1,207 +1,234 @@
-#!/usr/bin/env python
-
-# Copyright 2023 Jam Sadiq, Praveen Kumar
+#!/bin/env python
+# Copyright (C) 2015 Alexander Harvey Nitz, Ian Harry
 #
 # This program is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the
 # Free Software Foundation; either version 3 of the License, or (at your
 # option) any later version.
 #
 # This program is distributed in the hope that it will be useful, but
 # WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
 # Public License for more details.
+#
+# You should have received a copy of the GNU General Public License along
+# with this program; if not, write to the Free Software Foundation, Inc.,
+# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+""" Followup foreground events
+"""
+import os, argparse, logging
+import numpy
+from ligo.lw import table
+from ligo.lw import utils as ligolw_utils
+from pycbc.results import layout
+from pycbc.events import select_segments_by_definer
+import pycbc.workflow.minifollowups as mini
+import pycbc.version
+import pycbc.workflow as wf
+import pycbc.events
+from pycbc.workflow.core import resolve_url_to_file
+from pycbc.events import stat
+from pycbc.io import hdf
 
-import numpy, h5py, operator, argparse, logging
-from pycbc import init_logging
-import pycbc.conversions as convert
-from pycbc import libutils
-from pycbc.events import triggers
-akde = libutils.import_optional('awkde')
-kf = libutils.import_optional('sklearn.model_selection')
-
-
-parser = argparse.ArgumentParser(description=__doc__)
-parser.add_argument('--signal-file', help='File with parameters of GW signals '
-                    'for KDE calculation')
-parser.add_argument('--template-file', required=True, help='Hdf5 file with '
-                    'template masses and spins')
-parser.add_argument('--min-mass', type=float, default=None,
-                    help='Used only on signal masses: remove all' 
-                         'signal events with mass2 < min_mass')
-parser.add_argument('--nfold-signal', type=int,
-                    help='Number of k-folds for signal KDE cross validation')
-parser.add_argument('--nfold-template', type=int,
-                    help='Number of k-folds for template KDE cross validation')
-parser.add_argument('--fit-param', nargs='+', required=True,
-                    help='Parameters over which KDE is calculated')
-parser.add_argument('--log-param', nargs='+', choices=['True', 'False'], 
-                    required=True)
-parser.add_argument('--output-file', required=True, help='Name of .hdf output')
-parser.add_argument('--make-signal-kde', action='store_true')
-parser.add_argument('--make-template-kde', action='store_true')
-parser.add_argument('--fom-plot', help='Make a FOM plot for cross-validation'
-                    ' and save it as this file')
-parser.add_argument('--alpha-grid', type=float, nargs="+",
-                    help='Grid of choices of sensitivity parameter alpha for'
-                         ' local bandwidth')
-parser.add_argument('--bw-grid', type=float, nargs='+', 
-                    help='Grid of choices of global bandwidth')
-parser.add_argument('--verbose', action='count')
-args = parser.parse_args()
-init_logging(args.verbose)
-
-assert len(args.fit_param) == len(args.log_param)
-if args.make_signal_kde and args.make_template_kde:
-    parser.error("Choose only one option out of --make-signal-kde and \
-                 --make-template-kde")
-
-
-def kde_awkde(x, x_grid, alp=0.5, gl_bandwidth=None, ret_kde=False):
-    if gl_bandwidth is None:  # use default from awkde
-        kde = akde.GaussianKDE(alpha=alp, diag_cov=True)
-    else:
-        kde = akde.GaussianKDE(glob_bw=gl_bandwidth, alpha=alp, diag_cov=True)
+def add_wiki_row(outfile, cols):
+    """
+    Adds a wiki-formatted row to an output file from a list or a numpy array.
+    """
+    with open(outfile, 'a') as f:
+        f.write('||%s||\n' % '||'.join(map(str,cols)))
 
-    kde.fit(x)
-    y = kde.predict(x_grid)
+parser = argparse.ArgumentParser(description=__doc__[1:])
+parser.add_argument('--version', action='version', version=pycbc.version.git_verbose_msg)
+parser.add_argument('--bank-file',
+                    help="HDF format template bank file")
+parser.add_argument('--single-detector-file',
+                    help="HDF format merged single detector trigger files")
+parser.add_argument('--instrument', help="Name of interferometer e.g. H1")
+parser.add_argument('--veto-file',
+    help="The veto file to be used if vetoing triggers (optional).")
+parser.add_argument('--veto-segment-name',
+    help="If using veto file must also provide the name of the segment to use "
+         "as a veto.")
+parser.add_argument('--inspiral-segments',
+                    help="xml segment file containing the inspiral analysis "
+                         "times")
+parser.add_argument('--inspiral-data-read-name',
+                    help="Name of inspiral segmentlist containing data read in "
+                         "by each analysis job.")
+parser.add_argument('--inspiral-data-analyzed-name',
+                    help="Name of inspiral segmentlist containing data "
+                         "analyzed by each analysis job.")
+parser.add_argument('--min-snr', type=float, default=6.5,
+                    help="Minimum SNR to consider for loudest triggers")
+parser.add_argument('--non-coinc-time-only', default=False,
+                    action='store_true',
+                    help="If given remove (veto) single-detector triggers "
+                         "that occur during a time when at least one other "
+                         "instrument is taking science data.")
+parser.add_argument('--minimum-duration', default=None, type=float,
+                    help="If given only consider single-detector triggers "
+                         "with template duration larger than this.")
+parser.add_argument('--maximum-duration', default=None, type=float,
+                    help="If given only consider single-detector triggers "
+                         "with template duration smaller than this.")
+wf.add_workflow_command_line_group(parser)
+wf.add_workflow_settings_cli(parser, include_subdax_opts=True)
+stat.insert_statistic_option_group(parser,
+    default_ranking_statistic='single_ranking_only')
+args = parser.parse_args()
 
-    if ret_kde == True:
-        return kde, y
-    return y
+logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s',
+                    level=logging.INFO)
 
+workflow = wf.Workflow(args)
+workflow.ifos = [args.instrument]
+workflow.ifo_string = args.instrument
+
+wf.makedir(args.output_dir)
+
+# create a FileList that will contain all output files
+layouts = []
+
+tmpltbank_file = resolve_url_to_file(os.path.abspath(args.bank_file))
+sngl_file = resolve_url_to_file(
+    os.path.abspath(args.single_detector_file),
+    attrs={'ifos': args.instrument}
+)
+
+if args.veto_file is not None:
+    veto_file = resolve_url_to_file(
+        os.path.abspath(args.veto_file),
+        attrs={'ifos': args.instrument}
+    )
+else:
+    veto_file = None
+insp_segs = resolve_url_to_file(os.path.abspath(args.inspiral_segments))
+insp_data_seglists = select_segments_by_definer\
+        (args.inspiral_segments, segment_name=args.inspiral_data_read_name,
+         ifo=args.instrument)
+insp_data_seglists.coalesce()
+
+num_events = int(workflow.cp.get_opt_tags('workflow-sngl_minifollowups',
+                 'num-sngl-events', ''))
+
+# This helps speed up the processing to ignore a large fraction of triggers
+snr_mask = None
+if args.min_snr:
+    logging.info('Calculating Prefilter')
+    f = hdf.HFile(args.single_detector_file, 'r')
+    idx, _ = f.select(lambda snr: snr > args.min_snr,
+                      '{}/snr'.format(args.instrument),
+                      return_indices=True)
+    snr_mask = numpy.zeros(len(f['{}/snr'.format(args.instrument)]),
+                           dtype=bool)
+    snr_mask[idx] = True
+
+trigs = hdf.SingleDetTriggers(args.single_detector_file, args.bank_file,
+                              args.veto_file, args.veto_segment_name,
+                              None, args.instrument, premask=snr_mask)
+
+if args.non_coinc_time_only:
+    from pycbc.io.ligolw import LIGOLWContentHandler as h
+
+    segs_doc = ligolw_utils.load_filename(args.inspiral_segments,
+                                          contenthandler=h)
+    seg_def_table = table.Table.get_table(segs_doc, 'segment_definer')
+    def_ifos = seg_def_table.getColumnByName('ifos')
+    def_ifos = [str(ifo) for ifo in def_ifos]
+    ifo_list = list(set(def_ifos))
+    ifo_list.remove(args.instrument)
+    for ifo in ifo_list:
+        curr_veto_mask, segs = pycbc.events.veto.indices_outside_segments(
+            trigs.end_time, [args.inspiral_segments],
+            ifo=ifo, segment_name=args.inspiral_data_analyzed_name)
+        curr_veto_mask.sort()
+        trigs.apply_mask(curr_veto_mask)
+
+if args.minimum_duration is not None:
+    logging.info('applying minimum duration')
+    durations = trigs.template_duration
+    lgc_mask = durations > args.minimum_duration
+    trigs.apply_mask(lgc_mask)
+    logging.info('remaining triggers: %s', trigs.mask.sum())
+
+if args.maximum_duration is not None:
+    logging.info('applying maximum duration')
+    durations = trigs.template_duration
+    lgc_mask = durations < args.maximum_duration
+    trigs.apply_mask(lgc_mask)
+    logging.info('remaining triggers: %s', trigs.mask.sum())
+
+logging.info('finding loudest clustered events')
+rank_method = stat.get_statistic_from_opts(args, [args.instrument])
+trigs.mask_to_n_loudest_clustered_events(rank_method, n_loudest=num_events)
+
+if len(trigs.stat) < num_events:
+    num_events = len(trigs.stat)
+
+times = trigs.end_time
+tids = trigs.template_id
+
+# loop over number of loudest events to be followed up
+order = trigs.stat.argsort()[::-1]
+for rank, num_event in enumerate(order):
+    logging.info('Processing event: %s', num_event)
+
+    files = wf.FileList([])
+    time = times[num_event]
+    ifo_time = '%s:%s' %(args.instrument, str(time))
+    tid = trigs.mask[num_event]
+    ifo_tid = '%s:%s' %(args.instrument, str(tid))
+
+    layouts += (mini.make_sngl_ifo(workflow, sngl_file, tmpltbank_file,
+                                   tid, args.output_dir, args.instrument,
+                                   tags=args.tags + [str(rank)]),)
+    files += mini.make_trigger_timeseries(workflow, [sngl_file],
+                              ifo_time, args.output_dir, special_tids=ifo_tid,
+                              tags=args.tags + [str(rank)])
+    curr_params = {}
+    curr_params['mass1'] = trigs.mass1[num_event]
+    curr_params['mass2'] = trigs.mass2[num_event]
+    curr_params['spin1z'] = trigs.spin1z[num_event]
+    curr_params['spin2z'] = trigs.spin2z[num_event]
+    curr_params['f_lower'] = trigs.f_lower[num_event]
+    curr_params[args.instrument + '_end_time'] = time
+    # don't require precessing template info if not present
+    try:
+        curr_params['spin1x'] = trigs.spin1x[num_event]
+        curr_params['spin2x'] = trigs.spin2x[num_event]
+        curr_params['spin1y'] = trigs.spin1y[num_event]
+        curr_params['spin2y'] = trigs.spin2y[num_event]
+        curr_params['inclination'] = trigs.inclination[num_event]
+    except KeyError:
+        pass
+    try:
+        # Only present for precessing search
+        curr_params['u_vals'] = trigs.u_vals[num_event]
+    except:
+        pass
+
+    files += mini.make_single_template_plots(workflow, insp_segs,
+                            args.inspiral_data_read_name,
+                            args.inspiral_data_analyzed_name, curr_params,
+                            args.output_dir,
+                            tags=args.tags+[str(rank)])
+
+    files += mini.make_plot_waveform_plot(workflow, curr_params,
+                                        args.output_dir, [args.instrument],
+                                        tags=args.tags + [str(rank)])
+
+
+    files += mini.make_singles_timefreq(workflow, sngl_file, tmpltbank_file,
+                            time, args.output_dir,
+                            data_segments=insp_data_seglists,
+                            tags=args.tags + [str(rank)])
+
+    files += mini.make_qscan_plot(workflow, args.instrument, time,
+                                  args.output_dir,
+                                  data_segments=insp_data_seglists,
+                                  tags=args.tags + [str(rank)])
 
-def kfcv_awkde(sample, bwchoice, alphachoice, k=2):
-    """
-    Evaluate the K-fold cross validated log likelihood for an awKDE with
-    specific bandwidth and sensitivity (alpha) parameters
-    """
-    fomlist = []
-    kfold = kf.KFold(n_splits=k, shuffle=True, random_state=None)
-    for train_index, test_index in kfold.split(sample):
-        train, test = sample[train_index], sample[test_index]
-        y = kde_awkde(train, test, alp=alphachoice, gl_bandwidth=bwchoice)
-        # Figure of merit : log likelihood for training samples
-        fomlist.append(numpy.sum(numpy.log(y)))
-
-    # Return the sum over all K sets of training samples
-    return numpy.sum(fomlist)
-
-
-def optimizedparam(sampleval, bwgrid, alphagrid, nfold=2):
-    npoints, ndim = sampleval.shape
-    FOM = {}
-    for gbw in bwgrid:
-        for alphavals in alphagrid:
-            FOM[(gbw, alphavals)] = kfcv_awkde(sampleval, gbw, alphavals, \
-                                               k=nfold)
-    optval = max(FOM.items(), key=operator.itemgetter(1))[0]
-    optbw, optalpha = optval[0], optval[1]
-    maxFOM = FOM[(optbw, optalpha)]
-
-    # Plotting FOM parameters
-    if args.fom_plot:
-        import matplotlib.pyplot as plt
-        fig = plt.figure(figsize=(12,8))
-        ax = fig.add_subplot(111)
-        for bw in bwgrid:
-            FOMlist = [FOM[(bw, al)] for al in alphagrid]
-            ax.plot(alphagrid, FOMlist, label='{0:.3f}'.format(bw))
-        ax.plot(optalpha, maxFOM, 'ko', linewidth=10, label= \
-                r'$\alpha={0:.3f},bw={1:.3f}$'.format(optalpha, optbw))
-        ax.set_xlabel(r'$\alpha$', fontsize=15)
-        ax.set_ylabel(r'$FOM$', fontsize=15)
-        # Guess at a suitable range of FOM values to plot
-        ax.set_ylim(maxFOM - 0.5 * npoints, maxFOM + 0.2 * npoints)
-        ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.135), ncol=8)
-        plt.savefig(args.fom_plot)
-        plt.close()
-
-    return optbw, optalpha
-
-
-# Obtaining template parameters
-temp_file = h5py.File(args.template_file, 'r')
-mass1 = temp_file['mass1']
-tid = numpy.arange(len(mass1))  # Array of template ids
-mass_spin = triggers.get_mass_spin(temp_file, tid)
-
-f_dest = h5py.File(args.output_file, 'w')
-f_dest.create_dataset("template_id", data=tid)
-template_pars = []
-for param, slog in zip(args.fit_param, args.log_param):
-    pvals = triggers.get_param(param, args, *mass_spin)
-    # Write the KDE param values to output file
-    f_dest.create_dataset(param, data=pvals)
-    if slog in ['False']:
-        logging.info('Using param: %s', param)
-        template_pars.append(pvals)
-    elif slog in ['True']:
-        logging.info('Using log param: %s', param)
-        template_pars.append(numpy.log(pvals))
-    else:
-        raise ValueError("invalid log param argument, use 'True', or 'False'")
-
-# Copy standard data to output file
-f_dest.attrs['fit_param'] = args.fit_param
-f_dest.attrs['log_param'] = args.log_param
-with h5py.File(args.template_file, "r") as f_src:
-    f_src.copy(f_src["./"], f_dest["./"], "input_template_params")
-temp_samples = numpy.vstack((template_pars)).T
-
-if args.make_template_kde:
-    logging.info('Starting optimization of template KDE parameters')
-    optbw, optalpha = optimizedparam(temp_samples, alphagrid=args.alpha_grid,\
-                              bwgrid=args.bw_grid, nfold=args.nfold_template)
-    logging.info('Bandwidth %.4f, alpha %.2f' % (optbw, optalpha))
-    logging.info('Evaluating template KDE')
-    template_kde = kde_awkde(temp_samples, temp_samples, alp=optalpha, \
-                             gl_bandwidth=optbw)
-    f_dest.create_dataset("data_kde", data=template_kde)
-    f_dest.attrs['stat'] = "template-kde_file"
-
-# Obtaining signal parameters
-if args.make_signal_kde:
-    signal_pars = []
-    signal_file = numpy.genfromtxt(args.signal_file, dtype=float,
-                                   delimiter=',', names=True)
-    mass2_sgnl = signal_file['mass2']
-    N_original = len(mass2_sgnl)
-    if args.min_mass:
-        idx = mass2_sgnl > args.min_mass
-        mass2_sgnl = mass2_sgnl[idx]
-        logging.info('%i triggers out of %i with MASS2 > %s' %
-                         (len(mass2_sgnl), N_original, str(args.min_mass)))
-    else:
-        idx = numpy.full(N_original, True)
-    mass1_sgnl = signal_file['mass1'][idx]
-    assert min(mass1_sgnl - mass2_sgnl) > 0
- 
-    for param, slog in zip(args.fit_param, args.log_param):
-        pvals = signal_file[param][idx]
-        if slog in ['False']:
-            logging.info('Using param: %s', param)
-            signal_pars.append(pvals)
-        elif slog in ['True']:
-            logging.info('Using log param: %s', param)
-            signal_pars.append(numpy.log(pvals))
-        else:
-            raise ValueError("invalid log param argument, use 'True', \
-                             or 'False'")
-    
-    signal_samples = numpy.vstack((signal_pars)).T
-    logging.info('Starting optimization of signal KDE parameters')  
-    optbw, optalpha = optimizedparam(signal_samples, bwgrid=args.bw_grid, \
-                      alphagrid=args.alpha_grid, nfold=args.nfold_signal)
-    logging.info('Bandwidth %.4f, alpha %.2f' % (optbw, optalpha))
-    logging.info('Evaluating signal KDE')
-    signal_kde = kde_awkde(signal_samples, temp_samples, \
-                           alp=optalpha, gl_bandwidth=optbw)
-    f_dest.create_dataset("data_kde", data=signal_kde)
-    f_dest.attrs['stat'] = "signal-kde_file"
-
-f_dest.attrs['alpha'] = optalpha
-f_dest.attrs['bandwidth'] = optbw
-f_dest.close()
+    layouts += list(layout.grouper(files, 2))
 
-logging.info('Done!')
+workflow.save()
+layout.two_column_layout(args.output_dir, layouts)
```

### Comparing `PyCBC-2.2.0/bin/all_sky_search/pycbc_template_recovery_hist` & `PyCBC-2.2.1/bin/all_sky_search/pycbc_template_recovery_hist`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_aligned_bank_cat` & `PyCBC-2.2.1/bin/bank/pycbc_aligned_bank_cat`

 * *Files 19% similar despite different names*

```diff
@@ -26,15 +26,15 @@
 import argparse
 import numpy
 import pycbc.version
 import h5py
 from ligo.lw import utils
 from pycbc import tmpltbank
 # Old ligolw output functions no longer imported at package level
-import pycbc.tmpltbank.bank_output_utils as llw_output
+import pycbc.tmpltbank.bank_output_utils as bank_output
 import pycbc
 import pycbc.psd
 import pycbc.strain
 import pycbc.version
 from pycbc.waveform import get_waveform_filter_length_in_time
 from pycbc.io.ligolw import LIGOLWContentHandler
 
@@ -51,34 +51,33 @@
 parser.add_argument("--version", action="version", version=__version__)
 parser.add_argument("--verbose", action="store_true", default=False,
                     help="verbose output")
 parser.add_argument("-i", "--input-glob",
                     help="file glob the list of paramters")
 parser.add_argument("-I", "--input-files", nargs='+',
                     help="Explicit list of input files.")
-parser.add_argument("-o", "--output-file",  help="Output file name")
-parser.add_argument('--f-low-column', type=str, metavar='NAME',
-                    help='If given, store the lower frequency cutoff into '
-                         'column NAME of the single-inspiral table.')
-parser.add_argument("-m", "--metadata-file", metavar="METADATA_FILE",
+parser.add_argument("--metadata-file", metavar="METADATA_FILE",
                   help="XML file containing the process and process_params "
                   "tables that the aligned_bank code was run with.")
 
+
 # Insert the metric calculation options
 tmpltbank.insert_metric_calculation_options(parser)
 
 # Insert the PSD options
 pycbc.psd.insert_psd_option_group(parser)
 
 # Insert the data reading options
 pycbc.strain.insert_strain_option_group(parser)
 
 # Add the ethinca calculation options
 tmpltbank.insert_ethinca_metric_options(parser)
 
+tmpltbank.insert_base_bank_options(parser, match_req=False)
+
 options = parser.parse_args()
 
 if options.verbose:
     log_level = logging.DEBUG
 else:
     log_level = logging.WARN
 logging.basicConfig(format='%(asctime)s %(message)s', level=log_level)
@@ -161,40 +160,17 @@
 # moments (or read them in) and use the correct value of f0 and pn-order
 if options.metadata_file:
     outdoc = utils.load_filename(options.metadata_file,
                                  compress='auto',
                                  contenthandler=LIGOLWContentHandler)
 else:
     outdoc = None
-if options.output_file.endswith(('.xml','.xml.gz','.xmlgz')):
-    llw_output.output_sngl_inspiral_table(
-        options.output_file,
-        temp_bank,
-        metricParams,
-        ethincaParams,
-        programName=__program__,
-        optDict=options.__dict__,
-        outdoc=outdoc
-    )
-elif options.output_file.endswith(('.h5','.hdf','.hdf5')):
-    out_fp = h5py.File(options.output_file, 'w')
-    out_fp['mass1'] = temp_bank[:,0]
-    out_fp['mass2'] = temp_bank[:,1]
-    out_fp['spin1z'] = temp_bank[:,2]
-    out_fp['spin2z'] = temp_bank[:,3]
-    out_fp['f_lower'] = [options.f_low] * len(temp_bank[:,0])
-    tmplt_durations = []
-    for i in range(len(temp_bank[:,0])):
-        gwflit = get_waveform_filter_length_in_time
-        wfrm_length = gwflit('SPAtmplt', mass1=temp_bank[i,0],
-                             mass2=temp_bank[i,1], f_lower=options.f_low,
-                             phase_order=7)
-        tmplt_durations.append(wfrm_length)
-    out_fp['template_duration'] = tmplt_durations
-    approx = numpy.zeros(len(temp_bank[:,0]),
-                         dtype=h5py.string_dtype(encoding='utf-8'))
-    approx[:] = ['TaylorF2']
-    out_fp['approximant'] = approx
-    out_fp.close()
-else:
-    err_msg = "Unrecognized extension for file {}.".format(options.output_file)
-    raise ValueError(err_msg)
+
+bank_output.output_bank_to_file(
+    options.output_file,
+    temp_bank,
+    programName=__program__,
+    output_duration=True,
+    approximant="TaylorF2",
+    optDict=options.__dict__,
+    outdoc=outdoc
+)
```

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_aligned_stoch_bank` & `PyCBC-2.2.1/bin/bank/pycbc_aligned_stoch_bank`

 * *Files 2% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 import argparse
 import numpy
 import logging
 import pycbc
 import pycbc.version
 from pycbc import tmpltbank
 # Old ligolw output functions no longer imported at package level
-import pycbc.tmpltbank.bank_output_utils as llw_output
+import pycbc.tmpltbank.bank_output_utils as bank_output
 from pycbc.types import positive_float
 import pycbc.psd
 import pycbc.strain
 from pycbc.pnutils import named_frequency_cutoffs
 
 
 __author__  = "Ian Harry <ian.harry@astro.cf.ac.uk>"
@@ -177,21 +177,21 @@
 
 # Choose the frequency values to use for metric calculation
 if opts.vary_fupper==False:
     if ethincaParams.doEthinca==False:
         refFreq = metricParams.fUpper
     else:
         # use the maximum frequency for which the moments were calculated
-        fs = numpy.array(metricParams.evals.keys(), dtype=float)
+        fs = numpy.array(list(metricParams.evals.keys()), dtype=float)
         fs.sort()
         refFreq = fs.max()
 else:
     # determine upper frequency cutoffs corresponding to the min and max
     # total masses
-    fs = numpy.array(metricParams.evals.keys(), dtype=float)
+    fs = numpy.array(list(metricParams.evals.keys()), dtype=float)
     fs.sort()
     lowEve, highEve = tmpltbank.find_max_and_min_frequencies(\
                               opts.bank_fupper_formula, massRangeParams, fs)
     refFreq = lowEve
     fs = fs[fs >= lowEve]
     fs = fs[fs <= highEve]
 
@@ -293,17 +293,15 @@
 logging.info("Outputting bank")
 
 # Put the whole template bank in one list
 mass1, mass2, spin1z, spin2z = partitioned_bank_object.output_all_points()
 tempBank = zip(mass1, mass2, spin1z, spin2z)
 
 # Output to file
-llw_output.output_sngl_inspiral_table(
+bank_output.output_bank_to_file(
     opts.output_file,
     tempBank,
-    metricParams,
-    ethincaParams,
     programName=__program__,
     optDict=opts.__dict__
 )
 
 logging.info("Done")
```

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_bank_verification` & `PyCBC-2.2.1/bin/bank/pycbc_bank_verification`

 * *Files 0% similar despite different names*

```diff
@@ -155,15 +155,15 @@
 
 # Choose the frequency values to use for metric calculation
 if opts.vary_fupper==False:
     refFreq = metricParams.fUpper
 else:
     # determine upper frequency cutoffs corresponding to the min and max 
     # total masses
-    fs = numpy.array(metricParams.evals.keys(), dtype=float)
+    fs = numpy.array(list(metricParams.evals.keys()), dtype=float)
     fs.sort()
     lowEve, highEve = tmpltbank.find_max_and_min_frequencies(\
                               opts.bank_fupper_formula, massRangeParams, fs)
     refFreq = lowEve
     fs = fs[fs >= lowEve]
     fs = fs[fs <= highEve]
```

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_brute_bank` & `PyCBC-2.2.1/bin/bank/pycbc_brute_bank`

 * *Files 2% similar despite different names*

```diff
@@ -88,16 +88,15 @@
     else:
         waveform_transforms = None
 
     dists_joint = prior_from_config(cp=config_parser)
 
 fdict = {}
 if args.fixed_params:
-    for p, v in zip(args.fixed_params, args.fixed_values):
-        fdict[p] = v
+    fdict = {p: v for (p, v) in zip(args.fixed_params, args.fixed_values)}
 
 class Shrinker(object):
     def __init__(self, data):
         self.data = data
 
     def pop(self):
         if len(self.data) == 0:
@@ -317,26 +316,24 @@
 if args.input_file:
     f = h5py.File(args.input_file, 'r')
     params = {k: f[k][:] for k in f}
     bank, _ = bank.check_params(gen, params, args.minimal_match)
 
 
 def draw(rtype):
-    params = {}
 
     if rtype == 'uniform':
         if args.input_config is None:
-            for name, pmin, pmax in zip(args.params, args.min, args.max):
-                params[name] = numpy.random.uniform(pmin, pmax, size=size)
+            params = {name: numpy.random.uniform(pmin, pmax, size=size)
+                      for name, pmin, pmax in zip(args.params, args.min, args.max)}
         else:
             # `draw_samples_from_config` has its own fixed seed, so must overwrite it.
             random_seed = numpy.random.randint(low=0, high=2**32-1)
             samples = draw_samples_from_config(args.input_config, size, random_seed)
-            for name in samples.fieldnames:
-                params[name] = samples[name]
+            params = {name: samples[name] for name in samples.fieldnames}
             # Add `static_args` back.
             if static_args is not None:
                 for k in static_args.keys():
                     params[k] = numpy.array([static_args[k]]*size)
 
     elif rtype == 'kde':
         trail = 300
@@ -347,16 +344,15 @@
         p.remove('approximant')
         if args.input_config is not None:
             p = variable_args
         bdata = numpy.array([bank.key(k)[-trail:] for k in p])
 
         kde = gaussian_kde(bdata)
         points = kde.resample(size=size)
-        for k, v in zip(p, points):
-            params[k] = v
+        params = {k: v for k, v in zip(p, points)}
 
         # Add `static_args` back, some transformations may need them.
         if args.input_config is not None and static_args is not None:
             for k in static_args.keys():
                 params[k] = numpy.array([static_args[k]]*size)
 
         # Apply `waveform_transforms` defined in the .ini file to samples.
@@ -388,43 +384,39 @@
             from pycbc.conversions import mchirp_from_mass1_mass2
             mc = mchirp_from_mass1_mass2(params['mass1'], params['mass2'])
             l &= mc > args.min_mchirp
 
     else:
         l = dists_joint.contains(params)
 
-    for k in params:
-        params[k] = params[k][l]
+    params = {k: params[k][l] for k in params}
 
     return params
 
 def cdraw(rtype, ts, te):
     from pycbc.conversions import tau0_from_mass1_mass2
 
     p = draw(rtype)
     if  len(p[list(p.keys())[0]]) > 0:
         t = tau0_from_mass1_mass2(p['mass1'], p['mass2'],
                                   args.tau0_cutoff_frequency)
         l = (t < te) & (t > ts)
-        for k in p:
-            p[k] = p[k][l]
+        p = {k: p[k][l] for k in p}
 
     i = 0
     while len(p[list(p.keys())[0]]) < size:
 
         tp = draw(rtype)
-        for k in p:
-            p[k] = numpy.concatenate([p[k], tp[k]])
+        p = {k: numpy.concatenate([p[k], tp[k]]) for k in p}
 
         if  len(p[list(p.keys())[0]]) > 0:
             t = tau0_from_mass1_mass2(p['mass1'], p['mass2'],
                                       args.tau0_cutoff_frequency)
             l = (t < te) & (t > ts)
-            for k in p:
-                p[k] = p[k][l]
+            p = {k: p[k][l] for k in p}
 
         i += 1
         if i > 1000:
             break
 
     if len(p[list(p.keys())[0]]) == 0:
         return None
```

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_coinc_bank2hdf` & `PyCBC-2.2.1/bin/bank/pycbc_coinc_bank2hdf`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_geom_aligned_2dstack` & `PyCBC-2.2.1/bin/bank/pycbc_geom_aligned_2dstack`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_geom_aligned_bank` & `PyCBC-2.2.1/bin/bank/pycbc_geom_aligned_bank`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_geom_nonspinbank` & `PyCBC-2.2.1/bin/bank/pycbc_geom_nonspinbank`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 import logging
 import pycbc
 import pycbc.version
 import pycbc.psd
 import pycbc.strain
 from pycbc import tmpltbank
 # Old ligolw output functions no longer imported at package level
-import pycbc.tmpltbank.bank_output_utils as llw_output
+import pycbc.tmpltbank.bank_output_utils as bank_output
 from pycbc import pnutils
 
 
 __author__  = "Ian Harry <ian.harry@ligo.org>"
 __version__ = pycbc.version.git_verbose_msg
 __date__    = pycbc.version.date
 __program__ = "pycbc_geom_nonspinbank"
@@ -372,22 +372,17 @@
 
     tempBank.append([masses[0], masses[1], masses[2], masses[3]])
 
     # This can be used for debugging
     # print >>fileP, masses[0],masses[1],masses[2],masses[3],masses[4],\
     #  masses[5],masses[6],masses[7],masses[8],masses[9],v1,v2
 
-logging.info("Writing output")
+logging.info("Writing output to file %s", opts.output_file)
 
-# Currently this is hardcoded to dump as a sngl_inspiral table. But this is
-# easily changed.
-
-llw_output.output_sngl_inspiral_table(
+bank_output.output_bank_to_file(
     opts.output_file,
     tempBank,
-    metricParams,
-    ethincaParams,
     programName=__program__,
     optDict=opts.__dict__
 )
 
 logging.info("Done")
```

### Comparing `PyCBC-2.2.0/bin/bank/pycbc_tmpltbank_to_chi_params` & `PyCBC-2.2.1/bin/bank/pycbc_tmpltbank_to_chi_params`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/hwinj/pycbc_generate_hwinj` & `PyCBC-2.2.1/bin/hwinj/pycbc_generate_hwinj`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/hwinj/pycbc_generate_hwinj_from_xml` & `PyCBC-2.2.1/bin/hwinj/pycbc_generate_hwinj_from_xml`

 * *Files 2% similar despite different names*

```diff
@@ -45,15 +45,15 @@
 parser.add_argument('--injection-file', type=str, required=True,
              help='Path to the LIGOLW XML file that contains a sim_inspiral table.')
 parser.add_argument('--sample-rate', type=int, required=True,
              help='Sample rate that waveforms will be generated.')
 parser.add_argument("--tag", type=str, default='hwinjcbcsimid',
                     help="Prefix added to output filenames.")
 parser.add_argument('--ifos', nargs='+', default=['H1', 'L1'], required=True,
-                    choices=list(zip(*get_available_detectors()))[0],
+                    choices=get_available_detectors(),
                     help='List of IFOs to generate injections for.')
 # parse command line
 opts = parser.parse_args()
 
 # setup log
 logging_level = logging.DEBUG
 logging.basicConfig(format='%(asctime)s : %(message)s', level=logging_level)
```

### Comparing `PyCBC-2.2.0/bin/hwinj/pycbc_insert_frame_hwinj` & `PyCBC-2.2.1/bin/hwinj/pycbc_insert_frame_hwinj`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/hwinj/pycbc_plot_hwinj` & `PyCBC-2.2.1/bin/hwinj/pycbc_plot_hwinj`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference` & `PyCBC-2.2.1/bin/inference/pycbc_inference`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_create_fits` & `PyCBC-2.2.1/bin/inference/pycbc_inference_create_fits`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_extract_samples` & `PyCBC-2.2.1/bin/inference/pycbc_inference_extract_samples`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_model_stats` & `PyCBC-2.2.1/bin/inference/pycbc_inference_model_stats`

 * *Files 1% similar despite different names*

```diff
@@ -47,15 +47,15 @@
                          "Otherwise, an OSError is raised.")
 parser.add_argument("--nprocesses", type=int, default=1,
                     help="Number of processes to use. If not given then only "
                          "a single core will be used.")
 parser.add_argument("--config-file", nargs="+", type=str, default=None,
                     help="Override the config file stored in the input file "
                           "with the given file(s).")
-parser.add_argument("--verbose", action="store_true", default=False,
+parser.add_argument("--verbose", action="count", default=0,
                     help="Print logging messages.")
 parser.add_argument('--reconstruct-parameters', action="store_true",
                     help="Reconstruct marginalized parameters")
 
 
 # parse command line
 opts = parser.parse_args()
```

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_monitor` & `PyCBC-2.2.1/bin/inference/pycbc_inference_monitor`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_acceptance_rate` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_acceptance_rate`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_acf` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_acf`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_acl` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_acl`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_dynesty_run` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_dynesty_run`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_dynesty_traceplot` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_dynesty_traceplot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_gelman_rubin` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_gelman_rubin`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_geweke` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_geweke`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_inj_recovery` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_inj_recovery`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_mcmc_history` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_mcmc_history`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_movie` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_movie`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_posterior` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_posterior`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_pp` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_pp`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_prior` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_prior`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_samples` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_samples`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_skymap` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_skymap`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_plot_thermodynamic_integrand` & `PyCBC-2.2.1/bin/inference/pycbc_inference_plot_thermodynamic_integrand`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_pp_table_summary` & `PyCBC-2.2.1/bin/inference/pycbc_inference_pp_table_summary`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_start_from_samples` & `PyCBC-2.2.1/bin/inference/pycbc_inference_start_from_samples`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_inference_table_summary` & `PyCBC-2.2.1/bin/inference/pycbc_inference_table_summary`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/inference/pycbc_validate_test_posterior` & `PyCBC-2.2.1/bin/inference/pycbc_validate_test_posterior`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/live/pycbc_live_combine_single_fits` & `PyCBC-2.2.1/bin/live/pycbc_live_combine_single_fits`

 * *Files 6% similar despite different names*

```diff
@@ -56,15 +56,16 @@
 
     sngl_rank = fit_f0.attrs['sngl_ranking']
     fit_thresh = fit_f0.attrs['fit_threshold']
     fit_func = fit_f0.attrs['fit_function']
 
 live_times = {ifo : [] for ifo in args.ifos}
 
-trigger_file_times = []
+trigger_file_starts = []
+trigger_file_ends = []
 
 n_files = len(args.trfits_files)
 logging.info("Checking through %d files", n_files)
 
 for f in args.trfits_files:
 
     fits_f = h5py.File(f, 'r')
@@ -73,24 +74,26 @@
 
     assert fits_f.attrs['sngl_ranking'] == sngl_rank
     assert fits_f.attrs['fit_threshold'] == fit_thresh
     assert fits_f.attrs['fit_function'] == fit_func
     assert all(fits_f['bins_lower'][:] == bl)
     assert all(fits_f['bins_upper'][:] == bu)
 
-    # Get the time of the last trigger in the trigger_fits file
+    # Get the time of the first/last triggers in the trigger_fits file
     gps_last = 0
+    gps_first = np.inf
     for ifo in args.ifos:
         if ifo not in fits_f:
             continue
         else:
             trig_times = fits_f[ifo]['triggers']['end_time'][:]
-            gps_last = max(gps_last,
-                           trig_times.max())
-    trigger_file_times.append(gps_last)
+            gps_last = max(gps_last, trig_times.max())
+            gps_first = min(gps_first, trig_times.min())
+    trigger_file_starts.append(gps_first)
+    trigger_file_ends.append(gps_last)
 
     for ifo in args.ifos:
         if ifo not in fits_f:
             live_times[ifo].append(0)
             counts_all[ifo].append(-1 * np.ones_like(bl))
             alphas_all[ifo].append(-1 * np.ones_like(bl))
         else:
@@ -98,39 +101,39 @@
             counts_all[ifo].append(fits_f[ifo + '/counts'][:])
             alphas_all[ifo].append(fits_f[ifo + '/fit_coeff'][:])
             if any(np.isnan(fits_f[ifo + '/fit_coeff'][:])):
                 logging.info("nan in " + f + ", " + ifo)
                 logging.info(fits_f[ifo + '/fit_coeff'][:])
     fits_f.close()
 
-# Set up the date array, this is stored as an offset from the start date of
-# the combination to the end of the trigger_fits file.
-# This allows for missing or empty days
-
-trigger_file_times = np.array(trigger_file_times)
-ad_order = np.argsort(np.array(trigger_file_times))
-start_date_n = trigger_file_times[ad_order[0]]
-ad = trigger_file_times[ad_order] - start_date_n
+# Set up the date array, this is stored as an offset from the first trigger time of
+# the first file to the last trigger of the file
 
-# Get the counts and alphas sorted by bin rather than by date
+trigger_file_starts = np.array(trigger_file_starts)
+trigger_file_ends = np.array(trigger_file_ends)
+ad_order = np.argsort(trigger_file_starts)
+start_time_n = trigger_file_starts[ad_order[0]]
+ad = trigger_file_ends[ad_order] - start_time_n
+
+# Get the counts and alphas
 counts_bin = {ifo: [c for c in zip(*counts_all[ifo])] for ifo in args.ifos}
 alphas_bin = {ifo: [a for a in zip(*alphas_all[ifo])] for ifo in args.ifos}
 
 alphas_out = {ifo : np.zeros(len(alphas_bin[ifo])) for ifo in args.ifos}
 counts_out = {ifo : np.inf * np.ones(len(counts_bin[ifo])) for ifo in args.ifos}
 cons_alphas_out = {ifo : np.zeros(len(alphas_bin[ifo])) for ifo in args.ifos}
 cons_counts_out = {ifo : np.inf * np.ones(len(alphas_bin[ifo])) for ifo in args.ifos}
 
 logging.info("Writing results")
 fout = h5py.File(args.output, 'w')
 fout.attrs['fit_threshold'] = fit_thresh
 fout.attrs['conservative_percentile'] = args.conservative_percentile
 fout.attrs['ifos'] = args.ifos
 fout['bins_edges'] = list(bl) + [bu[-1]]
-fout['fits_dates'] = ad + start_date_n
+fout['fits_dates'] = ad + start_time_n
 
 for ifo in args.ifos:
     fout.create_group(ifo)
     fout[ifo].attrs['live_time'] = sum(live_times[ifo])
 
 save_allmeanalpha = {}
 for ifo in args.ifos:
@@ -139,21 +142,24 @@
     l_times = np.array(live_times[ifo])
     count_all = np.sum(counts_bin[ifo], axis=0) / l_times
     invalphan = np.array(counts_bin[ifo]) / np.array(alphas_bin[ifo])
     invalphan_all = np.mean(invalphan, axis=0)
     alpha_all = np.mean(counts_bin[ifo], axis=0) / invalphan_all
     meant = l_times.mean()
 
-    fout_ifo[f'separate_fits/live_times'] = l_times
-    fout_ifo[f'separate_fits/date'] = ad + start_date_n
+    fout_ifo[f'separate_fits/live_times'] = l_times[ad_order]
+    fout_ifo[f'separate_fits/start_time'] = trigger_file_starts[ad_order]
+    fout_ifo[f'separate_fits/end_time'] = trigger_file_ends[ad_order]
+
     for counter, a_c_u_l in enumerate(zip(alphas_bin[ifo],
                                           counts_bin[ifo], bu, bl)):
         a, c, u, l = a_c_u_l
-        a = np.array(a)
-        c = np.array(c)
+        # Sort alpha and counts by date
+        a = np.array(a)[ad_order]
+        c = np.array(c)[ad_order]
         invalphan = c / a
         mean_alpha = c.mean() / invalphan.mean()
         cons_alpha = np.percentile(a, 100 - args.conservative_percentile)
         cons_alphas_out[ifo][counter] = cons_alpha
         alphas_out[ifo][counter] = mean_alpha
         cons_count = np.percentile(c, args.conservative_percentile)
         cons_counts_out[ifo][counter] = cons_count * len(c)
```

### Comparing `PyCBC-2.2.0/bin/live/pycbc_live_plot_single_trigger_fits` & `PyCBC-2.2.1/bin/live/pycbc_live_plot_single_trigger_fits`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/live/pycbc_live_single_trigger_fits` & `PyCBC-2.2.1/bin/live/pycbc_live_single_trigger_fits`

 * *Files 4% similar despite different names*

```diff
@@ -172,15 +172,15 @@
 # criteria
 date_directory = os.path.join(args.top_directory, args.analysis_date)
 
 if not os.path.exists(date_directory):
     raise FileNotFoundError(f"The directory {date_directory} does not exist.")
 
 files = [f for f in os.listdir(date_directory)
-         if args.file_identifier in f]
+         if args.file_identifier in f and f.endswith('hdf')]
 
 events = {}
 counter = 0
 
 for filename in files:
     counter += 1
     if counter % 1000 == 0:
@@ -190,15 +190,14 @@
                 # In case of no triggers for an extended period
                 logging.info("%s: No data", ifo)
             else:
                 logging.info("%s: %d triggers in %.0fs", ifo,
                              events[ifo].data['snr'].size, live_time[ifo])
 
     f = os.path.join(date_directory, filename)
-    skipping_file = False
     # If there is an IOerror with the file, don't fail, just carry on
     try:
         h5py.File(f, 'r')
     except IOError:
         logging.info('IOError with file ' + f)
         continue
 
@@ -262,19 +261,20 @@
         triggers_cut[args.sngl_ranking] = sngls_value
 
         triggers_da = DictArray(data=triggers_cut)
 
         # If we are clustering, take the max sngl_ranking value
         if args.cluster:
             max_idx = sngls_value.argmax()
-            triggers_cut = triggers_da.select(max_idx)
+            # Make sure that the DictArray has array data, not float
+            triggers_da = triggers_da.select([max_idx])
 
-        if ifo in events:
+        if ifo in events:  # DictArray already exists for the ifo
             events[ifo] += triggers_da
-        else:
+        else:  # Set up a new dictionary entry
             events[ifo] = triggers_da
 
 logging.info("All events processed")
 
 logging.info("Number of events which meet all criteria:")
 for ifo in args.ifos:
     if ifo not in events:
```

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_foreground_minifollowup` & `PyCBC-2.2.1/bin/minifollowups/pycbc_foreground_minifollowup`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_injection_minifollowup` & `PyCBC-2.2.1/bin/minifollowups/pycbc_injection_minifollowup`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_page_coincinfo` & `PyCBC-2.2.1/bin/minifollowups/pycbc_page_coincinfo`

 * *Files 5% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
 # Public License for more details.
 #
 # You should have received a copy of the GNU General Public License along
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-""" Make tables describing a coincident foreground event"""
+""" Make tables describing a foreground event"""
 
 import h5py, argparse, logging, sys
 import matplotlib; matplotlib.use('Agg')
 import numpy, lal, datetime
 import pycbc.version, pycbc.results, pycbc.pnutils
 from pycbc.events import ranking
 from pycbc.results import followup
@@ -29,15 +29,15 @@
 parser.add_argument('--verbose', action='store_true')
 parser.add_argument('--single-trigger-files', nargs='+',
     help="HDF format single detector trigger files for the full data run")
 parser.add_argument('--bank-file',
     help="HDF format template bank file")
 parser.add_argument('--output-file')
 parser.add_argument('--statmap-file', required=True,
-    help="HDF format clustered coincident statmap file containing the result "
+    help="HDF format clustered statmap file containing the result "
          "triggers. Required")
 parser.add_argument('--statmap-file-subspace-name', default='background_exc',
     help="If given look in this 'sub-directory' of the HDF file for triggers, "
          "takes a default value of 'background_exc'.")
 trig_input = parser.add_mutually_exclusive_group(required=True)
 trig_input.add_argument('--n-loudest', type=int,
     help="Examine the n'th loudest trigger, use with statmap file")
@@ -67,35 +67,35 @@
 d = f[args.statmap_file_subspace_name]
 
 if args.n_loudest is not None:
     sorting = d[args.sort_variable][:].argsort()
     if args.sort_order == 'descending':
         sorting = sorting[::-1]
     n = sorting[args.n_loudest]
-    title = 'Parameters of coincident event ranked %s' % (args.n_loudest + 1)
+    title = 'Parameters of event ranked %s' % (args.n_loudest + 1)
     caption = ('Parameters of event ranked %s by %s %s in the search. The figures below'
                ' show the mini-followup data for this event.' % 
                (args.n_loudest + 1, args.sort_order, args.sort_variable))
 elif args.trigger_id is not None:
     n = args.trigger_id
-    title = 'Details of coincident trigger'
-    caption = ('Parameters of coincident event. The figures below show the '
+    title = 'Details of trigger'
+    caption = ('Parameters of event. The figures below show the '
                'mini-followup data for this event.')
 else:
     # It shouldn't be possible to get here!
     raise ValueError()
 
-# Make a table for the coincident information #################################
+# Make a table for the event information #################################
 
-hdrs = ["Coincident ranking statistic",
-           "Inclusive IFAR (yr)",
-           "Inclusive FAP",
-           "Exclusive IFAR (yr)",
-           "Exclusive FAP"
-        ]
+hdrs = ["Ranking statistic",
+        "Inclusive IFAR (yr)",
+        "Inclusive FAP",
+        "Exclusive IFAR (yr)",
+        "Exclusive FAP"
+       ]
 
 dsets = ['stat', 'ifar', 'fap', 'ifar_exc', 'fap_exc']
 formats = ['%5.2f', '%5.2f', '%5.2e', '%5.2f', '%5.2e']
 tbl = [[h, fmt % d[dst][n]] for fmt, dst, h in zip(formats, dsets, hdrs) if dst in d]
 headers, table = zip(*tbl)
 headers = list(headers)
 table = list(table)
```

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_page_injinfo` & `PyCBC-2.2.1/bin/minifollowups/pycbc_page_injinfo`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_page_snglinfo` & `PyCBC-2.2.1/bin/minifollowups/pycbc_page_snglinfo`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_plot_chigram` & `PyCBC-2.2.1/bin/minifollowups/pycbc_plot_chigram`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_plot_trigger_timeseries` & `PyCBC-2.2.1/bin/minifollowups/pycbc_plot_trigger_timeseries`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_single_template_plot` & `PyCBC-2.2.1/bin/minifollowups/pycbc_single_template_plot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/minifollowups/pycbc_sngl_minifollowup` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_minifollowups`

 * *Files 20% similar despite different names*

```diff
@@ -1,234 +1,210 @@
 #!/bin/env python
-# Copyright (C) 2015 Alexander Harvey Nitz, Ian Harry
+
+# Copyright (C) 2021 Francesco Pannarale
 #
 # This program is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the
 # Free Software Foundation; either version 3 of the License, or (at your
 # option) any later version.
 #
 # This program is distributed in the hope that it will be useful, but
 # WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
 # Public License for more details.
 #
 # You should have received a copy of the GNU General Public License along
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-""" Followup foreground events
+
 """
-import os, argparse, logging
-import numpy
-from ligo.lw import table
-from ligo.lw import utils as ligolw_utils
-from pycbc.results import layout
-from pycbc.events import select_segments_by_definer
+Temporary script to produce qscans of loudest triggers/missed injections
+"""
+
+# =============================================================================
+# Preamble
+# =============================================================================
+import os
+import argparse
+import logging
+import h5py
+import numpy as np
+from pycbc import init_logging
+import pycbc.workflow as wf
+from pycbc.workflow.core import resolve_url_to_file
 import pycbc.workflow.minifollowups as mini
 import pycbc.version
-import pycbc.workflow as wf
 import pycbc.events
-from pycbc.workflow.core import resolve_url_to_file
-from pycbc.events import stat
-from pycbc.io import hdf
+from pycbc.results import layout
+from pycbc.results.pygrb_postprocessing_utils import extract_ifos
+from pycbc.workflow.plotting import PlotExecutable
+from pycbc.workflow.grb_utils import build_veto_filelist, build_segment_filelist
+
+__author__ = "Francesco Pannarale <francesco.pannarale@ligo.org>"
+__version__ = pycbc.version.git_verbose_msg
+__date__ = pycbc.version.date
+__program__ = "pycbc_pygrb_minifollowupss"
 
+
+# =============================================================================
+# Functions
+# =============================================================================
 def add_wiki_row(outfile, cols):
     """
     Adds a wiki-formatted row to an output file from a list or a numpy array.
     """
     with open(outfile, 'a') as f:
-        f.write('||%s||\n' % '||'.join(map(str,cols)))
+        f.write('||%s||\n' % '||'.join(map(str, cols)))
+
 
+def make_timeseries_plot(workflow, trig_file, snr_type, central_time,
+                         shift_time, out_dir, ifo=None, tags=None):
+    """Adds a node for a timeseries of PyGRB results to the workflow"""
+
+    tags = [] if tags is None else tags
+
+    # Initialize job node with its tags
+    grb_name = workflow.cp.get('workflow', 'trigger-name')
+    extra_tags = ['GRB'+grb_name]
+    extra_tags += [snr_type]
+    if ifo is not None:
+        extra_tags += [ifo]
+    node = PlotExecutable(workflow.cp, 'pygrb_plot_snr_timeseries',
+                          ifos=workflow.ifos, out_dir=out_dir,
+                          tags=tags+extra_tags).create_node()
+    node.add_input_opt('--trig-file', trig_file)
+    # Pass the veto files
+    veto_files = build_veto_filelist(workflow)
+    node.add_input_list_opt('--veto-files', veto_files)
+    # Pass the segment files
+    seg_files = build_segment_filelist(workflow)
+    node.add_input_list_opt('--seg-files', seg_files)
+    # Other shared tuning values
+    for opt in ['chisq-index', 'chisq-nhigh', 'null-snr-threshold',
+                'veto-category', 'snr-threshold', 'newsnr-threshold',
+                'sngl-snr-threshold', 'null-grad-thresh', 'null-grad-val']:
+        node.add_opt('--'+opt, workflow.cp.get('workflow', opt))
+    node.new_output_file_opt(workflow.analysis_time, '.png',
+                             '--output-file', tags=extra_tags)
+    # Quantity to be displayed on the y-axis of the plot
+    node.add_opt('--y-variable', snr_type)
+    if ifo is not None:
+        node.add_opt('--ifo', ifo)
+    reset_central_time = shift_time - central_time
+    # Horizontal axis range the = prevents errors with negative times
+    x_lims = str(-5.+reset_central_time)+','+str(reset_central_time+5.)
+    node.add_opt('--x-lims='+x_lims)
+    # Plot title
+    if ifo is not None:
+        title_str = "'%s SNR at %.3f (s)'" %(ifo, central_time)
+        node.add_opt('--central-time', central_time)
+    else:
+        title_str = "'%s SNR at %.3f (s)'" %(snr_type.capitalize(), central_time)
+        node.add_opt('--central-time', central_time)
+    node.add_opt('--plot-title', title_str)
+
+    # Add job node to workflow
+    workflow += node
+
+    return node.output_files
+
+
+# =============================================================================
+# Main script starts here
+# =============================================================================
 parser = argparse.ArgumentParser(description=__doc__[1:])
 parser.add_argument('--version', action='version', version=pycbc.version.git_verbose_msg)
-parser.add_argument('--bank-file',
-                    help="HDF format template bank file")
-parser.add_argument('--single-detector-file',
-                    help="HDF format merged single detector trigger files")
-parser.add_argument('--instrument', help="Name of interferometer e.g. H1")
-parser.add_argument('--veto-file',
-    help="The veto file to be used if vetoing triggers (optional).")
-parser.add_argument('--veto-segment-name',
-    help="If using veto file must also provide the name of the segment to use "
-         "as a veto.")
-parser.add_argument('--inspiral-segments',
-                    help="xml segment file containing the inspiral analysis "
-                         "times")
-parser.add_argument('--inspiral-data-read-name',
-                    help="Name of inspiral segmentlist containing data read in "
-                         "by each analysis job.")
-parser.add_argument('--inspiral-data-analyzed-name',
-                    help="Name of inspiral segmentlist containing data "
-                         "analyzed by each analysis job.")
-parser.add_argument('--min-snr', type=float, default=6.5,
-                    help="Minimum SNR to consider for loudest triggers")
-parser.add_argument('--non-coinc-time-only', default=False,
-                    action='store_true',
-                    help="If given remove (veto) single-detector triggers "
-                         "that occur during a time when at least one other "
-                         "instrument is taking science data.")
-parser.add_argument('--minimum-duration', default=None, type=float,
-                    help="If given only consider single-detector triggers "
-                         "with template duration larger than this.")
-parser.add_argument('--maximum-duration', default=None, type=float,
-                    help="If given only consider single-detector triggers "
-                         "with template duration smaller than this.")
+parser.add_argument("-v", "--verbose", default=False, action="store_true",
+                    help="Verbose output")
+parser.add_argument('--trig-file',
+                    help="xml file containing the triggers found by PyGRB")
+parser.add_argument('--followups-file',
+                    help="HDF format file containing trigger/injections to follow up")
+parser.add_argument('--wiki-file',
+                    help="Name of file to save wiki-formatted table in")
+parser.add_argument('--veto-files', nargs='+', action="store",
+                    default=None, help="The location of the CATX veto " +
+                    "files provided as a list of space-separated values.")
+parser.add_argument("-a", "--seg-files", nargs="+", action="store",
+                    default=None, help="The location of the buffer, " +
+                    "onsource and offsource segment files.")
 wf.add_workflow_command_line_group(parser)
 wf.add_workflow_settings_cli(parser, include_subdax_opts=True)
-stat.insert_statistic_option_group(parser,
-    default_ranking_statistic='single_ranking_only')
 args = parser.parse_args()
 
-logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s',
-                    level=logging.INFO)
+init_logging(args.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
 workflow = wf.Workflow(args)
-workflow.ifos = [args.instrument]
-workflow.ifo_string = args.instrument
 
 wf.makedir(args.output_dir)
 
-# create a FileList that will contain all output files
+# Create a FileList that will contain all output files
 layouts = []
 
-tmpltbank_file = resolve_url_to_file(os.path.abspath(args.bank_file))
-sngl_file = resolve_url_to_file(
-    os.path.abspath(args.single_detector_file),
-    attrs={'ifos': args.instrument}
-)
-
-if args.veto_file is not None:
-    veto_file = resolve_url_to_file(
-        os.path.abspath(args.veto_file),
-        attrs={'ifos': args.instrument}
-    )
-else:
-    veto_file = None
-insp_segs = resolve_url_to_file(os.path.abspath(args.inspiral_segments))
-insp_data_seglists = select_segments_by_definer\
-        (args.inspiral_segments, segment_name=args.inspiral_data_read_name,
-         ifo=args.instrument)
-insp_data_seglists.coalesce()
-
-num_events = int(workflow.cp.get_opt_tags('workflow-sngl_minifollowups',
-                 'num-sngl-events', ''))
-
-# This helps speed up the processing to ignore a large fraction of triggers
-snr_mask = None
-if args.min_snr:
-    logging.info('Calculating Prefilter')
-    f = hdf.HFile(args.single_detector_file, 'r')
-    idx, _ = f.select(lambda snr: snr > args.min_snr,
-                      '{}/snr'.format(args.instrument),
-                      return_indices=True)
-    snr_mask = numpy.zeros(len(f['{}/snr'.format(args.instrument)]),
-                           dtype=bool)
-    snr_mask[idx] = True
-
-trigs = hdf.SingleDetTriggers(args.single_detector_file, args.bank_file,
-                              args.veto_file, args.veto_segment_name,
-                              None, args.instrument, premask=snr_mask)
-
-if args.non_coinc_time_only:
-    from pycbc.io.ligolw import LIGOLWContentHandler as h
-
-    segs_doc = ligolw_utils.load_filename(args.inspiral_segments,
-                                          contenthandler=h)
-    seg_def_table = table.Table.get_table(segs_doc, 'segment_definer')
-    def_ifos = seg_def_table.getColumnByName('ifos')
-    def_ifos = [str(ifo) for ifo in def_ifos]
-    ifo_list = list(set(def_ifos))
-    ifo_list.remove(args.instrument)
-    for ifo in ifo_list:
-        curr_veto_mask, segs = pycbc.events.veto.indices_outside_segments(
-            trigs.end_time, [args.inspiral_segments],
-            ifo=ifo, segment_name=args.inspiral_data_analyzed_name)
-        curr_veto_mask.sort()
-        trigs.apply_mask(curr_veto_mask)
-
-if args.minimum_duration is not None:
-    logging.info('applying minimum duration')
-    durations = trigs.template_duration
-    lgc_mask = durations > args.minimum_duration
-    trigs.apply_mask(lgc_mask)
-    logging.info('remaining triggers: %s', trigs.mask.sum())
-
-if args.maximum_duration is not None:
-    logging.info('applying maximum duration')
-    durations = trigs.template_duration
-    lgc_mask = durations < args.maximum_duration
-    trigs.apply_mask(lgc_mask)
-    logging.info('remaining triggers: %s', trigs.mask.sum())
-
-logging.info('finding loudest clustered events')
-rank_method = stat.get_statistic_from_opts(args, [args.instrument])
-trigs.mask_to_n_loudest_clustered_events(rank_method, n_loudest=num_events)
-
-if len(trigs.stat) < num_events:
-    num_events = len(trigs.stat)
-
-times = trigs.end_time
-tids = trigs.template_id
-
-# loop over number of loudest events to be followed up
-order = trigs.stat.argsort()[::-1]
-for rank, num_event in enumerate(order):
-    logging.info('Processing event: %s', num_event)
+# Read the file with the triggers/injections to follow up
+logging.info('Reading list of triggers/injections to followup')
+fp = h5py.File(args.followups_file, "r")
+
+# Initialize a wiki table and add the column headers
+if args.wiki_file:
+    wiki_file = os.path.join(args.output_dir, args.wiki_file)
+    add_wiki_row(wiki_file, fp.keys())
+
+# Establish the number of follow-ups to perform
+num_events = int(workflow.cp.get_opt_tags('workflow-minifollowups',
+                                          'num-events', ''))
+num_events = min(num_events, len(fp['BestNR'][:]))
+
+# Determine ifos used in the analysis
+trig_file = resolve_url_to_file(os.path.abspath(args.trig_file))
+ifos = extract_ifos(os.path.abspath(args.trig_file))
+num_ifos = len(ifos)
+
+# (Loudest) off/on-source events are on time-slid data so the
+# try will succeed, as it finds the time shift columns.
+is_injection_followup = True
+try:
+    time_shift = fp[ifos[0]+' time shift (s)'][0]
+    is_injection_followup = False
+except:
+    pass
+
 
+# Loop over triggers/injections to be followed up
+for num_event in range(num_events):
     files = wf.FileList([])
-    time = times[num_event]
-    ifo_time = '%s:%s' %(args.instrument, str(time))
-    tid = trigs.mask[num_event]
-    ifo_tid = '%s:%s' %(args.instrument, str(tid))
-
-    layouts += (mini.make_sngl_ifo(workflow, sngl_file, tmpltbank_file,
-                                   tid, args.output_dir, args.instrument,
-                                   tags=args.tags + [str(rank)]),)
-    files += mini.make_trigger_timeseries(workflow, [sngl_file],
-                              ifo_time, args.output_dir, special_tids=ifo_tid,
-                              tags=args.tags + [str(rank)])
-    curr_params = {}
-    curr_params['mass1'] = trigs.mass1[num_event]
-    curr_params['mass2'] = trigs.mass2[num_event]
-    curr_params['spin1z'] = trigs.spin1z[num_event]
-    curr_params['spin2z'] = trigs.spin2z[num_event]
-    curr_params['f_lower'] = trigs.f_lower[num_event]
-    curr_params[args.instrument + '_end_time'] = time
-    # don't require precessing template info if not present
-    try:
-        curr_params['spin1x'] = trigs.spin1x[num_event]
-        curr_params['spin2x'] = trigs.spin2x[num_event]
-        curr_params['spin1y'] = trigs.spin1y[num_event]
-        curr_params['spin2y'] = trigs.spin2y[num_event]
-        curr_params['inclination'] = trigs.inclination[num_event]
-    except KeyError:
-        pass
-    try:
-        # Only present for precessing search
-        curr_params['u_vals'] = trigs.u_vals[num_event]
-    except:
-        pass
-
-    files += mini.make_single_template_plots(workflow, insp_segs,
-                            args.inspiral_data_read_name,
-                            args.inspiral_data_analyzed_name, curr_params,
-                            args.output_dir,
-                            tags=args.tags+[str(rank)])
-
-    files += mini.make_plot_waveform_plot(workflow, curr_params,
-                                        args.output_dir, [args.instrument],
-                                        tags=args.tags + [str(rank)])
-
-
-    files += mini.make_singles_timefreq(workflow, sngl_file, tmpltbank_file,
-                            time, args.output_dir,
-                            data_segments=insp_data_seglists,
-                            tags=args.tags + [str(rank)])
-
-    files += mini.make_qscan_plot(workflow, args.instrument, time,
-                                  args.output_dir,
-                                  data_segments=insp_data_seglists,
-                                  tags=args.tags + [str(rank)])
+    logging.info('Processing event: %s', num_event)
+    gps_time = fp['GPS time'][num_event]
+    gps_time = gps_time.astype(float)
+    if wiki_file:
+        row = []
+        for key in fp.keys():
+            row.append(fp[key][num_event])
+        add_wiki_row(wiki_file, row)
+    # Handle off/on-source (loudest) triggers follow-up (which are on slid data)
+    if not is_injection_followup:
+        for ifo in ifos:
+            time_shift = fp[ifo+' time shift (s)'][num_event]
+            ifo_time = gps_time - time_shift
+            files += make_timeseries_plot(workflow, trig_file,
+                                          'single', gps_time, ifo_time,
+                                          args.output_dir, ifo=ifo,
+                                          tags=args.tags + [str(num_event)])
+            files += mini.make_qscan_plot(workflow, ifo, ifo_time,
+                                          args.output_dir,
+                                          tags=args.tags + [str(num_event)])
+    # Handle injections (which are on unslid data)
+    else:
+        for snr_type in ['reweighted', 'coherent']:
+            files += make_timeseries_plot(workflow, trig_file,
+                                          snr_type, gps_time, gps_time,
+                                          args.output_dir, ifo=None,
+                                          tags=args.tags + [str(num_event)])
+        for ifo in ifos:
+            files += mini.make_qscan_plot(workflow, ifo, gps_time,
+                                          args.output_dir,
+                                          tags=args.tags + [str(num_event)])
 
     layouts += list(layout.grouper(files, 2))
 
 workflow.save()
 layout.two_column_layout(args.output_dir, layouts)
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_banksim_plot_eff_fitting_factor` & `PyCBC-2.2.1/bin/plotting/pycbc_banksim_plot_eff_fitting_factor`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_banksim_plot_fitting_factors` & `PyCBC-2.2.1/bin/plotting/pycbc_banksim_plot_fitting_factors`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_banksim_table_point_injs` & `PyCBC-2.2.1/bin/plotting/pycbc_banksim_table_point_injs`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_create_html_snippet` & `PyCBC-2.2.1/bin/plotting/pycbc_create_html_snippet`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_faithsim_plots` & `PyCBC-2.2.1/bin/plotting/pycbc_faithsim_plots`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_ifar_catalog` & `PyCBC-2.2.1/bin/plotting/pycbc_ifar_catalog`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_mass_area_plot` & `PyCBC-2.2.1/bin/plotting/pycbc_mass_area_plot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_mchirp_plots` & `PyCBC-2.2.1/bin/plotting/pycbc_mchirp_plots`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_banktriggerrate` & `PyCBC-2.2.1/bin/plotting/pycbc_page_banktriggerrate`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_coinc_snrchi` & `PyCBC-2.2.1/bin/plotting/pycbc_page_coinc_snrchi`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_foreground` & `PyCBC-2.2.1/bin/plotting/pycbc_page_foreground`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 """
 import argparse
 import h5py, logging, numpy
 from pycbc.io import hdf
 import h5py, logging
 from pycbc.pnutils import mass1_mass2_to_mchirp_eta
 import pycbc.results, pycbc.results.followup
-from pycbc.results.versioning import save_fig_with_metadata
+from pycbc.results import save_fig_with_metadata
 import pycbc.version
 import sys
 
 parser = argparse.ArgumentParser()
 # General required options
 parser.add_argument("--version", action="version", version=pycbc.version.git_verbose_msg)
 parser.add_argument('--trigger-file')
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_foundmissed` & `PyCBC-2.2.1/bin/plotting/pycbc_page_foundmissed`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_ifar` & `PyCBC-2.2.1/bin/plotting/pycbc_page_ifar`

 * *Files 1% similar despite different names*

```diff
@@ -187,15 +187,14 @@
         back_ifar = fp['background/ifar'][:]
 
 # make figure
 fig = pylab.figure(1)
 
 # get a unique list of timeslide_ids and loop over them
 interval = fp.attrs['timeslide_interval']
-pifo, fifo = fp.attrs['pivot'], fp.attrs['fixed']
 ifo_joined = fp.attrs['ifos'].replace(' ','')
 p_starts = fp['segments'][ifo_joined]['start'][:]
 p_ends = fp['segments'][ifo_joined]['end'][:]
 pifo_segments = veto.start_end_to_segments(p_starts, p_ends)
 fifo_segments = copy.deepcopy(pifo_segments)
 min_tsid = (p_starts.min() - p_ends.max()) / interval
 max_tsid = -min_tsid
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_injtable` & `PyCBC-2.2.1/bin/plotting/pycbc_page_injtable`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_recovery` & `PyCBC-2.2.1/bin/plotting/pycbc_page_recovery`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_segments` & `PyCBC-2.2.1/bin/plotting/pycbc_page_segments`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_segplot` & `PyCBC-2.2.1/bin/plotting/pycbc_page_segplot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_segtable` & `PyCBC-2.2.1/bin/plotting/pycbc_page_segtable`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_sensitivity` & `PyCBC-2.2.1/bin/plotting/pycbc_page_sensitivity`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_snrchi` & `PyCBC-2.2.1/bin/plotting/pycbc_page_snrchi`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_snrifar` & `PyCBC-2.2.1/bin/plotting/pycbc_page_snrifar`

 * *Files 1% similar despite different names*

```diff
@@ -330,15 +330,15 @@
             # add sigma label
         pylab.annotate('%1.0f$\sigma$' % sigmas[ii],
                        (anntx, far_from_p(p, foreground_livetime,
                        far_back.max())), zorder=100)
         ax2.plot([],[])
     pylab.sca(ax1)
 
-pylab.xlabel(r'Coincident Ranking Statistic')
+pylab.xlabel(r'Ranking Statistic')
 pylab.yscale('log')
 pylab.ylim(plot_ymin, plot_ymax * 10.0)
 pylab.xlim(plot_xmin, plot_xmax)
 pylab.legend(loc="upper right", fontsize=9)
 pylab.grid()
     
 if args.cumulative:
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_snrratehist` & `PyCBC-2.2.1/bin/plotting/pycbc_page_snrratehist`

 * *Files 1% similar despite different names*

```diff
@@ -201,15 +201,15 @@
     left = numpy.searchsorted(fstat, le)
     right = numpy.searchsorted(fstat, re)
     count = (right - left) / f.attrs['foreground_time'] * lal.YRJUL_SI
     pylab.errorbar(bins[:-1] + bin_size / 2, count, xerr=bin_size/2,
                    label='Foreground', mec='none', fmt='o', ms=1, capthick=0,
                    elinewidth=4,  color='#ff6600')
 
-pylab.xlabel('Coincident ranking statistic (bin size = %.2f)' % bin_size)
+pylab.xlabel('Ranking statistic (bin size = %.2f)' % bin_size)
 pylab.ylabel('Trigger Rate (yr$^{-1})$')
 if args.x_min is not None:
     pylab.xlim(xmin=args.x_min)
 else:
     pylab.xlim(xmin=numpy.floor(histpeak))
 pylab.ylim(ymin=0.5 / f.attrs['background_time_exc'] * lal.YRJUL_SI)
 pylab.grid()
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_page_vetotable` & `PyCBC-2.2.1/bin/plotting/pycbc_page_vetotable`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_background_coincs` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_background_coincs`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_bank_bins` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_bank_bins`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_dq_likelihood_vs_time` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_dq_likelihood_vs_time`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_dq_percentiles` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_dq_percentiles`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_gate_triggers` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_gate_triggers`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_gating` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_gating`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_hist` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_hist`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_multiifo_dtphase` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_multiifo_dtphase`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_psd_file` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_psd_file`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_psd_timefreq` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_psd_timefreq`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_qscan` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_qscan`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_range` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_range`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_range_vs_mtot` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_range_vs_mtot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_singles_timefreq` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_singles_timefreq`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_singles_vs_params` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_singles_vs_params`

 * *Files 4% similar despite different names*

```diff
@@ -112,36 +112,34 @@
     'mincnt': 0,
     'linewidths': 0.03
 }
 if opts.log_x:
     hexbin_style['xscale'] = 'log'
 if opts.log_y:
     hexbin_style['yscale'] = 'log'
-if opts.min_z is not None:
-    hexbin_style['vmin'] = opts.min_z
-if opts.max_z is not None:
-    hexbin_style['vmax'] = opts.max_z
+minz = opts.min_z if opts.min_z else 1
+maxz = opts.max_z
+hexbin_style['norm'] = LogNorm(vmin=minz, vmax=maxz)
 
 logging.info('Plotting')
 fig = pl.figure()
 ax = fig.gca()
 
+
 if opts.z_var == 'density':
-    norm = LogNorm()
-    hb = ax.hexbin(x, y, norm=norm, vmin=1, **hexbin_style)
+    hb = ax.hexbin(x, y, **hexbin_style)
     fig.colorbar(hb, ticks=LogLocator(subs=range(10)))
 elif opts.z_var in ranking.sngls_ranking_function_dict:
     cb_style = {}
     z = trigs.get_ranking(opts.z_var)
 
     z = z[mask]
     min_z = z.min() if opts.min_z is None else opts.min_z
     max_z = z.max() if opts.max_z is None else opts.max_z
     if max_z / min_z > 10:
-        hexbin_style['norm'] = LogNorm()
         cb_style['ticks'] = LogLocator(subs=range(10))
     hb = ax.hexbin(x, y, C=z, reduce_C_function=max, **hexbin_style)
     fig.colorbar(hb, **cb_style)
 else:
     raise RuntimeError('z_var = %s is not recognized!' % (opts.z_var))
 
 ax.set_xlabel(opts.x_var)
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_throughput` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_throughput`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_trigrate` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_trigrate`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_vt_ratio` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_vt_ratio`

 * *Files 22% similar despite different names*

```diff
@@ -4,28 +4,29 @@
 This script compares the sensitivities (VTs) of two searches having consistent
 sets of injections. It reads two HDF files produced by pycbc_page_sensitivity's
 --hdf-out option, and plots the ratios of their VTs at various IFARs.
 """
 
 import sys
 import argparse
+import logging
 import h5py
 import matplotlib
 matplotlib.use('Agg')
 import matplotlib.pyplot as plt
 import numpy as np
 from pycbc.results import save_fig_with_metadata
 
 
 parser = argparse.ArgumentParser(description=__doc__)
-parser.add_argument('--vt-file-one', required=True,
-                    help='HDF file containing VT curves, data for '
+parser.add_argument('--vt-files-one', nargs='+',
+                    help='HDF files containing VT curves, data for '
                          'the numerator (top) of the ratio')
-parser.add_argument('--vt-file-two', required=True,
-                    help='HDF file containing VT curves, data for '
+parser.add_argument('--vt-files-two', nargs='+',
+                    help='HDF files containing VT curves, data for '
                          'the denominator (bottom) of the ratio')
 parser.add_argument('--desc-one',  required=True,
                     help='Descriptor tag for first set of data '
                          '(short, for use in subscript)')
 parser.add_argument('--desc-two', type=str, required=True,
                     help='Descriptor tag for second set of data '
                          '(short, for use in subscript)')
@@ -37,112 +38,145 @@
                          'from the VT files')
 parser.add_argument('--log-x', action='store_true',
                     help='Use logarithmic x-axis')
 parser.add_argument('--log-y', action='store_true',
                     help='Use logarithmic y-axis')
 args = parser.parse_args()
 
-# Load in the two datasets
-ftop = h5py.File(args.vt_file_one, 'r')
-fbottom = h5py.File(args.vt_file_two, 'r')
-
-# Find the index closest to the given IFAR value
-idxs = [np.argmin(np.abs(ftop['xvals'][:] - ifv)) for ifv in args.ifars]
-
-keys = ftop['data'].keys()
-# sanitise the input so that the files have the same binning parameter and bins
-if keys != fbottom['data'].keys():
-    parser.error('keys do not match for the given input files - '
-                 '{} vs {}'.format(keys, fbottom['data'].keys()))
+# Warn user if different numbers of files in numerator vs denominator
+if len(args.vt_files_one) != len(args.vt_files_two):
+    logging.warning(
+        'WATCH OUT! You gave different numbers of One and Two files!')
+
+
+# Load in the first numerator file
+with h5py.File(args.vt_files_one[0], 'r') as ftop_init:
+    # Find the index closest to the given IFAR value
+    idxs = [np.argmin(np.abs(ftop_init['xvals'][:] - ifv))
+            for ifv in args.ifars]
+    plot_ifars = ftop_init['xvals'][idxs]
+    # Get binning keys for reference
+    keys = list(ftop_init['data'].keys())
+
+# Dicts holding data for total VT and variances
+vt_top = {k: np.zeros_like(plot_ifars) for k in keys}
+vt_top_errsqhi = {k: np.zeros_like(plot_ifars) for k in keys}
+vt_top_errsqlow = {k: np.zeros_like(plot_ifars) for k in keys}
+vt_bot = {k: np.zeros_like(plot_ifars) for k in keys}
+vt_bot_errsqhi = {k: np.zeros_like(plot_ifars) for k in keys}
+vt_bot_errsqlow = {k: np.zeros_like(plot_ifars) for k in keys}
+
+# Cycle over inputs for numerator
+for ftop in args.vt_files_one:
+    with h5py.File(ftop, 'r') as f:
+        # Check the input bins
+        if list(f['data'].keys()) != keys:
+            raise ValueError(
+                f'keys do not match for the given input files - '
+                '{keys} v {list(f["data"].keys())}')
+        # Add the data
+        for k in keys:
+            vt_top[k] += f['data'][k][idxs]
+            # Variances add over files
+            vt_top_errsqhi[k] += f['errorhi'][k][idxs] ** 2.
+            vt_top_errsqlow[k] += f['errorlow'][k][idxs] ** 2.
+
+# Same for denominator
+for fbot in args.vt_files_two:
+    with h5py.File(fbot, 'r') as f:
+        if list(f['data'].keys()) != keys:
+            raise ValueError(
+                f'keys do not match for the given input files - '
+                '{keys} v {list(f["data"].keys())}')
+        for k in keys:
+            vt_bot[k] += f['data'][k][idxs]
+            vt_bot_errsqhi[k] += f['errorhi'][k][idxs] ** 2.
+            vt_bot_errsqlow[k] += f['errorlow'][k][idxs] ** 2.
 
 # make the plot pretty
 plt.rc('axes.formatter', limits=[-3, 4])
 plt.rc('figure', dpi=300)
 fig_mi = plt.figure(figsize=(10, 4))
 ax_mi = fig_mi.gca()
 
 ax_mi.grid(True, zorder=1)
 
 # read in labels for the different plotting points
-labels = ['$ ' + label.split('\\in')[-1] for label in ftop['data'].keys()]
+labels = ['$ ' + label.split('\\in')[-1] for label in keys]
 
 # read in the splitting parameter name from the first data set
-x_param = r'$' + tuple(ftop['data'].keys())[0].split('\\in')[0].strip('$').strip() + r'$'
+x_param = r'$' + tuple(keys)[0].split('\\in')[0].strip('$').strip() + r'$'
 
 # read in the positions from the labels
 xpos = np.array([float(l.split('[')[1].split(',')[0]) for l in labels])
 
 # offset different ifars by 1/20th of the mean distance between parameters
 try: 
     if args.log_x:
         xpos_logdiffmean = np.diff(np.log(xpos)).mean()
         xpos_add_dx = 0.05 * np.ones_like(xpos) * xpos_logdiffmean
     else:
         xpos_diffmean = np.diff(xpos).mean()
         xpos_add_dx = 0.05 * np.ones_like(xpos) * xpos_diffmean
 except IndexError:
-    #If there's only one value of xpos, then diff doesnt work
+    # If there's only one value of xpos, then diff doesn't work
     xpos_add_dx = 0.05
 
 # set the x ticks to be the positions given in the labels
 plt.xticks(xpos, labels, rotation='horizontal')
 
 colors = ['#7b85d4', '#f37738', '#83c995', '#d7369e', '#c4c9d8', '#859795']
 
 # loop through each IFAR and plot the VT ratio with error bars
-for count, idv in enumerate(idxs):
-    data1 = np.array([ftop['data'][key][idv] for key in keys])
-    errhigh1 = np.array([ftop['errorhigh'][key][idv] for key in keys])
-    errlow1 = np.array([ftop['errorlow'][key][idv] for key in keys])
+for j in range(len(idxs)):
+    data1 = np.array([vt_top[key][j] for key in keys])
+    errsqhi1 = np.array([vt_top_errsqhi[key][j] for key in keys])
+    errsqlow1 = np.array([vt_top_errsqlow[key][j] for key in keys])
     
-    data2 = np.array([fbottom['data'][key][idv] for key in keys])
-    errhigh2 = np.array([fbottom['errorhigh'][key][idv] for key in keys])
-    errlow2 = np.array([fbottom['errorlow'][key][idv] for key in keys])
-
-    ys = np.divide(data1, data2)
-    yerr_errlow = np.multiply(np.sqrt(np.divide(errlow1, data1)**2 +
-                np.divide(errlow2, data2)**2), ys)
-    yerr_errhigh = np.multiply(np.sqrt(np.divide(errhigh1, data1)**2 +
-                np.divide(errhigh2, data2)**2), ys)
+    data2 = np.array([vt_bot[key][j] for key in keys])
+    errsqhi2 = np.array([vt_bot_errsqhi[key][j] for key in keys])
+    errsqlow2 = np.array([vt_bot_errsqlow[key][j] for key in keys])
+
+    ys = data1 / data2
+    # fractional error propagation
+    yerr_low = (errsqlow1 / (data1**2.) + errsqlow2 / (data2**2.)) ** 0.5 * ys
+    yerr_hi = (errsqhi1 / (data1**2.) + errsqhi2 / (data2**2.)) ** 0.5 * ys
 
     if args.log_x:
         xvals = np.exp(np.log(xpos) +
-                       xpos_add_dx * (count -
-                                      float(len(args.ifars) - 1) / 2.))
+                       xpos_add_dx * (j - float(len(args.ifars) - 1) / 2.))
     else:
-        xvals = xpos + xpos_add_dx * (count -
-                                      float(len(args.ifars) - 1) / 2.)
+        xvals = xpos + xpos_add_dx * (j - float(len(args.ifars) - 1) / 2.)
     ax_mi.errorbar(xvals, ys,
-        yerr=[yerr_errlow, yerr_errhigh], fmt='o', markersize=7, linewidth=5,
-        label='IFAR = %d yr' % ftop['xvals'][idv], capsize=5,
-        capthick=2, mec='k', color=colors[count % len(colors)])
+        yerr=[yerr_low, yerr_hi], fmt='o', markersize=7, linewidth=5,
+        label='IFAR = %d yr' % plot_ifars[j], capsize=5,
+        capthick=2, mec='k', color=colors[j % len(colors)])
 
 if args.log_x:
     plt.xscale('log')
 if args.log_y:
     plt.yscale('log')
 plt.xticks(xpos, labels, rotation='horizontal')
 
 # get the limit of the x axes, and draw a black line in order to highlight
 # equal comparison
 xlimits = plt.xlim()
-plt.plot(xlimits, [1, 1], 'k', lw=3, zorder=0)
+plt.plot(xlimits, [1, 1], 'k', lw=2, zorder=0)
 plt.xlim(xlimits) # reassert the x limits so that the plot doesn't expand
 
 ax_mi.legend(bbox_to_anchor=(0.5, 1.01), ncol=len(args.ifars),
              loc='lower center')
 ax_mi.get_legend().get_title().set_fontsize('14')
 ax_mi.get_legend().get_frame().set_alpha(0.7)
 ax_mi.set_xlabel(x_param, size='large')
 ax_mi.set_ylabel(r'$\frac{VT(\mathrm{' + args.desc_one +'})}\
                          {VT(\mathrm{' + args.desc_two +'})}$', 
                  size='large')
 plt.tight_layout()
 
-title = 'VT sensitivity comparison between {} and {}'.format(
-        args.vt_file_one, args.vt_file_two)
-
-# write out to file
+title = f'VT sensitivity comparison between {args.desc_one} and ' \
+        f'{args.desc_two}'
 save_fig_with_metadata(fig_mi, args.outfile, cmd=' '.join(sys.argv),
                        title=title)
 
 plt.close()
+
```

### Comparing `PyCBC-2.2.0/bin/plotting/pycbc_plot_waveform` & `PyCBC-2.2.1/bin/plotting/pycbc_plot_waveform`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/population/pycbc_multiifo_pastro` & `PyCBC-2.2.1/bin/population/pycbc_multiifo_pastro`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/population/pycbc_population_plots` & `PyCBC-2.2.1/bin/population/pycbc_population_plots`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/population/pycbc_population_rates` & `PyCBC-2.2.1/bin/population/pycbc_population_rates`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_banksim` & `PyCBC-2.2.1/bin/pycbc_banksim`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_banksim_combine_banks` & `PyCBC-2.2.1/bin/pycbc_banksim_combine_banks`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_banksim_match_combine` & `PyCBC-2.2.1/bin/pycbc_banksim_match_combine`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_banksim_skymax` & `PyCBC-2.2.1/bin/pycbc_banksim_skymax`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_coinc_time` & `PyCBC-2.2.1/bin/pycbc_coinc_time`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_compress_bank` & `PyCBC-2.2.1/bin/pycbc_compress_bank`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_condition_strain` & `PyCBC-2.2.1/bin/pycbc_condition_strain`

 * *Files 2% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 """
 
 import logging
 import argparse
 import pycbc.strain
 import pycbc.version
 import pycbc.frame
+import pycbc.fft
 from pycbc.types import float32, float64
 
 
 def write_strain(file_name, channel, data):
     logging.info('Writing output strain to %s', file_name)
 
     if file_name.endswith('.gwf'):
@@ -71,16 +72,21 @@
                     help='Split the produced data into different frame files '
                          'of the given duration. The output file name should '
                          'contain the strings {start} and {duration}, which '
                          'will be replaced by the start GPS time and duration '
                          'in seconds')
 
 pycbc.strain.insert_strain_option_group(parser)
+pycbc.fft.insert_fft_option_group(parser)
 args = parser.parse_args()
 
+# Take in / deal with the FFT options
+pycbc.fft.verify_fft_options(args, parser)
+pycbc.fft.from_cli(args)
+
 if args.frame_duration is not None and args.frame_duration <= 0:
     parser.error('Frame duration should be positive integer, {} given'.format(args.frame_duration))
 
 logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)
 
 # read and condition strain as pycbc_inspiral would do
 out_strain = pycbc.strain.from_cli(args, dyn_range_fac=pycbc.DYN_RANGE_FAC,
```

### Comparing `PyCBC-2.2.0/bin/pycbc_copy_output_map` & `PyCBC-2.2.1/bin/pycbc_copy_output_map`

 * *Files 1% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 
 eval set -- "$CMD_OPTS"
 
 while true
 do
     case "$1" in
 	-h|--help)
-	    echo "usage: pycbc_copy_output_map [-h] input_map output_map [optional arguments]"
+	    echo "usage: pycbc_copy_output_map [-h] -i input_map -o output_map [optional arguments]"
 	    echo
 	    echo "required arguments:"
 	    echo "  -i, --input-file        the .map file that will be copied from"
 	    echo "  -o, --output-file       the name of the output file to be created"
 	    echo
 	    echo "optional arguments:"
 	    echo "  -h, --help              show this help message and exit"
```

### Comparing `PyCBC-2.2.0/bin/pycbc_create_injections` & `PyCBC-2.2.1/bin/pycbc_create_injections`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_data_store` & `PyCBC-2.2.1/bin/pycbc_data_store`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_faithsim` & `PyCBC-2.2.1/bin/pycbc_faithsim`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_faithsim_collect_results` & `PyCBC-2.2.1/bin/pycbc_faithsim_collect_results`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_fit_sngl_trigs` & `PyCBC-2.2.1/bin/pycbc_fit_sngl_trigs`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_get_ffinal` & `PyCBC-2.2.1/bin/pycbc_get_ffinal`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_hdf5_splitbank` & `PyCBC-2.2.1/bin/pycbc_hdf5_splitbank`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_hdf_splitinj` & `PyCBC-2.2.1/bin/pycbc_hdf_splitinj`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_inj_cut` & `PyCBC-2.2.1/bin/pycbc_inj_cut`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_inspiral` & `PyCBC-2.2.1/bin/pycbc_inspiral`

 * *Files 1% similar despite different names*

```diff
@@ -297,27 +297,29 @@
         #print(idx, out_vals['time_index'])
         
         out_vals_all.append(copy.deepcopy(out_vals))
         #print(out_vals_all)
     return out_vals_all, tparam
 
 with ctx:
-    # The following FFTW specific options needed to wait until
-    # we were inside the scheme context.
+    if opt.fft_backends == 'fftw':
 
-    # Import system wisdom.
-    if opt.fftw_import_system_wisdom:
-        fft.fftw.import_sys_wisdom()
-
-    # Read specified user-provided wisdom files
-    if opt.fftw_input_float_wisdom_file is not None:
-        fft.fftw.import_single_wisdom_from_filename(opt.fftw_input_float_wisdom_file)
+        # The following FFTW specific options needed to wait until
+        # we were inside the scheme context.
 
-    if opt.fftw_input_double_wisdom_file is not None:
-        fft.fftw.import_double_wisdom_from_filename(opt.fftw_input_double_wisdom_file)
+        # Import system wisdom.
+        if opt.fftw_import_system_wisdom:
+            fft.fftw.import_sys_wisdom()
+
+        # Read specified user-provided wisdom files
+        if opt.fftw_input_float_wisdom_file is not None:
+            fft.fftw.import_single_wisdom_from_filename(opt.fftw_input_float_wisdom_file)
+
+        if opt.fftw_input_double_wisdom_file is not None:
+            fft.fftw.import_double_wisdom_from_filename(opt.fftw_input_double_wisdom_file)
 
     flow = opt.low_frequency_cutoff
     flen = strain_segments.freq_len
     tlen = strain_segments.time_len
     delta_f = strain_segments.delta_f
 
 
@@ -495,14 +497,15 @@
 tstop = time.time()
 run_time = tstop - tstart
 event_mgr.save_performance(ncores, len(segments), len(bank), run_time, tsetup)
 
 logging.info("Writing out triggers")
 event_mgr.write_events(opt.output)
 
-if opt.fftw_output_float_wisdom_file:
-    fft.fftw.export_single_wisdom_to_filename(opt.fftw_output_float_wisdom_file)
+if opt.fft_backends == 'fftw':
+    if opt.fftw_output_float_wisdom_file:
+        fft.fftw.export_single_wisdom_to_filename(opt.fftw_output_float_wisdom_file)
 
-if opt.fftw_output_double_wisdom_file:
-    fft.fftw.export_double_wisdom_to_filename(opt.fftw_output_double_wisdom_file)
+    if opt.fftw_output_double_wisdom_file:
+        fft.fftw.export_double_wisdom_to_filename(opt.fftw_output_double_wisdom_file)
 
 logging.info("Finished")
```

### Comparing `PyCBC-2.2.0/bin/pycbc_inspiral_skymax` & `PyCBC-2.2.1/bin/pycbc_inspiral_skymax`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_live` & `PyCBC-2.2.1/bin/pycbc_live`

 * *Files 3% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 
 import sys
 import argparse, numpy, pycbc, logging, cProfile, h5py, lal, json
 import os.path
 import itertools
 import platform
 import subprocess
+import multiprocessing
 from multiprocessing.dummy import threading
 from matplotlib import use
 use('agg')
 from shutil import which
 from mpi4py import MPI as mpi
 from astropy.time import Time
 from pycbc.pool import BroadcastPool
@@ -75,14 +76,15 @@
     return numpy.maximum(ifar, nifar) / 2.
 
 
 class LiveEventManager(object):
     def __init__(self, args, bank):
         self.low_frequency_cutoff = args.low_frequency_cutoff
         self.bank = bank
+        self.skymap_only_ifos = [] if args.skymap_only_ifos is None else list(set(args.skymap_only_ifos))
 
         # Figure out what we are supposed to process within the pool of MPI processes
         self.comm = mpi.COMM_WORLD
         self.size = self.comm.Get_size()
         self.rank = self.comm.Get_rank()
 
         if self.rank > 0:
@@ -91,36 +93,51 @@
 
         self.path = args.output_path
         self.mc_area_args = mchirp_area.from_cli(args, parser)
         self.padata = livepau.PAstroData(args.p_astro_spec, args.bank_file)
         self.use_date_prefix = args.day_hour_output_prefix
         self.ifar_upload_threshold = args.ifar_upload_threshold
         self.pvalue_livetime = args.pvalue_combination_livetime
+        self.gracedb_server = args.gracedb_server
+        self.gracedb_search = args.gracedb_search
+        self.gracedb_labels = args.gracedb_labels
         self.gracedb_testing = not args.enable_production_gracedb_upload
         self.enable_gracedb_upload = args.enable_gracedb_upload
         self.run_snr_optimization = args.run_snr_optimization
+        self.snr_opt_label = args.snr_opt_label
         self.gracedb = None
 
         # Keep track of which events have been uploaded
         self.last_few_coincs_uploaded = []
 
         if self.run_snr_optimization:
             # preestimate the number of CPU cores that we can afford giving
             # to followup processes without slowing down the main search
-            available_cores = len(os.sched_getaffinity(0))
             bg_cores = len(tuple(itertools.combinations(ifos, 2)))
             analysis_cores = 1 + bg_cores
-            self.fu_cores = available_cores - analysis_cores
-            if self.fu_cores <= 0:
-                logging.warning('Insufficient number of CPU cores (%d) to '
-                                'run search and trigger followups. Uploaded '
-                                'triggers will momentarily increase the lag',
-                                available_cores)
+            if platform.system() != 'Darwin':
+                available_cores = len(os.sched_getaffinity(0))
+                self.fu_cores = available_cores - analysis_cores
+                if self.fu_cores <= 0:
+                    logging.warning(
+                        'Insufficient number of CPU cores (%d) to '
+                        'run search and trigger followups. Uploaded '
+                        'triggers will momentarily increase the lag',
+                        available_cores
+                    )
+                    self.fu_cores = 1
+            else:
+                # To enable mac testing, this is just set to 1
                 self.fu_cores = 1
 
+        if args.enable_embright_has_massgap:
+            if args.embright_massgap_max < self.mc_area_args['mass_bdary']['ns_max']:
+                parser.error('MAX_GAP value cannot be lower than MAX_NS limit')
+            self.mc_area_args['embright_mg_max'] = args.embright_massgap_max
+
     def commit_results(self, results):
         logging.info('Committing triggers')
         self.comm.gather(results, root=0)
 
     def barrier(self):
         self.comm.Barrier()
 
@@ -181,67 +198,94 @@
                 coinc_times[ifo],
                 check_state=False
             )
 
             if snr_series is not None:
                 out[ifo] = {'snr_series': snr_series}
 
-        # Determine if the other ifos can contribute to the coincident event
+        # Determine if the other ifos can contribute to the coincident event,
+        # if so then update the info in `triggers` appropriately
         for ifo in followup_ifos:
-            snr_series, ptime, pvalue, sigma2 = followup_event_significance(
+            pvalue_info = followup_event_significance(
                 ifo,
                 self.data_readers[ifo],
                 self.bank,
                 template_id,
                 coinc_times
             )
-            if snr_series is not None:
-                out[ifo] = {'snr_series': snr_series}
-                self.get_followup_info(ifos[0], ifo, triggers, snr_series,
-                                       ptime, pvalue, sigma2,
-                                       recalculate_ifar=recalculate_ifar)
+            if pvalue_info is None:
+                continue
+            out[ifo] = {'snr_series': pvalue_info['snr_series']}
+            self.get_followup_info(
+                ifos[0],
+                ifo,
+                triggers,
+                pvalue_info,
+                recalculate_ifar=recalculate_ifar and ifo not in self.skymap_only_ifos
+            )
 
         # the SNR time series sample rate can vary slightly due to
         # rounding errors, so force all of them to be identical
         fix_delta_t = None
         for ifo in out:
             if 'snr_series' not in out[ifo]:
                 continue
             if fix_delta_t is None:
                 fix_delta_t = out[ifo]['snr_series']._delta_t
             else:
                 out[ifo]['snr_series']._delta_t = fix_delta_t
 
         return out
 
-    def get_followup_info(self, coinc_ifo, ifo, triggers, snr_series, ptime,
-                          pvalue, sigma2, recalculate_ifar=False):
-        # Copy the common fields from the other detector
+    def get_followup_info(
+        self,
+        coinc_ifo,
+        ifo,
+        triggers,
+        pvalue_info,
+        recalculate_ifar=False
+    ):
+        peak_time = pvalue_info['peak_time']
+        pv = pvalue_info['pvalue']
+        pv_sat = pvalue_info['pvalue_saturated']
+
+        # Copy the common fields from the other detector;
         # ignore fields that contain detector-specific data
-        fields_to_ignore = set(['end_time', 'snr', 'stat', 'coa_phase',
-                                'chisq', 'chisq_dof', 'sg_chisq', 'sigmasq'])
+        fields_to_ignore = set([
+            'end_time', 'snr', 'stat', 'coa_phase',
+            'chisq', 'chisq_dof', 'sg_chisq', 'sigmasq'
+        ])
         for key in set(triggers):
-            if 'foreground/{}/'.format(coinc_ifo) in key:
-                _, _, name = key.split('/')
-                if name in fields_to_ignore:
-                    continue
-                triggers['foreground/{}/{}'.format(ifo, name)] = triggers[key]
+            if f'foreground/{coinc_ifo}/' not in key:
+                continue
+            _, _, name = key.split('/')
+            if name in fields_to_ignore:
+                continue
+            triggers[f'foreground/{ifo}/{name}'] = triggers[key]
 
         # Set the detector-specific fields for which we have data
-        snr_series_peak = snr_series.at_time(ptime, nearest_sample=True)
-        base = 'foreground/{}/'.format(ifo)
-        triggers[base + 'end_time'] = float(ptime)
+        snr_series_peak = pvalue_info['snr_series'].at_time(
+            peak_time,
+            nearest_sample=True
+        )
+        base = f'foreground/{ifo}/'
+        triggers[base + 'end_time'] = float(peak_time)
         triggers[base + 'snr'] = triggers[base + 'stat'] = abs(snr_series_peak)
         triggers[base + 'coa_phase'] = numpy.angle(snr_series_peak)
-        triggers[base + 'sigmasq'] = sigma2
+        triggers[base + 'sigmasq'] = pvalue_info['sigma2']
         if recalculate_ifar:
             # Calculate new ifar
             triggers['foreground/ifar'] = combine_ifar_pvalue(
-                    triggers['foreground/ifar'], pvalue, self.pvalue_livetime)
-            triggers['foreground/pvalue_{}'.format(ifo)] = pvalue
+                triggers['foreground/ifar'],
+                pv,
+                self.pvalue_livetime
+            )
+            triggers['foreground/ifar_saturated'] |= pv_sat
+            triggers[f'foreground/pvalue_{ifo}'] = pv
+            triggers[f'foreground/pvalue_{ifo}_saturated'] = pv_sat
 
     def setup_optimize_snr(
         self, results, live_ifos, triggering_ifos, fname, gid
     ):
         """Setup and start the network SNR optimization process for a
         candidate event. See arXiv:2008.07494 for details.
 
@@ -324,16 +368,21 @@
 
             recursively_save_dict_contents_to_group(hdfp,
                                                     'mc_area_args/',
                                                     self.mc_area_args)
 
         cmd += '--params-file {} '.format(curr_fname)
         cmd += '--approximant {} '.format(apr)
-        cmd += '--gracedb-server {} '.format(args.gracedb_server)
-        cmd += '--gracedb-search {} '.format(args.gracedb_search)
+        cmd += '--gracedb-server {} '.format(self.gracedb_server)
+        cmd += '--gracedb-search {} '.format(self.gracedb_search)
+
+        labels = self.snr_opt_label
+        labels += ' '.join(self.gracedb_labels or [])
+        cmd += f'--gracedb-labels {labels} '
+
         if not self.gracedb_testing:
             cmd += '--production '
         cmd += '--verbose '
 
         # set up a place for storing the SNR optimization results and log
         out_dir_path = os.path.join(os.path.dirname(fname), 'optimize_snr')
         makedir(out_dir_path)
@@ -342,17 +391,17 @@
         if self.enable_gracedb_upload:
             cmd += '--enable-gracedb-upload '
 
         cmd += '--cores {} '.format(self.fu_cores)
         if args.processing_scheme:
             # we will use the cores for multiple workers of the
             # optimization routine, so we force the processing scheme
-            # to a single core here.  this may be enforcing some
+            # to a single core here.  This may be enforcing some
             # assumptions about the optimal way to do ffts on the
-            # machine. however, the dominant cost of pycbc_optimize_snr
+            # machine. However, the dominant cost of pycbc_optimize_snr
             # is expected to be in waveform generation, which is
             # unlikely to benefit from a processing scheme with more
             # than 1 thread anyway.
             opt_scheme = args.processing_scheme.split(':')[0]
             cmd += '--processing-scheme {}:1 '.format(opt_scheme)
 
         log_fname = os.path.join(out_dir_path, 'optimize_snr.log')
@@ -361,38 +410,41 @@
         with open(log_fname, "w") as logfile:
             subprocess.Popen(
                 cmd, shell=True, stdout=logfile, stderr=logfile
             )
 
     def create_gdb(self):
         """
-        Function to create GraceDB session.
+        Create a GraceDB session that will persist throughout the execution
+        of PyCBC Live.
         """
         from ligo.gracedb.rest import GraceDb
+
         logging.info('Creating GraceDB session')
         # Set up dict for args that reload the certificate if it will expire in the
         # reload_buffer time. These args are documented here:
         # https://ligo-gracedb.readthedocs.io/en/latest/api.html#ligo.gracedb.rest.GraceDb
         # Because we do not change any of the request session values when running the
-        # code, it should remain thread safe. 
+        # code, it should remain thread safe.
         gdbargs = {'reload_certificate': True, 'reload_buffer': 300}
-        if args.gracedb_server:
-            gdbargs['service_url'] = args.gracedb_server 
+        if self.gracedb_server:
+            gdbargs['service_url'] = self.gracedb_server
         self.gracedb = GraceDb(**gdbargs)
 
     def upload_in_thread(self, event, fname, comment, results, live_ifos, ifos,
-                         upload_checks, optimize_snr_checks, gracedb_server, gracedb_search):
+                         upload_checks, optimize_snr_checks):
         gid = None
         if upload_checks:
             gid = event.upload(
                 fname,
-                gracedb_server=gracedb_server,
+                gracedb_server=self.gracedb_server,
                 testing=self.gracedb_testing,
                 extra_strings=[comment],
-                search=gracedb_search
+                search=self.gracedb_search,
+                labels=self.gracedb_labels
             )
         if optimize_snr_checks:
             self.setup_optimize_snr(
                 results,
                 live_ifos,
                 ifos,
                 fname,
@@ -409,15 +461,16 @@
                 break
 
         if 'foreground/ifar' not in coinc_results:
             return
 
         logging.info('computing followup data for coinc')
         coinc_ifos = coinc_results['foreground/type'].split('-')
-        followup_ifos = list(set(ifos) - set(coinc_ifos))
+        followup_ifos = set(ifos) - set(coinc_ifos)
+        followup_ifos = list(followup_ifos | set(self.skymap_only_ifos))
 
         double_ifar = coinc_results['foreground/ifar']
         if double_ifar < args.ifar_double_followup_threshold:
             coinc_results['foreground/NO_FOLLOWUP'] = True
             return
 
         sld = self.compute_followup_data(
@@ -455,15 +508,15 @@
         comment = comment.format(ppdets(coinc_ifos),
                                  args.ranking_statistic,
                                  ppdets(followup_ifos))
 
         ifar = coinc_results['foreground/ifar']
         upload_checks = self.enable_gracedb_upload and self.ifar_upload_threshold < ifar
         optimize_snr_checks = self.run_snr_optimization and self.ifar_upload_threshold < ifar
-        
+
         # Keep track of the last few coincs uploaded in order to
         # prevent singles being uploaded as well for coinc events
         self.last_few_coincs_uploaded.append(event.merger_time)
         # Only need to keep a few (10) events
         self.last_few_coincs_uploaded = \
             self.last_few_coincs_uploaded[-10:]
 
@@ -474,16 +527,15 @@
                 logging.info('Optimizing SNR for coinc above threshold ..')
                 # Tell snr optimized event about p_terr
                 if hasattr(event, 'p_terr') and event.p_terr is not None:
                     coinc_results['p_terr'] = event.p_terr
             live_ifos = [ifo for ifo in sld if 'snr_series' in sld[ifo]]
             thread_args = (
                 event, fname, comment, coinc_results, live_ifos, coinc_ifos,
-                upload_checks, optimize_snr_checks, args.gracedb_server, 
-                args.gracedb_search,
+                upload_checks, optimize_snr_checks
             )
             gdb_upload_thread = threading.Thread(target=self.upload_in_thread,
                                                  args=thread_args)
             gdb_upload_thread.start()
 
     def check_singles(self, results, psds):
         active = [k for k in results if results[k] is not None]
@@ -496,14 +548,15 @@
             if single is None:
                 continue
 
             sifar = single['foreground/ifar']
             logging.info(f'Found {ifo} single with ifar {sifar}')
 
             followup_ifos = [i for i in active if i is not ifo]
+            followup_ifos = list(set(followup_ifos) | set(self.skymap_only_ifos))
             # Don't recompute ifar considering other ifos
             sld = self.compute_followup_data(
                 [ifo],
                 single,
                 followup_ifos=followup_ifos,
                 recalculate_ifar=False
             )
@@ -562,17 +615,16 @@
                     logging.info('Optimizing SNR for single above threshold ..')
                     # Tell snr optimized event about p_terr
                     if hasattr(event, 'p_terr') and event.p_terr is not None:
                         single['p_terr'] = event.p_terr
                 live_ifos = [ifo for ifo in sld if 'snr_series' in sld[ifo]]
                 thread_args = (
                     event, fname, comment, single, live_ifos, [ifo],
-                    upload_checks, optimize_snr_checks, args.gracedb_server, 
-                    args.gracedb_search,
-                    )
+                    upload_checks, optimize_snr_checks
+                )
                 gdb_upload_thread = threading.Thread(target=self.upload_in_thread,
                                                      args=thread_args)
                 gdb_upload_thread.start()
 
     def get_out_dir_path(self, time_gps):
         """Compose and return the path to a directory to store files associated
         with times in a given day (specified via a GPS time).
@@ -744,14 +796,25 @@
 parser.add_argument('--autogating-width', type=float, default=0.25,
                     metavar='SECONDS', help='Half-width of the gating window.')
 parser.add_argument('--autogating-taper', type=float, metavar='SECONDS',
                     default=0.25,
                     help='Taper the strain before and after '
                          'each gating window over a duration '
                          'of SECONDS.')
+parser.add_argument('--autogating-duration', type=float, default=16,
+                    metavar='SECONDS',
+                    help='Amount of data in seconds to apply autogating on.')
+parser.add_argument('--autogating-psd-segment-length', type=float, default=2,
+                    metavar='SECONDS',
+                    help='Length in seconds of each segment used to estimate the PSD '
+                    'with Welchs method and median averaging.')
+parser.add_argument('--autogating-psd-stride', type=float, default=1,
+                    metavar='SECONDS',
+                    help='Length in seconds of the overlap between each segment used '
+                    'to estimate the PSD with Welchs method and median averaging.')
 
 parser.add_argument('--sync', action='store_true',
                     help="Imposes an MPI synchronization at each transfer of"
                          " single-detector triggers. Can help with debugging"
                          " and avoiding memory issues when running offline"
                          " analyses.")
 parser.add_argument('--increment-update-cache', action=MultiDetOptionAction, nargs='+')
@@ -803,30 +866,31 @@
 parser.add_argument('--enable-background-estimation', default=False, action='store_true')
 parser.add_argument('--ifar-double-followup-threshold', type=float, required=True,
                     help='Inverse-FAR threshold to followup double coincs with'
                          'additional detectors')
 parser.add_argument('--pvalue-combination-livetime', type=float, required=True,
                     help="Livetime used for p-value combination with followup "
                          "detectors, in years")
-parser.add_argument('--enable-single-detector-background', action='store_true', default=False)
-
 parser.add_argument('--enable-gracedb-upload', action='store_true', default=False,
                     help='Upload triggers to GraceDB')
 parser.add_argument('--enable-production-gracedb-upload', action='store_true', default=False,
                     help='Do not mark triggers uploaded to GraceDB as test '
                          'events. This option should *only* be enabled in '
                          'production analyses!')
 parser.add_argument('--enable-single-detector-upload', action='store_true', default=False,
                     help='Upload single ifo events to GraceDB')
 parser.add_argument('--gracedb-server', metavar='URL',
                     help='URL of GraceDB server API for uploading events. '
                          'If not provided, the default URL is used.')
 parser.add_argument('--gracedb-search', type=str, default='AllSky',
                     help='String going into the "search" field of the GraceDB '
                          'events')
+parser.add_argument('--gracedb-labels', metavar='LABEL', nargs='+',
+                    help='Apply the given list of labels to events uploaded '
+                         'to GraceDB.')
 parser.add_argument('--ifar-upload-threshold', type=float, required=True,
                     help='Inverse-FAR threshold for uploading candidate '
                          'triggers to GraceDB, in years.')
 parser.add_argument('--file-prefix', default='Live')
 
 parser.add_argument('--round-start-time', type=int, metavar='X',
                     help="Round up the start time to the nearest multiple of X"
@@ -839,27 +903,42 @@
                     help="Time in seconds to allow for a plan to be created")
 parser.add_argument('--run-snr-optimization', action='store_true',
                     default=False,
                     help='Run spawned followup processes to maximize SNR for '
                          'any trigger uploaded to GraceDB')
 parser.add_argument('--snr-opt-timeout', type=int, default=400, metavar='SECONDS',
                     help='Maximum allowed duration of followup process to maximize SNR')
+parser.add_argument('--snr-opt-label', type=str, default='SNR_OPTIMIZED',
+                    help='Label to apply to snr-optimized GraceDB uploads')
 
 
+parser.add_argument('--enable-embright-has-massgap', action='store_true', default=False,
+                    help='Estimate HasMassGap probability for EMBright info. Lower limit '
+                         'of the mass gap is equal to the maximum NS mass used for '
+                         'the source classification.')
+parser.add_argument('--embright-massgap-max', type=float, default=5.0, metavar='SOLAR MASSES',
+                    help='Upper limit of the mass gap, used for estimating '
+                         'HasMassGap probability.')
+parser.add_argument('--skymap-only-ifos', nargs='+',
+                    help="Detectors that only contribute in sky localization")
+
 scheme.insert_processing_option_group(parser)
 LiveSingle.insert_args(parser)
 fft.insert_fft_option_group(parser)
 Coincer.insert_args(parser)
 SingleDetSGChisq.insert_option_group(parser)
 mchirp_area.insert_args(parser)
 livepau.insert_live_pastro_option_group(parser)
 
 args = parser.parse_args()
+
 scheme.verify_processing_options(args, parser)
 fft.verify_fft_options(args, parser)
+ifos = set(args.channel_name.keys())
+analyze_singles = LiveSingle.verify_args(args, parser, ifos)
 
 if args.output_background is not None and len(args.output_background) != 2:
     parser.error('--output-background takes two parameters: period and path')
 if not args.enable_gracedb_upload and args.enable_single_detector_upload:
     parser.error('You are not allowed to enable single ifo upload without the '
                  '--enable-gracedb-upload option!')
 
@@ -878,18 +957,18 @@
         args.bank_file, args.sample_rate, total_pad, low_frequency_cutoff=lfc,
         approximant=args.approximant, increment=args.increment)
 if bank.min_f_lower < args.low_frequency_cutoff:
     parser.error('--low-frequency-cutoff ({} Hz) must not be larger than the '
                  'minimum f_lower across all templates '
                  '({} Hz)'.format(args.low_frequency_cutoff, bank.min_f_lower))
 
-ifos = set(args.channel_name.keys())
 logging.info('Analyzing data from detectors %s', ppdets(ifos))
 
 evnt = LiveEventManager(args, bank)
+logging.info('Detectors that only aid in the sky localization %s', ppdets(evnt.skymap_only_ifos))
 
 # include MPI rank and functional description into proctitle
 task_name = 'root' if evnt.rank == 0 else 'filtering'
 setproctitle('PyCBC Live rank {:d} [{}]'.format(evnt.rank, task_name))
 
 sg_chisq = SingleDetSGChisq.from_cli(args, bank, args.chisq_bins)
 
@@ -953,21 +1032,22 @@
         maxlen = args.max_length
     maxlen = int(maxlen)
     data_reader = {ifo: StrainBuffer.from_cli(ifo, args, maxlen)
                    for ifo in ifos}
     evnt.data_readers = data_reader
 
     # create single-detector background "estimators"
-    if args.enable_single_detector_background and evnt.rank == 0:
+    if analyze_singles and evnt.rank == 0:
         sngl_estimator = {ifo: LiveSingle.from_cli(args, ifo)
                           for ifo in ifos}
 
     # Create double coincident background estimator for every combo
     if args.enable_background_estimation and evnt.rank == 0:
-        ifo_combos = itertools.combinations(ifos, 2)
+        trigg_ifos = [ifo for ifo in ifos if ifo not in evnt.skymap_only_ifos]
+        ifo_combos = itertools.combinations(trigg_ifos, 2)
         estimators = []
         for combo in ifo_combos:
             logging.info('Will calculate %s background', ppdets(combo, "-"))
             estimators.append(Coincer.from_cli(
                 args, len(bank), args.analysis_chunk, list(combo)
             ))
 
@@ -1022,27 +1102,25 @@
                 status = data_reader[ifo].recalculate_psd()
                 psd_count[ifo] = args.psd_recompute_length - 1
             elif not status:
                 psd_count[ifo] = 0
             else:
                 psd_count[ifo] -= 1
 
-            if data_reader[ifo].psd is not None:
-                dist = data_reader[ifo].psd.dist
-                if dist < args.min_psd_abort_distance or dist > args.max_psd_abort_distance:
-                    logging.info("%s PSD dist %s outside acceptable range [%s, %s]",
-                                 ifo, dist, args.min_psd_abort_distance,
-                                 args.max_psd_abort_distance)
-                    status = False
+            status &= data_reader[ifo].check_psd_dist(
+                args.min_psd_abort_distance,
+                args.max_psd_abort_distance
+            )
 
             if status is True:
-                evnt.live_detectors.add(ifo)
-                if evnt.rank > 0:
-                    logging.info('Filtering %s', ifo)
-                    results[ifo] = mf.process_data(data_reader[ifo])
+                if ifo not in evnt.skymap_only_ifos:
+                    evnt.live_detectors.add(ifo)
+                    if evnt.rank > 0:
+                        logging.info('Filtering %s', ifo)
+                        results[ifo] = mf.process_data(data_reader[ifo])
             else:
                 logging.info('Insufficient data for %s analysis', ifo)
 
         if evnt.rank > 0:
             evnt.commit_results((results, data_end()))
         else:
             psds = {ifo: data_reader[ifo].psd for ifo in data_reader
@@ -1093,15 +1171,15 @@
 
                 # Pick the best coinc in this chunk
                 best_coinc = Coincer.pick_best_coinc(coinc_results)
 
                 evnt.check_coincs(list(results.keys()), best_coinc, psds)
 
             # Check for singles
-            if args.enable_single_detector_background:
+            if analyze_singles:
                 evnt.check_singles(results, psds)
 
             gates = {ifo: data_reader[ifo].gate_params for ifo in data_reader}
 
             # map the results file to an hdf file
             prefix = '{}-{}-{}-{}'.format(''.join(sorted(ifos)),
                                           args.file_prefix,
```

### Comparing `PyCBC-2.2.0/bin/pycbc_live_nagios_monitor` & `PyCBC-2.2.1/bin/pycbc_live_nagios_monitor`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_losc_segment_query` & `PyCBC-2.2.1/bin/pycbc_gwosc_segment_query`

 * *Files 2% similar despite different names*

```diff
@@ -37,16 +37,16 @@
     segment_list :  ligo.segments.segmentlist
         The interval returned by GWOSC
     segment_summary :  ligo.segments.segmentlist
         The segments returned by GWOSC
     """
 
     response = urlopen(
-        'https://www.gw-openscience.org/timeline/segments/json/O1/{}_{}/{}/{}/'.format(
-        ifo, segment_name, gps_start_time, duration))
+        f'https://www.gwosc.org/timeline/segments/json/O1/{ifo}_{segment_name}/{gps_start_time}/{duration}/'
+    )
 
     logging.info(response.info())
     json_segment_data = json.loads(response.read())
 
     summary_segment = ligo.segments.segmentlist([ligo.segments.segment(
                                                 json_segment_data['start'],
                                                 json_segment_data['end'])])
```

### Comparing `PyCBC-2.2.0/bin/pycbc_make_banksim` & `PyCBC-2.2.1/bin/pycbc_make_banksim`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_make_faithsim` & `PyCBC-2.2.1/bin/pycbc_make_faithsim`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_make_html_page` & `PyCBC-2.2.1/bin/pycbc_make_html_page`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_make_skymap` & `PyCBC-2.2.1/bin/pycbc_make_skymap`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_merge_inj_hdf` & `PyCBC-2.2.1/bin/pycbc_merge_inj_hdf`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_multi_inspiral` & `PyCBC-2.2.1/bin/pycbc_multi_inspiral`

 * *Files 14% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 """
 
 import logging
 import time
 from collections import defaultdict
 import argparse
 import numpy as np
+import h5py
 from pycbc import (
     detector, fft, init_logging, inject, opt, psd, scheme, strain, vetoes,
     waveform, DYN_RANGE_FAC
 )
 from pycbc.events import ranking, coherent as coh, EventManagerCoherent
 from pycbc.filter import MatchedFilterControl
 from pycbc.types import TimeSeries, zeros, float32, complex64
@@ -65,15 +66,16 @@
                     help="Method to use when clustering triggers. 'window' - "
                          "cluster within a fixed time window defined by the "
                          "cluster-window option (default); or 'template' - "
                          "cluster within windows defined by each template's "
                          "chirp length.")
 parser.add_argument("--cluster-window", type=float, default=0,
                     help="Length of clustering window in seconds.")
-parser.add_argument("--bank-veto-bank-file", type=str)
+parser.add_argument("--bank-veto-bank-file", type=str, help="Path to the "
+                    "bank file used to compute the the bank chi-square veto.")
 parser.add_argument("--chisq-bins", default=0)
 # Commenting out options which are not yet implemented
 # parser.add_argument("--chisq-threshold", type=float, default=0) 
 # parser.add_argument("--chisq-delta", type=float, default=0)
 parser.add_argument("--autochi-number-points", type=int, default=0)
 parser.add_argument("--autochi-stride", type=int, default=0)
 parser.add_argument("--autochi-onesided", action='store_true',
@@ -93,14 +95,17 @@
 parser.add_argument("--user-tag", type=str, metavar="TAG",
                     help="This is used to identify FULL_DATA jobs for "
                          "compatibility with pipedown post-processing. Option "
                          "will be removed when no longer needed.")
 # Arguments added for the coherent stuff
 parser.add_argument("--ra", type=float, help="Right ascension, in radians")
 parser.add_argument("--dec", type=float, help="Declination, in radians")
+parser.add_argument("--sky-grid", type=str,
+                    help="Sky-grid (hdf file) containing two datasets : "
+                    "ra and dec, both in radians")
 parser.add_argument("--coinc-threshold", type=float, default=0.0,
                     help="Triggers with coincident/coherent snr below this "
                          "value will be discarded.")
 parser.add_argument("--do-null-cut", action='store_true',
                     help="Apply a cut based on null SNR.")
 parser.add_argument("--null-min", type=float, default=5.25,
                     help="Triggers with null_snr above this value will be"
@@ -185,31 +190,50 @@
         args, segments, strain_dict, flen, delta_f, flow, args.instruments,
         dyn_range_factor=DYN_RANGE_FAC, precision='single')
     # Currently we are using the same matched-filter parameters for all
     # ifos. Therefore only one MatchedFilterControl needed. Maybe this can
     # change if needed. Segments is only used to get tlen etc. which is
     # same for all ifos, so just send the first ifo
     template_mem = zeros(tlen, dtype=complex64)
-    # Calculate time delays to each detector and apply time slide shifts
+    
+    #Read the sky grid or the single sky position
+    if args.sky_grid is not None and args.ra is not None and args.dec is not None:
+        parser.error('Give either a sky grid or a sky position, not both')
+    
+    if args.sky_grid is not None:
+        sky_grid = h5py.File(args.sky_grid, 'r')
+        ra = np.array(sky_grid['ra'])
+        dec = np.array(sky_grid['dec'])
+    if args.ra is not None and args.dec is not None:
+        ra = np.array([args.ra])
+        dec = np.array([args.dec])
+    
+    sky_positions = np.array([ra, dec])
+    num_sky_positions = sky_positions.shape[1]
+    positions_array = np.arange(num_sky_positions)
+
+    # Calculate time delays to each detector for each sky position and apply time slide shifts
     slide_ids = np.arange(1 + args.num_slides)
     time_slides = {
         ifo: args.slide_shift * slide_ids * n_ifo
         for n_ifo, ifo in enumerate(args.instruments)}
     time_delay_idx = {
-        slide: {
-            ifo: int(round(
-                (detector.Detector(ifo).time_delay_from_earth_center(
-                    args.ra, args.dec, t_gps)
-                + time_slides[ifo][slide])
-                * sample_rate
-                ))
-            for ifo in args.instruments
-            }
-        for slide in slide_ids
+            slide: {
+                position_index: {
+                    ifo: int(round(
+                        (detector.Detector(ifo).time_delay_from_earth_center(
+                            sky_positions[0][position_index], sky_positions[1][position_index], t_gps)
+                            + time_slides[ifo][slide])
+                        * sample_rate
+                        ))
+                        for ifo in args.instruments
+                } for position_index in positions_array
+            } for slide in slide_ids
         }
+    
     # Matched filter each ifo. Don't cluster here for a coherent search.
     # Clustering happens at the end of the template loop.
     # FIXME: The single detector SNR threshold should not necessarily be
     #        applied to every IFO (usually only 2 most sensitive in
     #        network)
     matched_filter = {
         ifo: MatchedFilterControl(
@@ -256,27 +280,27 @@
         'bank_chisq': None,
         'bank_chisq_dof': None,
         'cont_chisq': None,
         'slide_id': int
         }
     ifo_names = sorted(ifo_out_vals.keys())
     network_out_types = {
-        'latitude': float32,
-        'longitude': float32,
+        'dec': float32,
+        'ra': float32,
         'time_index': int,
         'coherent_snr': float32,
         'null_snr': float32,
         'nifo': int,
         'my_network_chisq': float32,
         'reweighted_snr': float32,
         'slide_id': int
         }
     network_out_vals = {
-        'latitude': None,
-        'longitude': None,
+        'dec': None,
+        'ra': None,
         'time_index': None,
         'coherent_snr': None,
         'null_snr': None,
         'nifo': None,
         'my_network_chisq': None, 
         'reweighted_snr': None,
         'slide_id': int
@@ -297,20 +321,25 @@
     nfilters = 0
     logging.info("Full template bank size: %d", n_bank)
     for ifo in args.instruments:
         bank.template_thinning(inj_filter_rejector[ifo])
     if not len(bank) == n_bank:
         n_bank = len(bank)
         logging.info("Template bank size after thinning: %d", n_bank)
+   
     # Antenna patterns
-    fp = {}
-    fc = {}
-    for ifo in args.instruments:
-        fp[ifo], fc[ifo] = detector.Detector(ifo).antenna_pattern(
-            args.ra, args.dec, polarization=0, t_gps=t_gps)
+    antenna_patterns = [[[0 for i in range(2)] for position_index in positions_array] for i in range(len(args.instruments))]
+    for i, ifo in enumerate(args.instruments):
+        for position_index in positions_array:
+            antenna_patterns[i][position_index] = detector.Detector(ifo).antenna_pattern(sky_positions[0][position_index], sky_positions[1][position_index], polarization=0, t_gps=t_gps)
+
+    ap = {}
+    for i, ifo in enumerate(args.instruments):
+        ap[ifo] = antenna_patterns[i]
+
     # Loop over templates
     for t_num, template in enumerate(bank):
         # Loop over segments
         for s_num,stilde in enumerate(segments[args.instruments[0]]):
             stilde = {ifo : segments[ifo][s_num] for ifo in args.instruments}
             # Filter check checks the 'inj_filter_rejector' options to
             # determine whether to filter this template/segment 
@@ -361,250 +390,258 @@
                         snr_ts[matched_filter[ifo].segments[s_num].analyze]
                         * norm)
                 norm_dict[ifo] = norm
                 corr_dict[ifo] = corr.copy()
                 idx[ifo] = ind.copy()
                 snrv_dict[ifo] = snrv.copy()
                 snr[ifo] = snrv * norm
+            
             # FIXME: wrong comment?
             # Move onto next segment if there are no triggers.
             if len(ifo_list)==0: continue
             # Loop through slides, staring with the zero-lag
             for slide in range(args.num_slides + 1):
                 logging.info(
-                    "Analyzing slide %d/%d", slide, args.num_slides)
-                # Save the indexes of triggers (if we have any)
-                # Even if we have none, need to keep an empty dictionary.
-                # Only do this if idx doesn't get time shifted out of the
-                # time we are looking at, i.e., require
-                # idx[ifo] - time_delay_idx[slide][ifo] to be in
-                # (0, len(snr_dict[ifo]))
-                idx_dict = {
+                        "Analyzing slide %d/%d", slide, args.num_slides)
+                for position_index in positions_array:
+                    logging.info(
+                        "Analyzing sky position %d/%d", position_index+1, len(positions_array))
+                    # Save the indexes of triggers (if we have any)
+                    # Even if we have none, need to keep an empty dictionary.
+                    # Only do this if idx doesn't get time shifted out of the
+                    # time we are looking at, i.e., require
+                    # idx[ifo] - time_delay_idx[slide][position_index][ifo] to be in
+                    # (0, len(snr_dict[ifo]))
+                    idx_dict = {
                     ifo: idx[ifo][
                         np.logical_and(
-                            idx[ifo] > time_delay_idx[slide][ifo],
-                            idx[ifo] - time_delay_idx[slide][ifo]
+                            idx[ifo] > time_delay_idx[slide][position_index][ifo],
+                            idx[ifo] - time_delay_idx[slide][position_index][ifo]
                                 < len(snr_dict[ifo]))
                         ]
                     for ifo in args.instruments
                     }
-                # Find triggers that are coincident (in geocent time) in
-                # multiple ifos. If a single ifo analysis then just use the
-                # indexes from that ifo.
-                if nifo > 1:
-                    coinc_idx = coh.get_coinc_indexes(
-                        idx_dict, time_delay_idx[slide])
-                else:
-                    coinc_idx = (
-                        idx_dict[args.instruments[0]]
-                        - time_delay_idx[slide][args.instruments[0]]
-                        )
-                logging.info("Found %d coincident triggers", len(coinc_idx))
-                for ifo in args.instruments:
-                    # Raise errror if this segment has no data
-                    # FIXME: raise this sooner?
-                    if len(snr_dict[ifo])==0:
-                        raise RuntimeError(
-                            'The SNR triggers dictionary is empty. This '
-                            'should not be possible.')
-                # Time delay is applied to indices
-                coinc_idx_det_frame = {
-                    ifo: (coinc_idx + time_delay_idx[slide][ifo]) % len(snr_dict[ifo])
-                    for ifo in args.instruments}
-                # Calculate the coincident and coherent snr. Check we have
-                # data before we try to compute the coherent snr
-                if len(coinc_idx) != 0 and nifo > 1:
-                    # Find coinc snr at trigger times and apply coinc snr
-                    # threshold
-                    rho_coinc, coinc_idx, coinc_triggers = \
-                        coh.coincident_snr(
-                            snr_dict, coinc_idx, args.coinc_threshold,
-                            time_delay_idx[slide])
-                    logging.info(
-                        "%d coincident tiggers above coincident SNR threshold",
-                        len(coinc_idx))
-                    if len(coinc_idx) != 0:
-                        logging.info(
-                            "Max coincident SNR = %.2f", max(rho_coinc))
-                # If there is only one ifo, then coinc_triggers is just the
-                # triggers from the ifo
-                elif len(coinc_idx) != 0 and nifo == 1:
-                    coinc_triggers = {
-                        args.instruments[0]: snr[args.instruments[0]][
-                            coinc_idx_det_frame[args.instruments[0]]
-                            ]
-                        }
-                else:
-                    coinc_triggers = {}
-                    logging.info("No triggers above coincident SNR threshold")
-                # If we have triggers above coinc threshold and more than 2
-                # ifos, then calculate the coherent statistics
-                if len(coinc_idx) != 0 and nifo > 2:
-                    if args.projection=='left+right':
-                        # Left polarized coherent SNR
-                        project_l = coh.get_projection_matrix(
-                            fp, fc, sigma, projection='left')
-                        (rho_coh_l, coinc_idx_l, coinc_triggers_l,
-                                rho_coinc_l) = \
-                            coh.coherent_snr(
-                                coinc_triggers, coinc_idx,
-                                args.coinc_threshold, project_l, rho_coinc)
-                        # Right polarized coherent SNR
-                        project_r = coh.get_projection_matrix(
-                            fp, fc, sigma, projection='right')
-                        (rho_coh_r, coinc_idx_r, coinc_triggers_r,
-                                rho_coinc_r) = \
-                            coh.coherent_snr(
-                                coinc_triggers, coinc_idx,
-                                args.coinc_threshold, project_r, rho_coinc)
-                        # Point by point, track the larger of the two and store it
-                        max_idx = np.argmax([rho_coh_l, rho_coh_r], axis=0)
-                        rho_coh = np.where(
-                            max_idx==0, rho_coh_l, rho_coh_r)
-                        coinc_idx = np.where(
-                            max_idx==0, coinc_idx_l, coinc_idx_r)
-                        coinc_triggers = {
-                            ifo: np.where(
-                                max_idx==0, coinc_triggers_l[ifo],
-                                coinc_triggers_r[ifo])
-                            for ifo in coinc_triggers_l}
-                        rho_coinc = np.where(
-                            max_idx==0, rho_coinc_l, rho_coinc_r)
+                    
+                    # Find triggers that are coincident (in geocent time) in
+                    # multiple ifos. If a single ifo analysis then just use the
+                    # indexes from that ifo.
+                    if nifo > 1:
+                        coinc_idx = coh.get_coinc_indexes(
+                            idx_dict, time_delay_idx[slide][position_index])
                     else:
-                        project = coh.get_projection_matrix(
-                            fp, fc, sigma, projection=args.projection)
-                        rho_coh, coinc_idx, coinc_triggers, rho_coinc = \
-                            coh.coherent_snr(
-                                coinc_triggers, coinc_idx,
-                                args.coinc_threshold, project, rho_coinc)
-                    logging.info(
-                        "%d triggers above coherent threshold", len(rho_coh))
-                    if len(coinc_idx) != 0:
-                        logging.info("Max coherent SNR = %.2f", max(rho_coh))
-                        #Find the null snr
-                        (null, rho_coh, rho_coinc, coinc_idx,
-                                coinc_triggers) =\
-                            coh.null_snr(
-                                rho_coh, rho_coinc, snrv=coinc_triggers,
-                                index=coinc_idx)
-                        if len(coinc_idx) != 0:
-                            logging.info("Max null SNR = %.2f", max(null))
-                        logging.info(
-                            "%d triggers above null threshold", len(null))
-                # We are now going to find the individual detector chi2
-                # values. To do this it is useful to find the indexes of
-                # the triggers in the detector frame.
-                if len(coinc_idx) != 0:
-                    # coinc_idx_det_frame is redefined to account for the
-                    # cuts to coinc_idx above
+                        coinc_idx = (
+                            idx_dict[args.instruments[0]]
+                            - time_delay_idx[slide][position_index][args.instruments[0]]
+                            )
+                    logging.info("Found %d coincident triggers", len(coinc_idx))
+                    for ifo in args.instruments:
+                        # Raise errror if this segment has no data
+                        # FIXME: raise this sooner?
+                        if len(snr_dict[ifo])==0:
+                            raise RuntimeError(
+                                'The SNR triggers dictionary is empty. This '
+                                'should not be possible.')
+                    # Time delay is applied to indices
                     coinc_idx_det_frame = {
-                        ifo: (coinc_idx + time_delay_idx[slide][ifo]) % len(snr_dict[ifo])
+                        ifo: (coinc_idx + time_delay_idx[slide][position_index][ifo]) % len(snr_dict[ifo])
                         for ifo in args.instruments}
-                    coherent_ifo_trigs = {
-                        ifo: snr_dict[ifo][coinc_idx_det_frame[ifo]]
-                        for ifo in args.instruments}
-                    # Calculate the power and autochi2 values for the coinc
-                    # indexes (this uses the snr timeseries before the time
-                    # delay, so we need to undo it. Same for normalisation)
-                    chisq = {}
-                    chisq_dof = {}
-                    for ifo in args.instruments:
-                        chisq[ifo], chisq_dof[ifo] = power_chisq.values(
-                            corr_dict[ifo],
-                            coherent_ifo_trigs[ifo] / norm_dict[ifo],
-                            norm_dict[ifo], stilde[ifo].psd,
-                            coinc_idx_det_frame[ifo]
-                            + stilde[ifo].analyze.start,
-                            template)
-                    # Calculate network chisq value
-                    network_chisq_dict = coh.network_chisq(
-                        chisq, chisq_dof, coherent_ifo_trigs)
-                    # Calculate chisq reweighted SNR
-                    if nifo > 2:
-                        reweighted_snr = ranking.newsnr(
-                            rho_coh, network_chisq_dict)
-                        # Calculate null reweighted SNR
-                        reweighted_snr = coh.reweight_snr_by_null(
-                            reweighted_snr, null, rho_coh)
-                    elif nifo == 2:
-                        reweighted_snr = ranking.newsnr(
-                            rho_coinc, network_chisq_dict)
-                    else:
-                        rho_sngl = abs(
-                            snr[args.instruments[0]][
+                    # Calculate the coincident and coherent snr. Check we have
+                    # data before we try to compute the coherent snr
+                    if len(coinc_idx) != 0 and nifo > 1:
+                        # Find coinc snr at trigger times and apply coinc snr
+                        # threshold
+                        rho_coinc, coinc_idx, coinc_triggers = \
+                            coh.coincident_snr(
+                                snr_dict, coinc_idx, args.coinc_threshold,
+                                time_delay_idx[slide][position_index])
+                        logging.info(
+                            "%d coincident tiggers above coincident SNR threshold",
+                            len(coinc_idx))
+                        if len(coinc_idx) != 0:
+                            logging.info(
+                                "Max coincident SNR = %.2f", max(rho_coinc))
+                    # If there is only one ifo, then coinc_triggers is just the
+                    # triggers from the ifo
+                    elif len(coinc_idx) != 0 and nifo == 1:
+                        coinc_triggers = {
+                            args.instruments[0]: snr[args.instruments[0]][
                                 coinc_idx_det_frame[args.instruments[0]]
                                 ]
-                            )
-                        reweighted_snr = ranking.newsnr(
-                            rho_sngl, network_chisq_dict)
-                    # Need all out vals to be the same length. This means
-                    # the entries that are single values need to be
-                    # repeated once per event.
-                    num_events = len(reweighted_snr)
-                    # the output will only be possible if
-                    # len(networkchi2) == num_events
-                    for ifo in args.instruments:
-                        (ifo_out_vals['bank_chisq'],
-                                ifo_out_vals['bank_chisq_dof']) =\
-                            bank_chisq.values(
-                                template, stilde[ifo].psd, stilde[ifo],
+                            }
+                    else:
+                        coinc_triggers = {}
+                        logging.info("No triggers above coincident SNR threshold")
+                    # If we have triggers above coinc threshold and more than 2
+                    # ifos, then calculate the coherent statistics
+                    if len(coinc_idx) != 0 and nifo > 2:
+                        if args.projection=='left+right':
+                            #Plus and cross polarization
+                            fp = {ifo: ap[ifo][position_index][0] for ifo in args.instruments}
+                            fc = {ifo: ap[ifo][position_index][1] for ifo in args.instruments}
+                            # Left polarized coherent SNR
+                            project_l = coh.get_projection_matrix(
+                                fp, fc, sigma, projection='left')
+                            (rho_coh_l, coinc_idx_l, coinc_triggers_l,
+                                    rho_coinc_l) = \
+                                coh.coherent_snr(
+                                    coinc_triggers, coinc_idx,
+                                    args.coinc_threshold, project_l, rho_coinc)
+                            # Right polarized coherent SNR
+                            project_r = coh.get_projection_matrix(
+                                fp, fc, sigma, projection='right')
+                            (rho_coh_r, coinc_idx_r, coinc_triggers_r,
+                                    rho_coinc_r) = \
+                                coh.coherent_snr(
+                                    coinc_triggers, coinc_idx,
+                                    args.coinc_threshold, project_r, rho_coinc)
+                            # Point by point, track the larger of the two and store it
+                            max_idx = np.argmax([rho_coh_l, rho_coh_r], axis=0)
+                            rho_coh = np.where(
+                                max_idx==0, rho_coh_l, rho_coh_r)
+                            coinc_idx = np.where(
+                                max_idx==0, coinc_idx_l, coinc_idx_r)
+                            coinc_triggers = {
+                                ifo: np.where(
+                                    max_idx==0, coinc_triggers_l[ifo],
+                                    coinc_triggers_r[ifo])
+                                for ifo in coinc_triggers_l}
+                            rho_coinc = np.where(
+                                max_idx==0, rho_coinc_l, rho_coinc_r)
+                        else:
+                            project = coh.get_projection_matrix(
+                                fp, fc, sigma, projection=args.projection)
+                            rho_coh, coinc_idx, coinc_triggers, rho_coinc = \
+                                coh.coherent_snr(
+                                    coinc_triggers, coinc_idx,
+                                    args.coinc_threshold, project, rho_coinc)
+                        logging.info(
+                            "%d triggers above coherent threshold", len(rho_coh))
+                        if len(coinc_idx) != 0:
+                            logging.info("Max coherent SNR = %.2f", max(rho_coh))
+                            #Find the null snr
+                            (null, rho_coh, rho_coinc, coinc_idx,
+                                    coinc_triggers) =\
+                                coh.null_snr(
+                                    rho_coh, rho_coinc, snrv=coinc_triggers,
+                                    index=coinc_idx)
+                            if len(coinc_idx) != 0:
+                                logging.info("Max null SNR = %.2f", max(null))
+                            logging.info(
+                                "%d triggers above null threshold", len(null))
+                    # We are now going to find the individual detector chi2
+                    # values. To do this it is useful to find the indexes of
+                    # the triggers in the detector frame.
+                    if len(coinc_idx) != 0:
+                        # coinc_idx_det_frame is redefined to account for the
+                        # cuts to coinc_idx above
+                        coinc_idx_det_frame = {
+                            ifo: (coinc_idx + time_delay_idx[slide][position_index][ifo]) % len(snr_dict[ifo])
+                            for ifo in args.instruments}
+                        coherent_ifo_trigs = {
+                            ifo: snr_dict[ifo][coinc_idx_det_frame[ifo]]
+                            for ifo in args.instruments}
+                            # Calculate the power and autochi2 values for the coinc
+                        # indexes (this uses the snr timeseries before the time
+                        # delay, so we need to undo it. Same for normalisation)
+                        chisq = {}
+                        chisq_dof = {}
+                        for ifo in args.instruments:
+                            chisq[ifo], chisq_dof[ifo] = power_chisq.values(
+                                corr_dict[ifo],
                                 coherent_ifo_trigs[ifo] / norm_dict[ifo],
-                                norm_dict[ifo],
+                                norm_dict[ifo], stilde[ifo].psd,
                                 coinc_idx_det_frame[ifo]
-                                + stilde[ifo].analyze.start)
-                        ifo_out_vals['cont_chisq'] = autochisq.values(
-                            snr_dict[ifo] / norm_dict[ifo],
-                            coinc_idx_det_frame[ifo], template,
-                            stilde[ifo].psd, norm_dict[ifo],
-                            stilde=stilde[ifo], low_frequency_cutoff=flow)
-                        ifo_out_vals['chisq'] = chisq[ifo]
-                        ifo_out_vals['chisq_dof'] = chisq_dof[ifo]
-                        ifo_out_vals['time_index'] = (
-                            coinc_idx_det_frame[ifo]
-                            + stilde[ifo].cumulative_index
-                            )
-                        ifo_out_vals['snr'] = coherent_ifo_trigs[ifo]
-                        # IFO is stored as an int
-                        ifo_out_vals['ifo'] = (
-                            [event_mgr.ifo_dict[ifo]] * num_events
-                            )
-                        # Time slide ID
-                        ifo_out_vals['slide_id'] = [slide] * num_events
-                        event_mgr.add_template_events_to_ifo(
-                            ifo, ifo_names,
-                            [ifo_out_vals[n] for n in ifo_names])
-                    if nifo>2:
-                        network_out_vals['coherent_snr'] = rho_coh
-                        network_out_vals['null_snr'] = null
-                    elif nifo==2:
-                        network_out_vals['coherent_snr'] = rho_coinc
-                    else:
-                        network_out_vals['coherent_snr'] = (
-                            abs(snr[args.instruments[0]][
-                                coinc_idx_det_frame[args.instruments[0]]
-                                ])
-                            )
-                    network_out_vals['reweighted_snr'] = reweighted_snr
-                    network_out_vals['my_network_chisq'] = (
-                        np.real(network_chisq_dict))
-                    network_out_vals['time_index'] = (
-                        coinc_idx + stilde[ifo].cumulative_index)
-                    network_out_vals['nifo'] = [nifo] * num_events
-                    network_out_vals['ra'] = [args.ra] * num_events
-                    network_out_vals['dec'] = [args.dec] * num_events
-                    network_out_vals['slide_id'] = [slide] * num_events
-                    event_mgr.add_template_events_to_network(
-                        network_names,
-                        [network_out_vals[n] for n in network_names])
-        if args.cluster_method == "window":
-            cluster_window = int(args.cluster_window * sample_rate)
-        elif args.cluster_method == "template":
-            cluster_window = int(template.chirp_length * sample_rate)
-        # Cluster template events by slide
-        for slide in range(args.num_slides + 1):
-            logging.info("Clustering slide %d", slide)
-            event_mgr.cluster_template_network_events(
-                'time_index', 'reweighted_snr', cluster_window, slide=slide)
-    event_mgr.finalize_template_events()
+                                + stilde[ifo].analyze.start,
+                                template)
+                        # Calculate network chisq value
+                        network_chisq_dict = coh.network_chisq(
+                            chisq, chisq_dof, coherent_ifo_trigs)
+                        # Calculate chisq reweighted SNR
+                        if nifo > 2:
+                            reweighted_snr = ranking.newsnr(
+                                rho_coh, network_chisq_dict)
+                            # Calculate null reweighted SNR
+                            reweighted_snr = coh.reweight_snr_by_null(
+                                reweighted_snr, null, rho_coh)
+                        elif nifo == 2:
+                            reweighted_snr = ranking.newsnr(
+                                rho_coinc, network_chisq_dict)
+                        else:
+                            rho_sngl = abs(
+                                snr[args.instruments[0]][
+                                    coinc_idx_det_frame[args.instruments[0]]
+                                    ]
+                                )
+                            reweighted_snr = ranking.newsnr(
+                                rho_sngl, network_chisq_dict)
+                        # Need all out vals to be the same length. This means
+                        # the entries that are single values need to be
+                        # repeated once per event.
+                        num_events = len(reweighted_snr)
+                        # the output will only be possible if
+                        # len(networkchi2) == num_events
+                        for ifo in args.instruments:
+                            (ifo_out_vals['bank_chisq'],
+                                    ifo_out_vals['bank_chisq_dof']) =\
+                                bank_chisq.values(
+                                    template, stilde[ifo].psd, stilde[ifo],
+                                    coherent_ifo_trigs[ifo] / norm_dict[ifo],
+                                    norm_dict[ifo],
+                                    coinc_idx_det_frame[ifo]
+                                    + stilde[ifo].analyze.start)
+                            ifo_out_vals['cont_chisq'] = autochisq.values(
+                                snr_dict[ifo] / norm_dict[ifo],
+                                coinc_idx_det_frame[ifo], template,
+                                stilde[ifo].psd, norm_dict[ifo],
+                                stilde=stilde[ifo], low_frequency_cutoff=flow)
+                            ifo_out_vals['chisq'] = chisq[ifo]
+                            ifo_out_vals['chisq_dof'] = chisq_dof[ifo]
+                            ifo_out_vals['time_index'] = (
+                                coinc_idx_det_frame[ifo]
+                                + stilde[ifo].cumulative_index
+                                )
+                            ifo_out_vals['snr'] = coherent_ifo_trigs[ifo]
+                            # IFO is stored as an int
+                            ifo_out_vals['ifo'] = (
+                                [event_mgr.ifo_dict[ifo]] * num_events
+                                )
+                            # Time slide ID
+                            ifo_out_vals['slide_id'] = [slide] * num_events
+                            event_mgr.add_template_events_to_ifo(
+                                ifo, ifo_names,
+                                [ifo_out_vals[n] for n in ifo_names])
+                        if nifo>2:
+                            network_out_vals['coherent_snr'] = rho_coh
+                            network_out_vals['null_snr'] = null
+                        elif nifo==2:
+                            network_out_vals['coherent_snr'] = rho_coinc
+                        else:
+                            network_out_vals['coherent_snr'] = (
+                                abs(snr[args.instruments[0]][
+                                    coinc_idx_det_frame[args.instruments[0]]
+                                    ])
+                                )
+                        network_out_vals['reweighted_snr'] = reweighted_snr
+                        network_out_vals['my_network_chisq'] = (
+                            np.real(network_chisq_dict))
+                        network_out_vals['time_index'] = (
+                            coinc_idx + stilde[ifo].cumulative_index)
+                        network_out_vals['nifo'] = [nifo] * num_events
+                        network_out_vals['dec'] = [sky_positions[1][position_index]] * num_events
+                        network_out_vals['ra'] = [sky_positions[0][position_index]] * num_events
+                        network_out_vals['slide_id'] = [slide] * num_events
+                        event_mgr.add_template_events_to_network(
+                            network_names,
+                            [network_out_vals[n] for n in network_names])
+            if args.cluster_method == "window":
+                cluster_window = int(args.cluster_window * sample_rate)
+            elif args.cluster_method == "template":
+                cluster_window = int(template.chirp_length * sample_rate)
+            # Cluster template events by slide
+            for slide in range(args.num_slides + 1):
+                logging.info("Clustering slide %d", slide)
+                event_mgr.cluster_template_network_events(
+                    'time_index', 'reweighted_snr', cluster_window, slide=slide)
+        event_mgr.finalize_template_events()
 event_mgr.write_events(args.output)
 logging.info("Finished")
 logging.info("Time to complete analysis: %d", int(time.time() - time_init))
```

### Comparing `PyCBC-2.2.0/bin/pycbc_optimal_snr` & `PyCBC-2.2.1/bin/pycbc_optimal_snr`

 * *Files 0% similar despite different names*

```diff
@@ -86,15 +86,15 @@
                     psd2[:] = np.inf
                     psd2[0:len(psd)] = psd
                 else:
                     psd2[:] = psd[0:self.length]
                 psd = psd2
             if self.f_low is not None and self.f_low < self.file_f_low:
                 # avoid using the PSD below the f_low given in the file
-                k = int(file_f_low / psd.delta_f)
+                k = int(self.file_f_low / psd.delta_f)
                 psd[0:k] = np.inf
             self._curr_psd[curr_pid] = psd
             self._curr_psd_index[curr_pid] = index
         return self._curr_psd[curr_pid]
 
 def parse_injection_range(num_inj, rangestr):
     part = int(rangestr.split('/')[0])
```

### Comparing `PyCBC-2.2.0/bin/pycbc_optimize_snr` & `PyCBC-2.2.1/bin/pycbc_optimize_snr`

 * *Files 10% similar despite different names*

```diff
@@ -11,28 +11,35 @@
 import pycbc
 from pycbc import (
     DYN_RANGE_FAC, fft, scheme, version, waveform
 )
 from pycbc.types import MultiDetOptionAction, zeros, load_frequencyseries
 from pycbc.filter import matched_filter_core
 import pycbc.waveform.bank
-from pycbc.conversions import (
-    mchirp_from_mass1_mass2, mtotal_from_mchirp_eta,
-    mass1_from_mtotal_eta, mass2_from_mtotal_eta
-)
+import pycbc.conversions as cv
 from pycbc.io import live
 from pycbc.io.hdf import load_hdf5_to_dict
 from pycbc.detector import Detector
 from pycbc.psd import interpolate
 
 try:
     import pyswarms as ps
 except:
     ps = None
 
+# Set a minimum mass for points tried in optimization allowing for
+# minimal slop relative to the lightest template
+MIN_CPT_MASS = 0.99
+
+# Set a large maximum total mass
+MAX_MTOTAL = 500.
+
+# Set a notional maximum and minimum possible eta
+MAX_ETA = 0.24999999999
+MIN_ETA = 0.01
 
 Nfeval = 0
 start_time = time.time()
 
 def callback_func(Xi, convergence=0):
     global Nfeval
     logging.info("Currently at %d %s", Nfeval, convergence)
@@ -52,29 +59,29 @@
     flen = argv[3]
     approximant = argv[4]
     flow = argv[5]
     f_end = argv[6]
     delta_f = argv[7]
     sample_rate = argv[8]
     distance = 1.0 / DYN_RANGE_FAC
-    mtotal = mtotal_from_mchirp_eta(v[0], v[1])
-    mass1 = mass1_from_mtotal_eta(mtotal, v[1])
-    mass2 = mass2_from_mtotal_eta(mtotal, v[1])
+    mtotal = cv.mtotal_from_mchirp_eta(v[0], v[1])
+    mass1 = cv.mass1_from_mtotal_eta(mtotal, v[1])
+    mass2 = cv.mass2_from_mtotal_eta(mtotal, v[1])
 
     # enforce broadly accepted search space boundaries
-    if mass1 < 1 or mass2 < 1 or mtotal > 500:
+    if mass1 < MIN_CPT_MASS or mass2 < MIN_CPT_MASS or mtotal > MAX_MTOTAL:
         return -numpy.inf, {}
 
     try:
         htilde = waveform.get_waveform_filter(
                 zeros(flen, dtype=numpy.complex64),
                 approximant=approximant,
                 mass1=mass1, mass2=mass2, spin1z=v[2], spin2z=v[3],
                 f_lower=flow, f_final=f_end, delta_f=delta_f,
-                delta_t=1.0/sample_rate, distance=distance)
+                delta_t=1./sample_rate, distance=distance)
     except RuntimeError:
         if raise_err:
             raise
         # assume a failure in the waveform approximant
         # due to the choice of parameters
         else:
             return -numpy.inf, {}
@@ -115,40 +122,56 @@
         argv = argv[0]
     nsnr, _ = compute_network_snr_core(v, *argv)
     return -nsnr
 
 
 def compute_minus_network_snr_pso(v, *argv, **kwargs):
     argv = kwargs['args']
-    nsnr_array = numpy.array([-compute_network_snr_core(v_i, *argv)[0] for v_i in v])
-    return nsnr_array
+    nsnr_array = numpy.array([compute_network_snr_core(v_i, *argv)[0] for v_i in v])
+    return -nsnr_array
 
 
-def optimize_di(bounds, cli_args, extra_args):
-    bounds = [
+def normalize_initial_point(initial_point, bounds):
+    return (initial_point - bounds[:,0]) / (bounds[:,1] - bounds[:,0])
+
+def optimize_di(bounds, cli_args, extra_args, initial_point):
+    bounds = numpy.array([
         bounds['mchirp'],
         bounds['eta'],
         bounds['spin1z'],
         bounds['spin2z']
-    ]
+    ])
+
+
+    # Currently only implemented for random seed initial array
+    rng = numpy.random.mtrand._rand
+    population_shape=(cli_args.di_popsize, 4)
+    population = rng.uniform(size=population_shape)
+    if cli_args.include_candidate_in_optimizer:
+        # Re-normalize the initial point into the correct range
+        point_init = normalize_initial_point(initial_point, bounds)
+        # add the initial point to the population
+        population = numpy.concatenate((population[:-1], point_init))
+
     results = differential_evolution(
         compute_minus_network_snr,
         bounds,
         maxiter=cli_args.di_maxiter,
         workers=(cli_args.cores or -1),
         popsize=cli_args.di_popsize,
         mutation=(0.5, 1),
         recombination=0.7,
         callback=callback_func,
-        args=extra_args
+        args=extra_args,
+        init=population
     )
     return results.x
 
 
-def optimize_shgo(bounds, cli_args, extra_args):
+def optimize_shgo(bounds, cli_args, extra_args, initial_point): # pylint: disable=unused-argument
     bounds = [
         bounds['mchirp'],
         bounds['eta'],
         bounds['spin1z'],
         bounds['spin2z']
     ]
     results = shgo(
@@ -157,38 +180,56 @@
         args=extra_args,
         iters=args.shgo_iters,
         n=args.shgo_samples,
         sampling_method="sobol"
     )
     return results.x
 
+def normalize_population(population, min_bounds, max_bounds):
+    norm_pop = min_bounds + population * (max_bounds - min_bounds)
+
+    return norm_pop
 
-def optimize_pso(bounds, cli_args, extra_args):
+def optimize_pso(bounds, cli_args, extra_args, initial_point):
     options = {
         'c1': cli_args.pso_c1,
         'c2': cli_args.pso_c2,
         'w': cli_args.pso_w
     }
-    min_bounds = [
+    min_bounds = numpy.array([
         bounds['mchirp'][0],
         bounds['eta'][0],
         bounds['spin1z'][0],
         bounds['spin2z'][0]
-    ]
-    max_bounds = [
+    ])
+    max_bounds = numpy.array([
         bounds['mchirp'][1],
         bounds['eta'][1],
         bounds['spin1z'][1],
         bounds['spin2z'][1]
-    ]
+    ])
+
+    # Manually generate the initial points, this is the same as the default
+    # method, but allows us to make some modifications
+    population = numpy.random.uniform(
+        low=0.0, high=1.0, size=(cli_args.pso_particles, 4)
+    )
+    population = normalize_population(population, min_bounds, max_bounds)
+
+    if cli_args.include_candidate_in_optimizer:
+        # add the initial point to the population
+        population = numpy.concatenate((population[:-1],
+                                        initial_point))
+
     optimizer = ps.single.GlobalBestPSO(
         n_particles=cli_args.pso_particles,
         dimensions=4,
         options=options,
-        bounds=(min_bounds, max_bounds)
+        bounds=(min_bounds, max_bounds),
+        init_pos=population
     )
     _, results = optimizer.optimize(
         compute_minus_network_snr_pso,
         iters=cli_args.pso_iters,
         n_processes=cli_args.cores,
         args=extra_args
     )
@@ -220,20 +261,27 @@
                     help='Locations of the PSD files produced '
                          'by PyCBC Live.')
 parser.add_argument('--approximant', required=True,
                     help='Waveform approximant string.')
 parser.add_argument('--snr-threshold', default=4.0,
                     help='If the SNR in ifo X is below this threshold do not '
                          'consider it part of the coincidence. Not implemented')
+parser.add_argument('--chirp-time-f-lower', default=20.,
+                    help='Starting frequency for chirp time window (Hz).')
+parser.add_argument('--chirp-time-window', default=2.,
+                    help='Chirp time window (s).')
 parser.add_argument('--gracedb-server', metavar='URL',
                     help='URL of GraceDB server API for uploading events. '
                          'If not provided, the default URL is used.')
 parser.add_argument('--gracedb-search', type=str, default='AllSky',
                     help='String going into the "search" field of the GraceDB '
                          'events')
+parser.add_argument('--gracedb-labels', metavar='LABEL', nargs='+',
+                    help='Apply the given list of labels to events uploaded '
+                         'to GraceDB.')
 parser.add_argument('--production', action='store_true',
                     help='Upload a production event rather than a test event')
 parser.add_argument('--enable-gracedb-upload', action='store_true', default=False,
                     help='Upload triggers to GraceDB')
 parser.add_argument('--output-path', required=True,
                     help='Path to a directory to store results in')
 parser.add_argument('--cores', type=int,
@@ -266,26 +314,37 @@
                     'The hyperparameter c1: the cognitive parameter.')
 parser.add_argument('--pso-c2', type=float, default=2.0,
                     help='Only relevant for --optimizer pso: '
                     'The hyperparameter c2: the social parameter.')
 parser.add_argument('--pso-w', type=float, default=0.01,
                     help='Only relevant for --optimizer pso: '
                     'The hyperparameter w: the inertia parameter.')
+parser.add_argument('--include-candidate-in-optimizer', action='store_true',
+                    help='Include parameters of the candidate event in the '
+                    'initialised array for the optimizer. Only relevant for '
+                    '--optimizer pso or differential_evolution')
+parser.add_argument('--seed', type=int, default=42,
+                    help='Seed to supply to the random generation of initial '
+                    'array to pass to the optimizer. Only relevant for '
+                    '--optimizer pso or differential_evolution')
 
 scheme.insert_processing_option_group(parser)
 fft.insert_fft_option_group(parser)
 
 # Input checking
 args = parser.parse_args()
 if args.snr_threshold != 4:
     parser.error("Sorry, the SNR threshold option doesn't work yet")
 if args.optimizer == 'pso' and ps == None:
     parser.error('You need to install pyswarms to use the pso optimizer.')
 pycbc.init_logging(args.verbose)
 
+if args.seed:
+    numpy.random.seed(args.seed)
+
 scheme.verify_processing_options(args, parser)
 fft.verify_fft_options(args, parser)
 
 scheme_context = scheme.from_cli(args)
 fft.from_cli(args)
 
 logging.info('Starting optimize SNR')
@@ -320,50 +379,86 @@
 f_end = float(fp['f_end'][()])
 delta_f = fp['delta_f'][()]
 sample_rate = fp['sample_rate'][()]
 
 extra_args = [data, coinc_times, coinc_ifos, flen,
               approximant, flow, f_end, delta_f, sample_rate]
 
-mchirp = mchirp_from_mass1_mass2(
-    fp['mass1'][()],
-    fp['mass2'][()]
-)
-minchirp = mchirp * (1 - mchirp / 50.0)
-maxchirp = mchirp * (1 + mchirp / 50.0)
-minchirp = 1 if minchirp < 1 else minchirp
-maxchirp = 80 if maxchirp > 80 else maxchirp
+# Determine chirp mass bounds from constant chirp time window
+tau0flow = args.chirp_time_f_lower
+tau0 = cv.tau0_from_mass1_mass2(fp['mass1'][()], fp['mass2'][()], tau0flow)
+# Arbitrarily set max mchirp to 200 Msun if tau0 - window is too small
+mintau0 = max(tau0 - args.chirp_time_window,
+              cv.tau0_from_mchirp(200., tau0flow))
+maxtau0 = tau0 + args.chirp_time_window
+minchirp = cv.mchirp_from_tau0(maxtau0, tau0flow)
+maxchirp = cv.mchirp_from_tau0(mintau0, tau0flow)
+# Check basic sanity
+assert minchirp > 0.5
+assert minchirp < maxchirp
+assert maxchirp < 250.
+
+# Establish minimum eta: find the most asymmetric mass point
+# nb function name is 'mass2' but it doesn't enforce m2 < m1!
+maxm1 = cv._mass2_from_mchirp_mass1(maxchirp, MIN_CPT_MASS)
+mineta = max(cv.eta_from_mass1_mass2(maxm1, MIN_CPT_MASS), MIN_ETA)
+
+# Use max chirp and max eta to find the upper bound on mass2
+maxm2 = cv.mass2_from_mchirp_eta(maxchirp, MAX_ETA)
+
+# Astrophysical spin bounds dependent on system masses - https://inspirehep.net/literature/1124003
+# If masses can be > 3 then allow them BH spin range
+minspin1z = -0.4 if maxm1 < 3 else -0.9
+maxspin1z = 0.4 if maxm1 < 3 else 0.9
+minspin2z = -0.4 if maxm2 < 3 else -0.9
+maxspin2z = 0.4 if maxm2 < 3 else 0.9
 
-# boundary of the optimization space (dict of (min, max) tuples)
+# Boundary of the optimization space
 bounds = {
     'mchirp': (minchirp, maxchirp),
-    'eta': (0.01, 0.2499),
-    'spin1z': (-0.9, 0.9),
-    'spin2z': (-0.9, 0.9)
+    'eta': (mineta, MAX_ETA),
+    'spin1z': (minspin1z, maxspin1z),
+    'spin2z': (minspin2z, maxspin2z)
 }
 
+if args.include_candidate_in_optimizer:
+    # Initial point from found candidate
+    mchirp_init = cv.mchirp_from_mass1_mass2(fp['mass1'][()], fp['mass2'][()])
+    eta_init = cv.eta_from_mass1_mass2(fp['mass1'][()], fp['mass2'][()])
+    spin1z_init = fp['spin1z'][()]
+    spin2z_init = fp['spin2z'][()]
+
+    initial_point = numpy.array([
+        mchirp_init,
+        eta_init,
+        spin1z_init,
+        spin2z_init,
+    ])[numpy.newaxis]
+else:
+    initial_point = None
+
 with scheme_context:
     logging.info('Starting optimization')
 
     optimize_func = optimize_funcs[args.optimizer]
-    opt_params = optimize_func(bounds, args, extra_args)
+    opt_params = optimize_func(bounds, args, extra_args, initial_point)
 
     logging.info('Optimization complete')
 
     fup_ifos = set(ifos) - set(coinc_ifos)
     for ifo in fup_ifos:
         coinc_times[ifo] = coinc_times[coinc_ifos[0]]
 
     extra_args[2] = ifos
 
     _, snr_series_dict = compute_network_snr_core(opt_params, *extra_args)
 
-mtotal = mtotal_from_mchirp_eta(opt_params[0], opt_params[1])
-mass1 = mass1_from_mtotal_eta(mtotal, opt_params[1])
-mass2 = mass2_from_mtotal_eta(mtotal, opt_params[1])
+mtotal = cv.mtotal_from_mchirp_eta(opt_params[0], opt_params[1])
+mass1 = cv.mass1_from_mtotal_eta(mtotal, opt_params[1])
+mass2 = cv.mass2_from_mtotal_eta(mtotal, opt_params[1])
 spin1z = opt_params[2]
 spin2z = opt_params[3]
 
 # Prepare for GraceDB upload
 coinc_results = {}
 skyloc_data = {}
 
@@ -507,15 +602,15 @@
                'parameters that maximize the SNR. The FAR of this trigger is '
                'copied from {0} and does not reflect this trigger\'s template '
                'parameters.')
     comment = comment.format(original_gid)
 
     gid = doc.upload(xml_path, gracedb_server=args.gracedb_server,
                      testing=(not args.production), extra_strings=[comment],
-                     search=args.gracedb_search)
+                     search=args.gracedb_search, labels=args.gracedb_labels)
     if gid is not None:
         logging.info('Event uploaded as %s', gid)
 
         # add a note to the original G event pointing to the optimized one
         from ligo.gracedb.rest import GraceDb
 
         gracedb = GraceDb(args.gracedb_server) \
```

### Comparing `PyCBC-2.2.0/bin/pycbc_process_sngls` & `PyCBC-2.2.1/bin/pycbc_process_sngls`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_randomize_inj_dist_by_optsnr` & `PyCBC-2.2.1/bin/pycbc_randomize_inj_dist_by_optsnr`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_single_template` & `PyCBC-2.2.1/bin/pycbc_single_template`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_source_probability_offline` & `PyCBC-2.2.1/bin/pycbc_source_probability_offline`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_split_inspinj` & `PyCBC-2.2.1/bin/pycbc_split_inspinj`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pycbc_splitbank` & `PyCBC-2.2.1/bin/pycbc_splitbank`

 * *Files 5% similar despite different names*

```diff
@@ -28,30 +28,27 @@
 """Splits a table in an xml file into multiple pieces."""
 
 import argparse
 from numpy import random, ceil
 from ligo.lw import ligolw
 from ligo.lw import lsctables
 from ligo.lw import utils as ligolw_utils
-from ligo.lw.utils import process as ligolw_process
 from pycbc import version
-from pycbc.io.ligolw import LIGOLWContentHandler
+from pycbc.io.ligolw import LIGOLWContentHandler, create_process_table
 from pycbc.conversions import mchirp_from_mass1_mass2
 from pycbc.pnutils import frequency_cutoff_from_name
 
 
 __author__  = "Alex Nitz <alex.nitz@ligo.org>"
-__version__ = version.git_verbose_msg
-__date__    = version.date
 __program__ = "pycbc_splitbank"
 
 
 # Command line parsing
 parser = argparse.ArgumentParser(description=__doc__)
-parser.add_argument('--version', action='version', version=__version__)
+parser.add_argument('--version', action='version', version=version.git_verbose_msg)
 
 group = parser.add_mutually_exclusive_group(required=True)
 group.add_argument('--templates-per-bank', metavar='SAMPLES',
                     help='number of templates in the output banks', type=int)
 group.add_argument('-n', '--number-of-banks', metavar='N',
                     help='Split template bank into N files', type=int)
 group.add_argument("-O", "--output-filenames", nargs='*', default=None,
@@ -145,18 +142,19 @@
 
 for num, (idx1, idx2) in enumerate(zip(index_list[:-1], index_list[1:])):
     assert(idx2 > idx1)
     # create a blank xml document and add the process id
     outdoc = ligolw.Document()
     outdoc.appendChild(ligolw.LIGO_LW())
 
-    process = ligolw_process.register_to_xmldoc(outdoc,
-                    __program__, args.__dict__, instruments=["G1"],
-                    version=version.version, cvs_repository=version.git_branch,
-                    cvs_entry_time=version.date)
+    process = create_process_table(
+        outdoc,
+        program_name=__program__,
+        options=args.__dict__
+    )
 
     sngl_inspiral_table = lsctables.New(tabletype, columns=used_columns)
     outdoc.childNodes[0].appendChild(sngl_inspiral_table)
 
     for i in range(idx2-idx1):
         row = tt.pop()
         row.process_id = process.process_id
```

### Comparing `PyCBC-2.2.0/bin/pycbc_stageout_failed_workflow` & `PyCBC-2.2.1/bin/pycbc_stageout_failed_workflow`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 eval set -- "$CMD_OPTS"
 
 while true
 do
     case "$1" in
 	-h|--help)
-	    echo "usage: pycbc_copy_output_map [-h] input_map output_map [optional arguments]"
+	    echo "usage: pycbc_copy_output_map [-h] -d workflow_directory [optional arguments]"
 	    echo
 	    echo "required arguments:"
 	    echo "  -d, --workflow-dir    the directory of the workflow to be staged out"
 	    echo
 	    echo "optional arguments:"
 	    echo "  -h, --help            show this help message and exit"
 	    echo
```

### Comparing `PyCBC-2.2.0/bin/pycbc_submit_dax` & `PyCBC-2.2.1/bin/pycbc_submit_dax`

 * *Files 26% similar despite different names*

```diff
@@ -13,17 +13,15 @@
 # log file would not contain any error messages.
 # SEE answer by Adam Spiers, which keeps STDERR a seperate stream -
 # I did not want to steal from him by simply adding his answer to mine.
 exec 2>&1
 
 LOCAL_PEGASUS_DIR=""
 PEGASUS_PROPERTIES=""
-NO_CREATE_PROXY=0
 NO_QUERY_DB=0
-NO_GRID=""
 SUBMIT_DAX="--submit"
 HTML_ENTITIES="{\"\'\": '&#39;', '(': '&#40;', ')': '&#41;', '+': '&#43;', '\"': '&quot;'}"
 
 # These will be changed by the bundle builder
 DATA_INLINE=False
 
 function expand_pegasus_files() {
@@ -50,39 +48,33 @@
         *) PEGASUS_PROPERTIES=$2 ; shift 2 ;;
       esac ;;
     -P|--append-pegasus-property)
       case "$2" in
         "") shift 2 ;;
         *) echo $2 >> extra-properties.conf ; shift 2 ;;
       esac ;;
-    -K|--no-create-proxy) NO_CREATE_PROXY=1 ; shift ;;
     -Q|--no-query-db) NO_QUERY_DB=1 ; shift ;;
     -n|--no-submit) SUBMIT_DAX="" ; shift ;;
     -l|--local-dir)
       case "$2" in
         "") shift 2 ;;
         *) LOCAL_PEGASUS_DIR=$2 ; shift 2 ;;
       esac ;;
-    -G|--no-grid) NO_GRID="--forward nogrid" ; shift ;;
     -h|--help)
       echo "usage: pycbc_submit_dax [-h] [optional arguments]"
       echo
       echo "optional arguments:"
       echo "  -h, --help              show this help message and exit"
       echo "  -p, --pegasus-properties FILE use the specified file as"
       echo "                               the pegasus properties file"
       echo "  -P, --append-pegasus-property STRING add the extra property"
       echo "                                          specified by the argument"
-      echo "  -K, --no-create-proxy   Do not run ligo-proxy-init and assume"
-      echo "                             that the user has a valid grid proxy"
       echo "  -n, --no-submit         Plan the DAX but do not submit it"
       echo "  -l, --local-dir         Directory to put condor files under"
       echo "  -Q, --no-query-db       Don't query the pegasus DB."
-      echo "  -G, --no-grid           Disable checks for grid proxy and"
-      echo "                             GLOBUS_LOCATION in pegasus-plan"
       echo
       echo "If the environment variable TMPDIR is set then this is prepended to the "
       echo "path to the temporary workflow execute directory passed to pegasus-plan."
       echo "If the --local-dir option is not given."
       echo
       echo "If the environment variable PEGASUS_FILE_DIRECTORY is set then the"
       echo "script will look there for configuration, "
@@ -91,57 +83,14 @@
       echo
       exit 0 ;;
     --) shift ; break ;;
     *) echo "Internal error!" ; exit 1 ;;
   esac
 done
 
-if [ $NO_CREATE_PROXY == 0 ]; then
-  # Force the user to create a new grid proxy
-  LIGO_USER_NAME=""
-  while true; do
-    read -p "Enter your LIGO.ORG username in (e.g. albert.einstein): " LIGO_USER_NAME
-    echo
-    if [ ! -z $LIGO_USER_NAME ] ; then
-      break
-    fi
-  done
-  unset X509_USER_PROXY
-  ligo-proxy-init -p $LIGO_USER_NAME || exit 1
-else
-  if [ ! -z ${X509_USER_PROXY} ] ; then
-    if [ -f ${X509_USER_PROXY} ] ; then
-      cp -a ${X509_USER_PROXY} /tmp/x509up_u`id -u`
-    fi
-  unset X509_USER_PROXY
-  fi
-fi
-
-if [ -z "${NO_GRID}" ] ; then
-  #Check that the proxy is valid
-  set +e
-  grid-proxy-info -exists
-  RESULT=$?
-  set -e
-  if [ ${RESULT} -eq 0 ] ; then
-    PROXY_TYPE=`grid-proxy-info -type | tr -d ' '`
-    if [ x${PROXY_TYPE} == 'xRFC3820compliantimpersonationproxy' ] ; then
-      grid-proxy-info
-    else
-      cp /tmp/x509up_u`id -u` /tmp/x509up_u`id -u`.orig
-      grid-proxy-init -hours 276 -cert /tmp/x509up_u`id -u`.orig -key /tmp/x509up_u`id -u`.orig
-      rm -f /tmp/x509up_u`id -u`.orig
-      grid-proxy-info
-    fi
-  else
-    echo "Error: Could not find a valid grid proxy to submit workflow."
-    exit 1
-  fi
-fi
-
 #Make a directory for the submit files
 SUBMIT_DIR=`mktemp --tmpdir=${LOCAL_PEGASUS_DIR} -d pycbc-tmp.XXXXXXXXXX`
 
 #Make sure the directory is world readable
 chmod 755 $SUBMIT_DIR
 
 # find the site-local template directory
@@ -166,15 +115,15 @@
 cat extra-properties.conf >> pegasus-properties.conf
 
 
 # Ian's bash-fu is not good enough to wrap this in the nice error handling that
 # it deserves!
 STORED_PLANNER_ARGS=`cat additional_planner_args.dat`
 
-pegasus-plan --conf ./pegasus-properties.conf --dir $SUBMIT_DIR $SUBMIT_DAX $NO_GRID ${STORED_PLANNER_ARGS}
+pegasus-plan --conf ./pegasus-properties.conf --dir $SUBMIT_DIR $SUBMIT_DAX ${STORED_PLANNER_ARGS}
 
 echo
 
 rm -f submitdir
 ln -sf $SUBMIT_DIR submitdir
 
 echo "pegasus-status --verbose --long $SUBMIT_DIR/work \$@" > status
@@ -182,50 +131,14 @@
 
 echo "pegasus-analyzer -r -v $SUBMIT_DIR/work \$@" > debug
 chmod 755 debug
 
 echo "pegasus-remove $SUBMIT_DIR/work \$@" > stop
 chmod 755 stop
 
-if [ -z "${NO_GRID}" ] ; then
-  cat << EOF > start
-#!/bin/bash
-
-if [ -f /tmp/x509up_u\`id -u\` ] ; then
-  unset X509_USER_PROXY
-else
-  if [ ! -z \${X509_USER_PROXY} ] ; then
-    if [ -f \${X509_USER_PROXY} ] ; then
-      cp -a \${X509_USER_PROXY} /tmp/x509up_u\`id -u\`
-    fi
-  fi
-  unset X509_USER_PROXY
-fi
-
-# Check that the proxy is valid
-grid-proxy-info -exists
-RESULT=\${?}
-if [ \${RESULT} -eq 0 ] ; then
-  PROXY_TYPE=\`grid-proxy-info -type | tr -d ' '\`
-  if [ x\${PROXY_TYPE} == 'xRFC3820compliantimpersonationproxy' ] ; then
-    grid-proxy-info
-  else
-    cp /tmp/x509up_u\`id -u\` /tmp/x509up_u\`id -u\`.orig
-    grid-proxy-init -hours 276 -cert /tmp/x509up_u\`id -u\`.orig -key /tmp/x509up_u\`id -u\`.orig
-    rm -f /tmp/x509up_u\`id -u\`.orig
-    grid-proxy-info
-  fi
-else
-  echo "Error: Could not find a valid grid proxy to submit workflow."
-  exit 1
-fi
-
-EOF
-fi
-
 echo "pegasus-run $SUBMIT_DIR/work \$@" > start
 
 chmod 755 start
 
 if [ -z ${SUBMIT_DAX} ] ; then
   echo
   echo "WARNING: DAX planned but not submitted. No dashboard entry has been created and"
```

### Comparing `PyCBC-2.2.0/bin/pycbc_upload_xml_to_gracedb` & `PyCBC-2.2.1/bin/pycbc_upload_xml_to_gracedb`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_grb_inj_combiner` & `PyCBC-2.2.1/bin/pygrb/pycbc_grb_inj_combiner`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_grb_inj_finder` & `PyCBC-2.2.1/bin/pygrb/pycbc_grb_inj_finder`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 #
-# Copyright (C) 2019 Duncan Macleod
+# Copyright (C) 2019 Duncan Macleod, Francesco Pannarale, Erin Vincent
 #
 # This program is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the
 # Free Software Foundation; either version 3 of the License, or (at your
 # option) any later version.
 #
 # This program is distributed in the hope that it will be useful, but
@@ -22,18 +22,15 @@
 
 import argparse
 import operator
 import os
 import re
 from collections import defaultdict
 from itertools import compress
-try:
-    from functools import reduce
-except ImportError:  # python < 2
-    pass
+from functools import reduce
 
 import tqdm
 
 import numpy
 
 import h5py
 
@@ -60,15 +57,14 @@
                    "[{elapsed} | ETA {remaining}]{postfix}")
 TQDM_KW = {
     "ascii": " -=#",
     "bar_format": TQDM_BAR_FORMAT,
     "smoothing": 0.05,
 }
 
-
 # -- utilities ----------------------------------
 
 def find_split_files(filelist, split):
     for fn in filelist:
         if (split is None and 'SPLIT' not in fn) or split in fn:
             yield fn
 
@@ -87,67 +83,66 @@
 def read_hdf5_triggers(inputfiles, verbose=False):
     """Merge several HDF5 files into a single file
 
     Parameters
     ----------
     inputfiles : `list` of `str`
         the paths of the input HDF5 files to merge
-
-    outputfile : `str`
-        the path of the output HDF5 file to write
     """
     datasets = {}
 
     def _scan_dataset(name, obj):
-        if not name.startswith("network") or not isinstance(obj, h5py.Dataset):
-            return
-        if NETWORK_IFO_EVENT_ID_REGEX.match(name):
+        if not isinstance(obj, h5py.Dataset) or "search" in name:
             return
-        shape = obj.shape
-        dtype = obj.dtype
-        try:
-            shape = numpy.sum(datasets[name][0] + shape, keepdims=True)
-        except KeyError:
-            pass
+        elif name.startswith("network") and \
+            NETWORK_IFO_EVENT_ID_REGEX.match(name):
+                return
         else:
-            assert dtype == datasets[name][1], (
-                "Cannot merge {0}/{1}, does not match dtype".format(
-                    obj.file.filename, name,
-                ))
-        datasets[name] = (shape, dtype)
+            shape = obj.shape
+            dtype = obj.dtype
+            try:
+                shape = numpy.sum(datasets[name][0] + shape, keepdims=True)
+            except KeyError:
+                pass
+            else:
+                assert dtype == datasets[name][1], (
+                    "Cannot merge {0}/{1}, does not match dtype".format(
+                        obj.file.filename, name,
+                    ))
+            datasets[name] = (shape, dtype)
 
     # get list of datasets
-    datasets = {}
     for filename in inputfiles:
         with h5py.File(filename, 'r') as h5f:
             h5f.visititems(_scan_dataset)
 
     position = defaultdict(int)
 
-    out = {}
+    out_dict = dict.fromkeys(datasets.keys(), {})
 
     # create datasets
     for dset, (shape, dtype) in datasets.items():
-        out[dset[8:]] = numpy.empty(shape, dtype=dtype)
+        out_dict[dset] = numpy.empty(shape, dtype=dtype)
 
     # copy dataset contents
     for filename in inputfiles:
         with h5py.File(filename, 'r') as h5in:
-            for dset in datasets:
-                data = h5in[dset][:]
-                size = data.shape[0]
-                pos = position[dset]
-                if EVENT_ID_REGEX.search(dset):
-                    out[dset[8:]][pos:pos+size] = data + pos
-                else:
-                    out[dset[8:]][pos:pos+size] = data
-                position[dset] += size
-
-    return out
+            # Skipping emtpy files
+            if len(h5in['network']['end_time_gc'][:]):
+                for dset in datasets:
+                    data = h5in[dset][:]
+                    size = data.shape[0]
+                    pos = position[dset]
+                    if EVENT_ID_REGEX.search(dset):
+                        out_dict[dset][pos:pos+size] = data + pos
+                    else:
+                        out_dict[dset][pos:pos+size] = data
+                    position[dset] += size
 
+    return out_dict
 
 # -- parse command line -------------------------
 
 parser = argparse.ArgumentParser(
     description=__doc__,
 )
 
@@ -223,15 +218,23 @@
 # -- find injections ----------------------------
 
 exclude = read_segment_files(args.exclude_segments)
 
 nexcluded = 0
 missed = []
 found = []
-trigs = defaultdict(list)
+
+# dictionary that will contain ifo names and 'network' as keys and information
+# on triggers to be written to file 
+out_trigs = {}
+# open first injection file
+firstfile = h5py.File(args.input_files[0], "r")
+for key in firstfile.keys():
+    # associate ifo names and 'network' with [] so we can append later 
+    out_trigs[key] = defaultdict(list)
 
 allinjections = None
 
 with tqdm.tqdm(args.inj_files, desc="Finding injections",
                disable=not args.verbose, unit="files",
                postfix=dict(found=0, missed=0, excluded=nexcluded),
                **TQDM_KW) as bar:
@@ -249,17 +252,17 @@
         keep = numpy.asarray([t not in exclude for t in injtime])
         nexcluded += (~keep).sum()
         injections = tmpinj[keep]
         injtime = injtime[keep]
 
         # read triggers
         triggers = read_hdf5_triggers(trigfiles)
-        time = triggers["end_time_gc"]
-        snr = triggers[args.rank_column]
-        event_id = triggers["event_id"]
+        time = triggers['network/end_time_gc']
+        snr = triggers['network/'+args.rank_column]
+        event_id = triggers['network/event_id']
         time_sorting = time.argsort()
 
         # determine found or missed
         _left = numpy.searchsorted(
             time[time_sorting],
             injtime - args.time_window,
             side='left',
@@ -276,16 +279,17 @@
                 start=len(allinjections if allinjections is not None else []),
         ):
             if not r - l:
                 missed.append(i)
             else:
                 found.append(i)
                 eid = l if r - l == 1 else event_id[l + snr[l:r].argmax()]
-                for col in triggers:
-                    trigs[col].append(triggers[col][i])
+                for x in triggers:
+                    ij = x.split('/')
+                    out_trigs[ij[0]][ij[1]].append(triggers[x][eid])
 
         # record all injections
         if allinjections is None:
             allinjections = injections
         else:
             allinjections = allinjections.append(injections)
 
@@ -316,19 +320,21 @@
         injs = allinjections[injlist]
         for field in injs.fieldnames:
             # h5py does not support unicode strings
             if 'U' not in str(injs[field].dtype):
                 grp[field] = injs[field]
             else:
                 grp[field] = [str(s) for s in injs[field]]
+                
     # write triggers
-    netg = h5out.create_group("network")
-    for col in trigs:
-        netg.create_dataset(
-            col,
-            data=trigs[col],
-            compression="gzip",
-            compression_opts=9,
-        )
-
+    for x in out_trigs:
+        xgroup = h5out.create_group(x)
+        for col in out_trigs[x]:
+            xgroup.create_dataset(
+                col,
+                data=out_trigs[x][col],
+                compression="gzip",
+                compression_opts=9,
+            )
+    
 if args.verbose:
     print("Found/missed written to {}".format(outfilename))
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_grb_trig_cluster` & `PyCBC-2.2.1/bin/pygrb/pycbc_grb_trig_cluster`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_grb_trig_combiner` & `PyCBC-2.2.1/bin/pygrb/pycbc_grb_trig_combiner`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_make_offline_grb_workflow` & `PyCBC-2.2.1/bin/pygrb/pycbc_make_offline_grb_workflow`

 * *Files 13% similar despite different names*

```diff
@@ -29,60 +29,61 @@
 
 import shutil
 import sys
 import os
 import argparse
 import logging
 import pycbc.workflow as _workflow
+from pycbc.workflow.core import configparser_value_to_file
 from ligo.segments import segment, segmentlist, segmentlistdict
 import matplotlib
 matplotlib.use('agg')
-from pycbc.results.legacy_grb import make_grb_segments_plot
+from pycbc.results.pygrb_plotting_utils import make_grb_segments_plot
 
 workflow_name = "pygrb_offline"
 logging.basicConfig(format="%(asctime)s:%(levelname)s : %(message)s",
                     level=logging.INFO)
 
 # Parse command line options and instantiate pycbc workflow object
 parser = argparse.ArgumentParser()
 parser.add_argument("--version", action="version", version=__version__)
 _workflow.add_workflow_command_line_group(parser)
 _workflow.add_workflow_settings_cli(parser)
 args = parser.parse_args()
 wflow = _workflow.Workflow(args, workflow_name)
 all_files = _workflow.FileList([])
 tags = []
-initDir = os.getcwd()
+init_dir = os.getcwd()
 
 logging.info("Generating %s workflow" % workflow_name)
 
 # Setup run directory
 if wflow.cp.has_option("workflow", "output-directory"):
-    baseDir = wflow.cp.get("workflow", "output-directory")
+    base_dir = wflow.cp.get("workflow", "output-directory")
 else:
-    baseDir = os.getcwd()
+    base_dir = os.getcwd()
 triggername = str(wflow.cp.get("workflow", "trigger-name"))
-runDir = os.path.join(baseDir, "GRB%s" % triggername)
-logging.info("Workflow will be generated in %s" % runDir)
-if not os.path.exists(runDir):
-    os.makedirs(runDir)
-os.chdir(runDir)
+run_dir = os.path.join(base_dir, "GRB%s" % triggername)
+logging.info("Workflow will be generated in %s" % run_dir)
+if not os.path.exists(run_dir):
+    os.makedirs(run_dir)
+os.chdir(run_dir)
 
 # SEGMENTS
 triggertime = int(wflow.cp.get("workflow", "trigger-time"))
 start = triggertime - int(wflow.cp.get("workflow-exttrig_segments",
                                        "max-duration"))
 end = triggertime + int(wflow.cp.get("workflow-exttrig_segments",
                                      "max-duration"))
 wflow.cp = _workflow.set_grb_start_end(wflow.cp, start, end)
 
 # Retrieve science segments
-currDir = os.getcwd()
-segDir = os.path.join(currDir, "segments")
-sciSegsFile = _workflow.get_segments_file(wflow, 'science', 'segments-science', segDir)
+curr_dir = os.getcwd()
+seg_dir = os.path.join(curr_dir, "segments")
+sciSegsFile = _workflow.get_segments_file(wflow, 'science', 'segments-science', seg_dir)
 
 sciSegs = {}
 for ifo in wflow.ifos:
     sciSegs[ifo] = sciSegsFile.segment_dict[ifo+':science']
 
 if wflow.cp.has_option("inspiral", "segment-start-pad"):
     pad_data = int(wflow.cp.get("inspiral", "pad-data"))
@@ -103,44 +104,44 @@
     wflow.cp.set("workflow-exttrig_segments", "min-before", str(deadtime))
     wflow.cp.set("workflow-exttrig_segments", "min-after", str(deadtime))
 
 # Do checks for no/single IFO case
 single_ifo = wflow.cp.has_option("workflow", "allow-single-ifo-search")
 if len(sciSegs.keys()) == 0:
     plot_met = make_grb_segments_plot(wflow, segmentlistdict(), triggertime,
-            triggername, segDir)
+            triggername, seg_dir)
     logging.warning("No science segments available.")
     sys.exit()
 elif len(sciSegs.keys()) < 2 and not single_ifo:
     plot_met = make_grb_segments_plot(wflow, segmentlistdict(sciSegs),
-            triggertime, triggername, segDir)
+            triggertime, triggername, seg_dir)
     msg = "Science segments exist only for %s. " % tuple(sciSegs.keys())[0]
     msg += "If you wish to enable single IFO running add the option "
     msg += "'allow-single-ifo-search' to the [workflow] section of your "
     msg += "configuration file."
     logging.warning(msg)
     sys.exit()
 else:
-    onSrc, offSrc = _workflow.generate_triggered_segment(wflow, segDir,
+    onSrc, offSrc = _workflow.generate_triggered_segment(wflow, seg_dir,
                                                          sciSegs)
 
 sciSegs = segmentlistdict(sciSegs)
 if onSrc is None:
     plot_met = make_grb_segments_plot(wflow, sciSegs, triggertime, triggername,
-            segDir, fail_criterion=offSrc)
+            seg_dir, fail_criterion=offSrc)
     logging.info("Making segment plot and exiting.")
     sys.exit()
 else:
     plot_met = make_grb_segments_plot(wflow, sciSegs, triggertime, triggername,
-            segDir, coherent_seg=offSrc[tuple(offSrc.keys())[0]][0])
+            seg_dir, coherent_seg=offSrc[tuple(offSrc.keys())[0]][0])
     segs_plot = _workflow.File(plot_met[0], plot_met[1], plot_met[2],
                                file_url=plot_met[3])
     segs_plot.add_pfn(segs_plot.cache_entry.path, site="local")
     sciSegs = offSrc
-    all_files.extend(_workflow.FileList([segs_plot]))
+    all_files.append(segs_plot)
 
 if len(sciSegs) == 1:
     logging.info("Generating a single IFO search.")
     mf_tag = "sngl"
 elif len(sciSegs) > 1:
     mf_tag = "coherent"
 
@@ -169,17 +170,17 @@
 else:
     wflow.analysis_time = segment(int(sciSegs[ifo][0][0]) + deadtime + padding,
                                   int(sciSegs[ifo][0][1]) - deadtime - padding)
 
 ext_file = None
 
 # DATAFIND
-dfDir = os.path.join(currDir, "datafind")
+df_dir = os.path.join(curr_dir, "datafind")
 datafind_files, _, sciSegs, _ = _workflow.setup_datafind_workflow(wflow,
-        sciSegs, dfDir, sciSegsFile)
+        sciSegs, df_dir, sciSegsFile)
 if wflow.cp.has_option("workflow-condition_strain", "do-gating"):
     new_seg = segment(sciSegs[ifo][0][0] + gate_pad,
                       sciSegs[ifo][0][1] - gate_pad)
     for iifo in sciSegs:
         sciSegs[iifo][0] = new_seg
     wflow.cp = _workflow.set_grb_start_end(wflow.cp, int(sciSegs[ifo][0][0]),
                                            int(sciSegs[ifo][0][1]))
@@ -191,15 +192,15 @@
 
 # GATING
 if wflow.cp.has_option("workflow-condition_strain", "do-gating"):
     logging.info("Creating gating jobs.")
     wflow.cp = _workflow.set_grb_start_end(wflow.cp, int(sciSegs[ifo][0][0]),
                                            int(sciSegs[ifo][0][1]))
     gating_nodes, gated_files = _workflow.make_gating_node(wflow,
-            datafind_files, outdir=dfDir)
+            datafind_files, outdir=df_dir)
     gating_method = wflow.cp.get("workflow-condition_strain",
                                  "gating-method")
     for gating_node in gating_nodes:
         if gating_method == "IN_WORKFLOW":
             wflow.add_node(gating_node)
         elif gating_method == "AT_RUNTIME":
             logging.info("Executing gating node...")
@@ -214,27 +215,27 @@
     for ifo in ifos:
         gated_frames = _workflow.FileList([gated_frame for gated_frame in \
                 gated_files if gated_frame.ifo == ifo])
         # TODO: Remove .lcf cache here
         gated_cache = _workflow.File(ifo, "gated",
                 segment(int(wflow.cp.get("workflow", "start-time")),
                         int(wflow.cp.get("workflow", "end-time"))),
-                extension="lcf", directory=dfDir)
+                extension="lcf", directory=df_dir)
         gated_cache.add_pfn(gated_cache.cache_entry.path, site="local")
         gated_frames.convert_to_lal_cache().tofile(\
                             open(gated_cache.storage_path, "w"))
-        datafind_files.extend(_workflow.FileList([gated_cache]))
+        datafind_files.append(gated_cache)
 
 datafind_veto_files.extend(datafind_files)
 ifo_list = sorted(sciSegs.keys())
 ifo = ifo_list[0]
 ifos = ''.join(ifo_list)
 wflow.ifos = ifos
 
-# Is this an IPN GRB?
+# Config file consistency check for IPN GRBs
 if wflow.cp.has_option("workflow-inspiral", "ipn-search-points") \
         and wflow.cp.has_option("workflow-injections", "ipn-sim-points"):
     wflow.cp.set("injections", "ipn-gps-time",
             wflow.cp.get("workflow", "trigger-time"))
     IPN = True
 elif wflow.cp.has_option("workflow-inspiral", "ipn-search-points") \
         or wflow.cp.has_option("workflow-injections", "ipn-sim-points"):
@@ -244,74 +245,82 @@
     msg += "IPN GRB please provide both, otherwise provide neither."
     logging.error(msg)
     sys.exit()
 else:
     IPN = False
 
 # Get bank_veto_bank.xml if running bank veto
-# TODO: Finalize config file location of bank veto bank
 if wflow.cp.has_option('workflow-inspiral', 'bank-veto-bank-file'):
-    bank_veto_file_path = wflow.cp.get('workflow-inspiral', 'bank-veto-bank-file')
-    bank_veto_file = _workflow.FileList([_workflow.resolve_url_to_file(bank_veto_file_path)])
+    bank_veto_file = configparser_value_to_file(wflow.cp, 'workflow-inspiral',
+                                                'bank-veto-bank-file')
+    bank_veto_file = _workflow.FileList([bank_veto_file])
     datafind_veto_files.extend(bank_veto_file)
 
 if IPN:
-    search_pts_file = _workflow.get_ipn_sky_files(wflow,
-            wflow.cp.get("workflow-inspiral", "ipn-search-points"),
-            tags=["SEARCH"])
-    datafind_veto_files.extend(_workflow.FileList([search_pts_file]))
-
-# Make ExtTrig xml file (needed for lalapps_inspinj and summary pages)
-# TODO: how will this work in O4?
-if wflow.cp.has_option("inspiral", "do-exttrig"):
-    ext_file = _workflow.make_exttrig_file(wflow.cp, ifos, sciSegs[ifo][0],
-                                           baseDir)
-    all_files.extend(_workflow.FileList([ext_file]))
+    file_attrs = {
+        'ifos': wflow.ifos,
+        'segs': wflow.analysis_time,
+        'exe_name': "IPN_SKY_POINTS",
+        'tags': ["SEARCH"]
+    }
+    search_pts_file = configparser_value_to_file(wflow.cp,
+                                                 'workflow-inspiral',
+                                                 'ipn-search-points',
+                                                 file_attrs=file_attrs)
+    datafind_veto_files.append(search_pts_file)
 
 all_files.extend(datafind_veto_files)
 
 # TEMPLATE BANK AND SPLIT BANK
 bank_files = _workflow.setup_tmpltbank_workflow(wflow, sciSegs,
-                                                datafind_files, dfDir)
-splitbank_files = _workflow.setup_splittable_workflow(wflow, bank_files, dfDir,
+                                                datafind_files, df_dir)
+splitbank_files = _workflow.setup_splittable_workflow(wflow, bank_files, df_dir,
                                                       tags=["inspiral"])
 all_files.extend(bank_files)
 all_files.extend(splitbank_files)
 
 # INJECTIONS
 injs = None
 inj_tags = []
 inj_files = None
 inj_caches = None
 inj_insp_files = None
 inj_insp_caches = None
 if wflow.cp.has_section("workflow-injections"):
-    injDir = os.path.join(currDir, "injections")
+    inj_dir = os.path.join(curr_dir, "injections")
     inj_caches = _workflow.FileList([])
     inj_insp_caches = _workflow.FileList([])
 
     # Generate injection files
     if IPN:
-        sim_pts_file = _workflow.get_ipn_sky_files(wflow,
-                wflow.cp.get("workflow-injections", "ipn-sim-points"),
-                tags=["SIM"])
-        all_files.extend(_workflow.FileList([sim_pts_file]))
-        inj_files, inj_tags = _workflow.setup_injection_workflow(wflow, injDir,
-                exttrig_file=sim_pts_file)
-    else:
-        inj_files, inj_tags = _workflow.setup_injection_workflow(wflow, injDir,
-                exttrig_file=ext_file)
+        # TODO: we used to pass this file to setup_injection_workflow as
+        # exttrig_file = sim_pts_file to then use it in lalapps_insping.
+        # The code below picks it up but does not pass it: get
+        # setup_injection_workflow or pycbc_create_injections handle it
+        # directly via configparser_value_to_file
+        file_attrs = {
+            'ifos': wflow.ifos,
+            'segs': wflow.analysis_time,
+            'exe_name': "IPN_SKY_POINTS",
+            'tags': ["SIM"]
+        }
+        sim_pts_file = configparser_value_to_file(wflow.cp,
+                                                  'workflow-inspiral',
+                                                  'ipn-sim-points',
+                                                  file_attrs=file_attrs)
+        all_files.append(sim_pts_file)
+    inj_files, inj_tags = _workflow.setup_injection_workflow(wflow, inj_dir)
     all_files.extend(inj_files)
     injs = inj_files
 
     # Either split template bank for injections jobs or use same split banks
     # as for standard matched filter jobs
     if wflow.cp.has_section("workflow-splittable-injections"):
         inj_splitbank_files = _workflow.setup_splittable_workflow(wflow,
-                bank_files, injDir, tags=["injections"])
+                bank_files, inj_dir, tags=["injections"])
         for inj_split in inj_splitbank_files:
             split_str = [s for s in inj_split.tagged_description.split("_") \
                          if ("BANK" in s and s[-1].isdigit())]
             if len(split_str) != 0:
                 inj_split.tagged_description += "%s_%d" % (inj_split.tag_str,
                        int(split_str[0].replace("BANK", "")))
         all_files.extend(inj_splitbank_files)
@@ -321,140 +330,103 @@
 
     # Split the injection files
     if wflow.cp.has_section("workflow-splittable-split_inspinj"):
         inj_split_files = _workflow.FileList([])
         for inj_file, inj_tag in zip(inj_files, inj_tags):
             file = _workflow.FileList([inj_file])
             inj_splits = _workflow.setup_splittable_workflow(wflow, file,
-                    injDir, tags=["split_inspinj", inj_tag])
+                    inj_dir, tags=["split_inspinj", inj_tag])
             for inj_split in inj_splits:
                 split_str = [s for s in \
                              inj_split.tagged_description.split("_") \
                              if ("SPLIT" in s and s[-1].isdigit())]
                 if len(split_str) != 0:
                     new = inj_split.tagged_description.replace(split_str[0],
                             "SPLIT_%s" % split_str[0].replace("SPLIT", ""))
                     inj_split.tagged_description = new
             inj_split_files.extend(inj_splits)
         all_files.extend(inj_split_files)
         injs = inj_split_files
 
     # Generate injection matched filter workflow
     inj_insp_files = _workflow.setup_matchedfltr_workflow(wflow, sciSegs,
-            datafind_veto_files, inj_splitbank_files, injDir, injs,
+            datafind_veto_files, inj_splitbank_files, inj_dir, injs,
             tags=[mf_tag + "_injections"])
     for inj_insp_file in inj_insp_files:
         split_str = [s for s in inj_insp_file.name.split("_") \
                      if ("SPLIT" in s and s[-1].isdigit())]
         if len(split_str) != 0:
             num = split_str[0].replace("SPLIT", "_")
             inj_insp_file.tagged_description += num
 
     # Make cache files (needed for post-processing)
     for inj_tag in inj_tags:
         files = _workflow.FileList([file for file in injs \
                                     if inj_tag in file.tag_str])
         inj_cache = _workflow.File(ifos, "injections", sciSegs[ifo][0],
-                                   extension="lcf", directory=injDir,
+                                   extension="lcf", directory=inj_dir,
                                    tags=[inj_tag])
         inj_cache.add_pfn(inj_cache.cache_entry.path, site="local")
-        inj_caches.extend(_workflow.FileList([inj_cache]))
+        inj_caches.append(inj_cache)
         inj_cache_entries = files.convert_to_lal_cache()
         inj_cache_entries.tofile(open(inj_cache.storage_path, "w"))
 
         files = _workflow.FileList([file for file in inj_insp_files \
                                     if inj_tag in file.tag_str])
         inj_insp_cache = _workflow.File(ifos, "inspiral_injections",
                                         sciSegs[ifo][0], extension="lcf",
-                                        directory=injDir, tags=[inj_tag])
+                                        directory=inj_dir, tags=[inj_tag])
         inj_insp_cache.add_pfn(inj_insp_cache.cache_entry.path, site="local")
-        inj_insp_caches.extend(_workflow.FileList([inj_insp_cache]))
+        inj_insp_caches.append(inj_insp_cache)
         inj_insp_cache_entries = files.convert_to_lal_cache()
         inj_insp_cache_entries.tofile(open(inj_insp_cache.storage_path, "w"))
 
     all_files.extend(inj_caches)
     all_files.extend(inj_insp_files)
     all_files.extend(inj_insp_caches)
 
 # MAIN MATCHED FILTERING
-inspDir = os.path.join(currDir, "inspiral")
+insp_dir = os.path.join(curr_dir, "inspiral")
 inspiral_files = _workflow.setup_matchedfltr_workflow(wflow, sciSegs,
-        datafind_veto_files, splitbank_files, inspDir,
+        datafind_veto_files, splitbank_files, insp_dir,
         tags=[mf_tag + "_no_injections"])
 all_files.extend(inspiral_files)
 # TODO: Remove .lcf caches here?
 inspiral_cache = _workflow.File(ifos, "inspiral", sciSegs[ifo][0],
-                                extension="lcf", directory=inspDir)
+                                extension="lcf", directory=insp_dir)
 inspiral_cache.add_pfn(inspiral_cache.cache_entry.path, site="local")
-all_files.extend(_workflow.FileList([inspiral_cache]))
+all_files.append(inspiral_cache)
 inspiral_cache_entries = inspiral_files.convert_to_lal_cache()
 inspiral_cache_entries.tofile(open(inspiral_cache.storage_path, "w"))
 
-# LONG TIME SLIDES
-# TODO: Eventually remove this once new time slides are finished
-# Long slides will likely not be done in the future
-long_slides = True if wflow.cp.has_option("workflow", "do-long-slides") else False
-if long_slides:
-    tsDir = os.path.join(currDir, "timeslides")
-    if wflow.cp.has_option("workflow", "num-long-slides"):
-        num_slides = int(wflow.cp.get("workflow", "num-long-slides"))
-    else:
-        num_slides = int((int(wflow.cp.get("workflow", "end-time")) - \
-                         int(wflow.cp.get("workflow", "start-time"))) / \
-                         float(int(wflow.cp.get("inspiral", "segment-length")) * \
-                               (len(ifo_list) - 1)))
-    logging.info("Doing {} long slides".format(num_slides))
-    inspiral_ts_files = _workflow.FileList([])
-    for slide in range(num_slides):
-        inspiral_slide_files = _workflow.setup_matchedfltr_workflow(wflow,
-                sciSegs, datafind_veto_files, splitbank_files, tsDir,
-                tags=[mf_tag + "_no_injections", "slide{}".format(slide)])
-        inspiral_ts_files.extend(inspiral_slide_files)
-        all_files.extend(inspiral_slide_files)
-else:
-    inspiral_ts_files=None
+# TODO: LONG TIME SLIDES
 
 # POST-PROCESSING
-ppDir = os.path.join(currDir, "post_processing")
-os.makedirs(ppDir)
+pp_dir = os.path.join(curr_dir, "post_processing")
+os.makedirs(pp_dir)
 post_proc_method = wflow.cp.get_opt_tags("workflow-postproc",
                                          "postproc-method", tags)
-
+pp_files = _workflow.FileList([])
+results_files = _workflow.FileList([])
 if post_proc_method == "PYGRB_OFFLINE":
-    pp_files = _workflow.setup_pygrb_pp_workflow(wflow, ppDir, segDir,
+    # The content and structure of pp_files are described in the definition
+    # of _workflow.setup_pygrb_pp_workflow
+    pp_files = _workflow.setup_pygrb_pp_workflow(wflow, pp_dir, seg_dir,
                                       sciSegs[ifo][0], inspiral_files,
                                       injs, inj_insp_files, inj_tags)
-
-# TODO: Remove all coh_ptf post-processing
-if post_proc_method in ["COH_PTF_WORKFLOW", "COH_PTF_OFFLINE",
-                        "COH_PTF_ONLINE"]:
-    from pylal import pygrb_cohptf_pp
-    # Add parsed config file so it can be linked from summary page
-    cp_file_name = workflow_name + ".ini"
-    cp_file_url = "file://localhost%s/%s" % (runDir, cp_file_name)
-    cp_file = _workflow.File(ifos, cp_file_name, sciSegs[ifo][0],
-                             file_url=cp_file_url)
-    cp_file.PFN(cp_file.cache_entry.path, site="local")
-
-    # Generate post-processing workflow
-    html_dir = wflow.cp.get("workflow", "html-dir")
-    pp_files = pygrb_cohptf_pp.setup_coh_PTF_post_processing(wflow,
-            inspiral_files, inspiral_cache, ppDir, segDir,
-            injection_trigger_files=inj_insp_files, injection_files=injs,
-            injection_trigger_caches=inj_insp_caches,
-            timeslide_trigger_files=inspiral_ts_files,
-            injection_caches=inj_caches, config_file=cp_file, web_dir=html_dir,
-            segments_plot=segs_plot, ifos=ifos, inj_tags=inj_tags)
-
-    # Retrieve style files for webpage
-    summary_files = _workflow.get_coh_PTF_files(wflow.cp, ifos, ppDir,
-                                                summary_files=True)
-
-    pp_files.extend(_workflow.FileList([cp_file]))
-    pp_files.extend(summary_files)
+    sec_name = 'workflow-pygrb_pp_workflow'
+    if not wflow.cp.has_section(sec_name):
+        msg = 'No {0} section found in configuration file.'.format(sec_name)
+        logging.info(msg)
+    else:
+        logging.info('Entering results module')
+        results_files = _workflow.setup_pygrb_results_workflow(wflow, pp_dir,
+                                                               pp_files[1],
+                                                               pp_files[-2])
+        logging.info('Leaving results module')
 
 all_files.extend(pp_files)
+all_files.extend(results_files)
 
 # COMPILE WORKFLOW AND WRITE DAX
 wflow.save()
 logging.info("Written dax.")
-
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_efficiency` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_efficiency`

 * *Files 3% similar despite different names*

```diff
@@ -94,43 +94,38 @@
 parser.add_argument("--onsource-output-file", default=None, required=True,
                     help="Exclusion distance output file.")
 parser.add_argument("-g", "--glitch-check-factor", action="store",
                     type=float, default=1.0, help="When deciding " +
                     "exclusion efficiencies this value is multiplied " +
                     "to the offsource around the injection trigger to " +
                     "determine if it is just a loud glitch.")
+parser.add_argument("--injection-set-name", action="store", type=str,
+                    default="", help="Name of the injection set to be " +
+                    "used in the plot title.")
 parser.add_argument("-C", "--cluster-window", action="store", type=float,
                     default=0.1, help="The cluster window used " +
                     "to cluster triggers in time.")
-ppu.pygrb_add_missed_injs_input_opt(parser)
 ppu.pygrb_add_injmc_opts(parser)
 ppu.pygrb_add_bestnr_opts(parser)
 opts = parser.parse_args()
 
 init_logging(opts.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
 # Check options
-if (opts.found_file is None) and (opts.missed_file is None):
-    do_injections = False
-elif (opts.found_file) and opts.missed_file:
-    do_injections = True
-else:
-    err_msg = "Must provide both found and missed file if running injections."
-    parser.error(err_msg)
+do_injections = opts.found_missed_file is not None
 
 if not opts.newsnr_threshold:
     opts.newsnr_threshold = opts.snr_threshold
 
 # Store options used multiple times in local variables
 outdir = os.path.split(os.path.abspath(opts.background_output_file))[0]
 trig_file = opts.offsource_file
 onsource_file = opts.onsource_file
-found_file = opts.found_file
-missed_file = opts.missed_file
-inj_set_name = os.path.split(os.path.abspath(missed_file))[1].split('INSPINJ')[1].split('_')[1]
+found_missed_file = opts.found_missed_file
+inj_set_name = opts.injection_set_name
 chisq_index = opts.chisq_index
 chisq_nhigh = opts.chisq_nhigh
 wf_err = opts.waveform_error
 cal_errs = {}
 cal_errs['G1'] = opts.g1_cal_error
 cal_errs['H1'] = opts.h1_cal_error
 cal_errs['K1'] = opts.k1_cal_error
@@ -372,15 +367,15 @@
     inj_sigma_mean = {}
     for ifo in ifos:
         inj_sigma_mean[ifo] = ((inj_sigma[ifo]*f_resp[ifo])/inj_sigma_tot).mean()
 
     logging.info("%d found injections analysed.", len(found_injs))
 
     # Missed injections (ones not recovered at all)
-    missed_injs = ppu.load_injections(missed_file, vetoes, sim_table=True)
+    missed_injs = ppu.load_injections(found_missed_file, vetoes, sim_table=True)
 
     # Process missed injections 'missed_inj'
     missed_inj = {}
     missed_inj['dist'] = np.asarray(missed_injs.get_column('distance'))
 
     logging.info("%d missed injections analysed.", len(missed_injs))
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_grb_info_table` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_grb_info_table`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_page_tables` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_page_tables`

 * *Files 2% similar despite different names*

```diff
@@ -44,14 +44,46 @@
 __date__ = pycbc.version.date
 __program__ = "pycbc_pygrb_page_tables"
 
 
 # =============================================================================
 # Functions
 # =============================================================================
+# Function to load trigger data
+def load_data(input_file, vetoes, ifos, opts, injections=False):
+    """Load data from a trigger/injection file"""
+
+    # Initialize the dictionary
+    data = {}
+    data['trig_time'] = None
+    data['coherent'] = None
+    data['reweighted_snr'] = None
+
+    if input_file:
+        if injections:
+            logging.info("Loading injections...")
+            # This will eventually become ppu.load_injections()
+            trigs_or_injs = ppu.load_triggers(input_file, vetoes)
+        else:
+            logging.info("Loading triggers...")
+            trigs_or_injs = ppu.load_triggers(input_file, vetoes)
+            
+        data['trig_time'] = trigs_or_injs['network/end_time_gc'][:]
+        num_trigs_or_injs = len(data['trig_time'])
+        data['coherent'] = trigs_or_injs['network/coherent_snr'][:]
+        data['reweighted_snr'] = trigs_or_injs['network/reweighted_snr'][:]
+
+        if injections:
+            logging.info("%d injections found.", num_trigs_or_injs)
+        else:
+            logging.info("%d triggers found.", num_trigs_or_injs)
+
+    return data
+
+
 def get_column(table, column_name):
     """Wrapper for glue.ligolw.lsctables.MultiInspiralTable.get_column
     method. Easier for h5py switch. Note: Must still replace
     .get_sigmasqs() and .get_snglsnr() methods throughout the scripts.
 
     Parameters
     ----------
@@ -186,24 +218,24 @@
 parser = ppu.pygrb_initialize_plot_parser(description=__doc__,
                                           version=__version__)
 parser.add_argument("-F", "--offsource-file", action="store", required=True,
                     default=None, help="Location of off-source trigger file")
 # As opposed to offsource-file and trig-file, this only contains onsource
 parser.add_argument("--onsource-file", action="store", default=None,
                     help="Location of on-source trigger file.")
-ppu.pygrb_add_missed_injs_input_opt(parser)
 ppu.pygrb_add_injmc_opts(parser)
 ppu.pygrb_add_bestnr_opts(parser)
 parser.add_argument("--num-loudest-off-trigs", action="store",
                     type=int, default=30, help="Number of loudest " +
                     "offsouce triggers to output details about.")
 parser.add_argument("--quiet-found-injs-output-file", default=None, #required=True,
                     help="Quiet-found injections html output file.")
 parser.add_argument("--missed-found-injs-output-file", default=None, #required=True,
                     help="Missed-found injections html output file.")
+# TODO: group hdf5 files into a single one
 parser.add_argument("--quiet-found-injs-h5-output-file", default=None, #required=True,
                     help="Quiet-found injections h5 output file.")
 parser.add_argument("--loudest-offsource-trigs-output-file", default=None, #required=True,
                     help="Loudest offsource triggers html output file.")
 parser.add_argument("--loudest-offsource-trigs-h5-output-file", default=None, #required=True,
                     help="Loudest offsource triggers h5 output file.")
 parser.add_argument("--loudest-onsource-trig-output-file", default=None, #required=True,
@@ -237,17 +269,17 @@
 if output_files.count(None) == len(output_files):
     msg = "Please specify at least one output file location."
     parser.error(msg)
 
 if opts.quiet_found_injs_output_file or opts.missed_found_injs_output_file or\
     opts.quiet_found_injs_h5_output_file:
     do_injections = True
-    if (opts.found_file is None) and (opts.missed_file is None):
-        err_msg = "Must provide both found and missed injections file "
-        err_msg += "locations if processing injections."
+    if opts.found_missed_file is None:
+        err_msg = "Must provide found-missed injections file "
+        err_msg += "location if processing injections."
         parser.error(err_msg)
 else:
     do_injections = False
 
 if opts.loudest_onsource_trig_output_file or opts.loudest_onsource_trig_h5_output_file:
     if opts.onsource_file is None:
         err_msg = "Must provide the on-source file location to output its "
@@ -256,16 +288,15 @@
 
 if not opts.newsnr_threshold:
     opts.newsnr_threshold = opts.snr_threshold
 
 # Store options used multiple times in local variables
 trig_file = opts.offsource_file
 onsource_file = opts.onsource_file
-found_file = opts.found_file
-missed_file = opts.missed_file
+found_missed_file = opts.found_missed_file
 chisq_index = opts.chisq_index
 chisq_nhigh = opts.chisq_nhigh
 wf_err = opts.waveform_error
 cal_errs = {}
 cal_errs['G1'] = opts.g1_cal_error
 cal_errs['H1'] = opts.h1_cal_error
 cal_errs['K1'] = opts.k1_cal_error
@@ -312,16 +343,15 @@
 
 # Extract IFOs and vetoes
 ifos, vetoes = ppu.extract_ifos_and_vetoes(trig_file, opts.veto_files,
                                            opts.veto_category)
 
 # Load triggers, time-slides, and segment dictionary
 logging.info("Loading triggers.")
-trigs = ppu.load_xml_table(trig_file, lsctables.MultiInspiralTable.tableName)
-logging.info("%d triggers loaded.", len(trigs))
+trig_data = load_data(trig_file, vetoes, ifos, opts)
 logging.info("Loading timeslides.")
 slide_dict = ppu.load_time_slides(trig_file)
 logging.info("Loading segments.")
 segment_dict = ppu.load_segment_dict(trig_file)
 
 # Construct trials
 logging.info("Constructing trials.")
@@ -583,16 +613,16 @@
     fap = sum(tot_off_snr > med_snr)/total_trials
 
 # =======================
 # Post-process injections
 # =======================
 if do_injections:
 
-    found_trig_table = ppu.load_injections(found_file, vetoes)
-    found_inj_table = ppu.load_injections(found_file, vetoes, sim_table=True)
+    found_trig_table = ppu.load_injections(found_missed_file, vetoes)
+    found_inj_table = ppu.load_injections(found_missed_file, vetoes, sim_table=True)
 
     logging.info("Missed/found injections/triggers loaded.")
 
     # Extract columns of found injections and triggers
     found_injs = lsctable_to_dict(
         found_inj_table, ifos, opts, trig_table=found_trig_table,
         background_bestnrs=full_time_veto_max_bestnr
@@ -614,15 +644,15 @@
     # 1) zero_fap 'g_found' -- > now acessed by indices: zero_fap
     # 2) nonzero_fap 'g_ifar' --> now acessed by indices: nonzero_fap
     # 3) missed because of vetoes 'g_missed2'--> now accessed by indices: vetoed_trigs
 
     logging.info("%d found injections analysed.", len(found_injs['mchirp']))
 
     # Missed injections (ones not recovered at all)
-    missed_inj_table = ppu.load_injections(missed_file, vetoes, sim_table=True)
+    missed_inj_table = ppu.load_injections(found_missed_file, vetoes, sim_table=True)
     missed_injs = lsctable_to_dict(missed_inj_table, ifos, opts)
 
     # Avoids a problem with formatting in the non-static html output file
     missed_na = [-0] * len(missed_injs['mchirp'])
 
     logging.info("%d missed injections analysed.", len(missed_injs['mchirp']))
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_chisq_veto` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_chisq_veto`

 * *Files 9% similar despite different names*

```diff
@@ -72,40 +72,62 @@
             trigs_or_injs = ppu.load_triggers(input_file, vetoes)
         
         num_trigs_or_injs = len(trigs_or_injs['network/end_time_gc'][:])
         
         if snr_type in ['coherent', 'null', 'reweighted']:
             data[snr_type] = trigs_or_injs['network/%s_snr' % snr_type][:]
         elif snr_type == 'single':
-            att = opts.ifo[0].lower()
-            data[snr_type] = trigs_or_injs['%s/snr_%s' % (opts.ifo, att)][:]
+            key = opts.ifo + '/snr_' + opts.ifo.lower()
+            # TODO: Undoes current L710 in EventManagerCoherent.write_to_hdf
+            # and is ugly hardcoded!
+            if opts.ifo.lower() != 'h1':
+                key = key[:-1]
+            data[snr_type] = trigs_or_injs[key][:]
 
         # Calculate coincident SNR
         elif snr_type == 'coincident':
             data[snr_type] = ppu.get_coinc_snr(trigs_or_injs, ifos)
 
         # Tags to find vetoes in HDF files
         veto_tags = {'standard': 'chisq',
                      'bank': 'bank_chisq',
                      'auto': 'cont_chisq'}
 
-        data[veto_type] = \
-            trigs_or_injs['%s/%s' % (opts.ifo, veto_tags[veto_type])][:]
+        # FIXME: network data contains my_network_chisq
+        chisq_key = 'network/my_network_chisq' if injections \
+            else opts.ifo + '/' + veto_tags[veto_type]
+        # Will not work until single IFO data is in found_missed_file
+        data[veto_type] = trigs_or_injs[chisq_key][:]
 
         # Floor single IFO chi-square at 0.005
         numpy.putmask(data[veto_type], data[veto_type] == 0, 0.005)
 
-        data['dof'] = numpy.unique(
-            trigs_or_injs['%s/%s_dof' % (opts.ifo, veto_tags[veto_type])][:])
-
-        logging.info("%d triggers found.", num_trigs_or_injs)
+        if not injections:
+            dof_key = '%s/%s_dof' % (opts.ifo, veto_tags[veto_type])
+            data['dof'] = numpy.unique(
+                trigs_or_injs[dof_key][:])
+
+        label = "injections" if injections else "triggers"
+           
+        logging.info("{0} {1} found.".format(num_trigs_or_injs, label))
 
     return data
 
 
+# Function to calculate chi-square weight for the reweighted SNR
+def new_snr_chisq(snr, new_snr, chisq_dof, chisq_index=4.0, chisq_nhigh=3.0):
+    """Returns the chi-square value needed to weight SNR into new SNR"""
+
+    chisqnorm = (snr/new_snr)**chisq_index
+    if chisqnorm <= 1:
+        return 1E-20
+
+    return chisq_dof * (2*chisqnorm - 1)**(chisq_nhigh/chisq_index)
+
+
 # Function that produces the contrours to be plotted
 def calculate_contours(trig_data, opts, veto_type, new_snrs=None):
     """Generate the contours for the veto plots"""
 
     # Read off the degrees of freedom
     dof = trig_data['dof'][0]
 
@@ -126,17 +148,17 @@
 
     # Initialise contours
     contours = numpy.zeros([len(new_snrs), len(snr_vals)],
                            dtype=numpy.float64)
     # Loop over SNR values and calculate chisq variable needed
     for j, snr in enumerate(snr_vals):
         for i, new_snr in enumerate(new_snrs):
-            contours[i][j] = plu.new_snr_chisq(snr, new_snr, dof,
-                                               opts.chisq_index,
-                                               opts.chisq_nhigh)
+            contours[i][j] = new_snr_chisq(snr, new_snr, dof,
+                                           opts.chisq_index,
+                                           opts.chisq_nhigh)
 
     return contours, snr_vals, cont_value
 
 
 # Function that produces the contrours to be plotted
 def contour_colors(opts, new_snrs=None):
     """Define the colours with which contours are plotted"""
@@ -157,46 +179,50 @@
 # Main script starts here
 # =============================================================================
 parser = ppu.pygrb_initialize_plot_parser(description=__doc__,
                                           version=__version__)
 parser.add_argument("-t", "--trig-file", action="store",
                     default=None, required=True,
                     help="The location of the trigger file")
+parser.add_argument("--found-missed-file",
+                    help="The hdf injection results file", required=False)
 parser.add_argument("-z", "--zoom-in", default=False, action="store_true",
                     help="Output file a zoomed in version of the plot.")
 parser.add_argument("-y", "--y-variable", required=True,
                     choices=['standard', 'bank', 'auto'],
                     help="Quantity to plot on the vertical axis.")
-parser.add_argument("--snr-type", default=None,
-                    choices=['coherent', 'single', 'coincident', 'null',
-                    'reweighted'], help="SNR value to plot on x-axis. Can be " +
+# TODO: get 'single' to work
+parser.add_argument("--snr-type", default='coherent',
+                    choices=['coherent', 'coincident', 'null', 'reweighted',
+                    'single'], help="SNR value to plot on x-axis. Can be " +
                     "coherent, coincident, null, reweighted, or single IFO SNR")
 ppu.pygrb_add_bestnr_opts(parser)
 opts = parser.parse_args()
 
 init_logging(opts.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
 # Check options
 trig_file = os.path.abspath(opts.trig_file)
-found_file = os.path.abspath(opts.found_file) if opts.found_file else None
+found_missed_file = os.path.abspath(opts.found_missed_file) if opts.found_missed_file else None
 zoom_in = opts.zoom_in
 veto_type = opts.y_variable
 ifo = opts.ifo
 snr_type = opts.snr_type
 # If this is false, coherent SNR is used on the horizontal axis
 # otherwise the single IFO SNR is used
 if snr_type == 'single':
     if ifo is None:
-        err_msg = "--ifo must be given when using --use-sngl-ifo-snr"
+        err_msg = "--ifo must be given to plot single IFO SNR veto"
         parser.error(err_msg)
 
 # Veto is intended as a single IFO quantity. Network chisq will be obsolete.
-if ifo is None:
-    err_msg = "--ifo must be given to plot single IFO SNR veto"
-    parser.error(err_msg)
+# TODO: fix vetoes
+#if ifo is None:
+#    err_msg = "--ifo must be given to plot single IFO SNR veto"
+#    parser.error(err_msg)
 
 # Prepare plot title and caption
 veto_labels = {'standard': "Chi Square Veto",
                'bank': "Bank Veto",
                'auto': "Auto Veto"}
 if opts.plot_title is None:
     opts.plot_title = "%s %s" %(ifo, veto_labels[veto_type])
@@ -207,15 +233,15 @@
     else:
         opts.plot_title += " vs Coherent SNR"
 if opts.plot_caption is None:
     opts.plot_caption = ("Blue crosses: background triggers.  " +
                          "Black line: veto line.  " +
                          "Gray shaded region: Vetoed area.  " +
                          "Yellow lines: contours of new SNR.")
-    if found_file:
+    if found_missed_file:
         opts.plot_caption = ("Red crosses: injections triggers.  ") +\
                             opts.plot_caption
 
 logging.info("Imported and ready to go.")
 
 # Set output directory
 outdir = os.path.split(os.path.abspath(opts.output_file))[0]
@@ -231,15 +257,15 @@
     err_msg = "The IFO selected with --ifo is unavailable in the data."
     raise RuntimeError(err_msg)
 
 # Extract trigger data
 trig_data = load_data(trig_file, vetoes, ifos, opts)
 
 # Extract (or initialize) injection data
-inj_data = load_data(found_file, vetoes, ifos, opts, injections=True)
+inj_data = load_data(found_missed_file, vetoes, ifos, opts, injections=True)
 
 # Sanity checks
 if trig_data[snr_type] is None and inj_data[snr_type] is None:
     err_msg = "No data to be plotted on the x-axis was found"
     raise RuntimeError(err_msg)
 if trig_data[veto_type] is None and inj_data[veto_type] is None:
     err_msg = "No data to be plotted on the y-axis was found"
@@ -255,22 +281,22 @@
 if snr_type == 'single':
     x_label = "%s SNR" % ifo
 if snr_type == 'coincident':
     x_label = "Coincident SNR"
 
 # Determine the minumum and maximum SNR value we are dealing with
 x_min = opts.sngl_snr_threshold
-x_max = 1.1*plu.axis_max_value(trig_data[snr_type], inj_data[snr_type], found_file)
+x_max = 1.1*plu.axis_max_value(trig_data[snr_type], inj_data[snr_type], found_missed_file)
 
 # Determine y-axis minimum value and label
 y_label = "%s Single %s" % (ifo, veto_labels[veto_type])
 y_min = 1
 
 # Determine the maximum bank veto value we are dealing with
-y_max = plu.axis_max_value(trig_data[veto_type], inj_data[veto_type], found_file)
+y_max = plu.axis_max_value(trig_data[veto_type], inj_data[veto_type], found_missed_file)
 
 # Determine contours for plots
 conts = None
 cont_value = None
 conts, snr_vals, cont_value = calculate_contours(trig_data, opts,
                                                  veto_type, new_snrs=None)
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_coh_ifosnr` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_coh_ifosnr`

 * *Files 3% similar despite different names*

```diff
@@ -65,28 +65,31 @@
     data['sigma_mean'] = dict((ifo, None) for ifo in ifos)
     data['sigma_max'] = None
     data['sigma_min'] = None
 
     if input_file:
         if injections:
             logging.info("Loading injections...")
-            # This will eventually become load_injections
-            trigs_or_injs = ppu.load_triggers(input_file, vetoes)
+            # TODO: This will eventually become load_injections
+            trigs = ppu.load_triggers(input_file, vetoes)
         else:
             logging.info("Loading triggers...")
-            trigs_or_injs = ppu.load_triggers(input_file, vetoes)
+            trigs = ppu.load_triggers(input_file, vetoes)
 
         num_trigs = len(trigs['network/end_time_gc'][:])
 
         # Load SNR data
         data['coherent'] = trigs['network/coherent_snr'][:]
 
         # Get single ifo SNR data
         for ifo in ifos:
-            att = ifo[0].lower()
+            att = ifo.lower()
+            # TODO: do we want pycbc_grb_inj_finder or other executables upstream to handle this?
+            if 'h' not in att:
+                att = att[0]
             data['single'][ifo] = trigs['%s/snr_%s' % (ifo, att)][:]
 
         # Get sigma for each ifo
         for ifo in ifos:
             sigma = dict((ifo, list(
                 trigs['%s/sigmasq' % ifo])) for ifo in ifos)
 
@@ -148,23 +151,25 @@
 # Main script starts here
 # =============================================================================
 parser = ppu.pygrb_initialize_plot_parser(description=__doc__,
                                           version=__version__)
 parser.add_argument("-t", "--trig-file", action="store",
                     default=None, required=True,
                     help="The location of the trigger file")
+parser.add_argument("--found-missed-file",
+                    help="The hdf injection results file", required=False)
 parser.add_argument("-z", "--zoom-in", default=False, action="store_true",
                     help="Output file a zoomed in version of the plot.")
 opts = parser.parse_args()
 
 init_logging(opts.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
 # Check options
 trig_file = os.path.abspath(opts.trig_file)
-found_file = os.path.abspath(opts.found_file) if opts.found_file else None
+found_file = os.path.abspath(opts.found_missed_file) if opts.found_missed_file else None
 zoom_in = opts.zoom_in
 ifo = opts.ifo
 if ifo is None:
     err_msg = "Please specify an interferometer"
     parser.error(err_msg)
 
 if opts.plot_title is None:
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_injs_results` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_injs_results`

 * *Files 11% similar despite different names*

```diff
@@ -40,312 +40,277 @@
 
 # =============================================================================
 # Functions
 # =============================================================================
 def process_var_strings(qty):
     """Add underscores to match HDF column name conventions"""
 
-    qty = qty.replace('effdist', 'eff_dist')
-    qty = qty.replace('effsitedist', 'eff_site_dist')
     qty = qty.replace('skyerror', 'sky_error')
     qty = qty.replace('cos', 'cos_')
     qty = qty.replace('abs', 'abs_')
-    qty = qty.replace('coaphase', 'coa_phase')
     qty = qty.replace('endtime', 'end_time')
+    qty = qty.replace('spin1a', 'spin1_a')
+    qty = qty.replace('spin2a', 'spin2_a')
 
     return qty
 
 
-def load_incl_data(injs, qty):
+def load_incl_data(input_file_handle, key, tag):
     """Extract data related to inclination from raw injection data"""
 
     local_dict = {}
 
     # Whether the user requests incl, |incl|, cos(incl), or cos(|incl|)
     # the following information is needed
-    local_dict['incl'] = injs['injections/inclination'][:]
+    # TODO: make sure it is clear that inclination is interpreted as theta_jn
+    local_dict['incl'] = input_file_handle[tag+'/thetajn'][:]
 
     # Requesting |incl| or cos(|incl|)
-    if 'abs_' in qty:
+    if 'abs_' in key:
         local_dict['abs_incl'] = 0.5*np.pi - abs(local_dict['incl'] - 0.5*np.pi)
 
     # Requesting cos(incl) or cos(|incl|): take cosine
-    if 'cos_' in qty:
-        angle = qty.replace('cos_', '')
+    if 'cos_' in key:
+        angle = key.replace('cos_', '')
         angle_data = local_dict[angle]
         data = np.cos(angle_data)
     # Requesting incl or abs_incl: convert to degrees
     else:
-        data = np.rad2deg(local_dict[qty])
+        data = np.rad2deg(local_dict[key])
 
     return data
 
+
 # Function to extract mass ratio or total mass data from a injection file
-def load_mass_data(injs, qty):
-    """Extract data related to mass ratio or total mass from raw
+def load_mass_data(input_file_handle, key, tag):
+    """Extract data related to mass ratio, chirp mass or total mass from raw
     injection data"""
 
-    if qty == 'mtotal':
-        data = injs['injections/mass1'][:] + injs['injections/mass2'][:]
-    elif qty == 'mchirp':
+    if key == 'mtotal':
+        data = input_file_handle[tag+'/mass1'][:] + \
+               input_file_handle[tag+'/mass2'][:]
+    elif key == 'mchirp':
         data, _ = pycbc.pnutils.mass1_mass2_to_mchirp_eta(
-            injs['injections/mass1'][:], injs['injections/mass2'][:])
+            input_file_handle[tag+'/mass1'][:],
+            input_file_handle[tag+'/mass2'][:])
     else:
-        data = injs['injections/mass2'][:]/injs['injections/mass1'][:]
+        data = input_file_handle[tag+'/mass2'][:]/\
+               input_file_handle[tag+'/mass1'][:]
         data = np.where(data > 1, 1./data, data)
 
     return data
 
 
-# Function to extract mass ratio or total mass data from a injection file
-def load_effdist_data(injs, qty, opts, ifos):
-    """Extract data related to effective distances from raw injection data"""
-
-    local_dict = {}
-
-    if qty == 'eff_site_dist':
-        data = injs['injections/eff_dist_%s' % opts.ifo[0].lower()][:]
+def load_sky_error_data(input_file_handle, key, tag, trig_data):
+    """Extract data related to sky_error from raw injection and trigger data"""
+    if tag == 'missed':
+        # Missed injections are assigned null values
+        data = np.full(len(input_file_handle[tag+'/mass1'][:]), None)
     else:
-        local_dict['eff_site_dist'] =\
-            dict((ifo, injs['injections/eff_dist_%s' % ifo[0].lower()][:])
-                 for ifo in ifos)
-        # Effective distance (inverse sum of inverse effective distances)
-        data = np.power(np.power(
-            local_dict['eff_site_dist'], -1).sum(0), -1)
-
-    return data
-
-# Function to extract spin related data from a injection file
-def load_spin_modulus_data(injs, qty):
-    """Extract data related to spin modulus from raw injection data"""
-
-    # Calculate the modulus
-    data = np.sqrt(injs['injections/%sx' % qty][:]**2 +
-                   injs['injections/%sy' % qty][:]**2 +
-                   injs['injections/%sz' % qty][:]**2)
+        inj = {}
+        inj['ra'] = input_file_handle[tag+'/ra'][:]
+        inj['dec'] = input_file_handle[tag+'/dec'][:]
+        trig = {}
+        trig['ra'] = np.rad2deg(trig_data['network/longitude'][:])
+        trig['dec'] = np.rad2deg(trig_data['network/latitude'][:])
+        data = np.arccos(np.cos(inj['dec'] - trig['dec']) -
+                         np.cos(inj['dec']) * np.cos(trig['dec']) *
+                         (1 - np.cos(inj['ra'] - trig['ra'])))
 
     return data
 
 
-# Function to extract desired data from a injection file
-def load_data(input_file, keys, opts, ifos):
+# These are keys in the found-missed-file under 'found' and 'missed' 
+# TODO: also in the found-missed injs file is 'inclination'
+easy_keys = ['distance', 'mass1', 'mass2', 'polarization',
+             'spin1_a', 'spin1x', 'spin1y', 'spin1z',
+             'spin2_a', 'spin2x', 'spin2y', 'spin2z',
+             'spin1_azimuthal', 'spin1_polar',
+             'spin2_azimuthal', 'spin2_polar',
+             'dec', 'ra', 'phi_ref']
+# Function to extract desired data from an injection file
+def load_data(input_file_handle, keys, tag, trig_data):
     """Create a dictionary containing the data specified by the
-    list of keys extracted from a injection file"""
+    list of keys extracted from an injection file"""
 
-    injs = h5py.File(input_file, 'r')
     data_dict = {}
 
-    easy_keys = ['coa_phase', 'distance', 'mass1', 'mass2',
-                 'polarization', 'spin1x', 'spin1y', 'spin1z',
-                 'spin2x', 'spin2y', 'spin2z']
-
-    for qty in keys:
-        if qty in easy_keys:
-            data_dict[qty] = injs['injections/%s' % qty][:]
-        elif qty == 'end_time':
-            data_dict[qty] = injs['injections/end_time'][:]
-            grb_time = ppu.get_grb_time(opts.seg_files)
-            data_dict[qty] -= grb_time
-        elif qty in ['latitude', 'longitude']:
-            data_dict[qty] = np.rad2deg(injs['injections/%s' % qty][:])
-        elif qty in ['mtotal', 'q', 'mchirp']:
-            data_dict[qty] = load_mass_data(injs, qty)
-        elif qty in ['eff_site_dist', 'eff_dist']:
-            data_dict[qty] = load_effdist_data(injs, qty, opts, ifos)
-        elif 'incl' in qty:
-            data_dict[qty] = load_incl_data(injs, qty)
-        # This handles spin1 and spin2, i.e. spin magnitudes, as components
-        # are dealt with in easy_keys (first if)
-        elif 'spin' in qty:
-            data_dict[qty] = load_spin_modulus_data(injs, qty)
+    for key in keys:
+        data_dict[key] = np.array([])
+        try:
+            if key in easy_keys:
+                data_dict[key] = f[tag][key][:]
+            elif key == 'end_time':
+                data_dict[key] = f[tag]['tc'][:]
+                #data_dict[key] -= grb_time
+            elif key in ['q', 'mchirp', 'mtotal']:
+                data_dict[key] = load_mass_data(input_file_handle, key, tag)
+            elif 'incl' in key:
+                data_dict[key] = load_incl_data(input_file_handle, key, tag)
+            elif key == 'sky_error':
+                data_dict[key] = load_sky_error_data(input_file_handle, key,
+                                                 tag, trig_data)
+        except KeyError:
+            #raise NotImplemented(key+' not allowed: returning empty entry')
+            logging.info(key+' not allowed yet')
+
+    logging.info("{0} {1} injections analysed.".format(len(data_dict[keys[0]]), tag))
 
     return data_dict
 
 
 def load_trig_data(input_file, vetoes):
     """Load data from a trigger file"""
 
     logging.info("Loading triggers...")
     trigs = ppu.load_triggers(input_file, vetoes)
 
     return trigs
 
 
-# Function to cherry-pick a subset of full_data specified by fap_mask.
-# Reweighted SNR values are picked separately since they are extracted
-# in different manners
-def grab_injs_subset(full_data, fap_mask):
-    """Separate out a subset of full_data based on fap_mask"""
-
-    data_subset = {}
-
-    for qty in full_data.keys():
-        data_subset[qty] = full_data[qty][fap_mask]
-
-    return data_subset
 
 
 # =============================================================================
 # Main script starts here
 # =============================================================================
 parser = ppu.pygrb_initialize_plot_parser(description=__doc__,
                                           version=__version__)
-parser.add_argument('--injection-file',
-                    help="The hdf injection file to plot", required=True)
-parser.add_argument('--offsource-file',
+parser.add_argument("--found-missed-file",
+                    help="The hdf injection results file", required=True)
+parser.add_argument("--trig-file",
                     help="The hdf offsource trigger file", required=True)            
-admitted_vars = ['coa_phase', 'distance', 'latitude', 'longitude','mass1',
-                 'mass2', 'polarization', 'spin1x', 'spin1y', 'spin1z',
-                 'spin2x', 'spin2y', 'spin2z', 'end_time', 'mtotal', 'q',
-                 'mchirp', 'eff_site_dist', 'eff_dist', 'incl', 'cos_incl',
-                 'abs_incl', 'cos_abs_incl', 'spin1', 'spin2', 'sky_error']
+parser.add_argument("--trigger-time",
+                    help="The GPS time of the external trigger", required=True)            
+# FIXME: not all of these work
+# TODO: effective distance
+admitted_vars = easy_keys + ['mtotal', 'q', 'mchirp',
+                             'spin1a', 'spin2a',
+                             'incl', 'cos_incl', 'abs_incl', 'cos_abs_incl',
+                             'cosincl', 'absincl', 'cosabsincl',
+                             'sky_error', 'skyerror', 'end_time', 'endtime',
+                             'latitude', 'longitude', 'coaphase', 'coa_phase',
+                             'eff_site_dist', 'eff_dist',
+                             'effsitedist', 'effdist']
 parser.add_argument("-x", "--x-variable", default=None, required=True,
                     choices=admitted_vars,
                     help="Quantity to plot on the horizontal axis. " +
                     "(Underscores may be omitted in specifying this option).")
 parser.add_argument("--x-log", action="store_true",
                     help="Use log horizontal axis")
 parser.add_argument("-y", "--y-variable", default=None, required=True,
                     choices=admitted_vars,
                     help="Quantity to plot on the vertical axis. " +
                     "(Underscores may be omitted in specifying this option).")
 parser.add_argument("--y-log", action="store_true",
                     help="Use log vertical axis")
-parser.add_argument('--colormap',default='cividis_r',
+parser.add_argument("--colormap",default='cividis_r',
                    help="Type of colormap to be used for the plots.")
-parser.add_argument('--gradient-far', action='store_true',
-                    help="Show far of found injections as a gradient")
-parser.add_argument('--far-type', choices=('inclusive', 'exclusive'),
+parser.add_argument("--far-type", choices=('inclusive', 'exclusive'),
                     default='inclusive',
                     help="Type of far to plot for the color. Choices are "
                          "'inclusive' or 'exclusive'. Default = 'inclusive'")
-parser.add_argument('--missed-on-top', action='store_true',
+parser.add_argument("--missed-on-top", action='store_true',
                     help="Plot missed injections on top of found ones and "
                          "high FAR on top of low FAR")
 opts = parser.parse_args()
 
 init_logging(opts.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
-# Check options
-if opts.injection_file is None:
-    err_msg = "Must provide injection file."
-    raise RuntimeError(err_msg)
-
 x_qty = process_var_strings(opts.x_variable)
 y_qty = process_var_strings(opts.y_variable)
 
 if 'eff_site_dist' in [x_qty, y_qty] and opts.ifo is None:
     err_msg = "A value for --ifo must be provided for "
     err_msg += "site specific effective distance"
     parser.error(err_msg)
 
 # Store options used multiple times in local variables
 outfile = opts.output_file
-trig_file = os.path.abspath(opts.offsource_file)
-f = h5py.File(opts.injection_file, 'r')
-grb_time = ppu.get_grb_time(opts.seg_files)
+trig_file = os.path.abspath(opts.trig_file)
+f = h5py.File(opts.found_missed_file, 'r')
+grb_time = opts.trigger_time
 
 # Set output directory
 logging.info("Setting output directory.")
 outdir = os.path.split(os.path.abspath(outfile))[0]
 if not os.path.isdir(outdir):
     os.makedirs(outdir)
 
 # Extract IFOs and vetoes
 ifos, vetoes = ppu.extract_ifos_and_vetoes(trig_file, opts.veto_files,
                                            opts.veto_category)
 
 # Load triggers. Time-slides not yet available
-logging.info("Loading triggers.")
 trig_data = load_trig_data(trig_file, vetoes)
 max_reweighted_snr = max(trig_data['network/reweighted_snr'][:])
 
 # =======================
 # Post-process injections
 # =======================
-# Triggers, missed injections (i.e., not recovered at all), and injections
-# recovered in some form. Trigs/injs at vetoed times are discarded.
-
-# Indices of all found injections
-found = f['found/injection_index'][:]
-# Indices of all missed injections
-missed = f['missed/all'][:]
-# Indices of injections found surviving vetoes
-found_after_vetoes = f['found_after_vetoes/injection_index'][:]
-# Indices of found but vetoed injections
-missed_after_vetoes = list(set(found) - set(found_after_vetoes))
-missed_after_vetoes = np.sort(missed_after_vetoes).astype(int)
-# Indices of injections found but not louder than background
-# are populated further down
-
-inj_data = load_data(opts.injection_file, [x_qty, y_qty], opts, ifos)
-logging.info("Triggers and missed/found injections loaded.")
-
-# Handle separately the special case of plotting the sky_error: this
-# quantity is not defined for *missed* injections
-found_trig = {}
-found_inj = {}
-if 'sky_error' in [x_qty, y_qty]:
-    found_inj['ra'] = np.rad2deg(f['injections/longitude'][found])
-    found_inj['dec'] = np.rad2deg(f['injections/latitude'][found])
-    found_trig['ra'] = np.rad2deg(trig_data['network/longitude'][:])
-    found_trig['dec'] = np.rad2deg(trig_data['network/latitude'][:])
-    found_inj['sky_error'] = np.arccos(np.cos(found_inj['dec'] - found_trig['dec']) -\
-                                       np.cos(found_inj['dec']) * np.cos(found_trig['dec']) *\
-                                       (1 - np.cos(found_inj['ra'] - found_trig['ra'])))
-
-    # Define inj_data only for found indices. Missed injections are assigned null values
-    inj_data['sky_error'] = np.full(len(f['injections/longitude'][:]), None)
-    inj_data['sky_error'][found] = found_inj['sky_error']
-
 # Extract the necessary data from the missed injections for the plot
-missed_inj = {}
-for qty in [x_qty, y_qty]:
-    missed_inj[qty] = inj_data[qty][missed]
-logging.info("%d missed injections analysed.", len(missed))
+missed_inj = load_data(f, [x_qty, y_qty], 'missed', trig_data)
 
 # Extract the necessary data from the found injections for the plot
-found_inj = {x_qty: inj_data[x_qty][found],
-             y_qty: inj_data[y_qty][found]}
+found_inj = load_data(f, [x_qty, y_qty], 'found', trig_data)
+
+# Injections found surviving vetoes
+found_after_vetoes = found_inj if 'found_after_vetoes' not in f.keys() else f['found_after_vetoes']
+# FIXME: Found but vetoed injections
+#missed_after_vetoes = list(set(found) - set(found_after_vetoes))
+#missed_after_vetoes = np.sort(missed_after_vetoes).astype(int)
+missed_after_vetoes = {x_qty: np.array([]), y_qty: np.array([])}
+# Injections found but not louder than background
+# are populated further down
 
 # Extract the detection statistic of injections found after vetoes
-found_after_vetoes_stat = f['found_after_vetoes/stat'][:]
+if len(list(f['network'].keys())) > 0:
+    found_after_vetoes_stat = f['network/reweighted_snr'][:] if 'found_after_vetoes' not in f.keys() else f['found_after_vetoes/stat'][:]
 
-# Separate triggers into:
-# 1) Found louder than background
-louder_mask = found_after_vetoes_stat > max_reweighted_snr
-louder_indices = found_after_vetoes[louder_mask]
-found_louder = grab_injs_subset(inj_data, louder_indices)
-found_louder['reweighted_snr'] = f['found_after_vetoes/stat'][louder_mask]
-
-# 2) Found quieter than background
-quieter_mask = (found_after_vetoes_stat <= max_reweighted_snr) & (found_after_vetoes_stat != 0)
-# Indices of injections found (bestnr > 0) but not louder than background (non-zero FAP)
-quieter_indices = found_after_vetoes[quieter_mask]
-found_quieter = grab_injs_subset(inj_data, quieter_indices)
-found_quieter['reweighted_snr'] = f['found_after_vetoes/stat'][quieter_mask]
-
-# Extract inclusive/exclusive IFAR for injections found quieter than background
-ifar_string = 'found_after_vetoes/ifar' if opts.far_type == 'inclusive' \
-    else 'found_after_vetoes/ifar_exc'
-found_quieter['ifar'] = f[ifar_string][quieter_mask]
-
-# 3) Missed due to vetoes
-vetoed = grab_injs_subset(inj_data, missed_after_vetoes)
-vetoed['reweighted_snr'] = []
-for vetoed_index in missed_after_vetoes:
-    found_index = list(found).index(vetoed_index)
-    vetoed['reweighted_snr'].append(f['found/stat'][found_index])
+    # Separate triggers into:
+    # 1) Found louder than background
+    louder_mask = found_after_vetoes_stat > max_reweighted_snr
+    found_louder = {}
+    for key in found_after_vetoes.keys():
+        found_louder[key] = found_after_vetoes[key][louder_mask]
+    found_louder['reweighted_snr'] = found_after_vetoes_stat[louder_mask]
     
-# Statistics: found on top (found-missed)
-FM = np.argsort(found_quieter['ifar'])
-# Statistics: missed on top (missed-found)
-MF = FM[::-1]
+    # 2) Found quieter than background: injections found (bestnr > 0)
+    #    but not louder than background (non-zero FAP)
+    quieter_mask = (found_after_vetoes_stat <= max_reweighted_snr) & (found_after_vetoes_stat != 0)
+    found_quieter = {}
+    for key in found_after_vetoes.keys():
+        found_quieter[key] = found_after_vetoes[key][quieter_mask]
+    found_quieter['reweighted_snr'] = found_after_vetoes_stat[quieter_mask]
+    
+    # TODO: ifar still missing
+    """
+    # Extract inclusive/exclusive IFAR for injections found quieter than background
+    ifar_string = 'found_after_vetoes/ifar' if opts.far_type == 'inclusive' \
+        else 'found_after_vetoes/ifar_exc'
+    found_quieter['ifar'] = f[ifar_string][quieter_mask]
+    """
+
+    # 3) Missed due to vetoes
+    # TODO: needs function to cherry-pick a subset of inj_data specified by
+    # a mask on FAP values.
+else:
+    found_louder = {x_qty: np.array([]), y_qty: np.array([])}
+    found_quieter = {x_qty: np.array([]), y_qty: np.array([])}
+# TMP FIX
+vetoed = missed_after_vetoes
 
-logging.info("%d found injections analysed.", len(found))
+# Statistics: found on top (found-missed)
+# TODO: ifar still missing
+#FM = np.argsort(found_quieter['ifar'])
+logging.info("WARNING: IFAR not implemented yet")
+logging.info("WARNING: Avoiding failure by setting it to -1 and shifting colorbar from [0,1] to [-2,0].")
+if found_quieter[x_qty].size:
+   FM = np.full(found_quieter['reweighted_snr'].shape, -1)
+   # Statistics: missed on top (missed-found)
+   MF = FM[::-1]
 
 # Post-processing of injections ends here
 
 # ==========
 # Make plots
 # ==========
 
@@ -356,31 +321,31 @@
 # Take care of axes labels
 axis_labels_dict = {'mchirp': "Chirp Mass (solar masses)",
                     'mtotal': "Total mass (solar masses)",
                     'q': "Mass ratio",
                     'distance': "Distance (Mpc)",
                     'eff_site_dist': "%s effective distance (Mpc)" % sitename.get(opts.ifo),
                     'eff_dist': "Inverse sum of effective distances (Mpc)",
-                    'end_time': "Time since %d (s)" % grb_time,
+                    'end_time': "Time since %s (s)" % grb_time,
                     'sky_error': "Rec. sky error (radians)",
                     'coa_phase': "Phase of complex SNR (radians)",
                     'latitude': "Latitude (degrees)",
                     'longitude': "Longitude (degrees)",
                     'incl': "Inclination (iota)",
                     'abs_incl': 'Magnitude of inclination (|iota|)',
                     'cos_incl': "cos(iota)",
                     'cos_abs_incl': "cos(|iota|)",
                     'mass1': "Mass of 1st binary component (solar masses)",
                     'mass2': "Mass of 2nd binary component (solar masses)",
                     'polarization': "Polarization phase (radians)",
-                    'spin1': "Spin on 1st binary component",
+                    'spin1_a': "Spin on 1st binary component",
                     'spin1x': "Spin x-component of 1st binary component",
                     'spin1y': "Spin y-component of 1st binary component",
                     'spin1z': "Spin z-component of 1st binary component",
-                    'spin2': "Spin on 2nd binary component",
+                    'spin2_a': "Spin on 2nd binary component",
                     'spin2x': "Spin x-component of 2nd binary component",
                     'spin2y': "Spin y-component of 2nd binary component",
                     'spin2z': "Spin z-component of 2nd binary component"}
 
 x_label = axis_labels_dict[x_qty]
 y_label = axis_labels_dict[y_qty]
 
@@ -400,50 +365,51 @@
 
 # Define the 'found' injection colour
 fnd_col = cmap(0)
 fnd_col = np.array([fnd_col])
 if not opts.missed_on_top:
     if missed_inj[x_qty].size and missed_inj[y_qty].size:
         ax.scatter(missed_inj[x_qty], missed_inj[y_qty], c="black", marker="x", s=10)
-    if vetoed[x_qty].size:
+    #FIXME: once vetoed is filled in properly
+    #if vetoed[x_qty].size:
+    if x_qty in vetoed.keys():
         ax.scatter(vetoed[x_qty], vetoed[y_qty], c="red", marker="x", s=10)
     if found_quieter[x_qty].size:
         p = ax.scatter(found_quieter[x_qty][FM], found_quieter[y_qty][FM],
                        c=found_quieter['reweighted_snr'][FM],
-                       cmap=cmap, vmin=0, vmax=1, s=40,
+                       cmap=cmap, vmin=-2, vmax=0, s=40,
                        edgecolor="w", linewidths=2.0)
         cb = plt.colorbar(p, label="p-value")
     if found_louder[x_qty].size:
         ax.scatter(found_louder[x_qty], found_louder[y_qty], c=fnd_col, marker="+", s=30)
 elif opts.missed_on_top:
     if found_louder[x_qty].size:
         ax.scatter(found_louder[x_qty], found_louder[y_qty], c=fnd_col, marker="+", s=15)
     if found_quieter[x_qty].size:
         p = ax.scatter(found_quieter[x_qty][MF], found_quieter[y_qty][MF],
                        c=found_quieter['reweighted_snr'][MF],
-                       cmap=cmap, vmin=0, vmax=1, s=40,
+                       cmap=cmap, vmin=-2, vmax=0, s=40,
                        edgecolor="w", linewidths=2.0)
         cb = plt.colorbar(p, label="p-value")
-    if vetoed[x_qty].size:
+    #FIXME: once vetoed is filled in properly
+    #if vetoed[x_qty].size:
+    if x_qty in vetoed.keys():
         ax.scatter(vetoed[x_qty], vetoed[y_qty], c="red", marker="x", s=40)
     if missed_inj[x_qty].size and missed_inj[y_qty].size:
         ax.scatter(missed_inj[x_qty], missed_inj[y_qty], c="black", marker="x", s=40)
 ax.grid()
 
 # Handle axis limits when plotting spins
-max_missed_inj = {}
-for key in ['spin1', 'spin2']:
-    for qty in [x_qty, y_qty]:
-        if key in qty:
-            max_missed_inj[qty] = missed_inj[qty].max()
-if "spin" in x_qty:
-    ax.set_xlim([0, np.ceil(10 * max(max_missed_inj[x_qty],
+if "spin" in x_qty and missed_inj['spin1_a'].size:
+    max_missed_inj = missed_inj['spin1_a'].max()
+    ax.set_xlim([0, np.ceil(10 * max(max_missed_inj,
                                      found_inj[x_qty].max())) / 10])
-if "spin" in y_qty:
-    ax.set_ylim([0, np.ceil(10 * max(max_missed_inj[y_qty],
+if "spin" in y_qty and missed_inj['spin2_a'].size:
+    max_missed_inj = missed_inj['spin2_a'].max()
+    ax.set_ylim([0, np.ceil(10 * max(max_missed_inj,
                                      found_inj[y_qty].max())) / 10])
 
 # Handle axis limits when plotting inclination
 if "incl" in x_qty or "incl" in y_qty:
     max_inc = np.pi
     #max_inc = max(np.concatenate((g_found[qty], g_ifar[qty], g_missed2[qty], missed_inj[qty])))
     max_inc_deg = np.rad2deg(max_inc)
@@ -507,19 +473,19 @@
                   'incl': "inclination",
                   'cos_incl': "inclination",
                   'abs_incl': "inclination",
                   'cos_abs_incl': "inclination",
                   'mass1': "mass",
                   'mass2': "mass",
                   'polarization': "polarization",
-                  'spin1': "spin",
+                  'spin1_a': "spin",
                   'spin1x': "spin x-component",
                   'spin1y': "spin y-component",
                   'spin1z': "spin z-component",
-                  'spin2': "spin",
+                  'spin2_a': "spin",
                   'spin2x': "spin x-component",
                   'spin2y': "spin y-component",
                   'spin2z': "spin z-component"}
 
     if "sky_error" in [x_qty, y_qty]:
         plot_title = "Sky error of recovered injections"
     else:
@@ -528,8 +494,8 @@
         plot_title += " and "+ title_dict[y_qty]
 
 # Wrap up
 plt.tight_layout()
 pycbc.results.save_fig_with_metadata(fig, outfile, cmd=' '.join(sys.argv),
                                      title=plot_title, caption=plot_caption)
 plt.close()
-logging.info("Plots complete.")
+logging.info("Plot complete.")
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_null_stats` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_null_stats`

 * *Files 3% similar despite different names*

```diff
@@ -71,16 +71,20 @@
 
         # Coherent SNR is always used
         data['coherent'] = trigs_or_injs['network/coherent_snr'][:]
 
         if null_stat_type == 'null':
             data[null_stat_type] = trigs_or_injs['network/%s_snr' % null_stat_type][:]
         elif null_stat_type == 'single':
-            att = opts.ifo[0].lower()
-            data[null_stat_type] = trigs_or_injs['%s/snr_%s' % (opts.ifo, att)][:]
+            key = opts.ifo + '/snr_' + opts.ifo.lower()
+            # TODO: Undoes current L710 in EventManagerCoherent.write_to_hdf
+            # and is ugly hardcoded!
+            if opts.ifo.lower() != 'h1':
+                key = key[:-1]
+            data[null_stat_type] = trigs_or_injs[key][:]
         elif null_stat_type == 'coincident':
             data[null_stat_type] = ppu.get_coinc_snr(trigs_or_injs, ifos)
 
         if injections:
             logging.info("%d injections found.", num_trigs_or_injs)
         else:
             logging.info("%d triggers found.", num_trigs_or_injs)
@@ -125,38 +129,40 @@
 # Main script starts here
 # =============================================================================
 parser = ppu.pygrb_initialize_plot_parser(description=__doc__,
                                           version=__version__)
 parser.add_argument("-t", "--trig-file", action="store",
                     default=None, required=True,
                     help="The location of the trigger file")
+parser.add_argument("--found-missed-file",
+                    help="The hdf injection results file", required=False)
 parser.add_argument("-z", "--zoom-in", default=False, action="store_true",
                     help="Output file a zoomed in version of the plot.")
 parser.add_argument("-y", "--y-variable", default=None,
                     choices=['coincident', 'null'], #TODO: overwhitened?
                     help="Quantity to plot on the vertical axis.")
 ppu.pygrb_add_bestnr_opts(parser)
 opts = parser.parse_args()
 
 init_logging(opts.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
 # Check options
 trig_file = os.path.abspath(opts.trig_file)
-found_file = os.path.abspath(opts.found_file) if opts.found_file else None
+found_missed_file = os.path.abspath(opts.found_missed_file) if opts.found_missed_file else None
 zoom_in = opts.zoom_in
 null_stat_type = opts.y_variable
 
 # Prepare plot title and caption
 y_labels = {'null': "Null SNR",
             'coincident': "Coincident SNR"} #TODO: overwhitened
 if opts.plot_title is None:
     opts.plot_title = y_labels[null_stat_type] + " vs Coherent SNR"
 if opts.plot_caption is None:
     opts.plot_caption = ("Blue crosses: background triggers.  ")
-    if found_file:
+    if found_missed_file:
         opts.plot_caption = opts.plot_caption +\
                             ("Red crosses: injections triggers.  ")
 
     if null_stat_type == 'coincident':
         opts.plot_caption += ("Green line: coincident SNR = coherent SNR.")
     else:
         opts.plot_caption = opts.plot_caption +\
@@ -177,28 +183,28 @@
 ifos, vetoes = ppu.extract_ifos_and_vetoes(trig_file, opts.veto_files,
                                         opts.veto_category)
 
 # Extract trigger data
 trig_data = load_data(trig_file, vetoes, ifos, opts)
 
 # Extract (or initialize) injection data
-inj_data = load_data(found_file, vetoes, ifos, opts, injections=True)
+inj_data = load_data(found_missed_file, vetoes, ifos, opts, injections=True)
 
 # Generate plots
 logging.info("Plotting...")
 
 # Contours
 snr_vals = None
 cont_colors = None
 shade_cont_value = None
 x_max = None
 # Coincident SNR plot case: we want a coinc=coh diagonal line on the plot
 if null_stat_type == 'coincident':
     cont_colors = ['g-']
-    x_max = plu.axis_max_value(trig_data['coherent'], inj_data['coherent'], found_file)
+    x_max = plu.axis_max_value(trig_data['coherent'], inj_data['coherent'], found_missed_file)
     snr_vals = [4, x_max]
     null_stat_conts = [[4, x_max]]
 # Overwhitened null stat (null SNR) and null stat  cases: newSNR contours
 else:
     cont_colors = ['k-', 'g-', 'm-']
     null_cont, snr_vals = calculate_contours(opts, new_snrs=None)
     null_stat_conts = [null_cont]
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_skygrid` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_skygrid`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_snr_timeseries` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_snr_timeseries`

 * *Files 6% similar despite different names*

```diff
@@ -65,16 +65,14 @@
     return trigs_or_injs
 
 
 # Find start and end times of trigger/injecton data relative to a given time
 def get_start_end_times(data_time, central_time):
     """Determine padded start and end times of data relative to central_time"""
 
-    #start = int(min(data.time)) - central_time
-    #end = int(max(data.time)) - central_time
     start = int(min(data_time)) - central_time
     end = int(max(data_time)) - central_time
     duration = end - start
     start -= duration*0.05
     end += duration*0.05
 
     return start, end
@@ -94,28 +92,29 @@
 # Main script starts here
 # =============================================================================
 parser = ppu.pygrb_initialize_plot_parser(description=__doc__,
                                           version=__version__)
 parser.add_argument("-t", "--trig-file", action="store",
                     default=None, required=True,
                     help="The location of the trigger file")
-parser.add_argument('--central-time', type=float, default=None,
-                    help="Center plot at the given GPS time. If omitted, "+
-                    "use the GRB trigger time")
+parser.add_argument("--found-missed-file",
+                    help="The hdf injection results file", required=False)
+parser.add_argument("--trigger-time", type=float, default=0,
+                    help="External GPS time.  Used to center the plot.")
 parser.add_argument("-y", "--y-variable", default=None,
                     choices=['coherent', 'single', 'reweighted', 'null'],
                     help="Quantity to plot on the vertical axis.")
 ppu.pygrb_add_bestnr_opts(parser)
 opts = parser.parse_args()
 
 init_logging(opts.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
 
 # Check options
 trig_file = os.path.abspath(opts.trig_file)
-found_file = os.path.abspath(opts.found_file) if opts.found_file else None
+inj_file = os.path.abspath(opts.found_missed_file) if opts.found_missed_file else None
 snr_type = opts.y_variable
 ifo = opts.ifo
 if snr_type == 'single' and ifo is None:
     err_msg = "Please specify an interferometer for a single IFO plot"
     parser.error(err_msg)
 
 logging.info("Imported and ready to go.")
@@ -129,42 +128,45 @@
 ifos, vetoes = ppu.extract_ifos_and_vetoes(trig_file, opts.veto_files,
                                         opts.veto_category)
 
 # Load trigger data
 trig_data = load_data(trig_file, vetoes)
 
 # Load (or initialize) injection data
-inj_data = load_data(found_file, vetoes, injections=True)
+inj_data = load_data(inj_file, vetoes, injections=True)
 
 # Specify HDF file keys for x quantity (time) and y quantity (SNR)
 if snr_type == 'single':
-    x_key = opts.ifo + '/' + 'end_time'
-    y_key = opts.ifo + '/' + 'snr_' + opts.ifo.lower()
+    x_key = opts.ifo + '/end_time'
+    y_key = opts.ifo + '/snr_' + opts.ifo.lower()
+    # TODO: Undoes current L710 in EventManagerCoherent.write_to_hdf
+    # and is ugly hardcoded!
+    if opts.ifo.lower() != 'h1':
+        y_key = y_key[:-1]
 else:
     x_key = 'network/end_time_gc'
     y_key = 'network/' + snr_type + '_snr'
 
 # Obtain times
 trig_data_time = trig_data[x_key][:]
-inj_data_time = inj_data[x_key][:] if found_file else None
+inj_data_time = inj_data[x_key][:] if inj_file else None
 
 # Obtain SNRs
 trig_data_snr = trig_data[y_key][:]
-inj_data_snr = inj_data[y_key][:] if found_file else None
+inj_data_snr = inj_data[y_key][:] if inj_file else None
 
-# Determine the central time (t=0): default is the GRB trigger time
-central_time = opts.central_time if opts.central_time is not None else\
-    ppu.get_grb_time(opts.seg_files)
+# Determine the central time (t=0) for the plot
+central_time = opts.trigger_time
 
 # Determine trigger data start and end times relative to the central time
 start, end = get_start_end_times(trig_data_time, central_time)
 
 # Reset trigger and injection times
 trig_data_time = reset_times(trig_data_time, central_time)
-if found_file:
+if inj_file:
     inj_data_time = reset_times(inj_data_time, central_time)
 
 # Generate plots
 logging.info("Plotting...")
 
 # Determine what goes on the vertical axis
 y_labels = {'coherent': "Coherent SNR",
@@ -174,15 +176,15 @@
 y_label = y_labels[snr_type]
 
 # Determine title and caption
 if opts.plot_title is None:
     opts.plot_title = y_label + " vs Time"
 if opts.plot_caption is None:
     opts.plot_caption = ("Blue crosses: background triggers.  ")
-    if found_file:
+    if inj_file:
         opts.plot_caption += ("Red crosses: injections triggers.")
 
 # Single IFO SNR versus time plots
 xlims = [start, end]
 if opts.x_lims:
     xlims = opts.x_lims
     xlims = map(float, xlims.split(','))
```

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_plot_stats_distribution` & `PyCBC-2.2.1/bin/pygrb/pycbc_pygrb_plot_stats_distribution`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/pygrb/pycbc_pygrb_pp_workflow` & `PyCBC-2.2.1/pycbc/workflow/grb_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,8 @@
-#!/usr/bin/env python
-#
-# Copyright (C) 2019 Francesco Pannarale
+# Copyright (C) 2015  Andrew Williamson
 #
 # This program is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the
 # Free Software Foundation; either version 3 of the License, or (at your
 # option) any later version.
 #
 # This program is distributed in the hope that it will be useful, but
@@ -12,144 +10,552 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
 # Public License for more details.
 #
 # You should have received a copy of the GNU General Public License along
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 
+#
+# =============================================================================
+#
+#                                   Preamble
+#
+# =============================================================================
+#
 
 """
-Workflow generator to run PyGRB offline post-processing.
+This library code contains functions and classes that are used in the
+generation of pygrb workflows. For details about pycbc.workflow see here:
+http://pycbc.org/pycbc/latest/html/workflow.html
 """
 
-# =============================================================================
-# Preamble
-# =============================================================================
-import sys
-import socket
-import logging
-import argparse
+import glob
 import os
-import pycbc.version
-from pycbc import init_logging
-import pycbc.workflow as _workflow
-from pycbc.workflow.plotting import PlotExecutable
-from pycbc.workflow.core import resolve_url_to_file
+import numpy as np
+from scipy.stats import rayleigh
+from gwdatafind.utils import filename_metadata
+from pycbc import makedir
+from pycbc.workflow.core import \
+    File, FileList, configparser_value_to_file, resolve_url_to_file,\
+    Executable, Node
+from pycbc.workflow.jobsetup import select_generic_executable
 from pycbc.workflow.pegasus_workflow import SubWorkflow
-from pycbc.results import create_versioning_page, layout
-from pycbc.results import pygrb_postprocessing_utils as ppu
-from pycbc.results.versioning import save_fig_with_metadata
-
-__author__ = "Francesco Pannarale  <francesco.pannarale@ligo.org>"
-__version__ = pycbc.version.git_verbose_msg
-__date__ = pycbc.version.date
-__program__ = "pycbc_pygrb_pp_workflow"
+from pycbc.workflow.plotting import PlotExecutable
 
 
-# =============================================================================
-# Functions
-# =============================================================================
-def opt_to_file(workflow, section, option):
-    """Fetch a file given its url location via the section
-    and option in the workflow configuraiton file."""
+def _select_grb_pp_class(wflow, curr_exe):
+    """
+    This function returns the class for PyGRB post-processing scripts.
+
+    Parameters
+    ----------
+    curr_exe : string
+        The name of the executable
+
+    Returns
+    -------
+    exe_class : Sub-class of pycbc.workflow.core.Executable that holds utility
+        functions appropriate for the given executable.  Instances of the class
+        ('jobs') **must** have methods
+        * job.create_node()
+        and
+        * job.get_valid_times(ifo, )
+    """
+    exe_path = wflow.cp.get('executables', curr_exe)
+    exe_name = os.path.basename(exe_path)
+    exe_to_class_map = {
+        'pycbc_grb_trig_combiner': PycbcGrbTrigCombinerExecutable,
+        'pycbc_grb_trig_cluster': PycbcGrbTrigClusterExecutable,
+        'pycbc_grb_inj_finder': PycbcGrbInjFinderExecutable,
+        'pycbc_grb_inj_combiner': PycbcGrbInjCombinerExecutable
+    }
+    if exe_name not in exe_to_class_map:
+        raise ValueError(f"No job class exists for executable {curr_exe}")
+
+    return exe_to_class_map[exe_name]
+
+
+def set_grb_start_end(cp, start, end):
+    """
+    Function to update analysis boundaries as workflow is generated
+
+    Parameters
+    ----------
+    cp : pycbc.workflow.configuration.WorkflowConfigParser object
+    The parsed configuration options of a pycbc.workflow.core.Workflow.
+
+    start : int
+    The start of the workflow analysis time.
+
+    end : int
+    The end of the workflow analysis time.
+
+    Returns
+    --------
+    cp : pycbc.workflow.configuration.WorkflowConfigParser object
+    The modified WorkflowConfigParser object.
+
+    """
+    cp.set("workflow", "start-time", str(start))
+    cp.set("workflow", "end-time", str(end))
+
+    return cp
+
+
+def make_gating_node(workflow, datafind_files, outdir=None, tags=None):
+    '''
+    Generate jobs for autogating the data for PyGRB runs.
+
+    Parameters
+    ----------
+    workflow: pycbc.workflow.core.Workflow
+        An instanced class that manages the constructed workflow.
+    datafind_files : pycbc.workflow.core.FileList
+        A FileList containing the frame files to be gated.
+    outdir : string
+        Path of the output directory
+    tags : list of strings
+        If given these tags are used to uniquely name and identify output files
+        that would be produced in multiple calls to this function.
+
+    Returns
+    --------
+    condition_strain_nodes : list
+        List containing the pycbc.workflow.core.Node objects representing the
+        autogating jobs.
+    condition_strain_outs : pycbc.workflow.core.FileList
+        FileList containing the pycbc.workflow.core.File objects representing
+        the gated frame files.
+    '''
+
+    cp = workflow.cp
+    if tags is None:
+        tags = []
+
+    condition_strain_class = select_generic_executable(workflow,
+                                                       "condition_strain")
+    condition_strain_nodes = []
+    condition_strain_outs = FileList([])
+    for ifo in workflow.ifos:
+        input_files = FileList([datafind_file for datafind_file in \
+                                datafind_files if datafind_file.ifo == ifo])
+        condition_strain_jobs = condition_strain_class(cp, "condition_strain",
+                ifos=ifo, out_dir=outdir, tags=tags)
+        condition_strain_node, condition_strain_out = \
+                condition_strain_jobs.create_node(input_files, tags=tags)
+        condition_strain_nodes.append(condition_strain_node)
+        condition_strain_outs.extend(FileList([condition_strain_out]))
+
+    return condition_strain_nodes, condition_strain_outs
+
+
+def fermi_core_tail_model(
+        sky_err, rad, core_frac=0.98, core_sigma=3.6, tail_sigma=29.6):
+    """Fermi systematic error model following
+    https://arxiv.org/abs/1909.03006, with default values valid
+    before 11 September 2019.
+
+    Parameters
+    ----------
+    core_frac : float
+        Fraction of the systematic uncertainty contained within the core
+        component.
+    core_sigma : float
+        Size of the GBM systematic core component.
+    tail_sigma : float
+        Size of the GBM systematic tail component.
+
+    Returns
+    _______
+    tuple
+        Tuple containing the core and tail probability distributions
+        as a function of radius.
+    """
+    scaledsq = sky_err**2 / -2 / np.log(0.32)
+    return (
+        frac * (1 - np.exp(-0.5 * (rad / np.sqrt(scaledsq + sigma**2))**2))
+        for frac, sigma
+        in zip([core_frac, 1 - core_frac], [core_sigma, tail_sigma]))
+
+
+def get_sky_grid_scale(
+        sky_error=0.0, containment=0.9, upscale=False, fermi_sys=False,
+        precision=1e-3, **kwargs):
+    """
+    Calculate the angular radius corresponding to a desired
+    localization uncertainty level. This is used to generate the search
+    grid and involves scaling up the standard 1-sigma value provided to
+    the workflow, assuming a normal probability profile. Fermi
+    systematic errors can be included, following
+    https://arxiv.org/abs/1909.03006, with default values valid before
+    11 September 2019. The default probability coverage is 90%.
+
+    Parameters
+    ----------
+    sky_error : float
+        The reported statistical 1-sigma sky error of the trigger.
+    containment : float
+        The desired localization probability to be covered by the sky
+        grid.
+    upscale : bool, optional
+        Whether to apply rescale to convert from 1 sigma -> containment
+        for non-Fermi triggers. Default = True as Swift reports 90%
+        radius directly.
+    fermi_sys : bool, optional
+        Whether to apply Fermi-GBM systematics via
+        ``fermi_core_tail_model``. Default = False.
+    precision : float, optional
+        Precision (in degrees) for calculating the error radius via
+        Fermi-GBM model.
+    **kwargs
+        Additional keyword arguments passed to `fermi_core_tail_model`.
+
+    Returns
+    _______
+
+    float
+        Sky error radius in degrees.
+    """
+    if fermi_sys:
+        lims = (0.5, 4)
+        radii = np.linspace(
+            lims[0] * sky_error, lims[1] * sky_error,
+            int((lims[1] - lims[0]) * sky_error / precision) + 1)
+        core, tail = fermi_core_tail_model(sky_error, radii, **kwargs)
+        out = radii[(abs(core + tail - containment)).argmin()]
+    else:
+        # Use Rayleigh distribution to go from 1 sigma containment to
+        # containment given by function variable. Interval method returns
+        # bounds of equal probability about the median, but we want 1-sided
+        # bound, hence use (2 * containment - 1)
+        out = sky_error
+        if upscale:
+            out *= rayleigh.interval(2 * containment - 1)[-1]
+    return out
+
+
+def setup_pygrb_pp_workflow(wf, pp_dir, seg_dir, segment, insp_files,
+                            inj_files, inj_insp_files, inj_tags):
+    """
+    Generate post-processing section of PyGRB offline workflow
+    """
+    pp_outs = FileList([])
+    # pp_outs is returned by this function. It is structured as follows:
+    # pp_outs[0]: [ALL_TIMES, ONSOURCE, OFFSOURCE, OFFTRIAL_1, ..., OFFTRIAL_N]
+    #             FileList (N can be set by the user and is 6 by default)
+    # pp_outs[1]: ALL_TIMES_CLUSTERED File
+    # pp_outs[2]: OFFSOURCE_CLUSTERED File
+    # pp_outs[3]: ONSOURCE_CLUSTERED File
+    # pp_outs[4]: OFFTRIAL_1_CLUSTERED File
+    # ...
+    # pp_outs[4+N]: OFFTRIAL_N_CLUSTERED File
+    # pp_outs[-2]: FOUNDMISSED FileList covering all injection sets
+    # pp_outs[-1]: FOUNDMISSED-FILTERED FileList covering all injection sets
+    #              in the same order as pp_outs[-2]
+
+    # Begin setting up trig combiner job(s)
+    # Select executable class and initialize
+    exe_class = _select_grb_pp_class(wf, "trig_combiner")
+    job_instance = exe_class(wf.cp, "trig_combiner")
+    # Create node for coherent no injections jobs
+    node, trig_files = job_instance.create_node(wf.ifos, seg_dir, segment,
+                                    insp_files, pp_dir)
+    wf.add_node(node)
+    pp_outs.append(trig_files)
+
+    # Trig clustering for each trig file
+    exe_class = _select_grb_pp_class(wf, "trig_cluster")
+    job_instance = exe_class(wf.cp, "trig_cluster")
+    for trig_file in trig_files:
+        # Create and add nodes
+        node, out_file = job_instance.create_node(trig_file, pp_dir)
+        wf.add_node(node)
+        pp_outs.append(out_file)
+
+    # Find injections from triggers
+    exe_class = _select_grb_pp_class(wf, "inj_finder")
+    job_instance = exe_class(wf.cp, "inj_finder")
+    inj_find_files = FileList([])
+    for inj_tag in inj_tags:
+        tag_inj_files = FileList([f for f in inj_files
+                                  if inj_tag in f.tags])
+        # The here stems from the injection group information
+        # being stored in the second tag. This could be improved
+        # depending on the final implementation of injections
+        tag_insp_files = FileList([f for f in inj_insp_files
+                                   if inj_tag in f.tags[1]])
+        node, inj_find_file = job_instance.create_node(
+                                           tag_inj_files, tag_insp_files,
+                                           pp_dir)
+        wf.add_node(node)
+        inj_find_files.append(inj_find_file)
+    pp_outs.append(inj_find_files)
+
+    # Combine injections
+    exe_class = _select_grb_pp_class(wf, "inj_combiner")
+    job_instance = exe_class(wf.cp, "inj_combiner")
+    inj_comb_files = FileList([])
+    for in_file in inj_find_files:
+        if 'DETECTION' not in in_file.tags:
+            node, inj_comb_file = job_instance.create_node(in_file,
+                                                           pp_dir,
+                                                           in_file.tags,
+                                                           segment)
+            wf.add_node(node)
+            inj_comb_files.append(inj_comb_file)
+    pp_outs.append(inj_comb_files)
+
+    return pp_outs
+
+
+class PycbcGrbTrigCombinerExecutable(Executable):
+    """ The class responsible for creating jobs
+    for ''pycbc_grb_trig_combiner''.
+    """
+
+    current_retention_level = Executable.ALL_TRIGGERS
+
+    def __init__(self, cp, name):
+        super().__init__(cp=cp, name=name)
+        self.trigger_name = cp.get('workflow', 'trigger-name')
+        self.trig_start_time = cp.get('workflow', 'start-time')
+        self.num_trials = int(cp.get('trig_combiner', 'num-trials'))
+
+    def create_node(self, ifo_tag, seg_dir, segment, insp_files,
+                    out_dir, tags=None):
+        node = Node(self)
+        node.add_opt('--verbose')
+        node.add_opt("--ifo-tag", ifo_tag)
+        node.add_opt("--grb-name", self.trigger_name)
+        node.add_opt("--trig-start-time", self.trig_start_time)
+        node.add_opt("--segment-dir", seg_dir)
+        node.add_input_list_opt("--input-files", insp_files)
+        node.add_opt("--user-tag", "PYGRB")
+        node.add_opt("--num-trials", self.num_trials)
+        # Prepare output file tag
+        user_tag = f"PYGRB_GRB{self.trigger_name}"
+        if tags:
+            user_tag += "_{}".format(tags)
+        # Add on/off source and off trial outputs
+        output_files = FileList([])
+        outfile_types = ['ALL_TIMES', 'OFFSOURCE', 'ONSOURCE']
+        for i in range(self.num_trials):
+            outfile_types.append("OFFTRIAL_{}".format(i+1))
+        for out_type in outfile_types:
+            out_name = "{}-{}_{}-{}-{}.h5".format(
+                       ifo_tag, user_tag, out_type,
+                       segment[0], segment[1]-segment[0])
+            out_file = File(ifo_tag, 'trig_combiner', segment,
+                            file_url=os.path.join(out_dir, out_name))
+            node.add_output(out_file)
+            output_files.append(out_file)
+
+        return node, output_files
+
+
+class PycbcGrbTrigClusterExecutable(Executable):
+    """ The class responsible for creating jobs
+    for ''pycbc_grb_trig_cluster''.
+    """
+
+    current_retention_level = Executable.ALL_TRIGGERS
+
+    def __init__(self, cp, name):
+        super().__init__(cp=cp, name=name)
+
+    def create_node(self, in_file, out_dir):
+        node = Node(self)
+        node.add_input_opt("--trig-file", in_file)
+        # Determine output file name
+        ifotag, filetag, segment = filename_metadata(in_file.name)
+        start, end = segment
+        out_name = "{}-{}_CLUSTERED-{}-{}.h5".format(ifotag, filetag,
+                                                     start, end-start)
+        out_file = File(ifotag, 'trig_cluster', segment,
+                        file_url=os.path.join(out_dir, out_name))
+        node.add_output(out_file)
+
+        return node, out_file
+
+
+class PycbcGrbInjFinderExecutable(Executable):
+    """The class responsible for creating jobs for ``pycbc_grb_inj_finder``
+    """
+    current_retention_level = Executable.ALL_TRIGGERS
+
+    def __init__(self, cp, exe_name):
+        super().__init__(cp=cp, name=exe_name)
+
+    def create_node(self, inj_files, inj_insp_files,
+                    out_dir, tags=None):
+        if tags is None:
+            tags = []
+        node = Node(self)
+        node.add_input_list_opt('--input-files', inj_insp_files)
+        node.add_input_list_opt('--inj-files', inj_files)
+        ifo_tag, desc, segment = filename_metadata(inj_files[0].name)
+        desc = '_'.join(desc.split('_')[:-1])
+        out_name = "{}-{}_FOUNDMISSED-{}-{}.h5".format(
+            ifo_tag, desc, segment[0], abs(segment))
+        out_file = File(ifo_tag, 'inj_finder', segment,
+                        os.path.join(out_dir, out_name), tags=tags)
+        node.add_output(out_file)
+        return node, out_file
+
+
+class PycbcGrbInjCombinerExecutable(Executable):
+    """The class responsible for creating jobs ``pycbc_grb_inj_combiner``
+    """
+    current_retention_level = Executable.ALL_TRIGGERS
+
+    def __init__(self, cp, exe_name):
+        super().__init__(cp=cp, name=exe_name)
+
+    def create_node(self, input_file, out_dir, ifo_tag, segment, tags=None):
+        if tags is None:
+            tags = []
+        node = Node(self)
+        node.add_input_opt('--input-files', input_file)
+        out_name = input_file.name.replace('.h5', '-FILTERED.h5')
+        out_file = File(ifo_tag, 'inj_combiner', segment,
+                        os.path.join(out_dir, out_name), tags=tags)
+        node.add_output_opt('--output-file', out_file)
+        return node, out_file
+
+
+def build_veto_filelist(workflow):
+    """Construct a FileList instance containing all veto xml files"""
+
+    veto_dir = workflow.cp.get('workflow', 'veto-directory')
+    veto_files = glob.glob(veto_dir + '/*CAT*.xml')
+    veto_files = [resolve_url_to_file(vf) for vf in veto_files]
+    veto_files = FileList(veto_files)
+
+    return veto_files
+
+
+def build_segment_filelist(workflow):
+    """Construct a FileList instance containing all segments txt files"""
 
-    path = workflow.cp.get(section, option)
+    seg_dir = workflow.cp.get('workflow', 'segment-dir')
+    file_names = ["bufferSeg.txt", "offSourceSeg.txt", "onSourceSeg.txt"]
+    seg_files = [os.path.join(seg_dir, fn) for fn in file_names]
+    seg_files = [resolve_url_to_file(sf) for sf in seg_files]
+    seg_files = FileList(seg_files)
 
-    return resolve_url_to_file(path)
+    return seg_files
 
 
-def make_pygrb_plot(workflow, exec_name, out_dir, # exclude=None, require=None,
-                    ifo=None, inj_set=None, tags=None):
+def make_pygrb_plot(workflow, exec_name, out_dir,
+                    ifo=None, inj_file=None, trig_file=None, tags=None):
     """Adds a node for a plot of PyGRB results to the workflow"""
 
     tags = [] if tags is None else tags
 
     # Initialize job node with its tags
     grb_name = workflow.cp.get('workflow', 'trigger-name')
     extra_tags = ['GRB'+grb_name]
-    if inj_set is not None:
-        extra_tags.append(inj_set) # TODO: why is inj_set repeated twice in output files?
+    # TODO: why is inj_set repeated twice in output files?
+    # if inj_set is not None:
+    #     extra_tags.append(inj_set)
     if ifo:
         extra_tags.append(ifo)
     node = PlotExecutable(workflow.cp, exec_name, ifos=workflow.ifos,
                           out_dir=out_dir,
                           tags=tags+extra_tags).create_node()
     # Pass the trigger file as an input File instance
-    if exec_name in ['pygrb_plot_chisq_veto', 'pygrb_plot_coh_ifosnr',
-                     'pygrb_plot_null_stats', 'pygrb_plot_skygrid',
-                     'pygrb_plot_snr_timeseries']:
-        trig_file = opt_to_file(workflow, 'workflow', 'trig-file')
-        node.add_input_opt('--trig-file', trig_file)
+    # if exec_name in ['pygrb_plot_chisq_veto', 'pygrb_plot_coh_ifosnr',
+    #                  'pygrb_plot_null_stats', 'pygrb_plot_skygrid',
+    #                  'pygrb_plot_snr_timeseries']:
+    if trig_file is not None:
+        node.add_input_opt('--trig-file', resolve_url_to_file(trig_file))
     # Pass the veto and segment files and options
-    node.add_opt('--veto-category', workflow.cp.get('workflow', 'veto-category'))
-    veto_files = ppu.build_veto_filelist(workflow)
-    node.add_input_list_opt('--veto-files', veto_files)
+    if workflow.cp.has_option('workflow', 'veto-category'):
+        node.add_opt('--veto-category',
+                     workflow.cp.get('workflow', 'veto-category'))
+    # FIXME: move to next if within previous one and else Raise error?
+    if workflow.cp.has_option('workflow', 'veto-files'):
+        veto_files = build_veto_filelist(workflow)
+        node.add_input_list_opt('--veto-files', veto_files)
+    # TODO: check this for pygrb_efficiency and pygrb_plot_stats_distribution
+    # They originally wanted seg_files
     if exec_name in ['pygrb_plot_injs_results', 'pygrb_efficiency',
                      'pygrb_plot_snr_timeseries',
                      'pygrb_plot_stats_distribution']:
-        seg_files = ppu.build_segment_filelist(workflow)
-        node.add_input_list_opt('--seg-files', seg_files)
+        trig_time = workflow.cp.get('workflow', 'trigger-time')
+        node.add_opt('--trigger-time', trig_time)
     # Other shared tuning values
     if exec_name not in ['pygrb_plot_skygrid', 'pygrb_plot_coh_ifosnr']:
-        if not (exec_name == 'pygrb_plot_snr_timeseries' and tags[0] != 'reweighted'):
-            for opt in ['chisq-index', 'chisq-nhigh',
-                        'null-snr-threshold', 'snr-threshold', 'newsnr-threshold',
-                        'sngl-snr-threshold', 'null-grad-thresh', 'null-grad-val']:
-                node.add_opt('--'+opt, workflow.cp.get('workflow', opt))
+        if not (exec_name == 'pygrb_plot_snr_timeseries' and
+                tags[0] != 'reweighted'):
+            for opt in ['chisq-index', 'chisq-nhigh', 'null-snr-threshold',
+                        'snr-threshold', 'newsnr-threshold',
+                        'sngl-snr-threshold', 'null-grad-thresh',
+                        'null-grad-val']:
+                if workflow.cp.has_option('workflow', opt):
+                    node.add_opt('--'+opt, workflow.cp.get('workflow', opt))
     # Pass the injection file as an input File instance
-    if inj_set is not None and exec_name not in ['pygrb_plot_skygrid',
-                                                 'pygrb_plot_stats_distribution']:
-        found_file = opt_to_file(workflow, 'injections-'+inj_set, 'found-file')
-        node.add_input_opt('--found-file', found_file)
+    if inj_file is not None and exec_name not in \
+            ['pygrb_plot_skygrid', 'pygrb_plot_stats_distribution']:
+        fm_file = resolve_url_to_file(inj_file)
+        node.add_input_opt('--found-missed-file', fm_file)
     # IFO option
     if ifo:
         node.add_opt('--ifo', ifo)
     # Additional input files (passed as File instances)
-    if exec_name in ['pygrb_plot_injs_results', 'pygrb_efficiency']:
-        missed_file = opt_to_file(workflow, 'injections-'+inj_set, 'missed-file')
-        node.add_input_opt('--missed-file', missed_file)
+    # if exec_name in ['pygrb_plot_injs_results', 'pygrb_efficiency']:
+    #     missed_file = inj_file
+    #     node.add_input_opt('--missed-file', missed_file)
+    # FIXME: need found-missed-file option
     # Output files and final input file (passed as a File instance)
     if exec_name == 'pygrb_efficiency':
         # In this case tags[0] is the offtrial number
-        onsource_file = opt_to_file(workflow, 'workflow', 'onsource-file')
+        onsource_file = configparser_value_to_file(workflow.cp,
+                                                   'workflow', 'onsource-file')
         node.add_input_opt('--onsource-file', onsource_file)
         node.new_output_file_opt(workflow.analysis_time, '.png',
-                                 '--background-output-file', tags=extra_tags+['max_background'])
+                                 '--background-output-file',
+                                 tags=extra_tags+['max_background'])
         node.new_output_file_opt(workflow.analysis_time, '.png',
-                                 '--onsource-output-file', tags=extra_tags+['onsource'])
+                                 '--onsource-output-file',
+                                 tags=extra_tags+['onsource'])
     else:
         node.new_output_file_opt(workflow.analysis_time, '.png',
                                  '--output-file', tags=extra_tags)
-        if exec_name in ['pygrb_plot_coh_ifosnr', 'pygrb_plot_null_stats'] and 'zoomin' in tags:
+        if exec_name in ['pygrb_plot_coh_ifosnr', 'pygrb_plot_null_stats'] \
+                and 'zoomin' in tags:
             node.add_opt('--zoom-in')
     # Quantity to be displayed on the y-axis of the plot
-    if exec_name in ['pygrb_plot_chisq_veto', 'pygrb_plot_null_stats', 'pygrb_plot_snr_timeseries']:
+    if exec_name in ['pygrb_plot_chisq_veto', 'pygrb_plot_null_stats',
+                     'pygrb_plot_snr_timeseries']:
         node.add_opt('--y-variable', tags[0])
     # Quantity to be displayed on the x-axis of the plot
     elif exec_name == 'pygrb_plot_stats_distribution':
         node.add_opt('--x-variable', tags[0])
     elif exec_name == 'pygrb_plot_injs_results':
         # Variables to plot on x and y axes
         node.add_opt('--y-variable', tags[0])
         node.add_opt('--x-variable', tags[1])
         # Flag to plot found over missed or missed over found
-        node.add_opt('--'+tags[2])
+        if tags[2] == 'missed-on-top':
+            node.add_opt('--'+tags[2])
         # Enable log axes
         subsection = '_'.join(tags[0:2])
         for log_flag in ['x-log', 'y-log']:
-            if workflow.cp.has_option_tags(exec_name, log_flag, tags=[subsection]):
+            if workflow.cp.has_option_tags(exec_name, log_flag,
+                                           tags=[subsection]):
                 node.add_opt('--'+log_flag)
 
     # Add job node to workflow
     workflow += node
 
     return node, node.output_files
 
 
-def make_info_table(workflow, out_dir, ifo=None, tags=None):
+def make_info_table(workflow, out_dir, tags=None):
     """Setup a job to create an html snippet with the GRB trigger information.
     """
 
     tags = [] if tags is None else tags
 
     # Exectuable
     exec_name = 'pygrb_grb_info_table'
@@ -171,544 +577,239 @@
                              '--output-file', tags=extra_tags)
     # Add job node to workflow
     workflow += node
 
     return node, node.output_files
 
 
-def make_pygrb_injs_tables(workflow, out_dir, # exclude=None, require=None,
+def make_pygrb_injs_tables(workflow, out_dir,  # exclude=None, require=None,
                            inj_set=None, tags=None):
-    """Adds a PyGRB job to make quiet-found and missed-found injection tables"""
+    """Adds a PyGRB job to make quiet-found and missed-found injection tables.
+    """
 
     tags = [] if tags is None else tags
 
     # Exectuable
     exec_name = 'pygrb_page_tables'
     # Initialize job node
     grb_name = workflow.cp.get('workflow', 'trigger-name')
     extra_tags = ['GRB'+grb_name]
+    # TODO: why is inj_set repeated twice in output files?
     if inj_set is not None:
-        extra_tags.append(inj_set) # TODO: why is inj_set repeated twice in output files?
+        extra_tags.append(inj_set)
     node = PlotExecutable(workflow.cp, exec_name,
                           ifos=workflow.ifos, out_dir=out_dir,
                           tags=tags+extra_tags).create_node()
     # Pass the veto and segment files and options
-    veto_files = ppu.build_veto_filelist(workflow)
-    node.add_input_list_opt('--veto-files', veto_files)
-    seg_files = ppu.build_segment_filelist(workflow)
-    node.add_input_list_opt('--seg-files', seg_files)
+    if workflow.cp.has_option('workflow', 'veto-files'):
+        veto_files = build_veto_filelist(workflow)
+        node.add_input_list_opt('--veto-files', veto_files)
+    trig_time = workflow.cp.get('workflow', 'trigger-time')
+    node.add_opt('--trigger-time', trig_time)
     # Other shared tuning values
     for opt in ['chisq-index', 'chisq-nhigh', 'null-snr-threshold',
                 'veto-category', 'snr-threshold', 'newsnr-threshold',
                 'sngl-snr-threshold', 'null-grad-thresh', 'null-grad-val']:
-        node.add_opt('--'+opt, workflow.cp.get('workflow', opt))
+        if workflow.cp.has_option('workflow', opt):
+            node.add_opt('--'+opt, workflow.cp.get('workflow', opt))
     # Handle input/output for injections
     if inj_set:
-        # Found/missed injection files (passed as File instances)
-        for f_or_m in ['found-file', 'missed-file']:
-            f_or_m_file = opt_to_file(workflow, 'injections-'+inj_set, f_or_m)
-            node.add_input_opt('--'+f_or_m, f_or_m_file)
+        # Found-missed injection file (passed as File instance)
+        fm_file = configparser_value_to_file(workflow.cp,
+                                             'injections-'+inj_set,
+                                             'found-missed-file')
+        node.add_input_opt('--found-missed-file', fm_file)
         # Missed-found and quiet-found injections html output files
         for mf_or_qf in ['missed-found', 'quiet-found']:
+            mf_or_qf_tags = [mf_or_qf.upper().replace('-', '_')]
             node.new_output_file_opt(workflow.analysis_time, '.html',
                                      '--'+mf_or_qf+'-injs-output-file',
-                                     tags=extra_tags+[mf_or_qf.upper().replace('-', '_')])
+                                     tags=extra_tags+mf_or_qf_tags)
         # Quiet-found injections h5 output file
         node.new_output_file_opt(workflow.analysis_time, '.h5',
                                  '--quiet-found-injs-h5-output-file',
                                  tags=extra_tags+['QUIET_FOUND'])
     # Handle input/output for onsource/offsource
     else:
         # Onsource input file (passed as File instance)
-        onsource_file = opt_to_file(workflow, 'workflow', 'onsource-file')
+        onsource_file = configparser_value_to_file(workflow.cp,
+                                                   'workflow', 'onsource-file')
         node.add_input_opt('--onsource-file', onsource_file)
-        # Loudest offsource triggers and onsource trigger html and h5 output files
-        for source_type in ['onsource-trig', 'offsource-trigs']:
+        # Loudest offsource triggers and onsource trigger html and h5 output
+        for src_type in ['onsource-trig', 'offsource-trigs']:
+            src_type_tags = [src_type.upper().replace('-', '_')]
             node.new_output_file_opt(workflow.analysis_time, '.html',
-                                     '--loudest-'+source_type+'-output-file',
-                                     tags=extra_tags+[source_type.upper().replace('-', '_')])
+                                     '--loudest-'+src_type+'-output-file',
+                                     tags=extra_tags+src_type_tags)
             node.new_output_file_opt(workflow.analysis_time, '.h5',
-                                     '--loudest-'+source_type+'-h5-output-file',
-                                     tags=extra_tags+[source_type.upper().replace('-', '_')])
+                                     '--loudest-'+src_type+'-h5-output-file',
+                                     tags=extra_tags+src_type_tags)
 
     # Add job node to the workflow
     workflow += node
 
     return node, node.output_files
 
 
 # Based on setup_single_det_minifollowups
 def setup_pygrb_minifollowups(workflow, followups_file,
                               dax_output, out_dir,
-                              tags=None):
-    """ Create plots that followup the the loudest PyGRB triggers or
+                              trig_file=None, tags=None):
+    """Create plots that followup the the loudest PyGRB triggers or
     missed injections from an HDF file.
+
     Parameters
     ----------
     workflow: pycbc.workflow.Workflow
         The core workflow instance we are populating
     followups_file: pycbc.workflow.File
         The File class holding the triggers/injections.
     dax_output: The directory that will contain the dax file.
     out_dir: path
         The directory to store minifollowups result plots and files
     tags: {None, optional}
         Tags to add to the minifollowups executables
     """
-    #ifos: List of available IFOs to loop over.
-
-    logging.info('Entering minifollowups module')
-
-    if not workflow.cp.has_section('workflow-minifollowups'):
-        msg = 'There is no [workflow-minifollowups] section in '
-        msg += 'configuration file'
-        logging.info(msg)
-        logging.info('Leaving minifollowups')
-        return
 
     tags = [] if tags is None else tags
-    _workflow.makedir(dax_output)
-
-    # Trun the trigger file into a File instance
-    trig_file = opt_to_file(workflow, 'workflow', 'trig-file')
+    # _workflow.makedir(dax_output)
+    makedir(dax_output)
 
     # Turn the config file into a File instance
-    #curr_ifo = single_trig_file.ifo
-    #config_path = os.path.abspath(dax_output + '/' + curr_ifo + \
+    # curr_ifo = single_trig_file.ifo
+    # config_path = os.path.abspath(dax_output + '/' + curr_ifo + \
     config_path = os.path.abspath(dax_output + '/' + \
-                                   '_'.join(tags) + '_minifollowup.ini')
+                                  '_'.join(tags) + '_minifollowup.ini')
     workflow.cp.write(open(config_path, 'w'))
     config_file = resolve_url_to_file(config_path)
 
-    #wikifile = curr_ifo + '_'.join(tags) + 'loudest_table.txt'
+    # wikifile = curr_ifo + '_'.join(tags) + 'loudest_table.txt'
     wikifile = '_'.join(tags) + 'loudest_table.txt'
 
     # Create the node
-    exe = _workflow.Executable(workflow.cp, 'pygrb_minifollowups',
-                               ifos=workflow.ifos, out_dir=dax_output,
-                               tags=tags)
+    exe = Executable(workflow.cp, 'pygrb_minifollowups',
+                     ifos=workflow.ifos, out_dir=dax_output,
+                     tags=tags)
     node = exe.create_node()
 
     # Grab and pass all necessary files
-    node.add_input_opt('--trig-file', trig_file)
-    veto_files = ppu.build_veto_filelist(workflow)
-    node.add_input_list_opt('--veto-files', veto_files)
-    seg_files = ppu.build_segment_filelist(workflow)
-    node.add_input_list_opt('--seg-files', seg_files)
+    if trig_file is not None:
+        node.add_input_opt('--trig-file', trig_file)
+    if workflow.cp.has_option('workflow', 'veto-files'):
+        veto_files = build_veto_filelist(workflow)
+        node.add_input_list_opt('--veto-files', veto_files)
+    trig_time = workflow.cp.get('workflow', 'trigger-time')
+    node.add_opt('--trigger-time', trig_time)
     node.add_input_opt('--config-files', config_file)
     node.add_input_opt('--followups-file', followups_file)
     node.add_opt('--wiki-file', wikifile)
     if tags:
         node.add_list_opt('--tags', tags)
     node.new_output_file_opt(workflow.analysis_time, '.dax', '--dax-file')
     node.new_output_file_opt(workflow.analysis_time, '.dax.map',
                              '--output-map')
 
     name = node.output_files[0].name
-    assert(name.endswith('.dax'))
+    assert name.endswith('.dax')
     map_file = node.output_files[1]
-    assert(map_file.name.endswith('.map'))
+    assert map_file.name.endswith('.map')
 
     node.add_opt('--workflow-name', name)
     node.add_opt('--output-dir', out_dir)
 
     workflow += node
 
     # Execute this in a sub-workflow
     fil = node.output_files[0]
     job = SubWorkflow(fil.name, is_planned=False)
     job.set_subworkflow_properties(map_file,
                                    staging_site=workflow.staging_site,
                                    cache_file=workflow.cache_file)
     job.add_into_workflow(workflow)
-    logging.info('Leaving minifollowups module')
-
 
-# =============================================================================
-# Main script
-# =============================================================================
-# Use the standard workflow command-line parsing routines.
-parser = argparse.ArgumentParser(description=__doc__[1:])
-parser.add_argument('--version', action='version', version=__version__)
-parser.add_argument("-v", "--verbose", default=False, action="store_true",
-                    help="Verbose output")
-_workflow.add_workflow_command_line_group(parser)
-_workflow.add_workflow_settings_cli(parser)
-args = parser.parse_args()
-
-init_logging(args.verbose, format="%(asctime)s: %(levelname)s: %(message)s")
-
-# Create the workflow objects
-logging.info("Generating %s workflow", args.workflow_name)
-container = _workflow.Workflow(args, name=args.workflow_name)
-wflow = _workflow.Workflow(args, args.workflow_name + '-main')
-finalize_wflow = _workflow.Workflow(args, args.workflow_name + '-finalization')
-
-logging.info("Post-processing output will be generated in %s", args.output_dir)
-if not os.path.exists(args.output_dir):
-    _workflow.makedir(args.output_dir)
-os.chdir(args.output_dir)
-args.output_dir = '.'
-
-# Setup results directory
-rdir = layout.SectionNumber('results', ['offsource_triggers_vs_time',
-                                        'signal_consistency',
-                                        'injections',
-                                        'loudest_offsource_events',
-                                        'exclusion_distances',
-                                        'open_box',
-                                        'workflow'])
-_workflow.makedir(rdir.base)
-_workflow.makedir(rdir['workflow'])
-
-# Input trigger file
-trig_file = wflow.cp.get('workflow', 'trig-file')
-# IFOs actually used: determined by data availability
-ifos = ppu.extract_ifos(trig_file)
-wflow.ifos = ifos
-
-plotting_nodes = []
-html_nodes = []
-
-# Logfile of this workflow
-wf_log_file = _workflow.File(wflow.ifos, 'workflow-log', wflow.analysis_time,
-                             extension='.txt', directory=rdir['workflow'])
-logfile = logging.FileHandler(filename=wf_log_file.storage_path, mode='w')
-logfile.setLevel(logging.INFO)
-formatter = logging.Formatter('%(asctime)s: %(levelname)s: %(message)s')
-logfile.setFormatter(formatter)
-logging.getLogger('').addHandler(logfile)
-logging.info("Created log file %s", wf_log_file.storage_path)
-
-
-# TODO: Pick up inifile, segments plot, GRB time and location, report IFO responses
-# Read the configuration file
-# typecast str from command line to File instances
-#cp = configuration.WorkflowConfigParser(opts.pp_config_file)
-
-out_dir = rdir.base
-_workflow.makedir(out_dir)
-files = _workflow.FileList([])
-
-#
-# Create information table about the GRB trigger and plot the search grid
-#
-info_table_node, grb_info_table = make_info_table(wflow, out_dir=out_dir)
-html_nodes.append(info_table_node)
-files.append(grb_info_table)
-#
-plot_node, skygrid_plot = make_pygrb_plot(wflow, 'pygrb_plot_skygrid', out_dir=out_dir)
-plotting_nodes.append(plot_node)
-files.append(skygrid_plot)
-summary_layout = [(grb_info_table[0],), (skygrid_plot[0],)]
-layout.two_column_layout(out_dir, summary_layout)
 
+def setup_pygrb_results_workflow(workflow, res_dir, trig_file,
+                                 inj_files, tags=None,
+                                 explicit_dependencies=None):
+    """Create subworkflow to produce plots, tables,
+    and results webpage for a PyGRB analysis.
 
-#
-# Plot SNR timeseries
-#
-out_dir = rdir['offsource_triggers_vs_time']
-_workflow.makedir(out_dir)
-
-# Coherent/Reweighted/Single IFO/Null SNR vs time
-out_dirs_dict = {'coherent' : 'offsource_triggers_vs_time/coh_snr_timeseries',
-                 'reweighted': 'offsource_triggers_vs_time/reweighted_snr_timeseries',
-                 'single': 'offsource_triggers_vs_time/single_ifo_snr_timeseries',
-                 'null': 'offsource_triggers_vs_time/null_snr_timeseries'}
-
-# Grab the name of the prefereed injection set for these plots
-tuning_inj_set = wflow.cp.get('workflow', 'tuning-inj-set')
-# Loop over timeseries request by the user
-timeseries = wflow.cp.get_subsections('pygrb_plot_snr_timeseries')
-for snr_type in timeseries:
-    out_dir = rdir[out_dirs_dict[snr_type]]
-    _workflow.makedir(out_dir)
-    files = _workflow.FileList([])
-    # Only single SNR timeseries requires looping over IFOs
-    ifos_to_loop = [None]
-    if snr_type == 'single':
-        ifos_to_loop = ifos
-    for ifo in ifos_to_loop:
-        timeseries_plots = _workflow.FileList([])
-        # Plots without and with injections
-        for inj_set in [None, tuning_inj_set]:
-            plot_node, output_files = \
-                make_pygrb_plot(wflow, 'pygrb_plot_snr_timeseries', out_dir,
-                                inj_set=inj_set, ifo=ifo, tags=[snr_type])
-            plotting_nodes.append(plot_node)
-            # We want a File, not a 1-element list with a File
-            # pycbc_pygrb_plot_snr_timeseries produces only one plot: take [0]
-            timeseries_plots.append(output_files[0])
-        files.append(timeseries_plots)
-    layout.two_column_layout(out_dir, files)
-
-
-#
-# Signal consistency plots
-#
-out_dir = rdir['signal_consistency']
-_workflow.makedir(out_dir)
-# Bank/auto/chisq veto vs Coherent SNR plots
-out_dir = rdir['signal_consistency/chi_squared_tests']
-_workflow.makedir(out_dir)
-files = _workflow.FileList([])
-# Loop over vetoes request by the user
-vetoes = wflow.cp.get_subsections('pygrb_plot_chisq_veto')
-for veto in vetoes:
-    chisq_veto_plots = _workflow.FileList([])
-    # Plots with and without injections
-    for inj_set in [tuning_inj_set, None]:
-        plot_node, output_files = \
-            make_pygrb_plot(wflow, 'pygrb_plot_chisq_veto', out_dir,
-                            inj_set=inj_set, tags=[veto])
-        plotting_nodes.append(plot_node)
-        # We want a File, not a 1-element list with a File
-        # pycbc_pygrb_plot_chisq_veto produces only one plot: take [0]
-        chisq_veto_plots.append(output_files[0])
-    files.append(chisq_veto_plots)
-layout.two_column_layout(out_dir, files)
-
-# Single detector chi-square plots: zoomed in and zoomed out
-out_dir = rdir['signal_consistency/individual_detectors']
-_workflow.makedir(out_dir)
-files = _workflow.FileList([])
-# Single IFO SNR vs Coherent SNR plots: zoomed in and zoomed out
-# Requires looping over IFOs
-if wflow.cp.has_section('pygrb_plot_coh_ifosnr'):
-    for ifo in ifos:
-        # Plots with and without injections
-        for inj_set in [tuning_inj_set, None]:
-            sngl_snr_plots = _workflow.FileList([])
-            for zoom_tag in [['zoomin'], None]:
-                # Single IFO SNR vs Coherent SNR
-                plot_node, output_files = \
-                    make_pygrb_plot(wflow, 'pygrb_plot_coh_ifosnr', out_dir,
-                                    inj_set=inj_set, ifo=ifo, tags=zoom_tag)
-                plotting_nodes.append(plot_node)
-                # We want a File, not a 1-element list with a File
-                # pycbc_pygrb_plot_cohifo_snr produces only one plot: take [0]
-                sngl_snr_plots.append(output_files[0])
-            files.append(sngl_snr_plots)
-    layout.two_column_layout(out_dir, files)
-else:
-    msg = 'No pygrb_plot_coh_ifosnr section found in the configuration file. '
-    msg += 'No coherent vs single detector SNR plots will be generated.'
-    logging.info(msg)
-
-# Null SNR/Overwhitened null stat vs Coherent SNR plots
-null_snr_out_dir = rdir['signal_consistency/null_snrs']
-_workflow.makedir(null_snr_out_dir)
-null_snr_files = _workflow.FileList([])
-# Coincident SNR vs Coherent SNR plots
-coinc_out_dir = rdir['signal_consistency/coincident_snr']
-_workflow.makedir(coinc_out_dir)
-coinc_files = _workflow.FileList([])
-# Loop over null statistics requested by the user (including coincident SNR)
-nstats = wflow.cp.get_subsections('pygrb_plot_null_stats')
-for nstat in nstats:
-    # Plots with and without injections, zoomed in and zoomed out
-    for inj_set in [tuning_inj_set, None]:
-        if nstat == 'coincident':
-            out_dir = coinc_out_dir
-            files = coinc_files
-        else:
-            out_dir = null_snr_out_dir
-            files = null_snr_files
-        null_stats_plots = _workflow.FileList([])
-        for zoom_tag in [['zoomin'], []]:
-            plot_node, output_files = \
-            make_pygrb_plot(wflow, 'pygrb_plot_null_stats', out_dir,
-                            inj_set=inj_set, tags=[nstat]+zoom_tag)
-            plotting_nodes.append(plot_node)
-            # We want a File, not a 1-element list with a File
-            # pycbc_pygrb_plot_null_stats produces only one plot: take [0]
-            null_stats_plots.append(output_files[0])
-        files.append(null_stats_plots)
-layout.two_column_layout(null_snr_out_dir, null_snr_files)
-layout.two_column_layout(coinc_out_dir, coinc_files)
+    Parameters
+    ----------
+    workflow: pycbc.workflow.Workflow
+        The core workflow instance we are populating
+    res_dir: The post-processing directory where
+        results (plots, etc.) will be stored
+    trig_file: The triggers File object
+    inj_files: FileList of injection results
+    tags: {None, optional}
+        Tags to add to the executables
+    excplicit_dependencies: nodes that must precede this
+    """
 
-#layout.group_layout(rdir['coincident_triggers'],
-#                    closed_box_ifars + all_snrifar + [bank_plot[0][0]])
+    tags = [] if tags is None else tags
+    dax_output = res_dir+'/webpage_daxes'
+    # _workflow.makedir(dax_output)
+    makedir(dax_output)
 
-#
-# Found/missed injections plots and tables
-#
-out_dir = rdir['injections']
-_workflow.makedir(out_dir)
-# Loop over injection plots requested by the user
-inj_sets = wflow.cp.get_subsections('injections')
-inj_plots = wflow.cp.get_subsections('pygrb_plot_injs_results')
-# The command above also picks up the injection set names so we remove them
-# from the set of requested injection plot types
-inj_plots = [inj_plot for inj_plot in inj_plots if inj_plot not in inj_sets]
-for inj_set in inj_sets:
-    out_dir = rdir['injections/'+inj_set]
-    _workflow.makedir(out_dir)
-    files = _workflow.FileList([])
-    # Generate plots: loop over inj_plots and found-missed/missed-found
-    for inj_plot in inj_plots:
-        y_qty, x_qty = inj_plot.split('_')
-        ifos_to_loop = [None]
-        if y_qty == 'effsitedist':
-            ifos_to_loop = ifos
-        for ifo in ifos_to_loop:
-            for fm_or_mf in ['found-missed', 'missed-found']:
-                plot_node, output_files = make_pygrb_plot(wflow, 'pygrb_plot_injs_results',
-                                                          out_dir, ifo=ifo, inj_set=inj_set,
-                                                          tags=[y_qty, x_qty, fm_or_mf])
-                plotting_nodes.append(plot_node)
-                # We want a File, not a 1-element list with a File
-                # pycbc_pygrb_plot_injs_results produces only one plot: take [0]
-                files.append(output_files[0])
-    # Generate quiet-found and missed-found html tables
-    # TODO:  pass lont/lofft arguments if inj_set=None
-    html_node, [mf_table, qf_table, qf_h5] =\
-         make_pygrb_injs_tables(wflow, out_dir, inj_set=inj_set)
-    html_nodes.append(html_node)
-
-    inj_layout = list(layout.grouper(files, 2)) + [(mf_table,), (qf_table,)]
-    layout.two_column_layout(out_dir, inj_layout)
-
-    # Follow up of loudest N quiet-found injections:
-    # qf_h5 includes quiet and missed found injections
-    out_dir = rdir['injections/'+inj_set+'loudest_quiet_found_followups']
-    setup_pygrb_minifollowups(wflow, qf_h5,
-                              'daxes', out_dir,
-                              tags=[inj_set, 'loudest_quiet_found_injs'])
+    # Turn the config file into a File instance
+    # config_path = os.path.abspath(dax_output + '/' + \
+    #                               '_'.join(tags) + 'webpage.ini')
+    # workflow.cp.write(open(config_path, 'w'))
+    # config_file = resolve_url_to_file(config_path)
 
+    # Create the node
+    exe = Executable(workflow.cp, 'pygrb_pp_workflow',
+                     ifos=workflow.ifos, out_dir=dax_output,
+                     tags=tags)
+    node = exe.create_node()
+    # Grab and pass all necessary files
+    node.add_input_opt('--trig-file', trig_file)
+    if workflow.cp.has_option('workflow', 'veto-files'):
+        veto_files = build_veto_filelist(workflow)
+        node.add_input_list_opt('--veto-files', veto_files)
+    # node.add_input_opt('--config-files', config_file)
+    node.add_input_list_opt('--inj-files', inj_files)
 
-#
-# FAP distributions
-#
-out_dir = rdir['loudest_offsource_events']
-_workflow.makedir(out_dir)
-#files = _workflow.FileList([])
-files = []
-# Loop over statistics requested by the user
-stats = wflow.cp.get_subsections('pygrb_plot_stats_distribution')
-for stat in stats:
-    plot_node, output_file = \
-        make_pygrb_plot(wflow, 'pygrb_plot_stats_distribution', out_dir,
-                        inj_set=tuning_inj_set, tags=[stat])
-    plotting_nodes.append(plot_node)
-    # We want a File, not a 1-element list with a File
-    # pycbc_pygrb_plot_stats_distribution produces only one plot: take [0]
-    files.append(output_file[0])
-##layout.single_layout(out_dir, files)
-##layout.single_layout(out_dir, (files))
-#layout.group_layout(out_dir, files)
-
-html_node, [lont_table, lont_h5, lofft_table, lofft_h5] =\
-     make_pygrb_injs_tables(wflow, out_dir, inj_set=None)
-html_nodes.append(html_node)
-
-lofft_layout = list(layout.grouper(files, 2)) + [(lofft_table,)]
-layout.two_column_layout(out_dir, lofft_layout)
-
-# Follow up N loudest offsource triggers (parent-child)
-out_dir = rdir['loudest_offsource_events/followups']
-setup_pygrb_minifollowups(wflow, lofft_h5,
-                          'daxes', out_dir,
-                          tags=['loudest_offsource_events'])
+    if tags:
+        node.add_list_opt('--tags', tags)
 
+    node.new_output_file_opt(workflow.analysis_time, '.dax',
+                             '--dax-file', tags=tags)
+    node.new_output_file_opt(workflow.analysis_time, '.map',
+                             '--output-map', tags=tags)
+    # + ['MAP'], use_tmp_subdirs=True)
+    name = node.output_files[0].name
+    assert name.endswith('.dax')
+    map_file = node.output_files[1]
+    assert map_file.name.endswith('.map')
+    node.add_opt('--workflow-name', name)
+    # This is the output dir for the products of this node, namely dax and map
+    node.add_opt('--output-dir', res_dir)
+    node.add_opt('--dax-file-directory', '.')
 
-#
-# Exclusion distance and efficiency plots based on offtrials
-#
-out_dir = rdir['exclusion_distances']
-_workflow.makedir(out_dir)
+    # Turn the config file into a File instance
+    config_path = os.path.abspath(dax_output + '/' + \
+                                  '_'.join(tags) + 'webpage.ini')
+    workflow.cp.write(open(config_path, 'w'))
+    config_file = resolve_url_to_file(config_path)
+    node.add_input_opt('--config-files', config_file)
 
-# Offtrials and injection sets request by the user
-num_trials = int(wflow.cp.get('trig_combiner', 'num-trials'))
-offtrials = ["offtrial_%s" % (i+1) for i in range(num_trials)]
-inj_sets = wflow.cp.get_subsections('injections')
-for offtrial in offtrials:
-    out_dir = rdir['exclusion_distances/'+offtrial]
-    _workflow.makedir(out_dir)
-    files = _workflow.FileList([])
-    for inj_set in inj_sets:
-        plot_node, output_files = \
-            make_pygrb_plot(wflow, 'pygrb_efficiency', out_dir,
-                            inj_set=inj_set, tags=[offtrial])
-        plotting_nodes.append(plot_node)
-        files.append(output_files)
-    layout.two_column_layout(out_dir, files)
+    # Track additional ini file produced by pycbc_pygrb_pp_workflow
+    out_file = File(workflow.ifos, 'pygrb_pp_workflow', workflow.analysis_time,
+                    file_url=os.path.join(dax_output, name+'.ini'))
+    node.add_output(out_file)
 
+    # Add node to the workflow workflow
+    workflow += node
+    if explicit_dependencies is not None:
+        for dep in explicit_dependencies:
+            workflow.add_explicit_dependancy(dep, node)
 
-#
-# Trigger Followups (TODO)
-#
+    # Execute this in a sub-workflow
+    job = SubWorkflow(name, is_planned=False)  # , _id='results')
+    job.set_subworkflow_properties(map_file,
+                                   staging_site=workflow.staging_site,
+                                   cache_file=workflow.cache_file)
+    job.add_into_workflow(workflow)
 
-# Make room for throughput histograms (TODO)
-base = rdir['workflow/throughput']
-_workflow.makedir(base)
-
-# Save global config file
-base = rdir['workflow/configuration']
-_workflow.makedir(base)
-ini_file_path = os.path.join(base, 'pygrb_offline_pp.ini')
-with open(ini_file_path, 'w') as ini_fh:
-    wflow.cp.write(ini_fh)
-ini_file = _workflow.FileList([_workflow.File(wflow.ifos, '',
-                                              wflow.analysis_time,
-                                              file_url='file://' + ini_file_path)])
-layout.single_layout(base, ini_file)
-
-# Create versioning information
-create_versioning_page(rdir['workflow/version'], wflow.cp)
-
-# Create the final log file
-log_file_html = _workflow.File(wflow.ifos, 'WORKFLOW-LOG', wflow.analysis_time,
-                               extension='.html', directory=rdir['workflow'])
-
-# Create a page to contain a dashboard link
-dashboard_file = _workflow.File(wflow.ifos, 'DASHBOARD', wflow.analysis_time,
-                                extension='.html', directory=rdir['workflow'])
-dashboard_str = """<center><p style="font-size:20px"><b><a href="PEGASUS_DASHBOARD_URL" target="_blank">Pegasus Dashboard Page</a></b></p></center>"""
-kwds = {'title' : 'Pegasus Dashboard',
-        'caption' : "Link to Pegasus Dashboard",
-        'cmd' : "PYCBC_SUBMIT_DAX_ARGV", }
-save_fig_with_metadata(dashboard_str, dashboard_file.storage_path, **kwds)
-
-# Create pages for the submission script to write data
-_workflow.makedir(rdir['workflow/dax'])
-_workflow.makedir(rdir['workflow/input_map'])
-_workflow.makedir(rdir['workflow/output_map'])
-_workflow.makedir(rdir['workflow/planning'])
-
-logging.info("Path for make_results_web_page: %s", os.path.join(os.getcwd(), rdir.base))
-_workflow.make_results_web_page(finalize_wflow, os.path.join(os.getcwd(), rdir.base),
-                                template='red')
-
-container += wflow
-container += finalize_wflow
-
-container.add_subworkflow_dependancy(wflow, finalize_wflow)
-
-container.save()
-logging.info("Dax written.")
-
-# Protect the open box results folder
-out_dir = rdir['open_box']
-_workflow.makedir(out_dir)
-os.chmod(out_dir, 0o0700)
-# TODO: follow up loudest offsource trigger
-#out_dir = rdir['open_box/loudest_event/followup']
-#setup_pygrb_minifollowups(wflow, lont_h5, #wflow.ifos,
-#                          'daxes', out_dir,
-#                          tags=['loudest_onsource_event'])
-
-# Close the log and flush to the html file
-logging.shutdown()
-with open(wf_log_file.storage_path, "r") as logfile:
-    logdata = logfile.read()
-log_str = """
-<p>Workflow generation script created workflow in output directory: %s</p>
-<p>Workflow name is: %s</p>
-<p>Workflow generation script run on host: %s</p>
-<pre>%s</pre>
-""" % (os.getcwd(), args.workflow_name, socket.gethostname(), logdata)
-kwds = {'title' : 'Workflow Generation Log',
-        'caption' : "Log of the workflow script %s" % sys.argv[0],
-        'cmd' :' '.join(sys.argv), }
-save_fig_with_metadata(log_str, log_file_html.storage_path, **kwds)
-layout.single_layout(rdir['workflow'], ([dashboard_file, log_file_html]))
+    return node.output_files
```

### Comparing `PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_combine_injection_comparisons` & `PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_combine_injection_comparisons`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_injection_set_comparison` & `PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_injection_set_comparison`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_plot_injections_found_both_workflows` & `PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_plot_injections_found_both_workflows`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_plot_injections_missed_one_workflow` & `PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_plot_injections_missed_one_workflow`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflow_comparisons/offline_search/pycbc_plot_vt_ratio_vs_ifar` & `PyCBC-2.2.1/bin/workflow_comparisons/offline_search/pycbc_plot_vt_ratio_vs_ifar`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_bank_verifier_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_bank_verifier_workflow`

 * *Files 1% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 import argparse
 import shutil
 
 from ligo import segments
 
 import pycbc.version
 import pycbc.workflow as wf
-from pycbc.results import (create_versioning_page, static_table, layout)
+from pycbc.results import (static_table, layout)
 from pycbc.workflow.jobsetup import (select_generic_executable,
                                      int_gps_time_to_str,
                                      PycbcCreateInjectionsExecutable,
                                      LalappsInspinjExecutable)
 from pycbc.workflow import setup_splittable_dax_generated
 
 # Boiler-plate stuff
@@ -378,13 +378,18 @@
     layout.two_column_layout(rdir['point_injection_sets/{}'.format(tag)],
                              curr_outs)
 
 # Save config file(s) to results directory
 layout.single_layout(conf_dir, conf_file)
 
 # Create versioning information
-create_versioning_page(rdir['workflow/version'], workflow.cp)
+wf.make_versioning_page(
+    workflow,
+    workflow.cp,
+    rdir['workflow/version'],
+)
+
 
 wf.make_results_web_page(workflow, os.path.join(os.getcwd(), rdir.base),
                          explicit_dependencies=plotting_nodes)
 
 workflow.save()
```

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_coinc_search_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_offline_search_workflow`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 
-# Copyright (C) 2013-2019 Ian W. Harry, Alex Nitz, Marton Tapai,
-#     Gareth Davies
+# Copyright (C) 2013-2023, Ian W. Harry, Alex Nitz, Marton Tapai,
+#     Gareth Cabourn Davies
 #
 # This program is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License as published by the
 # Free Software Foundation; either version 3 of the License, or (at your
 # option) any later version.
 #
 # This program is distributed in the hope that it will be useful, but
@@ -13,33 +13,32 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
 # Public License for more details.
 #
 # You should have received a copy of the GNU General Public License along
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 """
-Program for running multi-detector workflow analysis through coincidence and
-then generate post-processing and plots.
+Program for running offline analysis through event
+finding and ranking then generate post-processing
+and plots.
 """
 import pycbc
 import pycbc.version
-__author__  = "Alex Nitz <alex.nitz@ligo.org>"
 __version__ = pycbc.version.git_verbose_msg
 __date__    = pycbc.version.date
 __program__ = "pycbc_offline"
 
 import sys
 import socket
 import pycbc.events, pycbc.workflow as wf
 import os, argparse, logging
 import configparser as ConfigParser
 from ligo import segments
 import numpy, lal, datetime, itertools
-from pycbc.results import create_versioning_page, static_table, layout
-from pycbc.results.versioning import save_fig_with_metadata
+from pycbc.results import static_table, layout, save_fig_with_metadata
 from pycbc.results.metadata import html_escape
 
 
 def symlink_path(f, path):
     if f is None:
         return
     try:
@@ -65,28 +64,26 @@
 
 parser = argparse.ArgumentParser(description=__doc__[1:])
 parser.add_argument('--version', action='version', version=__version__)
 wf.add_workflow_command_line_group(parser)
 wf.add_workflow_settings_cli(parser)
 args = parser.parse_args()
 
-# FIXME: opts.tags is currently unused here.
+container = wf.Workflow(args, args.workflow_name)
+workflow = wf.Workflow(args, args.workflow_name + '-main')
+finalize_workflow = wf.Workflow(args, args.workflow_name + '-finalization')
 
 wf.makedir(args.output_dir)
-
-container = wf.Workflow(args, name=args.workflow_name)
-workflow = wf.Workflow(args, name=args.workflow_name + '-main')
-finalize_workflow = wf.Workflow(args, name=args.workflow_name + '-finalization')
 os.chdir(args.output_dir)
 
 rdir = layout.SectionNumber('results', ['analysis_time',
                                  'detector_sensitivity',
                                  'data_quality',
                                  'single_triggers',
-                                 'coincident_triggers',
+                                 'background_triggers',
                                  'injections',
                                  'search_sensitivity',
                                  'open_box_result',
                                  'workflow',
                                  ])
 
 wf.makedir(rdir.base)
@@ -135,14 +132,15 @@
 ssegs = {}
 for ifo in workflow.ifos:
     ssegs[ifo] = science_seg_file.segment_dict["%s:science" % ifo]
 
 hoft_tags=[]
 if 'hoft' in workflow.cp.get_subsections('workflow-datafind'):
     hoft_tags=['hoft']
+
 datafind_files, analyzable_file, analyzable_segs, analyzable_name = \
                                            wf.setup_datafind_workflow(workflow,
                                      ssegs, "datafind",
                                      seg_file=science_seg_file, tags=hoft_tags)
 
 final_veto_name = 'vetoes'
 final_veto_file = wf.get_segments_file(workflow, final_veto_name,
@@ -166,18 +164,17 @@
                                                   out_dir="bank",
                                                   tags=['full_data'])
 
 bank_tags = []
 if 'mass1_mass2' in workflow.cp.get_subsections('plot_bank'):
     bank_tags=['mass1_mass2']
 bank_plot = wf.make_template_plot(workflow, hdfbank,
-                                  rdir['coincident_triggers'],
+                                  rdir['background_triggers'],
                                   tags=bank_tags)
 
-
 ######################## Setup the FULL DATA run ##############################
 output_dir = "full_data"
 
 # setup the matchedfilter jobs
 ind_insps = insps = wf.setup_matchedfltr_workflow(workflow, analyzable_segs,
                                    datafind_files, splitbank_files_fd,
                                    output_dir, tags=['full_data'])
@@ -198,14 +195,15 @@
                                        analyzable_segs, analyzable_file,
                                        dq_segment_file,
                                        output_dir='dq',
                                        tags=['full_data'])
     statfiles += dq_label_files
     dqfiles += dq_label_files
     dqfile_labels += len(dq_label_files) * [dq_label]
+
 statfiles += wf.setup_trigger_fitting(workflow, insps, hdfbank,
                                       final_veto_file, final_veto_name,
                                       output_dir=output_dir,
                                       tags=['full_data'])
 
 # Set up the multi-ifo coinc jobs
 # final_bg_files contains coinc results using vetoes final_veto_files
@@ -215,17 +213,15 @@
 ifo_ids = {}
 
 # Get the ifo precedence values
 ifo_precedence_list = workflow.cp.get_opt_tags('workflow-coincidence', 'timeslide-precedence', ['full_data'])
 for ifo, _ in zip(*insps.categorize_by_attr('ifo')):
     ifo_ids[ifo] = ifo_precedence_list.index(ifo)
 
-# Generate the possible detector combinations from 2 detectors
-# up to the number of trigger files
-
+# Generate the possible detector combinations
 if workflow.cp.has_option_tags('workflow-data_quality', 'no-coinc-veto',
                                 tags=None):
     logging.info("no-coinc-veto option enabled, " +
                  "no longer passing veto segments to coinc jobs.")
     coinc_veto_file = None
 else:
     coinc_veto_file = final_veto_file
@@ -246,34 +242,58 @@
     # Optionally perform follow-up on triggers and rerank the candidates
     # Returns the input file if not enabled.
     no_fg_exc_files[ordered_ifo_list] = wf.rerank_coinc_followup(
                    workflow, bg_file, hdfbank, output_dir,
                    tags=ctagcomb)
 
 
-if len(insps) == 2:
-    final_bg_files = no_fg_exc_files
-else:
-    final_bg_files = {}
-    for ifocomb in ifo_combos(ifo_ids.keys()):
+# Are we analysing single-detector candidates?
+analyze_singles = workflow.cp.has_section('workflow-singles') \
+        and workflow.cp.has_option_tags('workflow-singles',
+                                        'analyze', tags=None)
+
+# The single-detector findtrigs and statmap jobs work differently
+# - set these up here
+
+for ifo in ifo_ids.keys():
+    if not analyze_singles:
+        continue
+    inspcomb = wf.select_files_by_ifo_combination([ifo], insps)
+    # Create coinc tag, and set up the findtrigs job for the combination
+    ctagsngl = ['full_data', '1det']
+    no_fg_exc_files[ifo] = wf.setup_sngls(
+        workflow, hdfbank, inspcomb, statfiles, final_veto_file,
+        final_veto_name, output_dir, tags=ctagsngl)
+
+ifo_sets = list(ifo_combos(ifo_ids.keys()))
+if analyze_singles:
+    ifo_sets += [(ifo,) for ifo in ifo_ids.keys()]
+
+final_bg_files = {}
+# set up exclude-zerolag jobs for each ifo combination
+for ifocomb in ifo_sets:
+    if len(ifocomb) > 1:
         _, _, ordered_ifo_list = wf.get_ordered_ifo_list(ifocomb, ifo_ids)
         # Create coinc tag
         coinctag = '{}det'.format(len(ifocomb))
-        ctagcomb = ['full_data', coinctag]
-        other_ifo_keys = list(no_fg_exc_files.keys())
-        other_ifo_keys.remove(ordered_ifo_list)
-        other_bg_files = {ctype: no_fg_exc_files[ctype]
-                          for ctype in other_ifo_keys}
-        final_bg_files[ordered_ifo_list] = wf.setup_exclude_zerolag(
-            workflow,
-            no_fg_exc_files[ordered_ifo_list],
-            wf.FileList(other_bg_files.values()),
-            output_dir, ordered_ifo_list,
-            tags=ctagcomb
-        )
+    else:
+        ordered_ifo_list = ifocomb[0]
+        coinctag= '1det'
+    other_ifo_keys = list(no_fg_exc_files.keys())
+    other_ifo_keys.remove(ordered_ifo_list)
+    ctagcomb = ['full_data', coinctag]
+    other_bg_files = {ctype: no_fg_exc_files[ctype]
+                      for ctype in other_ifo_keys}
+    final_bg_files[ordered_ifo_list] = wf.setup_exclude_zerolag(
+        workflow,
+        no_fg_exc_files[ordered_ifo_list],
+        wf.FileList(other_bg_files.values()),
+        output_dir, ordered_ifo_list,
+        tags=ctagcomb
+    )
 
 combined_bg_file = wf.setup_combine_statmap(
                                 workflow,
                                 wf.FileList(final_bg_files.values()),
                                 wf.FileList([]),
                                 output_dir,
                                 tags=['full_data'])
@@ -429,66 +449,45 @@
              tags=insp_file.tags + [subsec])
 
 ##################### COINC FULL_DATA plots ###################################
 
 # Main results with combined file (we mix open and closed box here, but
 # separate them in the result page)
 
-# FIXME: COMMENTED OUT JOBS ARE FAILING ... NEED FIXING!!
-#        (Currently that's most of the jobs :-( )
-#snrifar = wf.make_snrifar_plot(workflow, combined_bg_file,
-#                               rdir['open_box_result'],
-#                               tags=combined_bg_file.tags)
-#snrifar_cb = wf.make_snrifar_plot(workflow, combined_bg_file,
-#                                  rdir['coincident_triggers'], closed_box=True,
-#                                  tags=combined_bg_file.tags + ['closed'])
-#ratehist = wf.make_snrratehist_plot(workflow, combined_bg_file,
-#                                    rdir['open_box_result'],
-#                                    tags=combined_bg_file.tags)
-#snrifar_ifar = wf.make_snrifar_plot(workflow, combined_bg_file,
-#                                    rdir['open_box_result/significance'],
-#                                    cumulative=False,
-#                                    tags=combined_bg_file.tags + ['ifar'])
 ifar_ob = wf.make_ifar_plot(workflow, combined_bg_file,
                                     rdir['open_box_result'],
                                     tags=combined_bg_file.tags + ['open_box'],
                                     executable='page_ifar_catalog')
-#ifar_cb = wf.make_ifar_plot(workflow, combined_bg_file,
-#                                    rdir['coincident_triggers'],
-#                                    tags=combined_bg_file.tags + ['closed_box'],
-#                                    executable='page_ifar_catalog')
 table = wf.make_foreground_table(workflow, combined_bg_file,
                                  hdfbank, rdir['open_box_result'],
                                  singles=insps, extension='.html',
                                  tags=combined_bg_file.tags)
 
 fore_xmlall = wf.make_foreground_table(workflow, combined_bg_file,
                     hdfbank, rdir['open_box_result'], singles=insps,
                     extension='.xml', tags=["xmlall"])
 fore_xmlloudest = wf.make_foreground_table(workflow, combined_bg_file,
                     hdfbank, rdir['open_box_result'], singles=insps,
                     extension='.xml', tags=["xmlloudest"])
 
-#symlink_result(snrifar, 'open_box_result/significance')
-#symlink_result(ratehist, 'open_box_result/significance')
 symlink_result(table, 'open_box_result/significance')
 
 # Set html pages
 main_page = [(ifar_ob,), (table, )]
 layout.two_column_layout(rdir['open_box_result'], main_page)
 
 #detailed_page = [(snrifar, ratehist), (snrifar_ifar, ifar_ob), (table,)]
 #layout.two_column_layout(rdir['open_box_result/significance'], detailed_page)
 
 closed_page = [(bank_plot,)]
-layout.two_column_layout(rdir['coincident_triggers'], closed_page)
+layout.two_column_layout(rdir['background_triggers'], closed_page)
 
 # run minifollowups on the output of the loudest events
 mfup_dir_fg = rdir['open_box_result/loudest_events_followup']
-mfup_dir_bg = rdir['coincident_triggers/loudest_background_followup']
+mfup_dir_bg = rdir['background_triggers/loudest_background_followup']
 wf.setup_foreground_minifollowups(workflow, combined_bg_file,
                                   full_insps, hdfbank, insp_files_seg_file,
                                   data_analysed_name, trig_generated_name,
                                   'daxes', mfup_dir_fg,
                                   tags=combined_bg_file.tags + ['foreground'])
 
 wf.setup_foreground_minifollowups(workflow, combined_bg_file,
@@ -497,40 +496,44 @@
                                   'daxes', mfup_dir_bg,
                                   tags=combined_bg_file.tags + ['background'])
 
 # Sub-pages for each ifo combination
 
 snrifar_summ = []
 for key in final_bg_files:
-    # FIXME: Stop obfuscating this file!
     bg_file = final_bg_files[key]
-    open_dir = rdir['open_box_result/{}_coincidences'.format(key)]
-    closed_dir = rdir['coincident_triggers/{}_coincidences'.format(key)]
+    open_dir = rdir['open_box_result/{}_candidates'.format(key)]
+    closed_dir = rdir['background_triggers/{}_background'.format(key)]
     snrifar = wf.make_snrifar_plot(workflow, bg_file, open_dir,
                                    tags=bg_file.tags)
     snrifar_cb = wf.make_snrifar_plot(workflow, bg_file, closed_dir,
                                       closed_box=True,
                                       tags=bg_file.tags + ['closed'])
     ratehist = wf.make_snrratehist_plot(workflow, bg_file, open_dir,
                                         tags=bg_file.tags)
     snrifar_ifar = wf.make_snrifar_plot(workflow, bg_file, open_dir,
                                         cumulative=False,
                                         tags=bg_file.tags + ['ifar'])
     ifar_ob = wf.make_ifar_plot(workflow, bg_file, open_dir,
                                 tags=bg_file.tags + ['open_box'])
-    ifar_cb = wf.make_ifar_plot(workflow, bg_file, closed_dir,
-                                tags=bg_file.tags + ['closed_box'])
+    if len(key) > 2:
+        # don't do the background plot for single-detector stuff,
+        # as it is just blank
+        ifar_cb = wf.make_ifar_plot(workflow, bg_file, closed_dir,
+                                    tags=bg_file.tags + ['closed_box'])
+        closed_page = [(snrifar_cb, ifar_cb)]
+    else:
+        closed_page = [(snrifar_cb,)]
     table = wf.make_foreground_table(workflow, bg_file, hdfbank, open_dir,
                                      singles=insps, extension='.html',
                                      tags=bg_file.tags)
 
     detailed_page = [(snrifar, ratehist), (snrifar_ifar, ifar_ob), (table,)]
     layout.two_column_layout(open_dir, detailed_page)
 
-    closed_page = [(snrifar_cb, ifar_cb)]
     snrifar_summ += closed_page
     layout.two_column_layout(closed_dir, closed_page)
 
 #################### Plotting of data quality results #########################
 
 # DQ log likelihood plots
 for dqf,dql in zip(dqfiles,dqfile_labels):
@@ -556,14 +559,15 @@
 splitbank_files_inj = wf.setup_splittable_workflow(workflow, [hdfbank],
                                                    out_dir="bank",
                                                    tags=['injections'])
 
 # setup the injection files
 inj_files_base, inj_tags = wf.setup_injection_workflow(workflow,
                                                   output_dir="inj_files")
+
 inj_files = []
 for inj_file, tag in zip(inj_files_base, inj_tags):
     inj_files.append(wf.inj_to_hdf(workflow, inj_file, 'inj_files', [tag]))
 
 inj_coincs = wf.FileList()
 
 found_inj_dict ={}
@@ -601,15 +605,14 @@
 
         # Create coinc tag, and set up the coinc job for the combination
         coinctag = '{}det'.format(len(ifocomb))
         ctagcomb = [tag, 'injections', coinctag]
         curr_out = wf.setup_interval_coinc_inj(
             workflow,
             hdfbank,
-            full_insps,
             inspcomb,
             statfiles,
             final_bg_files[ordered_ifo_list],
             coinc_veto_file,
             final_veto_name,
             output_dir,
             pivot_ifo,
@@ -626,16 +629,36 @@
             tags=ctagcomb,
             injection_file=inj_file,
             ranking_file=final_bg_files[ordered_ifo_list]
         )
 
         inj_coinc[ordered_ifo_list] = curr_out
 
+    # get sngls for injections
+    for ifo in ifo_ids.keys():
+        if not analyze_singles:
+            continue
+        inspcomb = wf.select_files_by_ifo_combination([ifo], insps)
+        # Create sngls tag, and set up the findtrigs job for the combination
+        ctagsngl = [tag, 'injections', '1det']
+        inj_coinc[ifo] = wf.setup_sngls_inj(
+            workflow,
+            hdfbank,
+            inspcomb,
+            statfiles,
+            final_bg_files[ifo],
+            final_veto_file,
+            final_veto_name,
+            output_dir,
+            tags=ctagsngl
+        )
+
     combctags = [tag, 'injections']
     final_inj_bg_file_list = wf.FileList(inj_coinc.values())
+
     combined_inj_bg_file = wf.setup_combine_statmap(
         workflow,
         final_inj_bg_file_list,
         wf.FileList(final_bg_files.values()),
         output_dir,
         tags=combctags
     )
@@ -722,15 +745,14 @@
 
     # If option given, make throughput plots
     if workflow.cp.has_option_tags('workflow-matchedfilter',
                                    'plot-throughput', tags=[tag]):
         wf.make_throughput_plot(workflow, insps, rdir['workflow/throughput'],
                                 tags=[tag])
 
-
 ######################## Make combined injection plots ##########################
 if len(files_for_combined_injfind) > 0:
     sen_all = wf.make_sensitivity_plot(workflow, found_inj_comb,
                                        rdir['search_sensitivity'],
                                        require='all')
     layout.group_layout(rdir['search_sensitivity'], sen_all)
     inj_all = wf.make_foundmissed_plot(workflow, found_inj_comb,
@@ -743,22 +765,20 @@
                                      rdir['search_sensitivity'],
                                      require='summ')
     inj_s = wf.make_foundmissed_plot(workflow, found_inj_comb,
                                      rdir['injections'],
                                      require='summ')
     inj_summ = list(layout.grouper(inj_s + sen_s, 2))
 
-
 # Make analysis time summary
 analysis_time_summ = [time_file, seg_summ_plot]
 for f in analysis_time_summ:
     symlink_result(f, 'analysis_time')
 layout.single_layout(rdir['analysis_time'], (analysis_time_summ))
 
-
 ########################## Make full summary ####################################
 if len(files_for_combined_injfind) > 0:
     summ = ([(time_file,)] + [(seg_summ_plot,)] +
             [(seg_summ_table, veto_summ_table)] + det_summ + hist_summ +
             [(bank_plot,)] + inj_summ + snrifar_summ)
 
 else:
@@ -778,18 +798,20 @@
 ini_file_path = os.path.join(base, 'configuration.ini')
 with open(ini_file_path, 'w') as ini_fh:
     container.cp.write(ini_fh)
 ini_file = wf.FileList([wf.File(workflow.ifos, '', workflow.analysis_time,
                         file_url='file://' + ini_file_path)])
 layout.single_layout(base, ini_file)
 
-
 # Create versioning information
-create_versioning_page(rdir['workflow/version'], container.cp)
-
+wf.make_versioning_page(
+    workflow,
+    container.cp,
+    rdir['workflow/version'],
+)
 
 ############################ Finalization ####################################
 
 # Create the final log file
 log_file_html = wf.File(workflow.ifos, 'WORKFLOW-LOG', workflow.analysis_time,
                         extension='.html', directory=rdir['workflow'])
```

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_faithsim_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_faithsim_workflow`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_inference_inj_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_inference_inj_workflow`

 * *Files 1% similar despite different names*

```diff
@@ -351,16 +351,20 @@
         inj_recovery_plots.append(injrec_plot)
     # add to the results page
     pp_files = [(pp_table,)] + list(zip(pp_plots, inj_recovery_plots))
     layout.two_column_layout(rdir[pp_section], pp_files)
     # add to the main workflow
     workflow += pp_workflow
 
-# create versioning HTML pages
-results.create_versioning_page(rdir["workflow/version"], workflow.cp)
+# Create versioning information
+wf.make_versioning_page(
+    workflow,
+    workflow.cp,
+    rdir['workflow/version'],
+)
 
 # create node for making HTML pages
 plotting.make_results_web_page(finalize_workflow,
     os.path.join(os.getcwd(), rdir.base))
 
 # add finalize workflow to workflow and make it depend on the others
 workflow += finalize_workflow
```

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_inference_plots_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_inference_plots_workflow`

 * *Files 2% similar despite different names*

```diff
@@ -242,17 +242,21 @@
                              unique=str(num_event).zfill(zpad),
                              title=label, collapse=True)
 
     # build the psd page
     layout.single_layout(rdir['detector_sensitivity'], [psd_plot],
                          unique=str(num_event).zfill(zpad),
                          title=label, collapse=True)
-
-# create versioning HTML pages
-results.create_versioning_page(rdir["workflow/version"], container.cp)
+ 
+# Create versioning information
+wf.make_versioning_page(
+    workflow,
+    container.cp,
+    rdir['workflow/version'],
+)
 
 # create node for making HTML pages
 plotting.make_results_web_page(finalize_workflow,
     os.path.join(os.getcwd(), rdir.base))
 
 # add sub-workflows to workflow
 container += workflow
```

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_inference_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_inference_workflow`

 * *Files 0% similar despite different names*

```diff
@@ -33,14 +33,15 @@
 from pycbc.results import layout
 from pycbc.types import MultiDetOptionAction
 from pycbc.types import MultiDetOptionAppendAction
 from pycbc.workflow import configuration
 from pycbc.workflow import core
 from pycbc.workflow import datafind
 from pycbc.workflow import plotting
+from pycbc.workflow import versioning
 from pycbc import __version__
 import pycbc.workflow.inference_followups as inffu
 from pycbc.workflow.jobsetup import PycbcInferenceExecutable
 
 
 def read_events_from_config(cp):
     """Gets events to load from a config file.
@@ -303,16 +304,20 @@
                          unique=str(num_event).zfill(zpad),
                          title=label, collapse=True)
 
     # add the sub workflow to the main workflow
     workflow += sub_workflow
 
 
-# create versioning HTML pages
-results.create_versioning_page(rdir["workflow/version"], container.cp)
+# Create versioning information
+versioning.make_versioning_page(
+    workflow,
+    container.cp,
+    rdir['workflow/version'],
+)
 
 # create node for making HTML pages
 plotting.make_results_web_page(finalize_workflow,
     os.path.join(os.getcwd(), rdir.base))
 
 # add sub-workflows to workflow
 container += workflow
```

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_psd_estimation_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_psd_estimation_workflow`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_sbank_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_sbank_workflow`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/bin/workflows/pycbc_make_uberbank_workflow` & `PyCBC-2.2.1/bin/workflows/pycbc_make_uberbank_workflow`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/banksim/injection0.xml` & `PyCBC-2.2.1/examples/banksim/injection0.xml`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/pycbc_check_esd_saturation.sh` & `PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/pycbc_check_esd_saturation.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/pycbc_check_pcal_saturation.sh` & `PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/pycbc_check_pcal_saturation.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/cal/foton_filter_esd_saturation/run_pcal_saturation_example.sh` & `PyCBC-2.2.1/examples/cal/foton_filter_esd_saturation/run_pcal_saturation_example.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/catalog/data.py` & `PyCBC-2.2.1/examples/catalog/data.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/dataquality/on.py` & `PyCBC-2.2.1/examples/dataquality/on.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/detector/ant.py` & `PyCBC-2.2.1/examples/detector/ant.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/detector/delay.py` & `PyCBC-2.2.1/examples/detector/delay.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/detector/loc.py` & `PyCBC-2.2.1/examples/detector/loc.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from pycbc.detector import Detector, get_available_detectors
 
 # We can list the available detectors. This gives their detector abbreviation
 # along with a longer name. Note that some of these are not physical detectors
 # but may be useful for testing or study purposes
 
-for abv, long_name in get_available_detectors():
+for abv in get_available_detectors():
     d = Detector(abv)
 
     # Note that units are all in radians
-    print("{} {} Latitude {} Longitude {}".format(long_name, abv,
+    print("{} Latitude {} Longitude {}".format(abv,
                                                   d.latitude,
                                                   d.longitude))
```

### Comparing `PyCBC-2.2.0/examples/distributions/mass_examples.py` & `PyCBC-2.2.1/examples/distributions/mass_examples.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/distributions/mchirp_q_from_uniform_m1m2_example.py` & `PyCBC-2.2.1/examples/distributions/mchirp_q_from_uniform_m1m2_example.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/distributions/sampling_from_config_example.py` & `PyCBC-2.2.1/examples/distributions/sampling_from_config_example.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/distributions/spin_examples.py` & `PyCBC-2.2.1/examples/distributions/spin_examples.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/distributions/spin_spatial_distr_example.py` & `PyCBC-2.2.1/examples/distributions/spin_spatial_distr_example.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/faith/injection0.xml` & `PyCBC-2.2.1/examples/faith/injection0.xml`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/filter/chisq.py` & `PyCBC-2.2.1/examples/filter/chisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/filter/fir.py` & `PyCBC-2.2.1/examples/filter/fir.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/filter/pass.py` & `PyCBC-2.2.1/examples/filter/pass.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/filter/snr.py` & `PyCBC-2.2.1/examples/filter/snr.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/gw150914/audio.py` & `PyCBC-2.2.1/examples/gw150914/audio.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 try:
     from urllib.request import urlretrieve
 except ImportError:  # python < 3
     from urllib import urlretrieve
 
 # Read data and remove low frequency content
 fname = 'H-H1_LOSC_4_V2-1126259446-32.gwf'
-url = "https://www.gw-openscience.org/GW150914data/" + fname
+url = "https://www.gwosc.org/GW150914data/" + fname
 urlretrieve(url, filename=fname)
 h1 = highpass_fir(read_frame(fname, 'H1:LOSC-STRAIN'), 15.0, 8)
 
 # Calculate the noise spectrum and whiten
 psd = interpolate(welch(h1), 1.0 / 32)
 white_strain = (h1.to_frequencyseries() / psd ** 0.5 * psd.delta_f).to_timeseries()
```

### Comparing `PyCBC-2.2.0/examples/gw150914/gw150914_h1_snr.py` & `PyCBC-2.2.1/examples/gw150914/gw150914_h1_snr.py`

 * *Files 15% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from pycbc.filter import highpass_fir, matched_filter
 from pycbc.waveform import get_fd_waveform
 from pycbc.psd import welch, interpolate
 
 
 # Read data and remove low frequency content
 fname = 'H-H1_LOSC_4_V2-1126259446-32.gwf'
-url = "https://www.gw-openscience.org/GW150914data/" + fname
+url = "https://www.gwosc.org/GW150914data/" + fname
 urlretrieve(url, filename=fname)
 h1 = read_frame('H-H1_LOSC_4_V2-1126259446-32.gwf', 'H1:LOSC-STRAIN')
 h1 = highpass_fir(h1, 15, 8)
 
 # Calculate the noise spectrum
 psd = interpolate(welch(h1), 1.0 / h1.duration)
```

### Comparing `PyCBC-2.2.0/examples/gw150914/gw150914_shape.py` & `PyCBC-2.2.1/examples/gw150914/gw150914_shape.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/bbh-injection/run.sh` & `PyCBC-2.2.1/examples/inference/bbh-injection/run.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/bbh-injection/run_test.sh` & `PyCBC-2.2.1/examples/inference/bbh-injection/run_test.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/gw150914/plot.sh` & `PyCBC-2.2.1/examples/inference/gw150914/plot.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/gw150914/run.sh` & `PyCBC-2.2.1/examples/inference/gw150914/run.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/gw150914/run_test.sh` & `PyCBC-2.2.1/examples/inference/gw150914/run_test.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/hierarchical/run_test.sh` & `PyCBC-2.2.1/examples/inference/hierarchical/run_test.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/lisa_smbhb/advanced_plot.py` & `PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/advanced_plot.py`

 * *Files 0% similar despite different names*

```diff
@@ -36,15 +36,15 @@
               'mchirp': mchirp,
               'q': q,
               'mode_array': modes
             }
 
     plot_code = f"""
                 pycbc_inference_plot_posterior \
-                --input-file lisa_smbhb.hdf \
+                --input-file lisa_smbhb_ldc_pe.hdf \
                 --output-file lisa_smbhb_mass_tc_{p_index}.png \
                 --z-arg snr --plot-scatter --plot-marginal \
                 --plot-contours --contour-color black \
                 --parameters \
                     mass1_from_mchirp_q(mchirp,q):mass1 \
                     mass2_from_mchirp_q(mchirp,q):mass2 \
                     tc \
```

### Comparing `PyCBC-2.2.0/examples/inference/lisa_smbhb/get.sh` & `PyCBC-2.2.1/examples/inference/lisa_smbhb_ldc/get.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inference/margtime/run.sh` & `PyCBC-2.2.1/examples/inference/margtime/run.sh`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 --force \
 --verbose
 
 # This reconstructs any marginalized parameters
 OMP_NUM_THREADS=1 pycbc_inference_model_stats \
 --input-file marg_150914.hdf \
 --output-file demarg_150914.hdf \
---nprocesses 2 \
+--nprocesses 1 \
 --reconstruct-parameters \
 --force \
 --verbose
 
 pycbc_inference_plot_posterior \
 --input-file demarg_150914.hdf \
 --output-file demarg_150914.png \
```

### Comparing `PyCBC-2.2.0/examples/inference/margtime/run_inj.sh` & `PyCBC-2.2.1/examples/inference/margtime/run_inj.sh`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 --force \
 --verbose
 
 # This reconstructs any marginalized parameters
 OMP_NUM_THREADS=1 pycbc_inference_model_stats \
 --input-file marg_inj.hdf \
 --output-file demarg_inj.hdf \
---nprocesses 2 \
+--nprocesses 1 \
 --reconstruct-parameters \
 --force \
 --verbose
 
 pycbc_inference_plot_posterior \
 --input-file demarg_inj.hdf \
 --output-file demarg_inj.png \
```

### Comparing `PyCBC-2.2.0/examples/inference/samplers/run.sh` & `PyCBC-2.2.1/examples/inference/samplers/run.sh`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 #!/bin/sh
 for f in cpnest_stub.ini emcee_stub.ini emcee_pt_stub.ini dynesty_stub.ini ultranest_stub.ini epsie_stub.ini; do
         echo $f
 	pycbc_inference \
         --config-files `dirname $0`/simp.ini `dirname $0`/$f \
         --output-file $f.hdf \
-        --nprocesses 2 \
+        --nprocesses 1 \
         --seed 10 \
         --force
 done
 
 pycbc_inference_plot_posterior --input-file \
 emcee_stub.ini.hdf:emcee \
 emcee_pt_stub.ini.hdf:emcee_pt \
```

### Comparing `PyCBC-2.2.0/examples/inference/single/run_marg.sh` & `PyCBC-2.2.1/examples/inference/single/run_marg.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inspiral/check_GW150914_detection.py` & `PyCBC-2.2.1/examples/inspiral/check_GW150914_detection.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/inspiral/run.sh` & `PyCBC-2.2.1/examples/inspiral/run.sh`

 * *Files 10% similar despite different names*

```diff
@@ -53,30 +53,38 @@
 
 
 # Test pycbc inspiral by running over GW150914 with a limited template bank
 echo -e "\\n\\n>> [`date`] Getting template bank"
 wget -nv -nc https://github.com/gwastro/pycbc-config/raw/master/test/inspiral/SMALLER_BANK_FOR_GW150914.hdf
 
 echo -e "\\n\\n>> [`date`] Compressing template bank"
-pycbc_compress_bank --bank-file SMALLER_BANK_FOR_GW150914.hdf --output COMPRESSED_BANK.hdf --sample-rate 4096 --segment-length 256 --compression-algorithm mchirp --psd-model aLIGOZeroDetHighPower --low-frequency-cutoff 30 --approximant "SEOBNRv4_ROM"
+pycbc_compress_bank \
+    --bank-file SMALLER_BANK_FOR_GW150914.hdf \
+    --output COMPRESSED_BANK.hdf \
+    --sample-rate 4096 \
+    --segment-length 256 \
+    --compression-algorithm mchirp \
+    --psd-model aLIGOZeroDetHighPower \
+    --low-frequency-cutoff 30 \
+    --approximant "SEOBNRv4_ROM"
 
 echo -e "\\n\\n>> [`date`] Creating data file"
 pycbc_condition_strain \
---frame-type LOSC \
---sample-rate 2048 \
---pad-data 8 \
---autogating-width 0.25 \
---autogating-threshold 100 \
---autogating-cluster 0.5 \
---autogating-taper 0.25 \
---strain-high-pass 10 \
---channel-name H1:LOSC-STRAIN \
---gps-start-time 1126258578 \
---gps-end-time 1126259946 \
---output-strain-file DATA_FILE.gwf \
+    --frame-type GWOSC \
+    --sample-rate 2048 \
+    --pad-data 8 \
+    --autogating-width 0.25 \
+    --autogating-threshold 100 \
+    --autogating-cluster 0.5 \
+    --autogating-taper 0.25 \
+    --strain-high-pass 10 \
+    --channel-name H1:LOSC-STRAIN \
+    --gps-start-time 1126258578 \
+    --gps-end-time 1126259946 \
+    --output-strain-file DATA_FILE.gwf \
 
 
 start=`date +%s`
 inspiral_run fftw 16 openmp
 end=`date +%s`
 runtime_fftw_openmp_16=$((end-start))
```

### Comparing `PyCBC-2.2.0/examples/live/check_results.py` & `PyCBC-2.2.1/examples/live/check_results.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 import numpy as np
 import h5py
 import pycbc
 from pycbc.io import FieldArray
 from pycbc.io.ligolw import LIGOLWContentHandler
 from ligo.lw.utils import load_filename as load_xml_doc
 from ligo.lw import lsctables
+from pycbc import conversions as conv
 
 
 def close(a, b, c):
     return abs(a - b) <= c
 
 def check_single_results(args):
     single_fail = False
@@ -126,35 +127,39 @@
         found_fail = True
     else:
         log.info('%d found trigger(s) detected', n_found)
 
     # create field array to store properties of triggers
     dtype = [('mass1', float), ('mass2', float),
              ('spin1z', float), ('spin2z', float),
-             ('tc', float), ('net_snr', float)]
+             ('tc', float), ('net_snr', float),
+             ('ifar', float)]
     trig_props = FieldArray(n_found, dtype=dtype)
 
     # store properties of found triggers
     for x, ctrigfp in enumerate(found_trig_paths):
         log.info('Checking trigger %s', ctrigfp)
         xmldoc = load_xml_doc(
             ctrigfp, False, contenthandler=LIGOLWContentHandler)
         si_table = lsctables.SnglInspiralTable.get_table(xmldoc)
+        ci_table = lsctables.CoincInspiralTable.get_table(xmldoc)
 
         trig_props['tc'][x] = si_table[0].end
         trig_props['mass1'][x] = si_table[0].mass1
         trig_props['mass2'][x] = si_table[0].mass2
         trig_props['spin1z'][x] = si_table[0].spin1z
         trig_props['spin2z'][x] = si_table[0].spin2z
+        trig_props['ifar'][x] = conv.sec_to_year(1 / ci_table[0].combined_far)
 
         snr_list = si_table.getColumnByName('snr').asarray()
         trig_props['net_snr'][x] = sum(snr_list ** 2) ** 0.5
 
         log.info('Single-detector SNRs: %s', snr_list)
         log.info('Network SNR: %f', trig_props['net_snr'][x])
+        log.info('IFAR: %f', trig_props['ifar'][x])
         log.info('Merger time: %f', trig_props['tc'][x])
         log.info('Mass 1: %f', trig_props['mass1'][x])
         log.info('Mass 2: %f', trig_props['mass2'][x])
         log.info('Spin1z: %f', trig_props['spin1z'][x])
         log.info('Spin2z: %f', trig_props['spin2z'][x])
 
     # check if injections match trigger params
```

### Comparing `PyCBC-2.2.0/examples/live/generate_injections.py` & `PyCBC-2.2.1/examples/live/generate_injections.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/live/make_singles_fits_file.py` & `PyCBC-2.2.1/examples/live/make_singles_fits_file.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/live/run.sh` & `PyCBC-2.2.1/examples/live/run.sh`

 * *Files 0% similar despite different names*

```diff
@@ -191,20 +191,19 @@
 --round-start-time 4 \
 --start-time $gps_start_time \
 --end-time $gps_end_time \
 --src-class-mchirp-to-delta 0.01 \
 --src-class-eff-to-lum-distance 0.74899 \
 --src-class-lum-distance-to-delta -0.51557 -0.32195 \
 --run-snr-optimization \
---enable-single-detector-background \
+--sngl-ifar-est-dist conservative \
 --single-newsnr-threshold 9 \
 --single-duration-threshold 7 \
 --single-reduced-chisq-threshold 2 \
 --single-fit-file single_trigger_fits.hdf \
---sngl-ifar-est-dist conservative \
 --verbose
 
 # note that, at this point, some SNR optimization processes may still be
 # running, so the checks below may ignore their results
 
 # cat the logs of pycbc_optimize_snr so we can check them
 for opt_snr_log in `find output -type f -name optimize_snr.log | sort`
```

### Comparing `PyCBC-2.2.0/examples/make_skymap/GW170817.sh` & `PyCBC-2.2.1/examples/make_skymap/GW170817.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/make_skymap/simulated_data.sh` & `PyCBC-2.2.1/examples/make_skymap/simulated_data.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/multi_inspiral/check_faceon_faceaway_trigs.py` & `PyCBC-2.2.1/examples/multi_inspiral/check_faceon_faceaway_trigs.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/multi_inspiral/check_gw170817_trigs.py` & `PyCBC-2.2.1/examples/multi_inspiral/check_gw170817_trigs.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/multi_inspiral/faceon_faceaway.sh` & `PyCBC-2.2.1/examples/multi_inspiral/faceon_faceaway.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/multi_inspiral/run.sh` & `PyCBC-2.2.1/examples/multi_inspiral/run.sh`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 #!/bin/bash -e
 
 CONFIG_URL=https://github.com/gwastro/pycbc-config/raw/master/test/multi_inspiral
 BANK_FILE=gw170817_single_template.hdf
 BANK_VETO_FILE=bank_veto_bank.xml
-H1_FRAME=https://www.gw-openscience.org/eventapi/html/GWTC-1-confident/GW170817/v3/H-H1_GWOSC_4KHZ_R1-1187006835-4096.gwf
+H1_FRAME=https://www.gwosc.org/eventapi/html/GWTC-1-confident/GW170817/v3/H-H1_GWOSC_4KHZ_R1-1187006835-4096.gwf
 H1_CHANNEL=GWOSC-4KHZ_R1_STRAIN
 L1_FRAME=https://dcc.ligo.org/public/0144/T1700406/003/L-L1_CLEANED_HOFT_C02_T1700406_v3-1187008667-4096.gwf
 L1_CHANNEL=DCH-CLEAN_STRAIN_C02_T1700406_v3
-V1_FRAME=https://www.gw-openscience.org/eventapi/html/GWTC-1-confident/GW170817/v3/V-V1_GWOSC_4KHZ_R1-1187006835-4096.gwf
+V1_FRAME=https://www.gwosc.org/eventapi/html/GWTC-1-confident/GW170817/v3/V-V1_GWOSC_4KHZ_R1-1187006835-4096.gwf
 V1_CHANNEL=GWOSC-4KHZ_R1_STRAIN
 
 echo -e "\\n\\n>> [`date`] Getting template bank"
 wget -nv -nc ${CONFIG_URL}/${BANK_FILE}
 echo -e "\\n\\n>> [`date`] Bank veto bank"
 wget -nv -nc ${CONFIG_URL}/${BANK_VETO_FILE}
 for IFO in H1 L1 V1; do
@@ -30,15 +30,14 @@
 TRIG_END=$((GPS_END - END_PAD))
 OUTPUT=GW170817_test_output.hdf
 
 echo -e "\\n\\n>> [`date`] Running pycbc_multi_inspiral on GW170817 data"
 pycbc_multi_inspiral \
     --verbose \
     --projection left+right \
-    --processing-scheme mkl \
     --instruments H1 L1 V1 \
     --trigger-time ${EVENT} \
     --gps-start-time ${GPS_START} \
     --gps-end-time ${GPS_END} \
     --trig-start-time ${TRIG_START} \
     --trig-end-time ${TRIG_END} \
     --ra 3.44527994344 \
```

### Comparing `PyCBC-2.2.0/examples/noise/frequency.py` & `PyCBC-2.2.1/examples/noise/frequency.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/overlap.py` & `PyCBC-2.2.1/examples/overlap.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/psd/analytic.py` & `PyCBC-2.2.1/examples/psd/analytic.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/psd/estimate.py` & `PyCBC-2.2.1/examples/psd/estimate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/psd/read.py` & `PyCBC-2.2.1/examples/psd/read.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/search/check_job.py` & `PyCBC-2.2.1/examples/search/check_job.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/search/stats.sh` & `PyCBC-2.2.1/examples/search/stats.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/tmpltbank/testAligned.sh` & `PyCBC-2.2.1/examples/tmpltbank/testAligned.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/tmpltbank/testAligned2.sh` & `PyCBC-2.2.1/examples/tmpltbank/testAligned2.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/tmpltbank/testAligned3.sh` & `PyCBC-2.2.1/examples/tmpltbank/testAligned3.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/tmpltbank/testNonspin.sh` & `PyCBC-2.2.1/examples/tmpltbank/testNonspin.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/tmpltbank/testStoch.sh` & `PyCBC-2.2.1/examples/tmpltbank/testStoch.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/tmpltbank/testStoch4.sh` & `PyCBC-2.2.1/examples/tmpltbank/testStoch4.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/waveform/add_waveform.py` & `PyCBC-2.2.1/examples/waveform/add_waveform.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/waveform/match_waveform.py` & `PyCBC-2.2.1/examples/waveform/match_waveform.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/waveform/plot_detwaveform.py` & `PyCBC-2.2.1/examples/waveform/plot_detwaveform.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/waveform/plot_fd_td.py` & `PyCBC-2.2.1/examples/waveform/plot_fd_td.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/waveform/plot_freq.py` & `PyCBC-2.2.1/examples/waveform/plot_freq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/waveform/plot_phase.py` & `PyCBC-2.2.1/examples/waveform/plot_phase.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/data_checker/daily_test.py` & `PyCBC-2.2.1/examples/workflow/data_checker/daily_test.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/data_checker/get_data_example.py` & `PyCBC-2.2.1/examples/workflow/data_checker/get_data_example.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/dayhopecheck/dayhopecheck.py` & `PyCBC-2.2.1/examples/workflow/dayhopecheck/dayhopecheck.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/generic/multilevel_subworkflow_data/simple.py` & `PyCBC-2.2.1/examples/workflow/generic/multilevel_subworkflow_data/simple.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/generic/simple_subworkflow_data/simple.py` & `PyCBC-2.2.1/examples/workflow/generic/simple_subworkflow_data/simple.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/inference/bbh_inj-dynesty/create_inj_workflow.sh` & `PyCBC-2.2.1/examples/workflow/inference/bbh_inj-dynesty/create_inj_workflow.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/pygrb/ER7/run_er7_pygrb_offline.sh` & `PyCBC-2.2.1/examples/workflow/pygrb/ER7/run_er7_pygrb_offline.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/pygrb/ER8/run_er8_pygrb_offline.sh` & `PyCBC-2.2.1/examples/workflow/pygrb/ER8/run_er8_pygrb_offline.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/pygrb/ER8/run_er8_pygrb_online.sh` & `PyCBC-2.2.1/examples/workflow/pygrb/ER8/run_er8_pygrb_online.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/examples/workflow/pygrb/S6/run_s6_pygrb.sh` & `PyCBC-2.2.1/examples/workflow/pygrb/S6/run_s6_pygrb.sh`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/__init__.py` & `PyCBC-2.2.1/pycbc/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -154,15 +154,15 @@
 except ImportError:
     HAVE_CUDA=False
 
 # Check for MKL capability
 try:
     import pycbc.fft.mkl
     HAVE_MKL=True
-except ImportError:
+except (ImportError, OSError):
     HAVE_MKL=False
 
 # Check for openmp suppport, currently we pressume it exists, unless on
 # platforms (mac) that are silly and don't use the standard gcc.
 if sys.platform == 'darwin':
     HAVE_OMP = False
```

### Comparing `PyCBC-2.2.0/pycbc/_version.py` & `PyCBC-2.2.1/pycbc/_version.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/_version_helper.py` & `PyCBC-2.2.1/pycbc/_version_helper.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/bin_utils.py` & `PyCBC-2.2.1/pycbc/bin_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/boundaries.py` & `PyCBC-2.2.1/pycbc/boundaries.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/catalog/__init__.py` & `PyCBC-2.2.1/pycbc/catalog/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/catalog/catalog.py` & `PyCBC-2.2.1/pycbc/catalog/catalog.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 import json
 from pycbc.io import get_file
 
 # For the time being all quantities are the 1-d median value
 # FIXME with posteriors when available and we can just post-process that
 
 # LVC catalogs
-base_lvc_url = "https://www.gw-openscience.org/eventapi/jsonfull/{}/"
+base_lvc_url = "https://www.gwosc.org/eventapi/jsonfull/{}/"
 _catalogs = {'GWTC-1-confident': 'LVC',
              'GWTC-1-marginal': 'LVC',
              'Initial_LIGO_Virgo': 'LVC',
              'O1_O2-Preliminary': 'LVC',
              'O3_Discovery_Papers': 'LVC',
              'GWTC-2': 'LVC',
              'GWTC-2.1-confident': 'LVC',
```

### Comparing `PyCBC-2.2.0/pycbc/conversions.py` & `PyCBC-2.2.1/pycbc/conversions.py`

 * *Files 4% similar despite different names*

```diff
@@ -102,14 +102,16 @@
 #                           CBC mass functions
 #
 # =============================================================================
 #
 def primary_mass(mass1, mass2):
     """Returns the larger of mass1 and mass2 (p = primary)."""
     mass1, mass2, input_is_array = ensurearray(mass1, mass2)
+    if mass1.shape != mass2.shape:
+        raise ValueError("mass1 and mass2 must have same shape")
     mp = copy.copy(mass1)
     mask = mass1 < mass2
     mp[mask] = mass2[mask]
     return formatreturn(mp, input_is_array)
 
 
 def secondary_mass(mass1, mass2):
@@ -136,40 +138,40 @@
 def invq_from_mass1_mass2(mass1, mass2):
     """Returns the inverse mass ratio m2/m1, where m1 >= m2."""
     return secondary_mass(mass1, mass2) / primary_mass(mass1, mass2)
 
 
 def eta_from_mass1_mass2(mass1, mass2):
     """Returns the symmetric mass ratio from mass1 and mass2."""
-    return mass1*mass2 / (mass1+mass2)**2.
+    return mass1*mass2 / (mass1 + mass2)**2.
 
 
 def mchirp_from_mass1_mass2(mass1, mass2):
     """Returns the chirp mass from mass1 and mass2."""
-    return eta_from_mass1_mass2(mass1, mass2)**(3./5) * (mass1+mass2)
+    return eta_from_mass1_mass2(mass1, mass2)**(3./5) * (mass1 + mass2)
 
 
 def mass1_from_mtotal_q(mtotal, q):
     """Returns a component mass from the given total mass and mass ratio.
 
     If the mass ratio q is >= 1, the returned mass will be the primary
     (heavier) mass. If q < 1, the returned mass will be the secondary
     (lighter) mass.
     """
-    return q*mtotal / (1.+q)
+    return q*mtotal / (1. + q)
 
 
 def mass2_from_mtotal_q(mtotal, q):
     """Returns a component mass from the given total mass and mass ratio.
 
     If the mass ratio q is >= 1, the returned mass will be the secondary
     (lighter) mass. If q < 1, the returned mass will be the primary (heavier)
     mass.
     """
-    return mtotal / (1.+q)
+    return mtotal / (1. + q)
 
 
 def mass1_from_mtotal_eta(mtotal, eta):
     """Returns the primary mass from the total mass and symmetric mass
     ratio.
     """
     return 0.5 * mtotal * (1.0 + (1.0 - 4.0 * eta)**0.5)
@@ -181,15 +183,15 @@
     """
     return 0.5 * mtotal * (1.0 - (1.0 - 4.0 * eta)**0.5)
 
 
 def mtotal_from_mchirp_eta(mchirp, eta):
     """Returns the total mass from the chirp mass and symmetric mass ratio.
     """
-    return mchirp / (eta**(3./5.))
+    return mchirp / eta**(3./5.)
 
 
 def mass1_from_mchirp_eta(mchirp, eta):
     """Returns the primary mass from the chirp mass and symmetric mass ratio.
     """
     mtotal = mtotal_from_mchirp_eta(mchirp, eta)
     return mass1_from_mtotal_eta(mtotal, eta)
@@ -215,15 +217,15 @@
 
     .. math::
         a = \frac{\mathcal{M}^5}{m_1^3}.
 
     This has 3 solutions but only one will be real.
     """
     a = mchirp**5 / mass1**3
-    roots = numpy.roots([1,0,-a,-a*mass1])
+    roots = numpy.roots([1, 0, -a, -a * mass1])
     # Find the real one
     real_root = roots[(abs(roots - roots.real)).argmin()]
     return real_root.real
 
 mass2_from_mchirp_mass1 = numpy.vectorize(_mass2_from_mchirp_mass1)
 
 
@@ -257,15 +259,15 @@
         Force the returned mass to be real.
 
     Returns
     -------
     float
         The other component mass.
     """
-    roots = numpy.roots([eta, (2*eta - 1)*known_mass, eta*known_mass**2.])
+    roots = numpy.roots([eta, (2*eta - 1) * known_mass, eta * known_mass**2.])
     if force_real:
         roots = numpy.real(roots)
     if known_is_secondary:
         return roots[roots.argmax()]
     else:
         return roots[roots.argmin()]
 
@@ -294,33 +296,36 @@
     This is given by:
 
     .. math::
         \eta = \frac{q}{(1+q)^2}.
 
     Note that the mass ratio may be either < 1 or > 1.
     """
-    return q / (1.+q)**2
+    return q / (1. + q)**2
 
 
 def mass1_from_mchirp_q(mchirp, q):
     """Returns the primary mass from the given chirp mass and mass ratio."""
-    mass1 = (q**(2./5.))*((1.0 + q)**(1./5.))*mchirp
+    mass1 = q**(2./5.) * (1.0 + q)**(1./5.) * mchirp
     return mass1
 
+
 def mass2_from_mchirp_q(mchirp, q):
     """Returns the secondary mass from the given chirp mass and mass ratio."""
-    mass2 = (q**(-3./5.))*((1.0 + q)**(1./5.))*mchirp
+    mass2 = q**(-3./5.) * (1.0 + q)**(1./5.) * mchirp
     return mass2
 
+
 def _a0(f_lower):
     """Used in calculating chirp times: see Cokelaer, arxiv.org:0706.4437
        appendix 1, also lalinspiral/python/sbank/tau0tau3.py.
     """
     return 5. / (256. * (numpy.pi * f_lower)**(8./3.))
 
+
 def _a3(f_lower):
     """Another parameter used for chirp times"""
     return numpy.pi / (8. * (numpy.pi * f_lower)**(5./3.))
 
 
 def tau0_from_mtotal_eta(mtotal, eta, f_lower):
     r"""Returns :math:`\tau_0` from the total mass, symmetric mass ratio, and
@@ -328,14 +333,23 @@
     """
     # convert to seconds
     mtotal = mtotal * lal.MTSUN_SI
     # formulae from arxiv.org:0706.4437
     return _a0(f_lower) / (mtotal**(5./3.) * eta)
 
 
+def tau0_from_mchirp(mchirp, f_lower):
+    r"""Returns :math:`\tau_0` from the chirp mass and the given frequency.
+    """
+    # convert to seconds
+    mchirp = mchirp * lal.MTSUN_SI
+    # formulae from arxiv.org:0706.4437
+    return _a0(f_lower) / mchirp ** (5./3.)
+
+
 def tau3_from_mtotal_eta(mtotal, eta, f_lower):
     r"""Returns :math:`\tau_0` from the total mass, symmetric mass ratio, and
     the given frequency.
     """
     # convert to seconds
     mtotal = mtotal * lal.MTSUN_SI
     # formulae from arxiv.org:0706.4437
@@ -354,14 +368,22 @@
     r"""Returns :math:`\tau_3` from the component masses and given frequency.
     """
     mtotal = mass1 + mass2
     eta = eta_from_mass1_mass2(mass1, mass2)
     return tau3_from_mtotal_eta(mtotal, eta, f_lower)
 
 
+def mchirp_from_tau0(tau0, f_lower):
+    r"""Returns chirp mass from :math:`\tau_0` and the given frequency.
+    """
+    mchirp = (_a0(f_lower) / tau0) ** (3./5.)  # in seconds
+    # convert back to solar mass units
+    return mchirp / lal.MTSUN_SI
+
+
 def mtotal_from_tau0_tau3(tau0, tau3, f_lower,
                           in_seconds=False):
     r"""Returns total mass from :math:`\tau_0, \tau_3`."""
     mtotal = (tau3 / _a3(f_lower)) / (tau0 / _a0(f_lower))
     if not in_seconds:
         # convert back to solar mass units
         mtotal /= lal.MTSUN_SI
@@ -417,63 +439,151 @@
     mass_from_file = data[:, 0]
     lambda_from_file = data[:, 1]
     mass_src = mass/(1.0 + pycbc.cosmology.redshift(distance))
     lambdav = numpy.interp(mass_src, mass_from_file, lambda_from_file)
     return lambdav
 
 
+def ensure_obj1_is_primary(mass1, mass2, *params):
+    """
+    Enforce that the object labelled as 1 is the primary.
+
+    Parameters
+    ----------
+    mass1 : float, numpy.array
+        Mass values labelled as 1.
+    mass2 : float, numpy.array
+        Mass values labelled as 2.
+    *params :
+        The binary parameters to be swapped around when mass1 < mass2.
+        The list must have length 2N and it must be organized so that
+        params[i] and params[i+1] are the same kind of quantity, but
+        for object 1 and object 2, respsectively.
+        E.g., spin1z, spin2z, lambda1, lambda2.
+
+    Returns
+    -------
+    list :
+        A list with mass1, mass2, params as arrays, with elements, each
+        with elements re-arranged so that object 1 is the primary.
+    """
+    # Check params are 2N
+    if len(params) % 2 != 0:
+        raise ValueError("params must be 2N floats or arrays")
+    input_properties, input_is_array = ensurearray((mass1, mass2)+params)
+    # Check inputs are all the same length
+    shapes = [par.shape for par in input_properties]
+    if len(set(shapes)) != 1:
+        raise ValueError("Individual masses and params must have same shape")
+    # What needs to be swapped
+    mask = mass1 < mass2
+    # Output containter
+    output_properties = []
+    for i in numpy.arange(0, len(shapes), 2):
+        # primary (p)
+        p = copy.copy(input_properties[i])
+        # secondary (s)
+        s = copy.copy(input_properties[i+1])
+        # Swap
+        p[mask] = input_properties[i+1][mask]
+        s[mask] = input_properties[i][mask]
+        # Format and include in output object
+        output_properties.append(formatreturn(p, input_is_array))
+        output_properties.append(formatreturn(s, input_is_array))
+    # Release output
+    return output_properties
+
+
 def remnant_mass_from_mass1_mass2_spherical_spin_eos(
-        mass1, mass2, spin1a=0.0, spin1pol=0.0, eos='2H'):
+        mass1, mass2, spin1_a=0.0, spin1_polar=0.0, eos='2H',
+        spin2_a=0.0, spin2_polar=0.0, swap_companions=False,
+        ns_bh_mass_boundary=None, extrapolate=False):
     """
     Function that determines the remnant disk mass of an NS-BH system
     using the fit to numerical-relativity results discussed in
     Foucart, Hinderer & Nissanke, PRD 98, 081501(R) (2018).
     The BH spin may be misaligned with the orbital angular momentum.
     In such cases the ISSO is approximated following the approach of
     Stone, Loeb & Berger, PRD 87, 084053 (2013), which was originally
     devised for a previous NS-BH remnant mass fit of
     Foucart, PRD 86, 124007 (2012).
-    Note: NS spin is assumed to be 0!
+    Note: The NS spin does not play any role in this fit!
 
     Parameters
     -----------
     mass1 : float
         The mass of the black hole, in solar masses.
     mass2 : float
         The mass of the neutron star, in solar masses.
-    spin1a : float, optional
+    spin1_a : float, optional
         The dimensionless magnitude of the spin of mass1. Default = 0.
-    spin1pol : float, optional
+    spin1_polar : float, optional
         The tilt angle of the spin of mass1. Default = 0 (aligned w L).
     eos : str, optional
         Name of the equation of state being adopted. Default is '2H'.
+    spin2_a : float, optional
+        The dimensionless magnitude of the spin of mass2. Default = 0.
+    spin2_polar : float, optional
+        The tilt angle of the spin of mass2. Default = 0 (aligned w L).
+    swap_companions : boolean, optional
+        If mass2 > mass1, swap mass and spin of object 1 and 2 prior
+        to applying the fitting formula (otherwise fail). Default is False.
+    ns_bh_mass_boundary : float, optional
+        If mass2 is greater than this value, the neutron star is effectively
+        treated as a black hole and the returned value is 0. For consistency
+        with the eos, set this to the maximum mass allowed by the eos; set
+        a lower value for a more stringent cut. Default is None.
+    extrapolate : boolean, optional
+        Invoke extrapolation of NS baryonic mass and NS compactness in
+        scipy.interpolate.interp1d at low masses. If ns_bh_mass_boundary is
+        provided, it is applied at high masses, otherwise the equation of
+        state prescribes the maximum possible mass2. Default is False.
 
     Returns
     ----------
     remnant_mass: float
         The remnant mass in solar masses
     """
-    mass1, mass2, spin1a, spin1pol, input_is_array = ensurearray(
-        mass1, mass2, spin1a, spin1pol)
-    # mass1 must be greater than mass2
-    try:
-        if any(mass2 > mass1) and input_is_array:
-            raise ValueError(f'Require mass1 >= mass2')
-    except TypeError:
-        if mass2 > mass1 and not input_is_array:
-            raise ValueError(f'Require mass1 >= mass2. {mass1} < {mass2}')
-    ns_compactness, ns_b_mass = ns.initialize_eos(mass2, eos)
+    mass1, mass2, spin1_a, spin1_polar, spin2_a, spin2_polar, \
+        input_is_array = \
+        ensurearray(mass1, mass2, spin1_a, spin1_polar, spin2_a, spin2_polar)
+    # mass1 must be greater than mass2: swap the properties of 1 and 2 or fail
+    if swap_companions:
+        mass1, mass2, spin1_a, spin2_a, spin1_polar, spin2_polar = \
+            ensure_obj1_is_primary(mass1, mass2, spin1_a, spin2_a,
+                                   spin1_polar, spin2_polar)
+    else:
+        try:
+            if any(mass2 > mass1) and input_is_array:
+                raise ValueError(f'Require mass1 >= mass2')
+        except TypeError:
+            if mass2 > mass1 and not input_is_array:
+                raise ValueError(f'Require mass1 >= mass2. {mass1} < {mass2}')
     eta = eta_from_mass1_mass2(mass1, mass2)
-    remnant_mass = ns.foucart18(
-        eta, ns_compactness, ns_b_mass, spin1a, spin1pol)
+    # If a maximum NS mass is not provided, accept all values and
+    # let the EOS handle this (in ns.initialize_eos)
+    if ns_bh_mass_boundary is None:
+        mask = numpy.ones(ensurearray(mass2).size[0], dtype=bool)
+    # Otherwise perform the calculation only for small enough NS masses...
+    else:
+        mask = mass2 <= ns_bh_mass_boundary
+    # ...and return 0's otherwise
+    remnant_mass = numpy.zeros(ensurearray(mass2)[0].size)
+    ns_compactness, ns_b_mass = ns.initialize_eos(mass2[mask], eos,
+                                                  extrapolate=extrapolate)
+    remnant_mass[mask] = ns.foucart18(
+            eta[mask], ns_compactness, ns_b_mass,
+            spin1_a[mask], spin1_polar[mask])
     return formatreturn(remnant_mass, input_is_array)
 
 
 def remnant_mass_from_mass1_mass2_cartesian_spin_eos(
-        mass1, mass2, spin1x=0.0, spin1y=0.0, spin1z=0.0, eos='2H'):
+        mass1, mass2, spin1x=0.0, spin1y=0.0, spin1z=0.0, eos='2H',
+        spin2x=0.0, spin2y=0.0, spin2z=0.0, swap_companions=False,
+        ns_bh_mass_boundary=None, extrapolate=False):
     """
     Function that determines the remnant disk mass of an NS-BH system
     using the fit to numerical-relativity results discussed in
     Foucart, Hinderer & Nissanke, PRD 98, 081501(R) (2018).
     The BH spin may be misaligned with the orbital angular momentum.
     In such cases the ISSO is approximated following the approach of
     Stone, Loeb & Berger, PRD 87, 084053 (2013), which was originally
@@ -491,23 +601,52 @@
         The dimensionless x-component of the spin of mass1. Default = 0.
     spin1y : float, optional
         The dimensionless y-component of the spin of mass1. Default = 0.
     spin1z : float, optional
         The dimensionless z-component of the spin of mass1. Default = 0.
     eos: str, optional
         Name of the equation of state being adopted. Default is '2H'.
+    spin2x : float, optional
+        The dimensionless x-component of the spin of mass2. Default = 0.
+    spin2y : float, optional
+        The dimensionless y-component of the spin of mass2. Default = 0.
+    spin2z : float, optional
+        The dimensionless z-component of the spin of mass2. Default = 0.
+    swap_companions : boolean, optional
+        If mass2 > mass1, swap mass and spin of object 1 and 2 prior
+        to applying the fitting formula (otherwise fail). Default is False.
+    ns_bh_mass_boundary : float, optional
+        If mass2 is greater than this value, the neutron star is effectively
+        treated as a black hole and the returned value is 0. For consistency
+        with the eos, set this to the maximum mass allowed by the eos; set
+        a lower value for a more stringent cut. Default is None.
+    extrapolate : boolean, optional
+        Invoke extrapolation of NS baryonic mass and NS compactness in
+        scipy.interpolate.interp1d at low masses. If ns_bh_mass_boundary is
+        provided, it is applied at high masses, otherwise the equation of
+        state prescribes the maximum possible mass2. Default is False.
 
     Returns
     ----------
     remnant_mass: float
         The remnant mass in solar masses
     """
-    spin1a, _, spin1pol = _cartesian_to_spherical(spin1x, spin1y, spin1z)
+    spin1_a, _, spin1_polar = _cartesian_to_spherical(spin1x, spin1y, spin1z)
+    if swap_companions:
+        spin2_a, _, spin2_polar = _cartesian_to_spherical(spin2x,
+                                                          spin2y, spin2z)
+    else:
+        size = ensurearray(spin1_a)[0].size
+        spin2_a = numpy.zeros(size)
+        spin2_polar = numpy.zeros(size)
     return remnant_mass_from_mass1_mass2_spherical_spin_eos(
-        mass1, mass2, spin1a, spin1pol, eos=eos)
+        mass1, mass2, spin1_a=spin1_a, spin1_polar=spin1_polar, eos=eos,
+        spin2_a=spin2_a, spin2_polar=spin2_polar,
+        swap_companions=swap_companions,
+        ns_bh_mass_boundary=ns_bh_mass_boundary, extrapolate=extrapolate)
 
 
 #
 # =============================================================================
 #
 #                           CBC spin functions
 #
@@ -838,29 +977,29 @@
     """
     return optimal_orientation_from_detector(detector_name, tc)[1]
 
 def optimal_ra_from_detector(detector_name, tc):
     """For a given detector and GPS time, return the optimal orientation
     (directly overhead of the detector) in right ascension.
 
-
     Parameters
     ----------
     detector_name : string
         The name of the detector, e.g., 'H1'.
     tc : float
         The GPS time of the coalescence of the signal in the `ref_frame`.
 
     Returns
     -------
     float :
         The declination of the signal, in radians.
     """
     return optimal_orientation_from_detector(detector_name, tc)[0]
 
+
 #
 # =============================================================================
 #
 #                         Likelihood statistic parameter functions
 #
 # =============================================================================
 #
@@ -1601,14 +1740,15 @@
            'mass1_from_mtotal_eta', 'mass2_from_mtotal_eta',
            'mtotal_from_mchirp_eta', 'mass1_from_mchirp_eta',
            'mass2_from_mchirp_eta', 'mass2_from_mchirp_mass1',
            'mass_from_knownmass_eta', 'mass2_from_mass1_eta',
            'mass1_from_mass2_eta', 'eta_from_q', 'mass1_from_mchirp_q',
            'mass2_from_mchirp_q', 'tau0_from_mtotal_eta',
            'tau3_from_mtotal_eta', 'tau0_from_mass1_mass2',
+           'tau0_from_mchirp', 'mchirp_from_tau0',
            'tau3_from_mass1_mass2', 'mtotal_from_tau0_tau3',
            'eta_from_tau0_tau3', 'mass1_from_tau0_tau3',
            'mass2_from_tau0_tau3', 'primary_spin', 'secondary_spin',
            'chi_eff', 'chi_a', 'chi_p', 'phi_a', 'phi_s',
            'primary_xi', 'secondary_xi',
            'xi1_from_spin1x_spin1y', 'xi2_from_mass1_mass2_spin2x_spin2y',
            'chi_perp_from_spinx_spiny', 'chi_perp_from_mass1_mass2_xi2',
```

### Comparing `PyCBC-2.2.0/pycbc/coordinates.py` & `PyCBC-2.2.1/pycbc/coordinates.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/cosmology.py` & `PyCBC-2.2.1/pycbc/cosmology.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/detector.py` & `PyCBC-2.2.1/pycbc/detector.py`

 * *Files 4% similar despite different names*

```diff
@@ -44,16 +44,22 @@
 # presented in https://arxiv.org/pdf/gr-qc/0008066.pdf
 
 def gmst_accurate(gps_time):
     gmst = Time(gps_time, format='gps', scale='utc',
                 location=(0, 0)).sidereal_time('mean').rad
     return gmst
 
-
 def get_available_detectors():
+    """ List the available detectors """
+    dets = list(_ground_detectors.keys())
+    for pfx, name in get_available_lal_detectors():
+        dets += [pfx]
+    return dets
+
+def get_available_lal_detectors():
     """Return list of detectors known in the currently sourced lalsuite.
     This function will query lalsuite about which detectors are known to
     lalsuite. Detectors are identified by a two character string e.g. 'K1',
     but also by a longer, and clearer name, e.g. KAGRA. This function returns
     both. As LAL doesn't really expose this functionality we have to make some
     assumptions about how this information is stored in LAL. Therefore while
     we hope this function will work correctly, it's possible it will need
@@ -62,18 +68,19 @@
     """
     ld = lal.__dict__
     known_lal_names = [j for j in ld.keys() if "DETECTOR_PREFIX" in j]
     known_prefixes = [ld[k] for k in known_lal_names]
     known_names = [ld[k.replace('PREFIX', 'NAME')] for k in known_lal_names]
     return list(zip(known_prefixes, known_names))
 
+_ground_detectors = {}
 
-_custom_ground_detectors = {}
 def add_detector_on_earth(name, longitude, latitude,
-                          yangle=0, xangle=None, height=0):
+                          yangle=0, xangle=None, height=0,
+                          xlength=4000, ylength=4000):
     """ Add a new detector on the earth
 
     Parameters
     ----------
 
     name: str
         two-letter name to identify the detector
@@ -90,44 +97,69 @@
         The height in meters of the detector above the standard
         reference ellipsoidal earth
     """
     if xangle is None:
         # assume right angle detector if no separate xarm direction given
         xangle = yangle + np.pi / 2.0
 
-    # Calculate response in earth centered coordinates
-    # by rotation of response in coordinates aligned
-    # with the detector arms
-    a, b = cos(2*xangle), sin(2*xangle)
-    xresp = np.array([[-a, b, 0], [b, a, 0], [0, 0, 0]])
-    a, b = cos(2*yangle), sin(2*yangle)
-    yresp = np.array([[-a, b, 0], [b, a, 0], [0, 0, 0]])
-    resp = (yresp - xresp) / 4.0
-
+    # Rotation matrix to move detector to correct orientation
     rm1 = rotation_matrix(longitude * units.rad, 'z')
     rm2 = rotation_matrix((np.pi / 2.0 - latitude) * units.rad, 'y')
     rm = np.matmul(rm2, rm1)
 
-    resp = np.matmul(resp, rm)
-    resp = np.matmul(rm.T, resp)
+    # Calculate response in earth centered coordinates
+    # by rotation of response in coordinates aligned
+    # with the detector arms
+    resps = []
+    vecs = []
+    for angle in [yangle, xangle]:
+        a, b = cos(2 * angle), sin(2 * angle)
+        resp = np.array([[-a, b, 0], [b, a, 0], [0, 0, 0]])
+
+        # apply rotation
+        resp = np.matmul(resp, rm)
+        resp = np.matmul(rm.T, resp) / 4.0
+        resps.append(resp)
+
+        vec = np.matmul(rm.T, np.array([-np.cos(angle), np.sin(angle), 0]))
+        vecs.append(vec)
 
+    full_resp = (resps[0] - resps[1])
     loc = coordinates.EarthLocation.from_geodetic(longitude * units.rad,
                                                   latitude * units.rad,
                                                   height=height*units.meter)
-    loc = np.array([loc.x.value,
-                    loc.y.value,
-                    loc.z.value])
-    _custom_ground_detectors[name] = {'location': loc,
-                                      'response': resp,
-                                      'yangle': yangle,
-                                      'xangle': xangle,
-                                      'height': height,
-                                      'xaltitude': 0.0,
-                                      'yaltitude': 0.0,
-                                      }
+    loc = np.array([loc.x.value, loc.y.value, loc.z.value])
+    _ground_detectors[name] = {'location': loc,
+                               'response': full_resp,
+                               'xresp': resps[1],
+                               'yresp': resps[0],
+                               'xvec': vecs[1],
+                               'yvec': vecs[0],
+                               'yangle': yangle,
+                               'xangle': xangle,
+                               'height': height,
+                               'xaltitude': 0.0,
+                               'yaltitude': 0.0,
+                               'ylength': ylength,
+                               'xlength': xlength,
+                              }
+
+# Notation matches
+# Eq 4 of https://link.aps.org/accepted/10.1103/PhysRevD.96.084004
+def single_arm_frequency_response(f, n, arm_length):
+    """ The relative amplitude factor of the arm response due to
+    signal delay. This is relevant where the long-wavelength
+    approximation no longer applies)
+    """
+    n = np.clip(n, -0.999, 0.999)
+    phase = arm_length / constants.c.value * 2.0j * np.pi * f
+    a = 1.0 / 4.0 / phase
+    b = (1 - np.exp(-phase * (1 - n))) / (1 - n)
+    c = np.exp(-2.0 * phase) * (1 - np.exp(phase * (1 + n))) / (1 + n)
+    return a * (b - c) * 2.0  # We'll make this relative to the static resp
 
 def load_detector_config(config_files):
     """ Add custom detectors from a configuration file
 
     Parameters
     ----------
     config_files: str or list of strs
@@ -171,24 +203,25 @@
         reference_time: float
             Default is time of GW150914. In this case, the earth's rotation
         will be estimated from a reference time. If 'None', we will
         calculate the time for each gps time requested explicitly
         using a slower but higher precision method.
         """
         self.name = str(detector_name)
-
-        if detector_name in [pfx for pfx, name in get_available_detectors()]:
+        
+        lal_detectors = [pfx for pfx, name in get_available_lal_detectors()]
+        if detector_name in _ground_detectors:
+            self.info = _ground_detectors[detector_name]
+            self.response = self.info['response']
+            self.location = self.info['location']
+        elif detector_name in lal_detectors:
             lalsim = pycbc.libutils.import_optional('lalsimulation')
             self._lal = lalsim.DetectorPrefixToLALDetector(self.name)
             self.response = self._lal.response
             self.location = self._lal.location
-        elif detector_name in _custom_ground_detectors:
-            self.info = _custom_ground_detectors[detector_name]
-            self.response = self.info['response']
-            self.location = self.info['location']
         else:
             raise ValueError("Unkown detector {}".format(detector_name))
 
         loc = coordinates.EarthLocation(self.location[0],
                                         self.location[1],
                                         self.location[2],
                                         unit=meter)
@@ -253,15 +286,17 @@
         -------
         time: float
             The light travel time in seconds
         """
         d = self.location - det.location
         return float(d.dot(d)**0.5 / constants.c.value)
 
-    def antenna_pattern(self, right_ascension, declination, polarization, t_gps, polarization_type='tensor'):
+    def antenna_pattern(self, right_ascension, declination, polarization, t_gps,
+                        frequency=0,
+                        polarization_type='tensor'):
         """Return the detector response.
 
         Parameters
         ----------
         right_ascension: float or numpy.ndarray
             The right ascension of the source
         declination: float or numpy.ndarray
@@ -285,56 +320,76 @@
         cosgha = cos(gha)
         singha = sin(gha)
         cosdec = cos(declination)
         sindec = sin(declination)
         cospsi = cos(polarization)
         sinpsi = sin(polarization)
 
+        if frequency:
+            e0 = cosdec * cosgha
+            e1 = cosdec * -singha
+            e2 = sin(declination)
+            nhat = np.array([e0, e1, e2], dtype=object)
+
+            nx = nhat.dot(self.info['xvec'])
+            ny = nhat.dot(self.info['yvec'])
+
+            rx = single_arm_frequency_response(frequency, nx,
+                                               self.info['xlength'])
+            ry = single_arm_frequency_response(frequency, ny,
+                                               self.info['ylength'])
+            resp = ry * self.info['yresp'] -  rx * self.info['xresp']
+            ttype = np.complex128
+        else:
+            resp = self.response
+            ttype = np.float64
+
         x0 = -cospsi * singha - sinpsi * cosgha * sindec
         x1 = -cospsi * cosgha + sinpsi * singha * sindec
         x2 =  sinpsi * cosdec
 
         x = np.array([x0, x1, x2], dtype=object)
-        dx = self.response.dot(x)
+        dx = resp.dot(x)
 
         y0 =  sinpsi * singha - cospsi * cosgha * sindec
         y1 =  sinpsi * cosgha + cospsi * singha * sindec
         y2 =  cospsi * cosdec
 
         y = np.array([y0, y1, y2], dtype=object)
-        dy = self.response.dot(y)
+        dy = resp.dot(y)
 
         if polarization_type != 'tensor':
             z0 = -cosdec * cosgha
             z1 = cosdec * singha
             z2 = -sindec
             z = np.array([z0, z1, z2], dtype=object)
-            dz = self.response.dot(z)
+            dz = resp.dot(z)
 
         if polarization_type == 'tensor':
             if hasattr(dx, 'shape'):
-                fplus = (x * dx - y * dy).sum(axis=0).astype(np.float64)
-                fcross = (x * dy + y * dx).sum(axis=0).astype(np.float64)
+                fplus = (x * dx - y * dy).sum(axis=0).astype(ttype)
+                fcross = (x * dy + y * dx).sum(axis=0).astype(ttype)
             else:
                 fplus = (x * dx - y * dy).sum()
                 fcross = (x * dy + y * dx).sum()
             return fplus, fcross
 
         elif polarization_type == 'vector':
             if hasattr(dx, 'shape'):
-                fx = (z * dx + x * dz).sum(axis=0).astype(np.float64)
-                fy = (z * dy + y * dz).sum(axis=0).astype(np.float64)
+                fx = (z * dx + x * dz).sum(axis=0).astype(ttype)
+                fy = (z * dy + y * dz).sum(axis=0).astype(ttype)
             else:
                 fx = (z * dx + x * dz).sum()
                 fy = (z * dy + y * dz).sum()
+
             return fx, fy
 
         elif polarization_type == 'scalar':
             if hasattr(dx, 'shape'):
-                fb = (x * dx + y * dy).sum(axis=0).astype(np.float64)
+                fb = (x * dx + y * dy).sum(axis=0).astype(ttype)
                 fl = (z * dz).sum(axis=0)
             else:
                 fb = (x * dx + y * dy).sum()
                 fl = (z * dz).sum()
             return fb, fl
 
     def time_delay_from_earth_center(self, right_ascension, declination, t_gps):
```

### Comparing `PyCBC-2.2.0/pycbc/distributions/__init__.py` & `PyCBC-2.2.1/pycbc/distributions/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 # Promote some classes/functions to the distributions name space
 from pycbc.distributions.utils import draw_samples_from_config
 from pycbc.distributions.angular import UniformAngle, SinAngle, CosAngle, \
                                         UniformSolidAngle
 from pycbc.distributions.arbitrary import Arbitrary, FromFile
 from pycbc.distributions.gaussian import Gaussian
 from pycbc.distributions.power_law import UniformPowerLaw, UniformRadius
-from pycbc.distributions.sky_location import UniformSky
+from pycbc.distributions.sky_location import UniformSky, FisherSky
 from pycbc.distributions.uniform import Uniform
 from pycbc.distributions.uniform_log import UniformLog10
 from pycbc.distributions.spins import IndependentChiPChiEff
 from pycbc.distributions.qnm import UniformF0Tau
 from pycbc.distributions.joint import JointDistribution
 from pycbc.distributions.external import External, DistributionFunctionFromFile
 from pycbc.distributions.fixedsamples import FixedSamples
@@ -56,15 +56,16 @@
     UniformSky.name : UniformSky,
     UniformLog10.name : UniformLog10,
     UniformF0Tau.name : UniformF0Tau,
     External.name: External,
     DistributionFunctionFromFile.name: DistributionFunctionFromFile,
     FixedSamples.name: FixedSamples,
     MchirpfromUniformMass1Mass2.name: MchirpfromUniformMass1Mass2,
-    QfromUniformMass1Mass2.name: QfromUniformMass1Mass2
+    QfromUniformMass1Mass2.name: QfromUniformMass1Mass2,
+    FisherSky.name: FisherSky
 }
 
 def read_distributions_from_config(cp, section="prior"):
     """Returns a list of PyCBC distribution instances for a section in the
     given configuration file.
 
     Parameters
```

### Comparing `PyCBC-2.2.0/pycbc/distributions/angular.py` & `PyCBC-2.2.1/pycbc/distributions/angular.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/arbitrary.py` & `PyCBC-2.2.1/pycbc/distributions/arbitrary.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/bounded.py` & `PyCBC-2.2.1/pycbc/distributions/bounded.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/constraints.py` & `PyCBC-2.2.1/pycbc/distributions/constraints.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/external.py` & `PyCBC-2.2.1/pycbc/distributions/external.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/fixedsamples.py` & `PyCBC-2.2.1/pycbc/distributions/fixedsamples.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/gaussian.py` & `PyCBC-2.2.1/pycbc/distributions/gaussian.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/joint.py` & `PyCBC-2.2.1/pycbc/distributions/joint.py`

 * *Files 1% similar despite different names*

```diff
@@ -262,16 +262,15 @@
         params = self.apply_boundary_conditions(**params)
         result = True
         for dist in self.distributions:
             param_name = dist.params[0]
             contain_array = numpy.ones(len(params[param_name]), dtype=bool)
             # note: enable `__contains__` in `pycbc.distributions.bounded`
             # to handle array-like input, it doesn't work now.
-            for k in params[param_name]:
-                index = numpy.where(params[param_name] == k)[0][0]
+            for index, k in enumerate(params[param_name]):
                 contain_array[index] = {param_name: k} in dist
             result &= numpy.array(contain_array)
         result &= self.within_constraints(params)
         return result
 
     def __call__(self, **params):
         """Evaluate joint distribution for parameters.
```

### Comparing `PyCBC-2.2.0/pycbc/distributions/mass.py` & `PyCBC-2.2.1/pycbc/distributions/mass.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/power_law.py` & `PyCBC-2.2.1/pycbc/distributions/power_law.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/qnm.py` & `PyCBC-2.2.1/pycbc/distributions/qnm.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/spins.py` & `PyCBC-2.2.1/pycbc/distributions/spins.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/uniform.py` & `PyCBC-2.2.1/pycbc/distributions/uniform.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/uniform_log.py` & `PyCBC-2.2.1/pycbc/distributions/uniform_log.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/distributions/utils.py` & `PyCBC-2.2.1/pycbc/distributions/utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/dq.py` & `PyCBC-2.2.1/pycbc/dq.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 gravitational-wave detectors from public sources and/or dqsegdb.
 """
 
 import logging
 import json
 import numpy
 from ligo.segments import segmentlist, segment
-from pycbc.frame.losc import get_run
+from pycbc.frame.gwosc import get_run
 from pycbc.io import get_file
 
 
 def parse_veto_definer(veto_def_filename, ifos):
     """ Parse a veto definer file from the filename and return a dictionary
     indexed by ifo and veto definer category level.
 
@@ -97,15 +97,15 @@
                      'start_pad': start_pad[i],
                      'end_pad': end_pad[i],
                      }
         data[ifo[i]][curr_cat].append(veto_info)
     return data
 
 
-GWOSC_URL = 'https://www.gw-openscience.org/timeline/segments/json/{}/{}_{}/{}/{}/'
+GWOSC_URL = 'https://www.gwosc.org/timeline/segments/json/{}/{}_{}/{}/{}/'
 
 
 def query_dqsegdb2(detector, flag_name, start_time, end_time, server):
     """Utility function for better error reporting when calling dqsegdb2.
     """
     from dqsegdb2.query import query_segments
 
@@ -129,17 +129,17 @@
     """Return the times where the flag is active
 
     Parameters
     ----------
     ifo: string
         The interferometer to query (H1, L1).
     segment_name: string
-        The status flag to query from LOSC.
+        The status flag to query from GWOSC.
     start_time: int
-        The starting gps time to begin querying from LOSC
+        The starting gps time to begin querying from GWOSC
     end_time: int
         The end gps time of the query
     source: str, Optional
         Choice between "GWOSC" or "dqsegdb". If dqsegdb, the server option may
         also be given. The default is to try GWOSC first then try dqsegdb.
     server: str, Optional
         The server path. Only used with dqsegdb atm.
@@ -153,15 +153,15 @@
     ---------
     segments: ligo.segments.segmentlist
         List of segments
     """
     flag_segments = segmentlist([])
 
     if source in ['GWOSC', 'any']:
-        # Special cases as the LOSC convention is backwards from normal
+        # Special cases as the GWOSC convention is backwards from normal
         # LIGO / Virgo operation!!!!
         if (('_HW_INJ' in segment_name and 'NO' not in segment_name) or
                 'VETO' in segment_name):
             data = query_flag(ifo, 'DATA', start_time, end_time)
 
             if '_HW_INJ' in segment_name:
                 name = 'NO_' + segment_name
@@ -249,17 +249,17 @@
 
     Parameters
     ----------
     ifo: string or dict
         The interferometer to query (H1, L1). If a dict, an element for each
         flag name must be provided.
     segment_name: list of strings
-        The status flag to query from LOSC.
+        The status flag to query from GWOSC.
     start_time: int
-        The starting gps time to begin querying from LOSC
+        The starting gps time to begin querying from GWOSC
     end_time: int
         The end gps time of the query
     source: str, Optional
         Choice between "GWOSC" or "dqsegdb". If dqsegdb, the server option may
         also be given. The default is to try GWOSC first then try dqsegdb.
     server: str, Optional
         The server path. Only used with dqsegdb atm.
```

### Comparing `PyCBC-2.2.0/pycbc/events/coherent.py` & `PyCBC-2.2.1/pycbc/events/coherent.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/coinc.py` & `PyCBC-2.2.1/pycbc/events/coinc.py`

 * *Files 1% similar despite different names*

```diff
@@ -365,16 +365,16 @@
     if numpy.isfinite(slide):
         # for a time shifted coinc, time1 is greater than time2 by approximately timeslide_id*slide
         # adding this quantity gives a mean coinc time located around time1
         time = (time1 + time2 + timeslide_id * slide) / 2
     else:
         time = 0.5 * (time2 + time1)
 
-    tslide = timeslide_id.astype(numpy.float128)
-    time = time.astype(numpy.float128)
+    tslide = timeslide_id.astype(numpy.longdouble)
+    time = time.astype(numpy.longdouble)
 
     span = (time.max() - time.min()) + window * 10
     time = time + span * tslide
     logging.info('Clustering events over %s s window', window)
     cidx = cluster_over_time(stat, time, window, **kwargs)
     logging.info('%d triggers remaining', len(cidx))
     return cidx
@@ -420,16 +420,16 @@
 
     # shift all but the pivot ifo by (num_ifos-1) * timeslide_id * slide
     # this leads to a mean coinc time located around pivot time
     if numpy.isfinite(slide):
         nifos_minusone = (num_ifos - numpy.ones_like(num_ifos))
         time_avg = time_avg + (nifos_minusone * timeslide_id * slide)/num_ifos
 
-    tslide = timeslide_id.astype(numpy.float128)
-    time_avg = time_avg.astype(numpy.float128)
+    tslide = timeslide_id.astype(numpy.longdouble)
+    time_avg = time_avg.astype(numpy.longdouble)
 
     span = (time_avg.max() - time_avg.min()) + window * 10
     time_avg = time_avg + span * tslide
     logging.info('Clustering events over %s s window', window)
     cidx = cluster_over_time(stat, time_avg, window, **kwargs)
     logging.info('%d triggers remaining', len(cidx))
 
@@ -993,18 +993,33 @@
     @staticmethod
     def restore_state(filename):
         """Restore state of the background buffers from a file"""
         import pickle
         return pickle.load(filename)
 
     def ifar(self, coinc_stat):
-        """Return the far that would be associated with the coincident given.
+        """Map a given value of the coincident ranking statistic to an inverse
+        false-alarm rate (IFAR) using the interally stored background sample.
+
+        Parameters
+        ----------
+        coinc_stat: float
+            Value of the coincident ranking statistic to be converted.
+
+        Returns
+        -------
+        ifar: float
+            Inverse false-alarm rate in unit of years.
+        ifar_saturated: bool
+            True if `coinc_stat` is larger than all the available background,
+            in which case `ifar` is to be considered an upper limit.
         """
         n = self.coincs.num_greater(coinc_stat)
-        return self.background_time / lal.YRJUL_SI / (n + 1)
+        ifar = self.background_time / lal.YRJUL_SI / (n + 1)
+        return ifar, n == 0
 
     def set_singles_buffer(self, results):
         """Create the singles buffer
 
         This creates the singles buffer for each ifo. The dtype is determined
         by a representative sample of the single triggers in the results.
 
@@ -1230,15 +1245,15 @@
         if len(cstat) > 0:
             offsets = numpy.concatenate(offsets)
             ctime0 = numpy.concatenate(ctimes[self.ifos[0]]).astype(numpy.float64)
             ctime1 = numpy.concatenate(ctimes[self.ifos[1]]).astype(numpy.float64)
             logging.info("Clustering %s coincs", ppdets(self.ifos, "-"))
             cidx = cluster_coincs(cstat, ctime0, ctime1, offsets,
                                   self.timeslide_interval,
-                                  self.analysis_block,
+                                  self.analysis_block + 2*self.time_window,
                                   method='cython')
             offsets = offsets[cidx]
             zerolag_idx = (offsets == 0)
             bkg_idx = (offsets != 0)
 
             for ifo in self.ifos:
                 single_expire[ifo] = numpy.concatenate(single_expire[ifo])
@@ -1250,29 +1265,30 @@
         elif len(valid_ifos) > 0:
             self.coincs.increment(valid_ifos)
 
         # Collect coinc results for saving
         coinc_results = {}
         # Save information about zerolag triggers
         if num_zerolag > 0:
-            zerolag_results = {}
             idx = cidx[zerolag_idx][0]
             zerolag_cstat = cstat[cidx][zerolag_idx]
-            zerolag_results['foreground/ifar'] = self.ifar(zerolag_cstat)
-            zerolag_results['foreground/stat'] = zerolag_cstat
+            ifar, ifar_sat = self.ifar(zerolag_cstat)
+            zerolag_results = {
+                'foreground/ifar': ifar,
+                'foreground/ifar_saturated': ifar_sat,
+                'foreground/stat': zerolag_cstat,
+                'foreground/type': '-'.join(self.ifos)
+            }
             template = template_ids[idx]
             for ifo in self.ifos:
                 trig_id = trigger_ids[ifo][idx]
                 single_data = self.singles[ifo].data(template)[trig_id]
                 for key in single_data.dtype.names:
-                    path = 'foreground/%s/%s' % (ifo, key)
+                    path = f'foreground/{ifo}/{key}'
                     zerolag_results[path] = single_data[key]
-
-            zerolag_results['foreground/type'] = '-'.join(self.ifos)
-
             coinc_results.update(zerolag_results)
 
         # Save some summary statistics about the background
         coinc_results['background/time'] = numpy.array([self.background_time])
         coinc_results['background/count'] = len(self.coincs.data)
 
         # Save all the background triggers
```

### Comparing `PyCBC-2.2.0/pycbc/events/coinc_rate.py` & `PyCBC-2.2.1/pycbc/events/coinc_rate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/cuts.py` & `PyCBC-2.2.1/pycbc/events/cuts.py`

 * *Files 9% similar despite different names*

```diff
@@ -30,20 +30,24 @@
 import copy
 import numpy as np
 from pycbc.events import ranking
 from pycbc.io import hdf
 from pycbc.tmpltbank import bank_conversions as bank_conv
 from pycbc.io import get_chisq_from_file_choice
 
+# Only used to check isinstance:
+from pycbc.io.hdf import ReadByTemplate
+
 # sngl_rank_keys are the allowed names of reweighted SNR functions
 sngl_rank_keys = ranking.sngls_ranking_function_dict.keys()
 
 trigger_param_choices = list(sngl_rank_keys)
 trigger_param_choices += [cc + '_chisq' for cc in hdf.chisq_choices]
-trigger_param_choices += ['end_time', 'psd_var_val', 'sigmasq']
+trigger_param_choices += ['end_time', 'psd_var_val', 'sigmasq',
+                          'sigma_multiple']
 
 template_fit_param_choices = ['fit_by_fit_coeff', 'smoothed_fit_coeff',
                               'fit_by_count_above_thresh',
                               'smoothed_fit_count_above_thresh',
                               'fit_by_count_in_template',
                               'smoothed_fit_count_in_template']
 template_param_choices = bank_conv.conversion_options + \
@@ -184,15 +188,51 @@
     for inputstr in template_cut_strs:
         new_template_cut = convert_inputstr(inputstr, template_param_choices)
         check_update_cuts(template_cut_dict, new_template_cut)
 
     return trigger_cut_dict, template_cut_dict
 
 
-def apply_trigger_cuts(triggers, trigger_cut_dict):
+def sigma_multiple_cut_thresh(template_ids, statistic,
+                              cut_thresh, ifo):
+    """
+    Apply cuts based on a multiple of the median sigma value for the template
+
+    Parameters
+    ----------
+
+    template_ids:
+        template_id values for each of the triggers to be considered,
+        this will be used to associate a sigma threshold for each trigger
+    statistic:
+        A PyCBC ranking statistic instance. Used to get the median_sigma
+        value for the cuts. If fits_by_tid does not exist for the specified
+        ifo (where median_sigma lives), an error will be raised.
+    ifo:
+        The IFO for which we want to read median_sigma
+    cut_thresh: int or float
+        The multiple of median_sigma to compare triggers to
+
+    Returns
+    -------
+    idx_out: numpy array
+        An array of the indices of triggers which meet the criteria
+        set by the dictionary
+    """
+    statistic_classname = statistic.__class__.__name__
+    if not hasattr(statistic, 'fits_by_tid'):
+        raise ValueError("Cut parameter 'sigma_muliple' cannot "
+                         "be used when the ranking statistic " +
+                         statistic_classname + " does not use "
+                         "template fitting.")
+    tid_med_sigma = statistic.fits_by_tid[ifo]['median_sigma']
+    return cut_thresh * tid_med_sigma[template_ids]
+
+
+def apply_trigger_cuts(triggers, trigger_cut_dict, statistic=None):
     """
     Fetch/Calculate the parameter for triggers, and then
     apply the cuts defined in template_cut_dict
 
     Parameters
     ----------
     triggers: ReadByTemplate object or dictionary
@@ -201,14 +241,15 @@
         the values we cut on.
 
     trigger_cut_dict: dictionary
         Dictionary with tuples of (parameter, cut_function)
         as keys, cut_thresholds as values
         made using ingest_cuts_option_group function
 
+
     Returns
     -------
     idx_out: numpy array
         An array of the indices which meet the criteria
         set by the dictionary
     """
     idx_out = np.arange(len(triggers['snr']))
@@ -222,19 +263,37 @@
         if parameter.endswith('_chisq'):
             # parameter is a chisq-type thing
             chisq_choice = parameter.split('_')[0]
             # Currently calculated for all triggers - this seems inefficient
             value = get_chisq_from_file_choice(triggers, chisq_choice)
             # Apply any previous cuts to the value for comparison
             value = value[idx_out]
+        elif parameter == "sigma_multiple":
+            if isinstance(triggers, ReadByTemplate):
+                ifo_grp = triggers.file[triggers.ifo]
+                value = np.sqrt(ifo_grp['sigmasq'][idx_out])
+                template_ids = ifo_grp['template_id'][idx_out]
+                # Get a cut threshold value, this will be different
+                # depending on the template ID, so we rewrite cut_thresh
+                # as a value for each trigger, numpy comparison functions
+                # allow this
+                cut_thresh = sigma_multiple_cut_thresh(template_ids,
+                                                       statistic,
+                                                       cut_thresh,
+                                                       triggers.ifo)
+            else:
+                err_msg = "Cuts on 'sigma_multiple' are only implemented for "
+                err_msg += "triggers in a ReadByTemplate format. This code "
+                err_msg += f"uses a {type(triggers).__name__} format."
+                raise NotImplementedError(err_msg)
         elif ((not hasattr(triggers, "file") and parameter in triggers)
                 or (hasattr(triggers, "file")
                     and parameter in triggers.file[triggers.ifo])):
             # parameter can be read direct from the trigger dictionary / file
-            if parameter in triggers:
+            if not hasattr(triggers, 'file') and parameter in triggers:
                 value = triggers[parameter]
             else:
                 value = triggers.file[triggers.ifo][parameter]
             # Apply any previous cuts to the value for comparison
             value = value[idx_out]
         elif parameter in sngl_rank_keys:
             # parameter is a newsnr-type thing
@@ -266,15 +325,15 @@
         template fit cuts will be skipped. If a fit cut has been specified
         and fits_by_tid does not exist for all ifos, an error will be raised.
 
     ifos: list of strings
         List of IFOS used in this findtrigs instance.
         Templates must pass cuts in all IFOs.
 
-    parameter_cut_function: thresh
+    parameter_cut_function: tuple
         First entry: Which parameter is being used for the cut?
         Second entry: Cut function
 
     cut_thresh: float or int
         Cut threshold to the parameter according to the cut function
 
     template_ids: numpy array
```

### Comparing `PyCBC-2.2.0/pycbc/events/eventmgr.py` & `PyCBC-2.2.1/pycbc/events/eventmgr.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/eventmgr_cython.pyx` & `PyCBC-2.2.1/pycbc/events/eventmgr_cython.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/ranking.py` & `PyCBC-2.2.1/pycbc/events/ranking.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/significance.py` & `PyCBC-2.2.1/pycbc/events/significance.py`

 * *Files 1% similar despite different names*

```diff
@@ -155,18 +155,18 @@
 
     # Count the number of below-threshold background events louder than the
     # bg and foreground
     back_cnum[bg_below], fnlouder[fg_below] = \
         count_n_louder(back_stat[bg_below], fore_stat[fg_below], dec_facs)
 
     # As we have only counted the louder below-threshold events, need to
-    # add the above threshold events, whcih by definition are louder than
+    # add the above threshold events, which by definition are louder than
     # all the below-threshold events
-    back_cnum += n_above
-    fnlouder += n_above
+    back_cnum[bg_below] += n_above
+    fnlouder[fg_below] += n_above
 
     return back_cnum, fnlouder
 
 
 _significance_meth_dict = {
     'trigger_fit': n_louder_from_fit,
     'n_louder': count_n_louder
```

### Comparing `PyCBC-2.2.0/pycbc/events/simd_threshold_ccode.cpp` & `PyCBC-2.2.1/pycbc/events/simd_threshold_ccode.cpp`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/simd_threshold_cython.pyx` & `PyCBC-2.2.1/pycbc/events/simd_threshold_cython.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/stat.py` & `PyCBC-2.2.1/pycbc/events/stat.py`

 * *Files 1% similar despite different names*

```diff
@@ -117,15 +117,16 @@
         numpy.ndarray
             The array of single detector values
         """
         err_msg = "This function is a stub that should be overridden by the "
         err_msg += "sub-classes. You shouldn't be seeing this error!"
         raise NotImplementedError(err_msg)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for a single detector candidate
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
@@ -201,15 +202,16 @@
         Returns
         -------
         numpy.ndarray
             The array of single detector values
         """
         return self.get_sngl_ranking(trigs)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for a single detector candidate
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
@@ -654,15 +656,16 @@
         singles['snglstat'] = sngl_stat
         singles['coa_phase'] = trigs['coa_phase'][:]
         singles['end_time'] = trigs['end_time'][:]
         singles['sigmasq'] = trigs['sigmasq'][:]
         singles['snr'] = trigs['snr'][:]
         return numpy.array(singles, ndmin=1)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for a single detector candidate
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
@@ -896,15 +899,16 @@
         -------
         numpy.ndarray
             The array of single detector values
         """
 
         return self.lognoiserate(trigs)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for a single detector candidate
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
@@ -1035,15 +1039,16 @@
         _, _, thresh = self.find_fits(trigs)
         # shift by log of reference slope alpha
         logr_n += -1. * numpy.log(self.alpharef)
         # add threshold and rescale by reference slope
         stat = thresh - (logr_n / self.alpharef)
         return numpy.array(stat, ndmin=1, dtype=numpy.float32)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for single detector candidates
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
@@ -1166,15 +1171,16 @@
         singles['snglstat'] = sngl_stat
         singles['coa_phase'] = trigs['coa_phase'][:]
         singles['end_time'] = trigs['end_time'][:]
         singles['sigmasq'] = trigs['sigmasq'][:]
         singles['snr'] = trigs['snr'][:]
         return numpy.array(singles, ndmin=1)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for a single detector candidate
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
@@ -1484,15 +1490,16 @@
             # Should only be one ifo fit file provided
             assert len(self.ifos) == 1
         # Store benchmark log volume as single-ifo information since the coinc
         # method does not have access to template id
         singles['benchmark_logvol'] = self.benchmark_logvol[tnum]
         return numpy.array(singles, ndmin=1)
 
-    def rank_stat_single(self, single_info):
+    def rank_stat_single(self, single_info,
+                         **kwargs): # pylint:disable=unused-argument
         """
         Calculate the statistic for single detector candidates
 
         Parameters
         ----------
         single_info: tuple
             Tuple containing two values. The first is the ifo (str) and the
```

### Comparing `PyCBC-2.2.0/pycbc/events/threshold_cpu.py` & `PyCBC-2.2.1/pycbc/events/threshold_cpu.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/threshold_cuda.py` & `PyCBC-2.2.1/pycbc/events/threshold_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/trigger_fits.py` & `PyCBC-2.2.1/pycbc/events/trigger_fits.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/triggers.py` & `PyCBC-2.2.1/pycbc/events/triggers.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/events/veto.py` & `PyCBC-2.2.1/pycbc/events/veto.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/__init__.py` & `PyCBC-2.2.1/pycbc/fft/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/backend_cpu.py` & `PyCBC-2.2.1/pycbc/fft/backend_cpu.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 #  MA  02111-1307  USA
 
 from .core import _list_available
 
 _backend_dict = {'fftw' : 'fftw',
                  'mkl' : 'mkl',
                  'numpy' : 'npfft'}
-_backend_list = ['fftw','mkl','numpy']
+_backend_list = ['mkl', 'fftw', 'numpy']
 
 _alist, _adict = _list_available(_backend_list, _backend_dict)
 
 cpu_backend = None
 
 def set_backend(backend_list):
     global cpu_backend
```

### Comparing `PyCBC-2.2.0/pycbc/fft/backend_cuda.py` & `PyCBC-2.2.1/pycbc/fft/backend_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/backend_mkl.py` & `PyCBC-2.2.1/pycbc/fft/backend_mkl.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/backend_support.py` & `PyCBC-2.2.1/pycbc/fft/backend_support.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/class_api.py` & `PyCBC-2.2.1/pycbc/fft/class_api.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/core.py` & `PyCBC-2.2.1/pycbc/fft/core.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/cuda_pyfft.py` & `PyCBC-2.2.1/pycbc/fft/cuda_pyfft.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/cufft.py` & `PyCBC-2.2.1/pycbc/fft/cufft.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/fft_callback.py` & `PyCBC-2.2.1/pycbc/fft/fft_callback.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/fftw.py` & `PyCBC-2.2.1/pycbc/fft/fftw.py`

 * *Files 1% similar despite different names*

```diff
@@ -102,14 +102,18 @@
     try:
         double_threaded_libname = _fftw_threading_libnames[backend]['double']
         float_threaded_libname =  _fftw_threading_libnames[backend]['float']
     except KeyError:
         raise ValueError("Backend {0} for FFTW threading does not exist!".format(backend))
     if double_threaded_libname is not None:
         try:
+            # For reasons Ian doesn't understand we should not load libgomp
+            # first using RTLD_DEEPBIND, so force loading it here if needed
+            if backend == 'openmp':
+                get_ctypes_library('gomp', [], mode=ctypes.DEFAULT_MODE)
             # Note that the threaded libraries don't have their own pkg-config
             # files we must look for them wherever we look for double or single
             # FFTW itself.
             _double_threaded_lib = get_ctypes_library(
                 double_threaded_libname,
                 ['fftw3'],
                 mode=FFTW_RTLD_MODE
@@ -148,16 +152,17 @@
     # then it cycles in order through threaded backends,
     if backend is not None:
         retval = _init_threads(backend)
         # Since the user specified this backend raise an exception if the above failed
         if retval != 0:
             raise RuntimeError("Could not initialize FFTW threading backend {0}".format(backend))
     else:
-        # Note that we pop() from the end, so 'openmp' is the first thing tried
-        _backend_list = ['unthreaded','pthreads','openmp']
+        # Note that we pop() from the end, so 'pthreads'
+        # is the first thing tried
+        _backend_list = ['unthreaded','openmp', 'pthreads']
         while not _fftw_threaded_set:
             _next_backend = _backend_list.pop()
             retval = _init_threads(_next_backend)
 
 # Function to import system-wide wisdom files.
 
 def import_sys_wisdom():
```

### Comparing `PyCBC-2.2.0/pycbc/fft/fftw_pruned.py` & `PyCBC-2.2.1/pycbc/fft/fftw_pruned.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/fftw_pruned_cython.pyx` & `PyCBC-2.2.1/pycbc/fft/fftw_pruned_cython.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/func_api.py` & `PyCBC-2.2.1/pycbc/fft/func_api.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/mkl.py` & `PyCBC-2.2.1/pycbc/fft/mkl.py`

 * *Files 1% similar despite different names*

```diff
@@ -65,16 +65,16 @@
              }
 
 def check_status(status):
     """ Check the status of a mkl functions and raise a python exeption if
     there is an error.
     """
     if status:
+        lib.DftiErrorMessage.restype = ctypes.c_char_p
         msg = lib.DftiErrorMessage(status)
-        msg = ctypes.c_char_p(msg).value
         raise RuntimeError(msg)
 
 def create_descriptor(size, idtype, odtype, inplace):
     invec = zeros(1, dtype=idtype)
     outvec = zeros(1, dtype=odtype)
 
     desc = ctypes.c_void_p(1)
```

### Comparing `PyCBC-2.2.0/pycbc/fft/npfft.py` & `PyCBC-2.2.1/pycbc/fft/npfft.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/fft/parser_support.py` & `PyCBC-2.2.1/pycbc/fft/parser_support.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/autocorrelation.py` & `PyCBC-2.2.1/pycbc/filter/autocorrelation.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/fotonfilter.py` & `PyCBC-2.2.1/pycbc/filter/fotonfilter.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/matchedfilter.py` & `PyCBC-2.2.1/pycbc/filter/matchedfilter.py`

 * *Files 1% similar despite different names*

```diff
@@ -1795,30 +1795,35 @@
 
         return result, veto_info
 
 def followup_event_significance(ifo, data_reader, bank,
                                 template_id, coinc_times,
                                 coinc_threshold=0.005,
                                 lookback=150, duration=0.095):
-    """ Followup an event in another detector and determine its significance
+    """Given a detector, a template waveform and a set of candidate event
+    times in different detectors, perform an on-source/off-source analysis
+    to determine if the SNR in the first detector has a significant peak
+    in the on-source window. The significance is given in terms of a
+    p-value. See Dal Canton et al. 2021 (https://arxiv.org/abs/2008.07494)
+    for details.
     """
     from pycbc.waveform import get_waveform_filter_length_in_time
     tmplt = bank.table[template_id]
     length_in_time = get_waveform_filter_length_in_time(tmplt['approximant'],
                                                         tmplt)
 
     # calculate onsource time range
     from pycbc.detector import Detector
     onsource_start = -numpy.inf
     onsource_end = numpy.inf
     fdet = Detector(ifo)
 
     for cifo in coinc_times:
         time = coinc_times[cifo]
-        dtravel =  Detector(cifo).light_travel_time_to_detector(fdet)
+        dtravel = Detector(cifo).light_travel_time_to_detector(fdet)
         if time - dtravel > onsource_start:
             onsource_start = time - dtravel
         if time + dtravel < onsource_end:
             onsource_end = time + dtravel
 
     # Source must be within this time window to be considered a possible
     # coincidence
@@ -1832,23 +1837,23 @@
         bdur = data_reader.strain.duration * .75
 
     # Require all strain be valid within lookback time
     if data_reader.state is not None:
         state_start_time = data_reader.strain.end_time \
                 - data_reader.reduced_pad * data_reader.strain.delta_t - bdur
         if not data_reader.state.is_extent_valid(state_start_time, bdur):
-            return None, None, None, None
+            return None
 
     # We won't require that all DQ checks be valid for now, except at
     # onsource time.
     if data_reader.dq is not None:
         dq_start_time = onsource_start - duration / 2.0
         dq_duration = onsource_end - onsource_start + duration
         if not data_reader.dq.is_extent_valid(dq_start_time, dq_duration):
-            return None, None, None, None
+            return None
 
     # Calculate SNR time series for this duration
     htilde = bank.get_template(template_id, min_buffer=bdur)
     stilde = data_reader.overwhitened_data(htilde.delta_f)
 
     sigma2 = htilde.sigmasq(stilde.psd)
     snr, _, norm = matched_filter_core(htilde, stilde, h_norm=sigma2)
@@ -1862,27 +1867,35 @@
     bstart = float(snr.start_time) + length_in_time + trim_pad
     bkg = abs(snr.time_slice(bstart, onsource_start)).numpy()
 
     window = int((onsource_end - onsource_start) * snr.sample_rate)
     nsamples = int(len(bkg) / window)
 
     peaks = bkg[:nsamples*window].reshape(nsamples, window).max(axis=1)
-    pvalue = (1 + (peaks >= peak_value).sum()) / float(1 + nsamples)
+    num_louder_bg = (peaks >= peak_value).sum()
+    pvalue = (1 + num_louder_bg) / float(1 + nsamples)
+    pvalue_saturated = num_louder_bg == 0
 
     # Return recentered source SNR for bayestar, along with p-value, and trig
     peak_full = int((peak_time - snr.start_time) / snr.delta_t)
     half_dur_samples = int(snr.sample_rate * duration / 2)
     snr_slice = slice(peak_full - half_dur_samples,
                       peak_full + half_dur_samples + 1)
     baysnr = snr[snr_slice]
 
     logging.info('Adding %s to candidate, pvalue %s, %s samples', ifo,
                  pvalue, nsamples)
 
-    return baysnr * norm, peak_time, pvalue, sigma2
+    return {
+        'snr_series': baysnr * norm,
+        'peak_time': peak_time,
+        'pvalue': pvalue,
+        'pvalue_saturated': pvalue_saturated,
+        'sigma2': sigma2
+    }
 
 def compute_followup_snr_series(data_reader, htilde, trig_time,
                                 duration=0.095, check_state=True,
                                 coinc_window=0.05):
     """Given a StrainBuffer, a template frequency series and a trigger time,
     compute a portion of the SNR time series centered on the trigger for its
     rapid sky localization and followup.
```

### Comparing `PyCBC-2.2.0/pycbc/filter/matchedfilter_cpu.pyx` & `PyCBC-2.2.1/pycbc/filter/matchedfilter_cpu.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/matchedfilter_cuda.py` & `PyCBC-2.2.1/pycbc/filter/matchedfilter_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/matchedfilter_numpy.py` & `PyCBC-2.2.1/pycbc/filter/matchedfilter_numpy.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/qtransform.py` & `PyCBC-2.2.1/pycbc/filter/qtransform.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/resample.py` & `PyCBC-2.2.1/pycbc/filter/resample.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/simd_correlate.py` & `PyCBC-2.2.1/pycbc/filter/simd_correlate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/simd_correlate_ccode.cpp` & `PyCBC-2.2.1/pycbc/filter/simd_correlate_ccode.cpp`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/simd_correlate_cython.pyx` & `PyCBC-2.2.1/pycbc/filter/simd_correlate_cython.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/filter/zpk.py` & `PyCBC-2.2.1/pycbc/filter/zpk.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/frame/__init__.py` & `PyCBC-2.2.1/pycbc/frame/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from . frame import (locations_to_cache, read_frame, datafind_connection,
+from . frame import (locations_to_cache, read_frame,
                      query_and_read_frame, frame_paths, write_frame,
                      DataBuffer, StatusBuffer, iDQBuffer)
 
 from . store import (read_store)
 
 
 # Status flags for the calibration state vector
```

### Comparing `PyCBC-2.2.0/pycbc/frame/frame.py` & `PyCBC-2.2.1/pycbc/frame/frame.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 """
 
 import lalframe, logging
 import lal
 import numpy
 import math
 import os.path, glob, time
-import gwdatafind
+from gwdatafind import find_urls as find_frame_urls
 import pycbc
 from urllib.parse import urlparse
 from pycbc.types import TimeSeries, zeros
 
 
 # map LAL series types to corresponding functions and Numpy types
 _fr_type_map = {
@@ -252,30 +252,14 @@
             channel_data = _read_channel(channel, stream, start_time, duration)
             lalframe.FrStreamSeek(stream, start_time)
             all_data.append(channel_data)
         return all_data
     else:
         return _read_channel(channels, stream, start_time, duration)
 
-def datafind_connection(server=None):
-    """ Return a connection to the datafind server
-
-    Parameters
-    -----------
-    server : {SERVER:PORT, string}, optional
-       A string representation of the server and port.
-       The port may be ommitted.
-
-    Returns
-    --------
-    connection
-        The open connection to the datafind server.
-    """
-    return gwdatafind.connect(host=server)
-
 def frame_paths(frame_type, start_time, end_time, server=None, url_type='file'):
     """Return the paths to a span of frame files
 
     Parameters
     ----------
     frame_type : string
         The string representation of the frame type (ex. 'H1_ER_C00_L1')
@@ -284,30 +268,28 @@
     end_time : int
         The end time that we need the frames to span.
     server : {None, SERVER:PORT string}, optional
         Optional string to specify the datafind server to use. By default an
         attempt is made to use a local datafind server.
     url_type : string
         Returns only frame URLs with a particular scheme or head such
-        as "file" or "gsiftp". Default is "file", which queries locally
+        as "file" or "https". Default is "file", which queries locally
         stored frames. Option can be disabled if set to None.
     Returns
     -------
     paths : list of paths
         The list of paths to the frame files.
 
     Examples
     --------
     >>> paths = frame_paths('H1_LDAS_C02_L2', 968995968, 968995968+2048)
     """
     site = frame_type[0]
-    connection = datafind_connection(server)
-    connection.find_times(site, frame_type,
-                          gpsstart=start_time, gpsend=end_time)
-    cache = connection.find_frame_urls(site, frame_type, start_time, end_time,urltype=url_type)
+    cache = find_frame_urls(site, frame_type, start_time, end_time,
+                            urltype=url_type, host=server)
     return [urlparse(entry).path for entry in cache]
 
 def query_and_read_frame(frame_type, channels, start_time, end_time,
                          sieve=None, check_integrity=False):
     """Read time series from frame data.
 
     Query for the locatin of physical frames matching the frame type. Return
@@ -340,36 +322,35 @@
     Examples
     --------
     >>> ts = query_and_read_frame('H1_LDAS_C02_L2', 'H1:LDAS-STRAIN',
     >>>                               968995968, 968995968+2048)
     """
     # Allows compatibility with our standard tools
     # We may want to place this into a higher level frame getting tool
-    if frame_type == 'LOSC_STRAIN':
-        from pycbc.frame.losc import read_strain_losc
+    if frame_type in ['LOSC_STRAIN', 'GWOSC_STRAIN']:
+        from pycbc.frame.gwosc import read_strain_gwosc
         if not isinstance(channels, list):
             channels = [channels]
-        data = [read_strain_losc(c[:2], start_time, end_time)
+        data = [read_strain_gwosc(c[:2], start_time, end_time)
                 for c in channels]
         return data if len(data) > 1 else data[0]
-    if frame_type == 'LOSC':
-        from pycbc.frame.losc import read_frame_losc
-        return read_frame_losc(channels, start_time, end_time)
+    if frame_type in ['LOSC', 'GWOSC']:
+        from pycbc.frame.gwosc import read_frame_gwosc
+        return read_frame_gwosc(channels, start_time, end_time)
 
     logging.info('querying datafind server')
     paths = frame_paths(frame_type, int(start_time), int(numpy.ceil(end_time)))
     logging.info('found files: %s' % (' '.join(paths)))
     return read_frame(paths, channels,
                       start_time=start_time,
                       end_time=end_time,
                       sieve=sieve,
                       check_integrity=check_integrity)
 
 __all__ = ['read_frame', 'frame_paths',
-           'datafind_connection',
            'query_and_read_frame']
 
 def write_frame(location, channels, timeseries):
     """Write a list of time series to a single frame file.
 
     Parameters
     ----------
```

### Comparing `PyCBC-2.2.0/pycbc/frame/losc.py` & `PyCBC-2.2.1/pycbc/frame/gwosc.py`

 * *Files 26% similar despite different names*

```diff
@@ -10,174 +10,176 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Generals
 # Public License for more details.
 #
 # You should have received a copy of the GNU General Public License along
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 """
-This modules contains functions for getting data from the LOSC
+This modules contains functions for getting data from the Gravitational Wave
+Open Science Center (GWOSC).
 """
+import json
+from urllib.request import urlopen
 from pycbc.io import get_file
+from pycbc.frame import read_frame
+
+
+_GWOSC_URL = "https://www.gwosc.org/archive/links/%s/%s/%s/%s/json/"
 
-_losc_url = "https://www.gw-openscience.org/archive/links/%s/%s/%s/%s/json/"
 
 def get_run(time, ifo=None):
-    """ Return the run name for a given time
+    """Return the run name for a given time.
 
     Parameters
     ----------
     time: int
-        The gps time
+        The GPS time.
     ifo: str
-        The ifo prefix string. Optional and normally unused,
-        except for some special times where data releases
-        were made for a single detector under
-        unusual circumstances. For example, to get the data around GW170608
-        in the Hanford detector.
+        The interferometer prefix string. Optional and normally unused,
+        except for some special times where data releases were made for a
+        single detector under unusual circumstances. For example, to get
+        the data around GW170608 in the Hanford detector.
     """
+    cases = [
+        (
+            # ifo is only needed in this special case, otherwise,
+            # the run name is the same for all ifos
+            1180911618 <= time <= 1180982427 and ifo == 'H1',
+            'BKGW170608_16KHZ_R1'
+        ),
+        (1253977219 <= time <= 1320363336, 'O3b_16KHZ_R1'),
+        (1238166018 <= time <= 1253977218, 'O3a_16KHZ_R1'),
+        (1164556817 <= time <= 1187733618, 'O2_16KHZ_R1'),
+        (1126051217 <= time <= 1137254417, 'O1'),
+        (815011213 <= time <= 875318414, 'S5'),
+        (930787215 <= time <= 971568015, 'S6')
+    ]
+    for condition, name in cases:
+        if condition:
+            return name
+    raise ValueError(f'Time {time} not available in a public dataset')
 
-    # ifo is only needed in this special case, otherwise, the run name is
-    # the same for all ifos
-    if (1180911618 <= time <= 1180982427) and (ifo == 'H1'):
-        return 'BKGW170608_16KHZ_R1'
-    elif 1253977219 <= time <= 1320363336:
-        return 'O3b_16KHZ_R1'
-    elif 1238166018 <= time <= 1253977218:
-        return 'O3a_16KHZ_R1'
-    elif 1164556817 <= time <= 1187733618:
-        return 'O2_16KHZ_R1'
-    elif 1126051217 <= time <= 1137254417:
-        return 'O1'
-    elif 815011213 <= time <= 875318414:
-        return 'S5'
-    elif 930787215 <= time <= 971568015:
-        return 'S6'
-    else:
-        raise ValueError('Time %s not available in a public dataset' % time)
 
 def _get_channel(time):
     if time < 1164556817:
         return 'LOSC-STRAIN'
-    else:
-        return 'GWOSC-16KHZ_R1_STRAIN'
+    return 'GWOSC-16KHZ_R1_STRAIN'
 
-def losc_frame_json(ifo, start_time, end_time):
-    """ Get the information about the public data files in a duration of time
+
+def gwosc_frame_json(ifo, start_time, end_time):
+    """Get the information about the public data files in a duration of time.
 
     Parameters
     ----------
     ifo: str
-        The name of the IFO to find the information about.
+        The name of the interferometer to find the information about.
     start_time: int
-        The gps time in GPS seconds
+        The start time in GPS seconds.
     end_time: int
-        The end time in GPS seconds
+        The end time in GPS seconds.
 
     Returns
     -------
     info: dict
         A dictionary containing information about the files that span the
         requested times.
     """
-    import json
-    try:
-        from urllib.request import urlopen
-    except ImportError:  # python < 3
-        from urllib import urlopen
     run = get_run(start_time)
     run2 = get_run(end_time)
     if run != run2:
-        raise ValueError('Spanning multiple runs is not currently supported.'
-                         'You have requested data that uses '
-                         'both %s and %s' % (run, run2))
+        raise ValueError(
+            'Spanning multiple runs is not currently supported. '
+            f'You have requested data that uses both {run} and {run2}'
+        )
 
-    url = _losc_url % (run, ifo, int(start_time), int(end_time))
+    url = _GWOSC_URL % (run, ifo, int(start_time), int(end_time))
 
     try:
         return json.loads(urlopen(url).read().decode())
-    except Exception as e:
-        print(e)
-        raise ValueError('Failed to find gwf files for '
-            'ifo=%s, run=%s, between %s-%s' % (ifo, run, start_time, end_time))
+    except Exception as exc:
+        msg = ('Failed to find gwf files for '
+               f'ifo={ifo}, run={run}, between {start_time}-{end_time}')
+        raise ValueError(msg) from exc
 
-def losc_frame_urls(ifo, start_time, end_time):
-    """ Get a list of urls to losc frame files
+
+def gwosc_frame_urls(ifo, start_time, end_time):
+    """Get a list of URLs to GWOSC frame files.
 
     Parameters
     ----------
     ifo: str
-        The name of the IFO to find the information about.
+        The name of the interferometer to find the information about.
     start_time: int
-        The gps time in GPS seconds
+        The start time in GPS seconds.
     end_time: int
-        The end time in GPS seconds
+        The end time in GPS seconds.
 
     Returns
     -------
     frame_files: list
         A dictionary containing information about the files that span the
         requested times.
     """
-    data = losc_frame_json(ifo, start_time, end_time)['strain']
+    data = gwosc_frame_json(ifo, start_time, end_time)['strain']
     return [d['url'] for d in data if d['format'] == 'gwf']
 
-def read_frame_losc(channels, start_time, end_time):
-    """ Read channels from losc data
+
+def read_frame_gwosc(channels, start_time, end_time):
+    """Read channels from GWOSC data.
 
     Parameters
     ----------
     channels: str or list
         The channel name to read or list of channel names.
     start_time: int
-        The gps time in GPS seconds
+        The start time in GPS seconds.
     end_time: int
-        The end time in GPS seconds
+        The end time in GPS seconds.
 
     Returns
     -------
     ts: TimeSeries
         Returns a timeseries or list of timeseries with the requested data.
     """
-    from pycbc.frame import read_frame
     if not isinstance(channels, list):
         channels = [channels]
     ifos = [c[0:2] for c in channels]
     urls = {}
     for ifo in ifos:
-        urls[ifo] = losc_frame_urls(ifo, start_time, end_time)
+        urls[ifo] = gwosc_frame_urls(ifo, start_time, end_time)
         if len(urls[ifo]) == 0:
             raise ValueError("No data found for %s so we "
                              "can't produce a time series" % ifo)
 
-    fnames = {ifo:[] for ifo in ifos}
+    fnames = {ifo: [] for ifo in ifos}
     for ifo in ifos:
         for url in urls[ifo]:
             fname = get_file(url, cache=True)
             fnames[ifo].append(fname)
 
-    ts = [read_frame(fnames[channel[0:2]], channel,
-           start_time=start_time, end_time=end_time) for channel in channels]
-    if len(ts) == 1:
-        return ts[0]
-    else:
-        return ts
+    ts_list = [read_frame(fnames[channel[0:2]], channel,
+                          start_time=start_time, end_time=end_time)
+               for channel in channels]
+    if len(ts_list) == 1:
+        return ts_list[0]
+    return ts_list
 
-def read_strain_losc(ifo, start_time, end_time):
-    """ Get the strain data from the LOSC data
+
+def read_strain_gwosc(ifo, start_time, end_time):
+    """Get the strain data from the GWOSC data.
 
     Parameters
     ----------
     ifo: str
-        The name of the IFO to read data for. Ex. 'H1', 'L1', 'V1'
+        The name of the interferometer to read data for. Ex. 'H1', 'L1', 'V1'.
     start_time: int
-        The gps time in GPS seconds
+        The start time in GPS seconds.
     end_time: int
-        The end time in GPS seconds
+        The end time in GPS seconds.
 
     Returns
     -------
     ts: TimeSeries
         Returns a timeseries with the strain data.
     """
     channel = _get_channel(start_time)
-    return read_frame_losc('%s:%s' % (ifo, channel), start_time, end_time)
-
+    return read_frame_gwosc(f'{ifo}:{channel}', start_time, end_time)
```

### Comparing `PyCBC-2.2.0/pycbc/frame/store.py` & `PyCBC-2.2.1/pycbc/frame/store.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/burn_in.py` & `PyCBC-2.2.1/pycbc/inference/burn_in.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/entropy.py` & `PyCBC-2.2.1/pycbc/inference/entropy.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/evidence.py` & `PyCBC-2.2.1/pycbc/inference/evidence.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/gelman_rubin.py` & `PyCBC-2.2.1/pycbc/inference/gelman_rubin.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/geweke.py` & `PyCBC-2.2.1/pycbc/inference/geweke.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/__init__.py` & `PyCBC-2.2.1/pycbc/inference/io/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/base_hdf.py` & `PyCBC-2.2.1/pycbc/inference/io/base_hdf.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/base_mcmc.py` & `PyCBC-2.2.1/pycbc/inference/io/base_mcmc.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/base_multitemper.py` & `PyCBC-2.2.1/pycbc/inference/io/base_multitemper.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/base_nested_sampler.py` & `PyCBC-2.2.1/pycbc/inference/io/base_nested_sampler.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/base_sampler.py` & `PyCBC-2.2.1/pycbc/inference/io/base_sampler.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/cpnest.py` & `PyCBC-2.2.1/pycbc/inference/io/cpnest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/dynesty.py` & `PyCBC-2.2.1/pycbc/inference/io/dynesty.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/emcee.py` & `PyCBC-2.2.1/pycbc/inference/io/emcee.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/emcee_pt.py` & `PyCBC-2.2.1/pycbc/inference/io/emcee_pt.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/epsie.py` & `PyCBC-2.2.1/pycbc/inference/io/epsie.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/multinest.py` & `PyCBC-2.2.1/pycbc/inference/io/multinest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/posterior.py` & `PyCBC-2.2.1/pycbc/inference/io/posterior.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/ptemcee.py` & `PyCBC-2.2.1/pycbc/inference/io/ptemcee.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/txt.py` & `PyCBC-2.2.1/pycbc/inference/io/txt.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/io/ultranest.py` & `PyCBC-2.2.1/pycbc/inference/io/ultranest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/jump/__init__.py` & `PyCBC-2.2.1/pycbc/inference/jump/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/jump/angular.py` & `PyCBC-2.2.1/pycbc/inference/jump/angular.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/jump/bounded_normal.py` & `PyCBC-2.2.1/pycbc/inference/jump/bounded_normal.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/jump/discrete.py` & `PyCBC-2.2.1/pycbc/inference/jump/discrete.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/jump/normal.py` & `PyCBC-2.2.1/pycbc/inference/jump/normal.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/__init__.py` & `PyCBC-2.2.1/pycbc/inference/models/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 from .marginalized_gaussian_noise import MarginalizedPolarization
 from .marginalized_gaussian_noise import MarginalizedHMPolPhase
 from .marginalized_gaussian_noise import MarginalizedTime
 from .brute_marg import BruteParallelGaussianMarginalize
 from .brute_marg import BruteLISASkyModesMarginalize
 from .gated_gaussian_noise import (GatedGaussianNoise, GatedGaussianMargPol)
 from .single_template import SingleTemplate
-from .relbin import Relative
+from .relbin import Relative, RelativeTime, RelativeTimeDom
 from .hierarchical import HierarchicalModel, MultiSignalModel
 
 
 # Used to manage a model instance across multiple cores or MPI
 _global_instance = None
 
 
@@ -199,16 +199,18 @@
     MarginalizedTime,
     BruteParallelGaussianMarginalize,
     BruteLISASkyModesMarginalize,
     GatedGaussianNoise,
     GatedGaussianMargPol,
     SingleTemplate,
     Relative,
+    RelativeTime,
     HierarchicalModel,
     MultiSignalModel,
+    RelativeTimeDom,
 )}
 
 
 class _ModelManager(dict):
     """Sub-classes dictionary to manage the collection of available models.
 
     The first time this is called, any plugin models that are available will be
```

### Comparing `PyCBC-2.2.0/pycbc/inference/models/analytic.py` & `PyCBC-2.2.1/pycbc/inference/models/analytic.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/base.py` & `PyCBC-2.2.1/pycbc/inference/models/base.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/base_data.py` & `PyCBC-2.2.1/pycbc/inference/models/base_data.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/brute_marg.py` & `PyCBC-2.2.1/pycbc/inference/models/brute_marg.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/data_utils.py` & `PyCBC-2.2.1/pycbc/inference/models/data_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/gated_gaussian_noise.py` & `PyCBC-2.2.1/pycbc/inference/models/gated_gaussian_noise.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/gaussian_noise.py` & `PyCBC-2.2.1/pycbc/inference/models/gaussian_noise.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/models/hierarchical.py` & `PyCBC-2.2.1/pycbc/inference/models/hierarchical.py`

 * *Files 3% similar despite different names*

```diff
@@ -213,14 +213,22 @@
         if group is None or group == '/':
             prefix = ''
         else:
             prefix = group+'/'
         for lbl, model in self.submodels.items():
             model.write_metadata(fp, group=prefix+lbl)
 
+        # if all submodels support it, write a combined lognl parameter
+        try:
+            sampattrs = fp.getattrs(group=fp.samples_group)
+            lognl = [self.submodels[k].lognl for k in self.submodels]
+            sampattrs['{}lognl'.format(prefix)] = sum(lognl)
+        except AttributeError:
+            pass
+
     @classmethod
     def from_config(cls, cp, **kwargs):
         r"""Initializes an instance of this class from the given config file.
 
         Sub-models are initialized before initializing this class. The model
         section must have a ``submodels`` argument that lists the names of all
         the submodels to generate as a space-separated list. Each sub-model
@@ -544,14 +552,41 @@
                                "for the MultiSignal model isn't supported by"
                                "any of the constituent models.".format(ctypes))
 
         self.other_models = self.submodels.copy()
         self.other_models.pop(self.primary_model)
         self.other_models = list(self.other_models.values())
 
+    def write_metadata(self, fp, group=None):
+        """Adds metadata to the output files
+
+        Parameters
+        ----------
+        fp : pycbc.inference.io.BaseInferenceFile instance
+            The inference file to write to.
+        group : str, optional
+            If provided, the metadata will be written to the attrs specified
+            by group, i.e., to ``fp[group].attrs``. Otherwise, metadata is
+            written to the top-level attrs (``fp.attrs``).
+        """
+        super().write_metadata(fp, group=group)
+        sampattrs = fp.getattrs(group=fp.samples_group)
+        # if a group is specified, prepend the lognl names with it
+        if group is None or group == '/':
+            prefix = ''
+        else:
+            prefix = group.replace('/', '__')
+            if not prefix.endswith('__'):
+                prefix += '__'
+        try:
+            model = self.submodels[self.primary_model]
+            sampattrs['{}lognl'.format(prefix)] = model.lognl
+        except AttributeError:
+            pass
+
     def _loglikelihood(self):
         for lbl, model in self.submodels.items():
             # Update the parameters of each
             model.update(**{p.subname: self.current_params[p.fullname]
                             for p in self.param_map[lbl]})
 
         # Calculate the combined loglikelihood
```

### Comparing `PyCBC-2.2.0/pycbc/inference/models/marginalized_gaussian_noise.py` & `PyCBC-2.2.1/pycbc/inference/models/marginalized_gaussian_noise.py`

 * *Files 0% similar despite different names*

```diff
@@ -316,15 +316,15 @@
             hphc[det] = hp[slc].inner(hc[slc]).real
 
             snr_proxy = ((cplx_hpd[det] / hphp[det] ** 0.5).squared_norm() +
                          (cplx_hcd[det] / hchc[det] ** 0.5).squared_norm())
             snr_estimate[det] = (0.5 * snr_proxy) ** 0.5
 
         self.draw_ifos(snr_estimate, log=False, **self.kwargs)
-        self.snr_draw(snr_estimate)
+        self.snr_draw(snrs=snr_estimate)
 
         for det in wfs:
             if det not in self.dets:
                 self.dets[det] = Detector(det)
             fp, fc = self.dets[det].antenna_pattern(
                                     params['ra'],
                                     params['dec'],
```

### Comparing `PyCBC-2.2.0/pycbc/inference/models/relbin.py` & `PyCBC-2.2.1/pycbc/inference/models/relbin.py`

 * *Files 24% similar despite different names*

```diff
@@ -30,24 +30,32 @@
 import numpy
 import itertools
 from scipy.interpolate import interp1d
 
 from pycbc.waveform import (get_fd_waveform_sequence,
                             get_fd_det_waveform_sequence, fd_det_sequence)
 from pycbc.detector import Detector
-from pycbc.types import Array
+from pycbc.types import Array, TimeSeries
 
 from .gaussian_noise import BaseGaussianNoise
 from .relbin_cpu import (likelihood_parts, likelihood_parts_v,
                          likelihood_parts_multi, likelihood_parts_multi_v,
-                         likelihood_parts_det)
+                         likelihood_parts_det, likelihood_parts_vector,
+                         likelihood_parts_v_pol,
+                         likelihood_parts_v_time,
+                         likelihood_parts_v_pol_time,
+                         likelihood_parts_vectorp, snr_predictor,
+                         likelihood_parts_vectort,
+                         snr_predictor_dom)
 from .tools import DistMarg
 
 
-def setup_bins(f_full, f_lo, f_hi, chi=1.0, eps=0.5, gammas=None):
+def setup_bins(f_full, f_lo, f_hi, chi=1.0,
+               eps=0.1, gammas=None,
+               ):
     """Construct frequency bins for use in a relative likelihood
     model. For details, see [Barak, Dai & Venumadhav 2018].
 
     Parameters
     ----------
     f_full : array
         The full resolution array of frequencies being used in the analysis.
@@ -166,39 +174,39 @@
         variable_params,
         data,
         low_frequency_cutoff,
         fiducial_params=None,
         gammas=None,
         epsilon=0.5,
         earth_rotation=False,
+        earth_rotation_mode=2,
         marginalize_phase=True,
         **kwargs
     ):
 
         variable_params, kwargs = self.setup_marginalization(
                                variable_params,
                                marginalize_phase=marginalize_phase,
                                **kwargs)
 
         super(Relative, self).__init__(
             variable_params, data, low_frequency_cutoff, **kwargs
         )
 
-        # If the waveform handles the detector response internally, set
-        # self.det_response = True
-        self.no_det_response = False
+        # If the waveform needs us to apply the detector response,
+        # set flag to true (most cases for ground-based observatories).
+        self.still_needs_det_response = False
         if self.static_params['approximant'] in fd_det_sequence:
-            self.no_det_response = True
+            self.still_needs_det_response = True
 
         # reference waveform and bin edges
         self.f, self.df, self.end_time, self.det = {}, {}, {}, {}
         self.h00, self.h00_sparse = {}, {}
         self.fedges, self.edges = {}, {}
-        self.ta = {}
-        self.antenna_time = {}
+        self.ta, self.antenna_time = {}, {}
 
         # filtered summary data for linear approximation
         self.sdat = {}
 
         # store fiducial waveform params
         self.fid_params = self.static_params.copy()
         self.fid_params.update(fiducial_params)
@@ -223,23 +231,36 @@
                 ifo, f_lo, f_hi,
             )
 
             # prune low frequency samples to avoid waveform errors
             fpoints = Array(self.f[ifo].astype(numpy.float64))
             fpoints = fpoints[self.kmin[ifo]:self.kmax[ifo]+1]
 
-            if self.no_det_response:
+            if self.still_needs_det_response:
                 wave = get_fd_det_waveform_sequence(ifos=ifo,
                                                     sample_points=fpoints,
                                                     **self.fid_params)
                 curr_wav = wave[ifo]
+                self.ta[ifo] = 0
             else:
                 fid_hp, fid_hc = get_fd_waveform_sequence(sample_points=fpoints,
                                                           **self.fid_params)
-                curr_wav = fid_hp
+                # Apply detector response if not handled by
+                # the waveform generator
+                self.det[ifo] = Detector(ifo)
+                dt = self.det[ifo].time_delay_from_earth_center(
+                    self.fid_params["ra"],
+                    self.fid_params["dec"],
+                    self.fid_params["tc"],
+                )
+                self.ta[ifo] = self.fid_params["tc"] + dt
+                fp, fc = self.det[ifo].antenna_pattern(
+                    self.fid_params["ra"], self.fid_params["dec"],
+                    self.fid_params["polarization"], self.fid_params["tc"])
+                curr_wav = (fid_hp * fp + fid_hc * fc)
 
             # check for zeros at low and high frequencies
             # make sure only nonzero samples are included in bins
             numzeros_lo = list(curr_wav != 0j).index(True)
             if numzeros_lo > 0:
                 new_kmin = self.kmin[ifo] + numzeros_lo
                 f_lo = new_kmin * self.df[ifo]
@@ -252,81 +273,59 @@
                 new_kmax = self.kmax[ifo] - numzeros_hi
                 f_hi = new_kmax * self.df[ifo]
                 logging.info(
                     "WARNING! Fiducial waveform terminates below "
                     "high-frequency-cutoff, final bin frequency "
                     "will be %s Hz", f_hi)
 
-            # make copy of fiducial wfs, adding back in low frequencies
-            if self.no_det_response:
-                curr_wav.resize(len(self.f[ifo]))
-                curr_wav = numpy.roll(curr_wav, self.kmin[ifo])
-                # get detector-specific arrival times relative to end of data
-                self.ta[ifo] = -self.end_time[ifo]
-                tshift = numpy.exp(-2.0j * numpy.pi * self.f[ifo] * self.ta[ifo])
-                h00 = numpy.array(curr_wav) * tshift
-                self.h00[ifo] = h00
-            else:
-                fid_hp.resize(len(self.f[ifo]))
-                fid_hc.resize(len(self.f[ifo]))
-                hp0 = numpy.roll(fid_hp, self.kmin[ifo])
-                hc0 = numpy.roll(fid_hc, self.kmin[ifo])
-
-                self.det[ifo] = Detector(ifo)
-                dt = self.det[ifo].time_delay_from_earth_center(
-                    self.fid_params["ra"],
-                    self.fid_params["dec"],
-                    self.fid_params["tc"],
-                )
-                self.ta = self.fid_params["tc"] + dt - self.end_time[ifo]
-
-                fp, fc = self.det[ifo].antenna_pattern(
-                    self.fid_params["ra"], self.fid_params["dec"],
-                    self.fid_params["polarization"], self.fid_params["tc"])
-
-                tshift = numpy.exp(-2.0j * numpy.pi * self.f[ifo] * self.ta)
-
-                h00 = (hp0 * fp + hc0 * fc) * tshift
-                self.h00[ifo] = h00
+            self.ta[ifo] -= self.end_time[ifo]
+            curr_wav.resize(len(self.f[ifo]))
+            curr_wav = numpy.roll(curr_wav, self.kmin[ifo])
+
+            # We'll apply this to the data, in lieu of the ref waveform
+            # This makes it easier to compare target signal to reference later
+            tshift = numpy.exp(-2.0j * numpy.pi * self.f[ifo] * self.ta[ifo])
+            self.h00[ifo] = numpy.array(curr_wav) # * tshift
+            data_shifted = self.data[ifo] * numpy.conjugate(tshift)
 
-            # compute frequency bins
             logging.info("Computing frequency bins")
             fbin_ind = setup_bins(
                 f_full=self.f[ifo], f_lo=f_lo, f_hi=f_hi,
                 gammas=gammas, eps=float(epsilon),
             )
             logging.info("Using %s bins for this model", len(fbin_ind))
 
             self.fedges[ifo] = self.f[ifo][fbin_ind]
             self.edges[ifo] = fbin_ind
-
-            self.init_from_frequencies(h00, fbin_ind, ifo)
-            self.antenna_time[ifo] = self.setup_antenna(earth_rotation,
-                                                        self.fedges[ifo])
+            self.init_from_frequencies(data_shifted, self.h00, fbin_ind, ifo)
+            self.antenna_time[ifo] = self.setup_antenna(
+                                        earth_rotation,
+                                        int(earth_rotation_mode),
+                                        self.fedges[ifo])
         self.combine_layout()
 
-    def init_from_frequencies(self, h00, fbin_ind, ifo):
+    def init_from_frequencies(self, data, h00, fbin_ind, ifo):
         bins = numpy.array(
             [
                 (fbin_ind[i], fbin_ind[i + 1])
                 for i in range(len(fbin_ind) - 1)
             ]
         )
 
         # store low res copy of fiducial waveform
-        self.h00_sparse[ifo] = h00.copy().take(fbin_ind)
+        self.h00_sparse[ifo] = h00[ifo].copy().take(fbin_ind)
 
         # compute summary data
         logging.info(
             "Calculating summary data at frequency resolution %s Hz",
             self.df[ifo],
         )
 
-        a0, a1 = self.summary_product(self.data[ifo], h00, bins, ifo)
-        b0, b1 = self.summary_product(h00, h00, bins, ifo)
+        a0, a1 = self.summary_product(data, h00[ifo], bins, ifo)
+        b0, b1 = self.summary_product(h00[ifo], h00[ifo], bins, ifo)
         self.sdat[ifo] = {"a0": a0, "a1": a1, "b0": abs(b0), "b1": abs(b1)}
 
     def combine_layout(self):
         # determine the unique ifo layouts
         self.edge_unique = []
         self.ifo_map = {}
         for ifo in self.fedges:
@@ -339,38 +338,70 @@
                         self.ifo_map[ifo] = i
                         break
                 else:
                     self.ifo_map[ifo] = len(self.edge_unique)
                     self.edge_unique.append(Array(self.fedges[ifo]))
         logging.info("%s unique ifo layouts", len(self.edge_unique))
 
-    def setup_antenna(self, earth_rotation, fedges):
+    def setup_antenna(self, earth_rotation, mode, fedges):
         # Calculate the times to evaluate fp/fc
+        self.earth_rotation = earth_rotation
         if earth_rotation is not False:
             logging.info("Enabling frequency-dependent earth rotation")
             from pycbc.waveform.spa_tmplt import spa_length_in_time
 
             times = spa_length_in_time(
                 phase_order=-1,
                 mass1=self.fid_params["mass1"],
                 mass2=self.fid_params["mass2"],
-                f_lower=fedges,
+                f_lower=numpy.array(fedges) / mode * 2.0,
             )
             atimes = self.fid_params["tc"] - times
             self.lik = likelihood_parts_v
             self.mlik = likelihood_parts_multi_v
         else:
             atimes = self.fid_params["tc"]
-            if self.no_det_response:
+            if self.still_needs_det_response:
                 self.lik = likelihood_parts_det
             else:
                 self.lik = likelihood_parts
-            self.mlik = likelihood_parts_multi
+                self.mlik = likelihood_parts_multi
         return atimes
 
+    @property
+    def likelihood_function(self):
+        self.lformat = None
+        if self.marginalize_vector_params:
+            p = self.current_params
+
+            vmarg = set(k for k in self.marginalize_vector_params
+                        if not numpy.isscalar(p[k]))
+
+            if self.earth_rotation:
+                if set(['tc', 'polarization']).issubset(vmarg):
+                    self.lformat = 'earth_time_pol'
+                    return likelihood_parts_v_pol_time
+                elif set(['polarization']).issubset(vmarg):
+                    self.lformat = 'earth_pol'
+                    return likelihood_parts_v_pol
+                elif set(['tc']).issubset(vmarg):
+                    self.lformat = 'earth_time'
+                    return likelihood_parts_v_time
+            else:
+                if set(['ra', 'dec', 'tc']).issubset(vmarg):
+                    return likelihood_parts_vector
+                elif set(['tc', 'polarization']).issubset(vmarg):
+                    return likelihood_parts_vector
+                elif set(['tc']).issubset(vmarg):
+                    return likelihood_parts_vectort
+                elif set(['polarization']).issubset(vmarg):
+                    return likelihood_parts_vectorp
+
+        return self.lik
+
     def summary_product(self, h1, h2, bins, ifo):
         """ Calculate the summary values for the inner product <h1|h2>
         """
         # calculate coefficients
         h12 = numpy.conjugate(h1) * h2 / self.psds[ifo]
 
         # constant terms
@@ -386,28 +417,30 @@
                 for l, h in bins])
 
         return a0, a1
 
     def get_waveforms(self, params):
         """ Get the waveform polarizations for each ifo
         """
-        if self.no_det_response:
+        if self.still_needs_det_response:
             wfs = {}
             for ifo in self.data:
                 wfs.update(get_fd_det_waveform_sequence(
                         ifos=ifo, sample_points=self.fedges[ifo], **params))
             return wfs
 
         wfs = []
         for edge in self.edge_unique:
             hp, hc = get_fd_waveform_sequence(sample_points=edge, **params)
             hp = hp.numpy()
             hc = hc.numpy()
             wfs.append((hp, hc))
         wf_ret = {ifo: wfs[self.ifo_map[ifo]] for ifo in self.data}
+
+        self.wf_ret = wf_ret
         return wf_ret
 
     @property
     def multi_signal_support(self):
         """ The list of classes that this model supports in a multi-signal
         likelihood
         """
@@ -485,54 +518,64 @@
 
         Returns
         -------
         float
             The value of the log likelihood ratio.
         """
         # get model params
-        p = self.current_params.copy()
-        p.update(self.static_params)
+        p = self.current_params
         wfs = self.get_waveforms(p)
-
+        lik = self.likelihood_function
         norm = 0.0
         filt = 0j
         self._current_wf_parts = {}
-        for ifo in self.data:
+        pol_phase = numpy.exp(-2.0j * p['polarization'])
 
+        for ifo in self.data:
             freqs = self.fedges[ifo]
             sdat = self.sdat[ifo]
             h00 = self.h00_sparse[ifo]
             end_time = self.end_time[ifo]
             times = self.antenna_time[ifo]
 
             # project waveform to detector frame if waveform does not deal
             # with detector response. Otherwise, skip detector response.
 
-            if self.no_det_response:
-                dtc = -end_time
+            if self.still_needs_det_response:
                 channel = wfs[ifo].numpy()
-                filter_i, norm_i = self.lik(freqs, dtc, channel, h00,
-                                            sdat['a0'], sdat['a1'],
-                                            sdat['b0'], sdat['b1'])
+                filter_i, norm_i = lik(freqs, 0.0, channel, h00,
+                                       sdat['a0'], sdat['a1'],
+                                       sdat['b0'], sdat['b1'])
             else:
                 hp, hc = wfs[ifo]
                 det = self.det[ifo]
                 fp, fc = det.antenna_pattern(p["ra"], p["dec"],
-                                             p["polarization"], times)
+                                             0.0, times)
                 dt = det.time_delay_from_earth_center(p["ra"], p["dec"], times)
-                dtc = p["tc"] + dt - end_time
+                dtc = p["tc"] + dt - end_time - self.ta[ifo]
+
+                if self.lformat == 'earth_pol':
+                    filter_i, norm_i = lik(freqs, fp, fc, dtc, pol_phase,
+                                           hp, hc, h00,
+                                           sdat['a0'], sdat['a1'],
+                                           sdat['b0'], sdat['b1'])
+                else:
+                    f = (fp + 1.0j * fc) * pol_phase
+                    fp = f.real.copy()
+                    fc = f.imag.copy()
+                    filter_i, norm_i = lik(freqs, fp, fc, dtc,
+                                           hp, hc, h00,
+                                           sdat['a0'], sdat['a1'],
+                                           sdat['b0'], sdat['b1'])
+                    self._current_wf_parts[ifo] = (fp, fc, dtc, hp, hc, h00)
 
-                filter_i, norm_i = self.lik(freqs, fp, fc, dtc,
-                                            hp, hc, h00,
-                                            sdat['a0'], sdat['a1'],
-                                            sdat['b0'], sdat['b1'])
-                self._current_wf_parts[ifo] = (fp, fc, dtc, hp, hc, h00)
             filt += filter_i
             norm += norm_i
-        return self.marginalize_loglr(filt, norm)
+        loglr = self.marginalize_loglr(filt, norm)
+        return loglr
 
     def write_metadata(self, fp, group=None):
         """Adds writing the fiducial parameters and epsilon to file's attrs.
 
         Parameters
         ----------
         fp : pycbc.inference.io.BaseInferenceFile instance
@@ -546,14 +589,25 @@
         if group is None:
             attrs = fp.attrs
         else:
             attrs = fp[group].attrs
         for p, v in self.fid_params.items():
             attrs["{}_ref".format(p)] = v
 
+    def max_curvature_from_reference(self):
+        """ Return the maximum change in slope between frequency bins
+        relative to the reference waveform.
+        """
+        dmax = 0
+        for ifo in self.data:
+            r = self.wf_ret[ifo][0] / self.h00_sparse[ifo]
+            d = abs(numpy.diff(r / abs(r).min(), n=2)).max()
+            dmax = d if dmax < d else dmax
+        return dmax
+
     @staticmethod
     def extra_args_from_config(cp, section, skip_args=None, dtypes=None):
         """Adds reading fiducial waveform parameters from config file."""
         # add fiducial params to skip list
         skip_args += [
             option for option in cp.options(section) if option.endswith("_ref")
         ]
@@ -585,7 +639,207 @@
             "polarization": numpy.pi,
         }
         fid_params.update(
             {p: opt_params[p] for p in opt_params if p not in fid_params}
         )
         args.update({"fiducial_params": fid_params, "gammas": gammas})
         return args
+
+
+class RelativeTime(Relative):
+    """ Heterodyne likelihood optimized for time marginalization. In addition
+    it supports phase (dominant-mode), sky location, and polarization
+    marginalization.
+    """
+    name = "relative_time"
+
+    def __init__(self, *args,
+                 sample_rate=4096,
+                 **kwargs):
+        super(RelativeTime, self).__init__(*args, **kwargs)
+        self.sample_rate = float(sample_rate)
+        self.setup_peak_lock(sample_rate=self.sample_rate, **kwargs)
+        self.draw_ifos(self.ref_snr, **kwargs)
+
+    @property
+    def ref_snr(self):
+        if not hasattr(self, '_ref_snr'):
+            wfs = {ifo: (self.h00_sparse[ifo],
+                         self.h00_sparse[ifo]) for ifo in self.h00_sparse}
+            self._ref_snr = self.get_snr(wfs)
+        return self._ref_snr
+
+    def get_snr(self, wfs):
+        """ Return hp/hc maximized SNR time series
+        """
+        delta_t = 1.0 / self.sample_rate
+        snrs = {}
+        for ifo in wfs:
+            sdat = self.sdat[ifo]
+            dtc = self.tstart[ifo] - self.end_time[ifo] - self.ta[ifo]
+
+            snr = snr_predictor(self.fedges[ifo],
+                                dtc - delta_t * 2.0, delta_t,
+                                self.num_samples[ifo] + 4,
+                                wfs[ifo][0], wfs[ifo][1],
+                                self.h00_sparse[ifo],
+                                sdat['a0'], sdat['a1'],
+                                sdat['b0'], sdat['b1'])
+            snrs[ifo] = TimeSeries(snr, delta_t=delta_t,
+                                   epoch=self.tstart[ifo] - delta_t * 2.0)
+        return snrs
+
+    def _loglr(self):
+        r"""Computes the log likelihood ratio,
+
+        .. math::
+
+            \log \mathcal{L}(\Theta) = \sum_i
+                \left<h_i(\Theta)|d_i\right> -
+                \frac{1}{2}\left<h_i(\Theta)|h_i(\Theta)\right>,
+
+        at the current parameter values :math:`\Theta`.
+
+        Returns
+        -------
+        float
+            The value of the log likelihood ratio.
+        """
+        # get model params
+        p = self.current_params
+        wfs = self.get_waveforms(p)
+        lik = self.likelihood_function
+        norm = 0.0
+        filt = 0j
+        pol_phase = numpy.exp(-2.0j * p['polarization'])
+
+        self.snr_draw(wfs)
+        p = self.current_params
+
+        for ifo in self.data:
+            freqs = self.fedges[ifo]
+            sdat = self.sdat[ifo]
+            h00 = self.h00_sparse[ifo]
+            end_time = self.end_time[ifo]
+            times = self.antenna_time[ifo]
+
+            hp, hc = wfs[ifo]
+            det = self.det[ifo]
+            fp, fc = det.antenna_pattern(p["ra"], p["dec"],
+                                         0, times)
+            times = det.time_delay_from_earth_center(p["ra"], p["dec"], times)
+            dtc = p["tc"] - end_time - self.ta[ifo]
+
+            if self.lformat == 'earth_time_pol':
+                filter_i, norm_i = lik(
+                                       freqs, fp, fc, times, dtc, pol_phase,
+                                       hp, hc, h00,
+                                       sdat['a0'], sdat['a1'],
+                                       sdat['b0'], sdat['b1'])
+            else:
+                f = (fp + 1.0j * fc) * pol_phase
+                fp = f.real.copy()
+                fc = f.imag.copy()
+                if self.lformat == 'earth_time':
+                    filter_i, norm_i = lik(
+                                           freqs, fp, fc, times, dtc,
+                                           hp, hc, h00,
+                                           sdat['a0'], sdat['a1'],
+                                           sdat['b0'], sdat['b1'])
+                else:
+                    filter_i, norm_i = lik(freqs, fp, fc, times + dtc,
+                                           hp, hc, h00,
+                                           sdat['a0'], sdat['a1'],
+                                           sdat['b0'], sdat['b1'])
+            filt += filter_i
+            norm += norm_i
+        loglr = self.marginalize_loglr(filt, norm)
+        return loglr
+
+
+class RelativeTimeDom(RelativeTime):
+    """ Heterodyne likelihood optimized for time marginalization and only
+    dominant-mode waveforms. This enables the ability to do inclination
+    marginalization in addition to the other forms supportedy by RelativeTime.
+    """
+    name = "relative_time_dom"
+
+    def get_snr(self, wfs):
+        """ Return hp/hc maximized SNR time series
+        """
+        delta_t = 1.0 / self.sample_rate
+        snrs = {}
+        self.sh = {}
+        self.hh = {}
+        for ifo in wfs:
+            sdat = self.sdat[ifo]
+            dtc = self.tstart[ifo] - self.end_time[ifo] - self.ta[ifo]
+
+            sh, hh = snr_predictor_dom(self.fedges[ifo],
+                                       dtc - delta_t * 2.0, delta_t,
+                                       self.num_samples[ifo] + 4,
+                                       wfs[ifo][0],
+                                       self.h00_sparse[ifo],
+                                       sdat['a0'], sdat['a1'],
+                                       sdat['b0'], sdat['b1'])
+            snr = TimeSeries(abs(sh[2:-2]) / hh ** 0.5, delta_t=delta_t,
+                             epoch=self.tstart[ifo])
+            self.sh[ifo] = TimeSeries(sh, delta_t=delta_t,
+                                      epoch=self.tstart[ifo] - delta_t * 2.0)
+            self.hh[ifo] = hh
+            snrs[ifo] = snr
+
+        return snrs
+
+    def _loglr(self):
+        r"""Computes the log likelihood ratio,
+
+        .. math::
+
+            \log \mathcal{L}(\Theta) = \sum_i
+                \left<h_i(\Theta)|d_i\right> -
+                \frac{1}{2}\left<h_i(\Theta)|h_i(\Theta)\right>,
+
+        at the current parameter values :math:`\Theta`.
+
+        Returns
+        -------
+        float
+            The value of the log likelihood ratio.
+        """
+        # calculate <d-h|d-h> = <h|h> - 2<h|d> + <d|d> up to a constant
+        p = self.current_params
+
+        p2 = p.copy()
+        p2.pop('inclination')
+        wfs = self.get_waveforms(p2)
+
+        sh_total = hh_total = 0
+        ic = numpy.cos(p['inclination'])
+        ip = 0.5 * (1.0 + ic * ic)
+        pol_phase = numpy.exp(-2.0j * p['polarization'])
+
+        snrs = self.get_snr(wfs)
+        self.snr_draw(snrs=snrs)
+
+        for ifo in self.sh:
+            if self.precalc_antenna_factors:
+                fp, fc, dt = self.get_precalc_antenna_factors(ifo)
+            else:
+                dt = self.det[ifo].time_delay_from_earth_center(p['ra'],
+                                                                p['dec'],
+                                                                p['tc'])
+                fp, fc = self.det[ifo].antenna_pattern(p['ra'], p['dec'],
+                                                       0, p['tc'])
+            dts = p['tc'] + dt
+            f = (fp + 1.0j * fc) * pol_phase
+
+            # Note, this includes complex conjugation already
+            # as our stored inner products were hp* x data
+            htf = (f.real * ip + 1.0j * f.imag * ic)
+
+            sh = self.sh[ifo].at_time(dts, interpolate='quadratic')
+            sh_total += sh * htf
+            hh_total += self.hh[ifo] * abs(htf) ** 2.0
+
+        loglr = self.marginalize_loglr(sh_total, hh_total)
+        return loglr
```

### Comparing `PyCBC-2.2.0/pycbc/inference/models/single_template.py` & `PyCBC-2.2.1/pycbc/inference/models/single_template.py`

 * *Files 0% similar despite different names*

```diff
@@ -200,15 +200,15 @@
 
         sh_total = hh_total = 0
 
         ic = numpy.cos(p['inclination'])
         ip = 0.5 * (1.0 + ic * ic)
         pol_phase = numpy.exp(-2.0j * p['polarization'])
 
-        self.snr_draw(self.snr)
+        self.snr_draw(snrs=self.snr)
 
         for ifo in self.sh:
             dt = self.det[ifo].time_delay_from_earth_center(p['ra'], p['dec'],
                                                             p['tc'])
             self.dts[ifo] = p['tc'] + dt
 
             fp, fc = self.det[ifo].antenna_pattern(p['ra'], p['dec'],
```

### Comparing `PyCBC-2.2.0/pycbc/inference/models/tools.py` & `PyCBC-2.2.1/pycbc/inference/models/tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,15 +45,15 @@
     cdf = numpy.exp(loglr).cumsum()
     cdf /= cdf[-1]
     xl = numpy.searchsorted(cdf, x)
     return xl
 
 
 class DistMarg():
-    """Help class to add bookkeeping for distance marginalization"""
+    """Help class to add bookkeeping for likelihood marginalization"""
 
     marginalize_phase = None
     distance_marginalization = None
     distance_interpolator = None
 
     def setup_marginalization(self,
                               variable_params,
@@ -62,14 +62,15 @@
                               marginalize_distance_param='distance',
                               marginalize_distance_samples=int(1e4),
                               marginalize_distance_interpolator=False,
                               marginalize_distance_snr_range=None,
                               marginalize_distance_density=None,
                               marginalize_vector_params=None,
                               marginalize_vector_samples=1e3,
+                              marginalize_sky_initial_samples=1e6,
                               **kwargs):
         """ Setup the model for use with distance marginalization
 
         This function sets up precalculations for distance / phase
         marginalization. For distance margininalization it modifies the
         model to internally remove distance as a parameter.
 
@@ -115,20 +116,24 @@
                                       *dists, **old_prior.kwargs)
             kwargs['prior'] = prior
             return dprior
 
         self.reconstruct_phase = False
         self.reconstruct_distance = False
         self.reconstruct_vector = False
+        self.precalc_antennna_factors = False
 
         # Handle any requested parameter vector / brute force marginalizations
         self.marginalize_vector_params = {}
         self.marginalized_vector_priors = {}
         self.vsamples = int(marginalize_vector_samples)
 
+        self.marginalize_sky_initial_samples = \
+            int(float(marginalize_sky_initial_samples))
+
         for param in str_to_tuple(marginalize_vector_params, str):
             logging.info('Marginalizing over %s, %s points from prior',
                          param, self.vsamples)
             self.marginalized_vector_priors[param] = pop_prior(param)
 
         # Remove in the future, backwards compatibility
         if 'polarization_samples' in kwargs:
@@ -271,36 +276,63 @@
                                       phase=self.marginalize_phase,
                                       interpolator=interpolator,
                                       distance=distance,
                                       skip_vector=skip_vector,
                                       return_complex=return_complex,
                                       return_peak=return_peak)
 
-    def snr_draw(self, snrs):
+    def premarg_draw(self):
+        """ Choose random samples from prechosen set"""
+
+        # Update the current proposed times and the marginalization values
+        logw = self.premarg['logw_partial']
+        choice = numpy.random.randint(0, len(logw), size=self.vsamples)
+
+        for k in self.snr_params:
+            self.marginalize_vector_params[k] = self.premarg[k][choice]
+
+        self._current_params.update(self.marginalize_vector_params)
+        self.sample_idx = self.premarg['sample_idx'][choice]
+
+        # Update the importance weights for each vector sample
+        logw = self.marginalize_vector_weights + logw[choice]
+        self.marginalize_vector_weights = logw - logsumexp(logw)
+        return self.marginalize_vector_params
+
+    def snr_draw(self, wfs=None, snrs=None, size=None):
         """ Improve the monte-carlo vector marginalization using the SNR time
         series of each detector
         """
-        p = self.current_params
-
-        if (not numpy.isscalar(p['tc']) and
-            'tc' in self.marginalized_vector_priors and
-            not ('ra' in self.marginalized_vector_priors
-                 or 'dec' in self.marginalized_vector_priors)):
-            self.draw_times(snrs)
-        elif (not numpy.isscalar(p['tc']) and
-              'tc' in self.marginalized_vector_priors and
-              'ra' in self.marginalized_vector_priors and
-              'dec' in self.marginalized_vector_priors):
-            self.draw_sky_times(snrs)
+        try:
+            p = self.current_params
+            set_scalar = numpy.isscalar(p['tc'])
+        except:
+            set_scalar = False
+
+        if not set_scalar:
+            if hasattr(self, 'premarg'):
+                return self.premarg_draw()
+
+            if snrs is None:
+                snrs = self.get_snr(wfs)
+            if ('tc' in self.marginalized_vector_priors and
+                not ('ra' in self.marginalized_vector_priors
+                     or 'dec' in self.marginalized_vector_priors)):
+                return self.draw_times(snrs, size=size)
+            elif ('tc' in self.marginalized_vector_priors and
+                  'ra' in self.marginalized_vector_priors and
+                  'dec' in self.marginalized_vector_priors):
+                return self.draw_sky_times(snrs, size=size)
         else:
             # OK, we couldn't do anything with the requested monte-carlo
             # marginalizations.
-            pass
+            self.precalc_antenna_factors = None
+            return None
 
-    def draw_times(self, snrs):
+    def draw_times(self, snrs, size=None):
         """ Draw times consistent with the incoherent network SNR
 
         Parameters
         ----------
         snrs: dist of TimeSeries
         """
         if not hasattr(self, 'tinfo'):
@@ -308,16 +340,18 @@
             tcmin, tcmax = self.marginalized_vector_priors['tc'].bounds['tc']
             tcave = (tcmax + tcmin) / 2.0
             ifos = list(snrs.keys())
             if hasattr(self, 'keep_ifos'):
                 ifos = self.keep_ifos
             d = {ifo: Detector(ifo, reference_time=tcave) for ifo in ifos}
             self.tinfo = tcmin, tcmax, tcave, ifos, d
+            self.snr_params = ['tc']
 
         tcmin, tcmax, tcave, ifos, d = self.tinfo
+        vsamples = size if size is not None else self.vsamples
 
         # Determine the weights for the valid time range
         ra = self._current_params['ra']
         dec = self._current_params['dec']
 
         # Determine the common valid time range
         iref = ifos[0]
@@ -357,30 +391,35 @@
                                         snr.end_time + idel,
                                         mode='nearest')
             logweight += snrv.squared_norm().numpy()
         logweight /= 2.0
 
         # Draw proportional to the incoherent likelihood
         # Draw first which time sample
-        tci = draw_sample(logweight, size=self.vsamples)
+        tci = draw_sample(logweight, size=vsamples)
         # Second draw a subsample size offset so that all times are covered
         tct = numpy.random.uniform(-snr.delta_t / 2.0,
                                    snr.delta_t / 2.0,
-                                   size=self.vsamples)
+                                   size=vsamples)
         tc = tct + tci * snr.delta_t + float(snr.start_time) - dt
 
         # Update the current proposed times and the marginalization values
+        logw = - logweight[tci]
         self.marginalize_vector_params['tc'] = tc
-        self._current_params['tc'] = tc
+        self.marginalize_vector_params['logw_partial'] = logw
 
-        # Update the importance weights for each vector sample
-        logw = self.marginalize_vector_weights - logweight[tci]
-        self.marginalize_vector_weights = logw - logsumexp(logw)
+        if self._current_params is not None:
+            # Update the importance weights for each vector sample
+            logw = self.marginalize_vector_weights + logw
+            self._current_params.update(self.marginalize_vector_params)
+            self.marginalize_vector_weights = logw - logsumexp(logw)
 
-    def draw_sky_times(self, snrs):
+        return self.marginalize_vector_params
+
+    def draw_sky_times(self, snrs, size=None):
         """ Draw ra, dec, and tc together using SNR timeseries to determine
         monte-carlo weights.
         """
         # First setup
         # precalculate dense sky grid and make dict and or array of the results
         ifos = list(snrs.keys())
         if hasattr(self, 'keep_ifos'):
@@ -388,52 +427,58 @@
         ikey = ''.join(ifos)
 
         # No good SNR peaks, go with prior draw
         if len(ifos) == 0:
             return
 
         def make_init():
-            logging.info('pregenerating sky pointings')
-            size = int(1e6)
-            logging.info('drawing samples')
+            self.snr_params = ['tc', 'ra', 'dec']
+            size = self.marginalize_sky_initial_samples
+            logging.info('drawing samples: %s', size)
             ra = self.marginalized_vector_priors['ra'].rvs(size=size)['ra']
             dec = self.marginalized_vector_priors['dec'].rvs(size=size)['dec']
             tcmin, tcmax = self.marginalized_vector_priors['tc'].bounds['tc']
             tcave = (tcmax + tcmin) / 2.0
-            d = {ifo: Detector(ifo, reference_time=tcave) for ifo in ifos}
+            d = {ifo: Detector(ifo, reference_time=tcave) for ifo in self.data}
 
             # What data structure to hold times? Dict of offset -> list?
             logging.info('sorting into time delay dict')
             dts = []
             for i in range(len(ifos) - 1):
                 dt = d[ifos[0]].time_delay_from_detector(d[ifos[i+1]],
                                                          ra, dec, tcave)
                 dt = numpy.rint(dt / snrs[ifos[0]].delta_t)
                 dts.append(dt)
 
-            dtc = d[ifos[0]].time_delay_from_earth_center(ra, dec, tcave)
+            fp, fc, dtc = {}, {}, {}
+            for ifo in self.data:
+                fp[ifo], fc[ifo] = d[ifo].antenna_pattern(ra, dec, 0.0, tcave)
+                dtc[ifo] = d[ifo].time_delay_from_earth_center(ra, dec, tcave)
 
             dmap = {}
             for i, t in enumerate(tqdm.tqdm(zip(*dts))):
-                v = ra[i], dec[i], dtc[i]
                 if t not in dmap:
                     dmap[t] = []
-                dmap[t].append(v)
+                dmap[t].append(i)
 
             if len(ifos) == 1:
-                dmap[()] = list(zip(ra, dec, dtc))
-            return ifos, dmap, tcmin, tcmax
+                dmap[()] = numpy.arange(0, size, 1).astype(int)
+
+            return dmap, tcmin, tcmax, fp, fc, ra, dec, dtc
 
         if not hasattr(self, 'tinfo'):
             self.tinfo = {}
 
         if ikey not in self.tinfo:
+            logging.info('pregenerating sky pointings')
             self.tinfo[ikey] = make_init()
 
-        ifos, dmap, tcmin, tcmax = self.tinfo[ikey]
+        dmap, tcmin, tcmax, fp, fc, ra, dec, dtc = self.tinfo[ikey]
+
+        vsamples = size if size is not None else self.vsamples
 
         # draw times from each snr time series
         # Is it worth doing this if some detector has low SNR?
         sref = None
         iref = None
         idx = []
         dx = []
@@ -447,76 +492,86 @@
 
             start = max(tmin, snrs[ifo].start_time)
             end = min(tmax, snrs[ifo].end_time)
 
             snr = snr.time_slice(start, end, mode='nearest')
 
             w = snr.squared_norm().numpy() / 2.0
-            i = draw_sample(w, size=self.vsamples)
+            i = draw_sample(w, size=vsamples)
 
             if sref is not None:
                 mcweight -= w[i]
                 delt = float(snr.start_time - sref.start_time)
                 i += round(delt / sref.delta_t)
                 dx.append(iref - i)
             else:
                 sref = snr
                 iref = i
                 mcweight = -w[i]
 
             idx.append(i)
 
         # check if delay is in dict, if not, throw out
-        ra = []
-        dec = []
-        dtc = []
         ti = []
+        ix = []
         wi = []
-        rand = numpy.random.uniform(0, 1, size=self.vsamples)
-        for i in range(self.vsamples):
+        rand = numpy.random.uniform(0, 1, size=vsamples)
+        for i in range(vsamples):
             t = tuple(x[i] for x in dx)
             if t in dmap:
                 randi = int(rand[i] * (len(dmap[t])))
-                xra, xdec, xdtc = dmap[t][randi]
-                ra.append(xra)
-                dec.append(xdec)
-                dtc.append(xdtc)
+                ix.append(dmap[t][randi])
                 wi.append(len(dmap[t]))
                 ti.append(i)
 
         # If we had really poor efficiency at finding a point, we should
         # give up and just use the original random draws
-        if len(ra) < 0.05 * self.vsamples:
+        if len(ra) < 0.05 * vsamples:
             return
 
         # fill back to fixed size with repeat samples
         # sample order is random, so this should be OK statistically
+        ix = numpy.resize(numpy.array(ix, dtype=int), vsamples)
+        self.sample_idx = ix
+        self.precalc_antenna_factors = fp, fc, dtc
+
+        ra = ra[ix]
+        dec = dec[ix]
+        dtc = {ifo: dtc[ifo][ix] for ifo in dtc}
 
-        ra = numpy.resize(numpy.array(ra), self.vsamples)
-        dec = numpy.resize(numpy.array(dec), self.vsamples)
-        dtc = numpy.resize(numpy.array(dtc), self.vsamples)
-        ti = numpy.resize(numpy.array(ti, dtype=int), self.vsamples)
-        wi = numpy.resize(numpy.array(wi), self.vsamples)
+        ti = numpy.resize(numpy.array(ti, dtype=int), vsamples)
+        wi = numpy.resize(numpy.array(wi), vsamples)
 
         # Second draw a subsample size offset so that all times are covered
         tct = numpy.random.uniform(-snr.delta_t / 2.0,
                                    snr.delta_t / 2.0,
                                    size=len(ti))
 
-        tc = tct + iref[ti] * snr.delta_t + float(sref.start_time) - dtc
+        tc = tct + iref[ti] * snr.delta_t + float(sref.start_time) - dtc[ifos[0]]
 
         # Update the current proposed times and the marginalization values
+        logw_sky = mcweight[ti] + numpy.log(wi)
         self.marginalize_vector_params['tc'] = tc
         self.marginalize_vector_params['ra'] = ra
         self.marginalize_vector_params['dec'] = dec
-        self._current_params.update(self.marginalize_vector_params)
+        self.marginalize_vector_params['logw_partial'] = logw_sky
 
-        # Update the importance weights for each vector sample
-        logw = self.marginalize_vector_weights + mcweight[ti] + numpy.log(wi)
-        self.marginalize_vector_weights = logw - logsumexp(logw)
+        if self._current_params is not None:
+            # Update the importance weights for each vector sample
+            logw = self.marginalize_vector_weights + logw_sky
+            self._current_params.update(self.marginalize_vector_params)
+            self.marginalize_vector_weights = logw - logsumexp(logw)
+
+        return self.marginalize_vector_params
+
+    def get_precalc_antenna_factors(self, ifo):
+        """ Get the antenna factors for marginalized samples if they exist """
+        ix = self.sample_idx
+        fp, fc, dtc = self.precalc_antenna_factors
+        return fp[ifo][ix], fc[ifo][ix], dtc[ifo][ix]
 
     def setup_peak_lock(self,
                         sample_rate=4096,
                         snrs=None,
                         peak_lock_snr=None,
                         peak_lock_ratio=1e4,
                         peak_lock_region=4,
@@ -560,15 +615,17 @@
         # to identifiable peaks
         if peak_lock_snr is not None:
             peak_lock_snr = float(peak_lock_snr)
             peak_lock_ratio = float(peak_lock_ratio)
             peak_lock_region = int(peak_lock_region)
 
             for ifo in snrs:
-                z = snrs[ifo].time_slice(tstart, tstart + tmax, mode='nearest')
+                s = max(tstart, snrs[ifo].start_time)
+                e = min(tstart + tmax, snrs[ifo].end_time)
+                z = snrs[ifo].time_slice(s, e, mode='nearest')
                 peak_snr, imax = z.abs_max_loc()
                 times = z.sample_times
                 peak_time = times[imax]
 
                 logging.info('%s: Max Ref SNR Peak of %s at %s',
                              ifo, peak_snr, peak_time)
 
@@ -603,21 +660,25 @@
                 logging.info('%s: use region %s-%s, %s points',
                              ifo, ts, te, self.num_samples[ifo])
 
         self.tend = self.tstart.copy()
         for ifo in snrs:
             self.tend[ifo] += self.num_samples[ifo] / sample_rate
 
-    def draw_ifos(self, snrs, peak_snr_threshold=4.0, log=True, **kwargs):
+    def draw_ifos(self, snrs, peak_snr_threshold=4.0, log=True,
+                  precalculate_marginalization_points=False,
+                  **kwargs):
         """ Helper utility to determine which ifos we should use based on the
         reference SNR time series.
         """
         if 'tc' not in self.marginalized_vector_priors:
             return
 
+        peak_snr_threshold = float(peak_snr_threshold)
+
         tcmin, tcmax = self.marginalized_vector_priors['tc'].bounds['tc']
         ifos = list(snrs.keys())
         keep_ifos = []
         psnrs = []
         for ifo in ifos:
             snr = snrs[ifo]
             start = max(tcmin - EARTH_RADIUS, snr.start_time)
@@ -630,14 +691,20 @@
 
         if log:
             logging.info("Ifos used for SNR based draws:"
                          " %s, snrs: %s, peak_snr_threshold=%s",
                          keep_ifos, psnrs, peak_snr_threshold)
 
         self.keep_ifos = keep_ifos
+
+        if precalculate_marginalization_points:
+            num_points = int(float(precalculate_marginalization_points))
+            self.premarg = self.snr_draw(size=num_points, snrs=snrs).copy()
+            self.premarg['sample_idx'] = self.sample_idx
+
         return keep_ifos
 
     @property
     def current_params(self):
         """ The current parameters
 
         If a parameter has been vector marginalized, the likelihood should
@@ -664,35 +731,35 @@
         def get_loglr():
             p = self.current_params.copy()
             p.update(rec)
             self.update(**p)
             return self.loglr
 
         if self.marginalize_vector_params:
-            logging.info('Reconstruct vector')
+            logging.debug('Reconstruct vector')
             self.reconstruct_vector = True
             self.reset_vector_params()
             loglr = get_loglr()
             xl = draw_sample(loglr + self.marginalize_vector_weights)
             for k in self.marginalize_vector_params:
                 rec[k] = self.marginalize_vector_params[k][xl]
             self.reconstruct_vector = False
 
         if self.distance_marginalization:
-            logging.info('Reconstruct distance')
+            logging.debug('Reconstruct distance')
             # call likelihood to get vector output
             self.reconstruct_distance = True
             _, weights = self.distance_marginalization
             loglr = get_loglr()
             xl = draw_sample(loglr + numpy.log(weights))
             rec['distance'] = self.dist_locs[xl]
             self.reconstruct_distance = False
 
         if self.marginalize_phase:
-            logging.info('Reconstruct phase')
+            logging.debug('Reconstruct phase')
             self.reconstruct_phase = True
             s, h = get_loglr()
             phasev = numpy.linspace(0, numpy.pi*2.0, int(1e4))
             # This assumes that the template was conjugated in inner products
             loglr = (numpy.exp(-2.0j * phasev) * s).real + h
             xl = draw_sample(loglr)
             rec['coa_phase'] = phasev[xl]
```

### Comparing `PyCBC-2.2.0/pycbc/inference/option_utils.py` & `PyCBC-2.2.1/pycbc/inference/option_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/__init__.py` & `PyCBC-2.2.1/pycbc/inference/sampler/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/base.py` & `PyCBC-2.2.1/pycbc/inference/sampler/base.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/base_cube.py` & `PyCBC-2.2.1/pycbc/inference/sampler/base_cube.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/base_mcmc.py` & `PyCBC-2.2.1/pycbc/inference/sampler/base_mcmc.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/base_multitemper.py` & `PyCBC-2.2.1/pycbc/inference/sampler/base_multitemper.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/cpnest.py` & `PyCBC-2.2.1/pycbc/inference/sampler/cpnest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/dummy.py` & `PyCBC-2.2.1/pycbc/inference/sampler/dummy.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/dynesty.py` & `PyCBC-2.2.1/pycbc/inference/sampler/dynesty.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/emcee.py` & `PyCBC-2.2.1/pycbc/inference/sampler/emcee.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/emcee_pt.py` & `PyCBC-2.2.1/pycbc/inference/sampler/emcee_pt.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/epsie.py` & `PyCBC-2.2.1/pycbc/inference/sampler/epsie.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/multinest.py` & `PyCBC-2.2.1/pycbc/inference/sampler/multinest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/ptemcee.py` & `PyCBC-2.2.1/pycbc/inference/sampler/ptemcee.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inference/sampler/ultranest.py` & `PyCBC-2.2.1/pycbc/inference/sampler/ultranest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inject/inject.py` & `PyCBC-2.2.1/pycbc/inject/inject.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/inject/injfilterrejector.py` & `PyCBC-2.2.1/pycbc/inject/injfilterrejector.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/io/__init__.py` & `PyCBC-2.2.1/pycbc/io/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/io/hdf.py` & `PyCBC-2.2.1/pycbc/io/hdf.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-# Convenience classes for accessing hdf5 trigger files
-# The 'get_column()' method is implemented parallel to
-# legacy pylal.SnglInspiralUtils functions
+"""
+Convenience classes for accessing hdf5 trigger files
+"""
 
 import h5py
 import numpy as np
 import logging
 import inspect
 import pickle
 
@@ -123,14 +123,18 @@
         groups: list of strings
             List of keys into each file. Required by the files option.
         """
         # Check that input fits with how the DictArray is set up
         if data and files:
             raise RuntimeError('DictArray can only have data or files as '
                                'input, not both.')
+        if data is None and files is None:
+            raise RuntimeError('DictArray needs either data or files at'
+                               'initialization. To set up an empty instance'
+                               'use DictArray(data={})')
         if files and not groups:
             raise RuntimeError('If files are given then need groups.')
 
         self.data = data
         self.groups = groups
         if files:
             self.data = {}
@@ -154,28 +158,34 @@
     def _return(self, data):
         return self.__class__(data=data)
 
     def __len__(self):
         return len(self.data[tuple(self.data.keys())[0]])
 
     def __add__(self, other):
+        if self.data == {}:
+            logging.debug('Adding data to a DictArray instance which'
+                          ' was initialized with an empty dict')
+            return self._return(data=other)
+
         data = {}
         for k in self.data:
             try:
                 data[k] = np.concatenate([self.data[k], other.data[k]])
             except KeyError:
                 logging.info('%s does not exist in other data' % k)
         return self._return(data=data)
 
     def select(self, idx):
         """ Return a new DictArray containing only the indexed values
         """
         data = {}
         for k in self.data:
-            data[k] = self.data[k][idx]
+            # Make sure each entry is an array (not a scalar)
+            data[k] = np.array(self.data[k][idx])
         return self._return(data=data)
 
     def remove(self, idx):
         """ Return a new DictArray that does not contain the indexed values
         """
         data = {}
         for k in self.data:
@@ -269,28 +279,28 @@
                              self.timeslide_id,
                              interval,
                              window)
         return self.select(cid)
 
 
 class FileData(object):
-
     def __init__(self, fname, group=None, columnlist=None, filter_func=None):
         """
         Parameters
         ----------
         group : string
             Name of group to be read from the file
         columnlist : list of strings
             Names of columns to be read; if None, use all existing columns
         filter_func : string
             String should evaluate to a Boolean expression using attributes
             of the class instance derived from columns: ex. 'self.snr < 6.5'
         """
         if not fname: raise RuntimeError("Didn't get a file!")
+
         self.fname = fname
         self.h5file = HFile(fname, "r")
         if group is None:
             if len(self.h5file.keys()) == 1:
                 group, = self.h5file.keys()
             else:
                 raise RuntimeError("Didn't get a group!")
@@ -324,14 +334,17 @@
                     if column in self.filter_func:
                         setattr(self, column, self.group[column][:])
                 self._mask = eval(self.filter_func)
             return self._mask
 
     def get_column(self, col):
         """
+        Method designed to be analogous to legacy pylal.SnglInspiralUtils
+        functionality
+
         Parameters
         ----------
         col : string
             Name of the dataset to be returned
 
         Returns
         -------
@@ -1011,90 +1024,77 @@
 
     def to_coinc_hdf_object(self, file_name):
         ofd = h5py.File(file_name,'w')
 
         # Some fields are special cases
         logging.info("Outputting search results")
         time = self.get_end_time()
-        ofd.create_dataset('time', data=time, dtype=np.float32)
+        # time will be used later to determine active ifos
+        ofd['time'] = time
 
         if self._inclusive:
-            ifar = self.get_coincfile_array('ifar')
-            ofd.create_dataset('ifar', data=ifar, dtype=np.float32)
+            ofd['ifar'] = self.get_coincfile_array('ifar')
+            ofd['p_value'] = self.get_coincfile_array('fap')
 
-        ifar_exc = self.get_coincfile_array('ifar_exc')
-        ofd.create_dataset('ifar_exclusive', data=ifar_exc,
-                           dtype=np.float32)
-
-        if self._inclusive:
-            fap = self.get_coincfile_array('fap')
-            ofd.create_dataset('p_value', data=fap,
-                               dtype=np.float32)
-
-        fap_exc = self.get_coincfile_array('fap_exc')
-        ofd.create_dataset('p_value_exclusive', data=fap_exc,
-                           dtype=np.float32)
+        ofd['ifar_exclusive'] = self.get_coincfile_array('ifar_exc')
+        ofd['p_value_exclusive'] = self.get_coincfile_array('fap_exc')
 
         # Coinc fields
         for field in ['stat']:
-            vals = self.get_coincfile_array(field)
-            ofd.create_dataset(field, data=vals, dtype=np.float32)
+            ofd[field] = self.get_coincfile_array(field)
 
         logging.info("Outputting template information")
         # Bank fields
         for field in ['mass1','mass2','spin1z','spin2z']:
-            vals = self.get_bankfile_array(field)
-            ofd.create_dataset(field, data=vals, dtype=np.float32)
+            ofd[field] = self.get_bankfile_array(field)
 
         mass1 = self.get_bankfile_array('mass1')
         mass2 = self.get_bankfile_array('mass2')
-        mchirp, _ = mass1_mass2_to_mchirp_eta(mass1, mass2)
-
-        ofd.create_dataset('chirp_mass', data=mchirp, dtype=np.float32)
+        ofd['chirp_mass'], _ = mass1_mass2_to_mchirp_eta(mass1, mass2)
 
         logging.info("Outputting single-trigger information")
         logging.info("reduced chisquared")
         chisq_vals_valid = self.get_snglfile_array_dict('chisq')
         chisq_dof_vals_valid = self.get_snglfile_array_dict('chisq_dof')
         for ifo in self.ifos:
             chisq_vals = chisq_vals_valid[ifo][0]
             chisq_valid = chisq_vals_valid[ifo][1]
             chisq_dof_vals = chisq_dof_vals_valid[ifo][0]
             rchisq = chisq_vals / (2. * chisq_dof_vals - 2.)
             rchisq[np.logical_not(chisq_valid)] = -1.
-            ofd.create_dataset(ifo + '_chisq', data=rchisq,
-                               dtype=np.float64)
+            ofd[ifo + '_chisq'] = rchisq
 
         # Single-detector fields
         for field in ['sg_chisq', 'end_time', 'sigmasq',
                       'psd_var_val']:
             logging.info(field)
             try:
                 vals_valid = self.get_snglfile_array_dict(field)
             except KeyError:
                 logging.info(field + " is not present in the "
                              "single-detector files")
+
             for ifo in self.ifos:
+                # Some of the values will not be valid for all IFOs,
+                # the `valid` parameter out of get_snglfile_array_dict
+                # tells us this, and we set the values to -1
                 vals = vals_valid[ifo][0]
                 valid = vals_valid[ifo][1]
                 vals[np.logical_not(valid)] = -1.
-                ofd.create_dataset(ifo + '_'+field, data=vals,
-                                   dtype=np.float32)
+                ofd[f'{ifo}_{field}'] = vals
 
         snr_vals_valid = self.get_snglfile_array_dict('snr')
         network_snr_sq = np.zeros_like(snr_vals_valid[self.ifos[0]][0])
         for ifo in self.ifos:
             vals = snr_vals_valid[ifo][0]
             valid = snr_vals_valid[ifo][1]
             vals[np.logical_not(valid)] = -1.
-            ofd.create_dataset(ifo + '_snr', data=vals,
-                               dtype=np.float32)
+            ofd[ifo + '_snr'] = vals
             network_snr_sq[valid] += vals[valid] ** 2.0
-        network_snr = np.sqrt(network_snr_sq)
-        ofd.create_dataset('network_snr', data=network_snr, dtype=np.float32)
+        ofd['network_snr'] = np.sqrt(network_snr_sq)
 
         logging.info("Triggered detectors")
         # Create a n_ifos by n_events matrix, with the ifo letter if the
         # event contains a trigger from the ifo, empty string if not
         triggered_matrix = [[ifo[0] if v else ''
                              for v in snr_vals_valid[ifo][1]]
                             for ifo in self.ifos]
```

### Comparing `PyCBC-2.2.0/pycbc/io/ligolw.py` & `PyCBC-2.2.1/pycbc/io/ligolw.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,14 +31,15 @@
 import pycbc.version as pycbc_version
 
 
 __all__ = (
     'default_null_value',
     'return_empty_sngl',
     'return_search_summary',
+    'create_process_table',
     'legacy_row_id_converter',
     'get_table_columns',
     'LIGOLWContentHandler'
 )
 
 ROWID_PYTYPE = int
 ROWID_TYPE = FromPyType[ROWID_PYTYPE]
```

### Comparing `PyCBC-2.2.0/pycbc/io/live.py` & `PyCBC-2.2.1/pycbc/io/live.py`

 * *Files 5% similar despite different names*

```diff
@@ -88,31 +88,27 @@
         if 'skyloc_data' in kwargs:
             sld = kwargs['skyloc_data']
             assert len({sld[ifo]['snr_series'].delta_t for ifo in sld}) == 1, \
                     "delta_t for all ifos do not match"
             snr_ifos = sld.keys()  # Ifos with SNR time series calculated
             self.snr_series = {ifo: sld[ifo]['snr_series'] for ifo in snr_ifos}
             # Extra ifos have SNR time series but not sngl inspiral triggers
-            extra_ifos = list(set(snr_ifos) - set(self.et_ifos))
 
             for ifo in snr_ifos:
                 # Ifos used for sky loc must have a PSD
                 assert ifo in self.psds
                 self.snr_series[ifo].start_time += self.time_offset
         else:
             self.snr_series = None
             snr_ifos = self.et_ifos
-            extra_ifos = []
 
         # Set up the bare structure of the xml document
         outdoc = ligolw.Document()
         outdoc.appendChild(ligolw.LIGO_LW())
 
-        # FIXME is it safe (in terms of downstream operations) to let
-        # `program_name` default to the actual script name?
         proc_id = create_process_table(outdoc, program_name='pycbc',
                                        detectors=snr_ifos).process_id
 
         # Set up coinc_definer table
         coinc_def_table = lsctables.New(lsctables.CoincDefTable)
         coinc_def_id = lsctables.CoincDefID(0)
         coinc_def_row = lsctables.CoincDef()
@@ -187,24 +183,14 @@
 
         # Set merger time to the mean of trigger peaks over coinc_results ifos
         self.merger_time = \
             numpy.mean([coinc_results[f'foreground/{ifo}/end_time'] for ifo in
                         self.et_ifos]) \
             + self.time_offset
 
-        # For extra detectors used only for sky loc, respect BAYESTAR's
-        # assumptions and checks
-        bayestar_check_fields = ('mass1 mass2 mtotal mchirp eta spin1x '
-                                 'spin1y spin1z spin2x spin2y spin2z').split()
-        for sngl in sngl_inspiral_table:
-            if sngl.ifo in extra_ifos:
-                for bcf in bayestar_check_fields:
-                    setattr(sngl, bcf, getattr(sngl_populated, bcf))
-                sngl.end = lal.LIGOTimeGPS(self.merger_time)
-
         outdoc.childNodes[0].appendChild(coinc_event_map_table)
         outdoc.childNodes[0].appendChild(sngl_inspiral_table)
 
         # Set up the coinc inspiral table
         coinc_inspiral_table = lsctables.New(lsctables.CoincInspiralTable)
         coinc_inspiral_row = lsctables.CoincInspiral()
         # This seems to be used as FAP, which should not be in gracedb
@@ -257,32 +243,32 @@
                 kwargs['padata'].do_pastro_calc(trigger_data, horizons)
         elif 'p_terr' in kwargs:
             self.p_astro, self.p_terr = 1 - kwargs['p_terr'], kwargs['p_terr']
         else:
             self.p_astro, self.p_terr = None, None
 
         # Source probabilities and hasmassgap estimation
+        self.probabilities = None
+        self.hasmassgap = None
         if 'mc_area_args' in kwargs:
             eff_distances = [sngl.eff_distance for sngl in sngl_inspiral_table]
             self.probabilities = calc_probabilities(coinc_inspiral_row.mchirp,
                                                     coinc_inspiral_row.snr,
                                                     min(eff_distances),
                                                     kwargs['mc_area_args'])
-            kwargs['hasmassgap_args'] = copy.deepcopy(kwargs['mc_area_args'])
-            kwargs['hasmassgap_args']['mass_gap'] = True
-            kwargs['hasmassgap_args']['mass_bdary']['ns_max'] = 3.0
-            kwargs['hasmassgap_args']['mass_bdary']['gap_max'] = 5.0
-            self.hasmassgap = calc_probabilities(
-                                  coinc_inspiral_row.mchirp,
-                                  coinc_inspiral_row.snr,
-                                  min(eff_distances),
-                                  kwargs['hasmassgap_args'])['Mass Gap']
-        else:
-            self.probabilities = None
-            self.hasmassgap = None
+            if 'embright_mg_max' in kwargs['mc_area_args']:
+                hasmg_args = copy.deepcopy(kwargs['mc_area_args'])
+                hasmg_args['mass_gap'] = True
+                hasmg_args['mass_bdary']['gap_max'] = \
+                    kwargs['mc_area_args']['embright_mg_max']
+                self.hasmassgap = calc_probabilities(
+                                      coinc_inspiral_row.mchirp,
+                                      coinc_inspiral_row.snr,
+                                      min(eff_distances),
+                                      hasmg_args)['Mass Gap']
 
         # Combine p astro and source probs
         if self.p_astro is not None and self.probabilities is not None:
             self.astro_probs = {cl: pr * self.p_astro for
                                 cl, pr in self.probabilities.items()}
             self.astro_probs['Terrestrial'] = self.p_terr
         else:
@@ -336,15 +322,15 @@
             self.pastro_file = os.path.join(save_dir, 'pa_pterr.json')
             with open(self.pastro_file, 'w') as pastrof:
                 json.dump({'p_astro': self.p_astro, 'p_terr': self.p_terr},
                           pastrof)
             logging.info('P_astro file saved as %s', self.pastro_file)
 
     def upload(self, fname, gracedb_server=None, testing=True,
-               extra_strings=None, search='AllSky'):
+               extra_strings=None, search='AllSky', labels=None):
         """Upload this candidate to GraceDB, and annotate it with a few useful
         plots and comments.
 
         Parameters
         ----------
         fname: str
             The name to give the xml file associated with this trigger
@@ -352,14 +338,16 @@
             URL to the GraceDB web API service for uploading the event.
             If omitted, the default will be used.
         testing: bool
             Switch to determine if the upload should be sent to gracedb as a
             test trigger (True) or a production trigger (False).
         search: str
             String going into the "search" field of the GraceDB event.
+        labels: list
+            Optional list of labels to tag the new event with.
         """
         import matplotlib
         matplotlib.use('Agg')
         import pylab as pl
 
         if fname.endswith('.xml.gz'):
             self.basename = fname.replace('.xml.gz', '')
@@ -369,67 +357,82 @@
             raise ValueError("Upload filename must end in .xml or .xml.gz, got"
                              " %s" % fname)
 
         # First make sure the event is saved on disk
         # as GraceDB operations can fail later
         self.save(fname)
 
+        # hardware injections need to be maked with the INJ tag
+        if self.is_hardware_injection:
+            labels = (labels or []) + ['INJ']
+
+        # connect to GraceDB if we are not reusing a connection
+        if not hasattr(self, 'gracedb'):
+            logging.info('Connecting to GraceDB')
+            gdbargs = {'reload_certificate': True, 'reload_buffer': 300}
+            if gracedb_server:
+                gdbargs['service_url'] = gracedb_server
+            try:
+                from ligo.gracedb.rest import GraceDb
+                self.gracedb = GraceDb(**gdbargs)
+            except Exception as exc:
+                logging.error('Failed to create GraceDB client')
+                logging.error(exc)
+
+        # create GraceDB event
+        logging.info('Uploading %s to GraceDB', fname)
+        group = 'Test' if testing else 'CBC'
         gid = None
         try:
-            if not hasattr(self, 'gracedb'):
-                from ligo.gracedb.rest import GraceDb
-                gdbargs = {'reload_certificate': True, 'reload_buffer': 300}
-                self.gracedb = GraceDb(gracedb_server, **gdbargs) \
-                    if gracedb_server else GraceDb(**gdbargs)
-            # create GraceDB event
-            group = 'Test' if testing else 'CBC'
-            r = self.gracedb.create_event(group, "pycbc", fname, search).json()
-            gid = r["graceid"]
+            response = self.gracedb.create_event(
+                group,
+                "pycbc",
+                fname,
+                search=search,
+                labels=labels
+            )
+            gid = response.json()["graceid"]
             logging.info("Uploaded event %s", gid)
-
-            if self.is_hardware_injection:
-                self.gracedb.write_label(gid, 'INJ')
-                logging.info("Tagging event %s as an injection", gid)
-
         except Exception as exc:
-            logging.error('Something failed during the upload of event %s on '
-                          'GraceDB. The event may not have been uploaded!',
-                          fname)
+            logging.error('Failed to create GraceDB event')
             logging.error(str(exc))
 
         # Upload em_bright properties JSON
-        if self.hasmassgap is not None:
+        if self.hasmassgap is not None and gid is not None:
             try:
                 self.gracedb.write_log(
                     gid, 'EM Bright properties JSON file upload',
                     filename=self.embright_file,
                     tag_name=['em_bright']
                 )
                 logging.info('Uploaded em_bright properties for %s', gid)
             except Exception as exc:
                 logging.error('Failed to upload em_bright properties file '
                               'for %s', gid)
                 logging.error(str(exc))
 
         # Upload multi-cpt p_astro JSON
-        if self.astro_probs is not None:
+        if self.astro_probs is not None and gid is not None:
             try:
                 self.gracedb.write_log(
                     gid, 'Multi-component p_astro JSON file upload',
                     filename=self.multipa_file,
                     tag_name=['p_astro'],
                     label='PASTRO_READY'
                 )
                 logging.info('Uploaded multi p_astro for %s', gid)
             except Exception as exc:
-                logging.error('Failed to upload multi p_astro file for %s', gid)
+                logging.error(
+                    'Failed to upload multi p_astro file for %s',
+                    gid
+                )
                 logging.error(str(exc))
 
         # If there is p_astro but no probabilities, upload p_astro JSON
-        if hasattr(self, 'pastro_file'):
+        if hasattr(self, 'pastro_file') and gid is not None:
             try:
                 self.gracedb.write_log(
                     gid, '2-component p_astro JSON file upload',
                     filename=self.pastro_file,
                     tag_name=['sig_info']
                 )
                 logging.info('Uploaded p_astro for %s', gid)
@@ -469,15 +472,15 @@
             for ifo in sorted(self.psds):
                 # Undo dynamic range factor
                 curr_psd = self.psds[ifo].astype(numpy.float64)
                 curr_psd /= pycbc.DYN_RANGE_FAC ** 2.0
                 curr_psd.save(snr_series_fname, group='%s/psd' % ifo)
 
         # Upload SNR series in HDF format and plots
-        if self.snr_series is not None:
+        if self.snr_series is not None and gid is not None:
             try:
                 self.gracedb.write_log(
                     gid, 'SNR timeseries HDF file upload',
                     filename=snr_series_fname
                 )
                 self.gracedb.write_log(
                     gid, 'SNR timeseries plot upload',
@@ -507,32 +510,39 @@
             colors = [source_color(label) for label in labels]
             fig, ax = pl.subplots()
             ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                    textprops={'fontsize': 15})
             ax.axis('equal')
             fig.savefig(self.prob_plotf)
             pl.close()
-            try:
-                self.gracedb.write_log(
-                    gid, 'Source probabilities JSON file upload',
-                    filename=self.prob_file,
-                    tag_name=['pe']
-                )
-                logging.info('Uploaded source probabilities for %s', gid)
-                self.gracedb.write_log(
-                    gid, 'Source probabilities plot upload',
-                    filename=self.prob_plotf,
-                    tag_name=['pe']
-                )
-                logging.info('Uploaded source probabilities pie chart for %s',
-                             gid)
-            except Exception as exc:
-                logging.error(
-                    'Failed to upload source probability results for %s', gid)
-                logging.error(str(exc))
+            if gid is not None:
+                try:
+                    self.gracedb.write_log(
+                        gid,
+                        'Source probabilities JSON file upload',
+                        filename=self.prob_file,
+                        tag_name=['pe']
+                    )
+                    logging.info('Uploaded source probabilities for %s', gid)
+                    self.gracedb.write_log(
+                        gid,
+                        'Source probabilities plot upload',
+                        filename=self.prob_plotf,
+                        tag_name=['pe']
+                    )
+                    logging.info(
+                        'Uploaded source probabilities pie chart for %s',
+                        gid
+                    )
+                except Exception as exc:
+                    logging.error(
+                        'Failed to upload source probability results for %s',
+                        gid
+                    )
+                    logging.error(str(exc))
 
         if gid is not None:
             try:
                 # Add code version info
                 gracedb_tag_with_version(self.gracedb, gid)
                 # Add any annotations to the event log
                 for text in (extra_strings or []):
```

### Comparing `PyCBC-2.2.0/pycbc/io/record.py` & `PyCBC-2.2.1/pycbc/io/record.py`

 * *Files 0% similar despite different names*

```diff
@@ -1913,12 +1913,14 @@
         return coordinates.cartesian_to_spherical_polar(
                                      self.spin2x, self.spin2y, self.spin2z)
 
     @property
     def remnant_mass(self):
         """Returns the remnant mass for an NS-BH binary."""
         return conversions.remnant_mass_from_mass1_mass2_cartesian_spin_eos(
-                                     self.mass1, self.mass2, self.spin1x,
-                                     self.spin1y, self.spin1z)
+                                     self.mass1, self.mass2,
+                                     spin1x=self.spin1x,
+                                     spin1y=self.spin1y,
+                                     spin1z=self.spin1z)
 
 
 __all__ = ['FieldArray', 'WaveformArray']
```

### Comparing `PyCBC-2.2.0/pycbc/libutils.py` & `PyCBC-2.2.1/pycbc/libutils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/mchirp_area.py` & `PyCBC-2.2.1/pycbc/mchirp_area.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/neutron_stars/eos_utils.py` & `PyCBC-2.2.1/pycbc/neutron_stars/eos_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -29,25 +29,25 @@
     """
     Load the data of an NS non-rotating equilibrium sequence generated
     using the equation of state (EOS) chosen by the user.
     File format is: grav mass (Msun), baryonic mass (Msun), compactness
 
     Parameters
     -----------
-    eos_name: string
+    eos_name : string
         NS equation of state label ('2H' is the only supported
         choice at the moment)
 
     Returns
     ----------
-    ns_sequence: numpy.array
+    ns_sequence : numpy.array
         contains the sequence data in the form NS gravitational
         mass (in solar masses), NS baryonic mass (in solar
         masses), NS compactness (dimensionless)
-    max_ns_g_mass: float
+    max_ns_g_mass : float
         the maximum NS gravitational mass (in solar masses) in
         the sequence (this is the mass of the most massive stable
         NS)
     """
     ns_sequence_file = os.path.join(
         NS_DATA_DIRECTORY, 'equil_{}.dat'.format(eos_name))
     if eos_name not in NS_SEQUENCES:
@@ -57,99 +57,113 @@
             'contain: NS gravitational mass (in solar masses), NS baryonic '
             'mass (in solar masses), NS compactness (dimensionless)')
     ns_sequence = np.loadtxt(ns_sequence_file)
     max_ns_g_mass = max(ns_sequence[:, 0])
     return (ns_sequence, max_ns_g_mass)
 
 
-def interp_grav_mass_to_baryon_mass(ns_g_mass, ns_sequence):
+def interp_grav_mass_to_baryon_mass(ns_g_mass, ns_sequence, extrapolate=False):
     """
     Determines the baryonic mass of an NS given its gravitational
     mass and an NS equilibrium sequence (in solar masses).
 
     Parameters
     -----------
-    ns_g_mass: float
+    ns_g_mass : float
         NS gravitational mass (in solar masses)
-    ns_sequence: numpy.array
-        contains the sequence data in the form NS gravitational
+    ns_sequence : numpy.array
+        Contains the sequence data in the form NS gravitational
         mass (in solar masses), NS baryonic mass (in solar
         masses), NS compactness (dimensionless)
+    extrapolate : boolean, optional
+        Invoke extrapolation in scipy.interpolate.interp1d.
+        Default is False (so ValueError is raised for ns_g_mass out of bounds)
 
     Returns
     ----------
     float
     """
     x = ns_sequence[:, 0]
     y = ns_sequence[:, 1]
-    f = interp1d(x, y)
-
+    fill_value = "extrapolate" if extrapolate else np.nan
+    f = interp1d(x, y, fill_value=fill_value)
     return f(ns_g_mass)
 
 
-def interp_grav_mass_to_compactness(ns_g_mass, ns_sequence):
+def interp_grav_mass_to_compactness(ns_g_mass, ns_sequence, extrapolate=False):
     """
     Determines the dimensionless compactness parameter of an NS given
     its gravitational mass and an NS equilibrium sequence.
 
     Parameters
     -----------
-    ns_g_mass: float
+    ns_g_mass : float
         NS gravitational mass (in solar masses)
-    ns_sequence: numpy.array
-        contains the sequence data in the form NS gravitational
+    ns_sequence : numpy.array
+        Contains the sequence data in the form NS gravitational
         mass (in solar masses), NS baryonic mass (in solar
         masses), NS compactness (dimensionless)
+    extrapolate : boolean, optional
+        Invoke extrapolation in scipy.interpolate.interp1d.
+        Default is False (so ValueError is raised for ns_g_mass out of bounds)
 
     Returns
     ----------
     float
     """
     x = ns_sequence[:, 0]
     y = ns_sequence[:, 2]
-    f = interp1d(x, y)
-
+    fill_value = "extrapolate" if extrapolate else np.nan
+    f = interp1d(x, y, fill_value=fill_value)
     return f(ns_g_mass)
 
 
-def initialize_eos(ns_mass, eos):
+def initialize_eos(ns_mass, eos, extrapolate=False):
     """Load an equation of state and return the compactness and baryonic
     mass for a given neutron star mass
 
     Parameters
     ----------
     ns_mass : {float, array}
-        The mass of the neutron star, in solar masses.
+        The gravitational mass of the neutron star, in solar masses.
     eos : str
         Name of the equation of state.
+    extrapolate : boolean, optional
+        Invoke extrapolation in scipy.interpolate.interp1d in the low-mass
+        regime. In the high-mass regime, the maximum NS mass supported by the
+        equation of state is not allowed to be exceeded. Default is False
+        (so ValueError is raised whenever ns_mass is out of bounds).
 
     Returns
     -------
     ns_compactness : float
         Compactness parameter of the neutron star.
     ns_b_mass : float
         Baryonic mass of the neutron star.
     """
     if isinstance(ns_mass, np.ndarray):
         input_is_array = True
     if eos in NS_SEQUENCES:
         ns_seq, ns_max = load_ns_sequence(eos)
+        # Never extrapolate beyond the maximum NS mass allowed by the EOS
         try:
             if any(ns_mass > ns_max) and input_is_array:
                 raise ValueError(
                     f'Maximum NS mass for {eos} is {ns_max}, received masses '
                     f'up to {max(ns_mass[ns_mass > ns_max])}')
         except TypeError:
             if ns_mass > ns_max and not input_is_array:
                 raise ValueError(
                     f'Maximum NS mass for {eos} is {ns_max}, received '
                     f'{ns_mass}')
         # Interpolate NS compactness and rest mass
-        ns_compactness = interp_grav_mass_to_compactness(ns_mass, ns_seq)
-        ns_b_mass = interp_grav_mass_to_baryon_mass(ns_mass, ns_seq)
+        ns_compactness = interp_grav_mass_to_compactness(
+            ns_mass, ns_seq, extrapolate=extrapolate)
+        ns_b_mass = interp_grav_mass_to_baryon_mass(
+            ns_mass, ns_seq, extrapolate=extrapolate)
     elif eos in lalsim.SimNeutronStarEOSNames:
         #eos_obj = lalsim.SimNeutronStarEOSByName(eos)
         #eos_fam = lalsim.CreateSimNeutronStarFamily(eos_obj)
         #r_ns = lalsim.SimNeutronStarRadius(ns_mass * lal.MSUN_SI, eos_obj)
         #ns_compactness = lal.G_SI * ns_mass * lal.MSUN_SI / (r_ns * lal.C_SI**2)
         raise NotImplementedError(
             'LALSimulation EOS interface not yet implemented!')
```

### Comparing `PyCBC-2.2.0/pycbc/neutron_stars/ns_data/equil_2H.dat` & `PyCBC-2.2.1/pycbc/neutron_stars/ns_data/equil_2H.dat`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/neutron_stars/pg_isso_solver.py` & `PyCBC-2.2.1/pycbc/neutron_stars/pg_isso_solver.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/noise/gaussian.py` & `PyCBC-2.2.1/pycbc/noise/gaussian.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/noise/reproduceable.py` & `PyCBC-2.2.1/pycbc/noise/reproduceable.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/opt.py` & `PyCBC-2.2.1/pycbc/opt.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/pnutils.py` & `PyCBC-2.2.1/pycbc/pnutils.py`

 * *Files 0% similar despite different names*

```diff
@@ -471,14 +471,16 @@
     "SEOBNRv2RD": lambda p: get_freq("fSEOBNRv2RD", p["mass1"], p["mass2"],
                                      p["spin1z"], p["spin2z"]),
     "SEOBNRv2Peak": lambda p: get_freq("fSEOBNRv2Peak", p["mass1"], p["mass2"],
                                        p["spin1z"], p["spin2z"]),
     "SEOBNRv4RD": lambda p: get_freq("fSEOBNRv4RD", p["mass1"], p["mass2"],
                                      p["spin1z"], p["spin2z"]),
     "SEOBNRv4Peak": lambda p: get_freq("fSEOBNRv4Peak", p["mass1"], p["mass2"],
+                                       p["spin1z"], p["spin2z"]),
+    "SEOBNRv5Peak": lambda p: get_freq("fSEOBNRv5Peak", p["mass1"], p["mass2"],
                                        p["spin1z"], p["spin2z"])
     }
 
 def frequency_cutoff_from_name(name, m1, m2, s1z, s2z):
     """
     Returns the result of evaluating the frequency cutoff function
     specified by 'name' on a template with given parameters.
@@ -508,14 +510,17 @@
     """Wrapper of lalsimulation template duration approximate formula"""
     m1, m2, s1z, s2z, f_low = float(m1), float(m2), float(s1z), float(s2z),\
                               float(f_low)
     if approximant == "SEOBNRv2":
         chi = lalsimulation.SimIMRPhenomBComputeChi(m1, m2, s1z, s2z)
         time_length = lalsimulation.SimIMRSEOBNRv2ChirpTimeSingleSpin(
                                 m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, chi, f_low)
+    elif approximant == 'IMRPhenomXAS':
+        time_length = lalsimulation.SimIMRPhenomXASDuration(
+                           m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, s1z, s2z, f_low)
     elif approximant == "IMRPhenomD":
         time_length = lalsimulation.SimIMRPhenomDChirpTime(
                            m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, s1z, s2z, f_low)
     elif approximant == "SEOBNRv4":
         # NB for no clear reason this function has f_low as first argument
         time_length = lalsimulation.SimIMRSEOBNRv4ROMTimeOfFrequency(
                            f_low, m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, s1z, s2z)
```

### Comparing `PyCBC-2.2.0/pycbc/pool.py` & `PyCBC-2.2.1/pycbc/pool.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/population/fgmc_functions.py` & `PyCBC-2.2.1/pycbc/population/fgmc_functions.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/population/fgmc_laguerre.py` & `PyCBC-2.2.1/pycbc/population/fgmc_laguerre.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/population/fgmc_plots.py` & `PyCBC-2.2.1/pycbc/population/fgmc_plots.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/population/live_pastro.py` & `PyCBC-2.2.1/pycbc/population/live_pastro.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,14 +27,35 @@
            len(spec_json['sig_per_yr_binned']) + 1
     assert 'ref_bns_horizon' in spec_json  # float
     assert 'netsnr_thresh' in spec_json  # float
 
     return spec_json
 
 
+def check_template_param_bin_farlim_data(spec_json):
+    """
+    Parameters
+    ----------
+    spec_json: JSON dictionary-like object
+        Result of parsing json file containing static data
+
+    Returns
+    -------
+    spec_json: dictionary
+    """
+    # Standard template param bin checks
+    check_template_param_bin_data(spec_json)
+
+    # In addition, need limiting FAR and SNR values
+    assert 'limit_far' in spec_json
+    assert 'limit_snr' in spec_json
+
+    return spec_json
+
+
 def read_template_bank_param(spec_d, bankf):
     """
     Parameters
     ----------
     spec_d: dictionary
         Prerequisite data for p astro calc
     bankf: string
@@ -202,15 +223,15 @@
 
     # Get noise rate density
     if 'bg_fac' not in padata.spec:
         expfac = 6.
     else:
         expfac = padata.spec['bg_fac']
 
-    # Ifos over trigger threshold
+    # List of ifos over trigger threshold
     tr_ifos = trdata['triggered']
 
     # FAR is in Hz, therefore convert to rate per year (per SNR)
     dnoise = noise_density_from_far(trdata['far'], expfac) * lal_s_per_yr
     logging.debug('FAR %.3g, noise density per yr per SNR %.3g',
                   trdata['far'], dnoise)
     # Scale by fraction of templates in bin
@@ -234,17 +255,41 @@
     logging.debug('After triggered ifo rate rescaling %.3g', dsig)
 
     p_astro = dsig / (dsig + dnoise)
     logging.debug('p_astro %.4g', p_astro)
     return p_astro, 1 - p_astro
 
 
+def template_param_bin_types_farlim_pa(padata, trdata, horizons):
+    """
+    Parameters
+    ----------
+    padata: PAstroData instance
+        Static information on p astro calculation
+    trdata: dictionary
+        Trigger properties
+    horizons: dictionary
+        BNS horizon distances keyed on ifo
+
+    Returns
+    -------
+    p_astro, p_terr: tuple of floats
+    """
+    # If the network SNR and FAR indicate saturation of the FAR estimate,
+    # set them to specified fixed values
+    trdata = padata.apply_significance_limits(trdata)
+
+    # Now perform standard calculation with event types
+    return template_param_bin_types_pa(padata, trdata, horizons)
+
+
 __all__ = [
     "check_template_param_bin_data",
     "read_template_bank_param",
     "noise_density_from_far",
     "signal_pdf_from_snr",
     "signal_rate_rescale",
     "signal_rate_trig_type",
     "template_param_bin_pa",
     "template_param_bin_types_pa",
+    "template_param_bin_types_farlim_pa",
 ]
```

### Comparing `PyCBC-2.2.0/pycbc/population/live_pastro_utils.py` & `PyCBC-2.2.1/pycbc/population/live_pastro_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -23,26 +23,31 @@
                                    'up p_astro calculation')
 
     return live_pastro_group
 
 
 # Choices of p astro calc method
 _check_spec = {
-      'template_param_bins': livepa.check_template_param_bin_data,
-      'template_param_bins_types': livepa.check_template_param_bin_data
+    'template_param_bins': livepa.check_template_param_bin_data,
+    'template_param_bins_types': livepa.check_template_param_bin_data,
+    'template_param_bins_types_farlim':
+        livepa.check_template_param_bin_farlim_data
 }
 
 _read_bank = {
-      'template_param_bins': livepa.read_template_bank_param,
-      'template_param_bins_types': livepa.read_template_bank_param
+    'template_param_bins': livepa.read_template_bank_param,
+    'template_param_bins_types': livepa.read_template_bank_param,
+    'template_param_bins_types_farlim': livepa.read_template_bank_param
 }
 
 _do_calc = {
-      'template_param_bins': livepa.template_param_bin_pa,
-      'template_param_bins_types': livepa.template_param_bin_types_pa
+    'template_param_bins': livepa.template_param_bin_pa,
+    'template_param_bins_types': livepa.template_param_bin_types_pa,
+    'template_param_bins_types_farlim':
+        livepa.template_param_bin_types_farlim_pa
 }
 
 
 class PAstroData():
     """ Class for managing live p_astro calculation persistent info """
     def __init__(self, specfile, bank):
         """
@@ -67,14 +72,40 @@
             except KeyError as ke:
                 raise ValueError("Can't find 'method' in p_astro spec file!") \
                     from ke
             logging.info('Setting up p_astro data with method %s', self.method)
             self.spec = _check_spec[self.method](self.spec_json)
             self.bank = _read_bank[self.method](self.spec, bank)
 
+    def apply_significance_limits(self, trigger_data):
+        """
+        If the network SNR and FAR indicate saturation of the FAR estimate,
+        set them to the fixed values given in the specification.
+        """
+        # This only happens for double or triple events
+        if len(trigger_data['triggered']) == 1:
+            return trigger_data
+
+        if len(trigger_data['triggered']) > 1:
+            farlim = self.spec['limit_far']
+            snrlim = self.spec['limit_snr']
+            # Only do anything if FAR and SNR are beyond given limits
+            if trigger_data['far'] > farlim or \
+                    trigger_data['network_snr'] < snrlim:
+                return trigger_data
+
+            logging.debug('Truncating FAR and SNR from %f, %f to %f, %f',
+                          trigger_data['far'], trigger_data['network_snr'],
+                          farlim, snrlim)
+            trigger_data['network_snr'] = snrlim
+            trigger_data['far'] = farlim
+            return trigger_data
+
+        raise RuntimeError('Number of triggered ifos must be >0 !')
+
     def do_pastro_calc(self, trigger_data, horizons):
         """ No-op, or call the despatch dictionary to evaluate p_astro """
         if not self.do:
             return None, None
 
         logging.info('Computing p_astro')
         p_astro, p_terr = _do_calc[self.method](self, trigger_data, horizons)
```

### Comparing `PyCBC-2.2.0/pycbc/population/population_models.py` & `PyCBC-2.2.1/pycbc/population/population_models.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/population/rates_functions.py` & `PyCBC-2.2.1/pycbc/population/rates_functions.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/population/scale_injections.py` & `PyCBC-2.2.1/pycbc/population/scale_injections.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/psd/__init__.py` & `PyCBC-2.2.1/pycbc/psd/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/psd/analytical.py` & `PyCBC-2.2.1/pycbc/psd/analytical.py`

 * *Files 5% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 space-borne ones, see `pycbc.psd.analytical_space` module.
 """
 import numbers
 from pycbc.types import FrequencySeries
 from pycbc.psd.analytical_space import (
     analytical_psd_lisa_tdi_1p5_XYZ, analytical_psd_lisa_tdi_2p0_XYZ,
     analytical_psd_lisa_tdi_1p5_AE, analytical_psd_lisa_tdi_1p5_T,
-    sh_transformed_psd_lisa_tdi_XYZ)
+    sh_transformed_psd_lisa_tdi_XYZ, analytical_psd_lisa_tdi_AE_confusion)
 import lal
 import numpy
 
 # build a list of usable PSD functions from lalsimulation
 _name_prefix = 'SimNoisePSD'
 _name_suffix = 'Ptr'
 _name_blacklist = ('FromFile', 'MirrorTherm', 'Quantum', 'Seismic', 'Shot', 'SuspTherm')
@@ -169,8 +169,9 @@
 pycbc_analytical_psds = {
     'flat_unity' : flat_unity,
     'analytical_psd_lisa_tdi_1p5_XYZ' : analytical_psd_lisa_tdi_1p5_XYZ,
     'analytical_psd_lisa_tdi_2p0_XYZ' : analytical_psd_lisa_tdi_2p0_XYZ,
     'analytical_psd_lisa_tdi_1p5_AE' : analytical_psd_lisa_tdi_1p5_AE,
     'analytical_psd_lisa_tdi_1p5_T' : analytical_psd_lisa_tdi_1p5_T,
     'sh_transformed_psd_lisa_tdi_XYZ' : sh_transformed_psd_lisa_tdi_XYZ,
+    'analytical_psd_lisa_tdi_AE_confusion' : analytical_psd_lisa_tdi_AE_confusion,
 }
```

### Comparing `PyCBC-2.2.0/pycbc/psd/analytical_space.py` & `PyCBC-2.2.1/pycbc/psd/analytical_space.py`

 * *Files 10% similar despite different names*

```diff
@@ -31,20 +31,22 @@
 import numpy as np
 from astropy import constants
 from pycbc.psd.read import from_numpy_arrays
 
 
 def psd_lisa_acc_noise(f, acc_noise_level=3e-15):
     """ The PSD of LISA's acceleration noise.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     acc_noise_level : float
         The level of acceleration noise.
+
     Returns
     -------
     s_acc_nu : float or numpy.array
         The PSD value or array for acceleration noise.
     Notes
     -----
         Pease see Eq.(11-13) in <LISA-LCST-SGS-TN-001> for more details.
@@ -54,20 +56,22 @@
     s_acc_nu = (2*np.pi*f/constants.c.value)**2 * s_acc_d
 
     return s_acc_nu
 
 
 def psd_lisa_oms_noise(f, oms_noise_level=15e-12):
     """ The PSD of LISA's OMS noise.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     s_oms_nu : float or numpy.array
         The PSD value or array for OMS noise.
     Notes
     -----
         Pease see Eq.(9-10) in <LISA-LCST-SGS-TN-001> for more details.
@@ -76,69 +80,75 @@
     s_oms_nu = s_oms_d * (2*np.pi*f/constants.c.value)**2
 
     return s_oms_nu
 
 
 def lisa_psd_components(f, acc_noise_level=3e-15, oms_noise_level=15e-12):
     """ The PSD of LISA's acceleration and OMS noise.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     [low_freq_component, high_freq_component] : list
         The PSD value or array for acceleration and OMS noise.
     """
     low_freq_component = psd_lisa_acc_noise(f, acc_noise_level)
     high_freq_component = psd_lisa_oms_noise(f, oms_noise_level)
 
     return [low_freq_component, high_freq_component]
 
 
 def omega_length(f, len_arm=2.5e9):
     """ The function to calculate 2*pi*f*LISA_arm_length.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     len_arm : float
         The arm length of LISA.
+
     Returns
     -------
     omega_len : float or numpy.array
         The value of 2*pi*f*LISA_arm_length.
     """
     omega_len = 2*np.pi*f * len_arm/constants.c.value
 
     return omega_len
 
 
 def analytical_psd_lisa_tdi_1p5_XYZ(length, delta_f, low_freq_cutoff,
                                     len_arm=2.5e9, acc_noise_level=3e-15,
                                     oms_noise_level=15e-12):
     """ The TDI-1.5 analytical PSD (X,Y,Z channel) for LISA.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
     len_arm : float
         The arm length of LISA, in the unit of "m".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     fseries : FrequencySeries
         The TDI-1.5 PSD (X,Y,Z channel) for LISA.
     Notes
     -----
         Pease see Eq.(19) in <LISA-LCST-SGS-TN-001> for more details.
@@ -160,28 +170,30 @@
     return fseries
 
 
 def analytical_psd_lisa_tdi_2p0_XYZ(length, delta_f, low_freq_cutoff,
                                     len_arm=2.5e9, acc_noise_level=3e-15,
                                     oms_noise_level=15e-12):
     """ The TDI-2.0 analytical PSD (X,Y,Z channel) for LISA.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
     len_arm : float
         The arm length of LISA, in the unit of "m".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     fseries : FrequencySeries
         The TDI-2.0 PSD (X,Y,Z channel) for LISA.
     Notes
     -----
         Pease see Eq.(20) in <LISA-LCST-SGS-TN-001> for more details.
@@ -203,28 +215,30 @@
     return fseries
 
 
 def analytical_csd_lisa_tdi_1p5_XY(length, delta_f, low_freq_cutoff,
                                    len_arm=2.5e9, acc_noise_level=3e-15,
                                    oms_noise_level=15e-12):
     """ The cross-spectrum density between LISA's TDI channel X and Y.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
     len_arm : float
         The arm length of LISA, in the unit of "m".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     fseries : FrequencySeries
         The CSD between LISA's TDI-1.5 channel X and Y.
     Notes
     -----
         Pease see Eq.(56) in <LISA-LCST-SGS-MAN-001(Radler)> for more details.
@@ -245,28 +259,30 @@
     return fseries
 
 
 def analytical_psd_lisa_tdi_1p5_AE(length, delta_f, low_freq_cutoff,
                                    len_arm=2.5e9, acc_noise_level=3e-15,
                                    oms_noise_level=15e-12):
     """ The PSD of LISA's TDI-1.5 channel A and E.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
     len_arm : float
         The arm length of LISA, in the unit of "m".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     fseries : FrequencySeries
         The PSD of LISA's TDI-1.5 channel A and E.
     Notes
     -----
         Pease see Eq.(58) in <LISA-LCST-SGS-MAN-001(Radler)> for more details.
@@ -289,28 +305,30 @@
     return fseries
 
 
 def analytical_psd_lisa_tdi_1p5_T(length, delta_f, low_freq_cutoff,
                                   len_arm=2.5e9, acc_noise_level=3e-15,
                                   oms_noise_level=15e-12):
     """ The PSD of LISA's TDI-1.5 channel T.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
     len_arm : float
         The arm length of LISA, in the unit of "m".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     fseries : FrequencySeries
         The PSD of LISA's TDI-1.5 channel T.
     Notes
     -----
         Pease see Eq.(59) in <LISA-LCST-SGS-MAN-001(Radler)> for more details.
@@ -331,58 +349,60 @@
 
     return fseries
 
 
 def averaged_lisa_fplus_sq_approx(f, len_arm=2.5e9):
     """ An approximant for LISA's squared antenna response function,
     averaged over sky and polarization angle.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     len_arm : float
         The arm length of LISA, in the unit of "m".
+
     Returns
     -------
     fp_sq_approx : float or numpy.array
         The sky and polarization angle averaged squared antenna response.
     Notes
     -----
         Pease see Eq.(36) in <LISA-LCST-SGS-TN-001> for more details.
     """
-    from os import getcwd, path
-    from urllib import request
     from scipy.interpolate import interp1d
+    from astropy.utils.data import download_file
 
     if len_arm != 2.5e9:
         raise Exception("Currently only support 'len_arm=2.5e9'.")
-    cwd = getcwd()
-    if path.exists(cwd+"/AvFXp2_Raw.npy") is False:
-        url = "https://zenodo.org/record/7497853/files/AvFXp2_Raw.npy"
-        request.urlretrieve(url, cwd+"/AvFXp2_Raw.npy")
-    freqs, fp_sq = np.load(cwd+"/AvFXp2_Raw.npy")
+    # Download the numerical LISA averaged response.
+    url = "https://zenodo.org/record/7497853/files/AvFXp2_Raw.npy"
+    file_path = download_file(url, cache=True)
+    freqs, fp_sq = np.load(file_path)
     # Padding the end.
     freqs = np.append(freqs, 2)
     fp_sq = np.append(fp_sq, 0.0012712348970728724)
     fp_sq_interp = interp1d(freqs, fp_sq, kind='linear',
                             fill_value="extrapolate")
     fp_sq_approx = fp_sq_interp(f)/16
 
     return fp_sq_approx
 
 
 def averaged_response_lisa_tdi_1p5(f, len_arm=2.5e9):
     """ LISA's TDI-1.5 response function to GW,
     averaged over sky and polarization angle.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     len_arm : float
         The arm length of LISA, in the unit of "m".
+
     Returns
     -------
     response_tdi_1p5 : float or numpy.array
         The sky and polarization angle averaged TDI-1.5 response to GW.
     Notes
     -----
         Pease see Eq.(39) in <LISA-LCST-SGS-TN-001> for more details.
@@ -393,20 +413,22 @@
 
     return response_tdi_1p5
 
 
 def averaged_response_lisa_tdi_2p0(f, len_arm=2.5e9):
     """ LISA's TDI-2.0 response function to GW,
     averaged over sky and polarization angle.
+
     Parameters
     ----------
     f : float or numpy.array
         The frequency or frequency range, in the unit of "Hz".
     len_arm : float
         The arm length of LISA, in the unit of "m".
+
     Returns
     -------
     response_tdi_2p0 : float or numpy.array
         The sky and polarization angle averaged TDI-2.0 response to GW.
     Notes
     -----
         Pease see Eq.(40) in <LISA-LCST-SGS-TN-001> for more details.
@@ -420,28 +442,30 @@
 
 def sensitivity_curve_lisa_semi_analytical(length, delta_f, low_freq_cutoff,
                                            len_arm=2.5e9,
                                            acc_noise_level=3e-15,
                                            oms_noise_level=15e-12):
     """ The semi-analytical LISA's sensitivity curve (6-links),
     averaged over sky and polarization angle.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
     len_arm : float
         The arm length of LISA, in the unit of "m".
     acc_noise_level : float
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
+
     Returns
     -------
     fseries : FrequencySeries
         The sky and polarization angle averaged semi-analytical
         LISA's sensitivity curve (6-links).
     Notes
     -----
@@ -464,22 +488,24 @@
 
     return fseries
 
 
 def sensitivity_curve_lisa_SciRD(length, delta_f, low_freq_cutoff):
     """ The analytical LISA's sensitivity curve in SciRD,
     averaged over sky and polarization angle.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
         Low-frequency cutoff for output FrequencySeries.
+
     Returns
     -------
     fseries : FrequencySeries
         The sky and polarization angle averaged analytical
         LISA's sensitivity curve in SciRD.
     Notes
     -----
@@ -500,14 +526,15 @@
 
 def sensitivity_curve_lisa_confusion(length, delta_f, low_freq_cutoff,
                                      len_arm=2.5e9, acc_noise_level=3e-15,
                                      oms_noise_level=15e-12,
                                      base_model="semi", duration=1.0):
     """ The LISA's sensitivity curve with Galactic confusion noise,
     averaged over sky and polarization angle.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
@@ -518,28 +545,29 @@
         The level of acceleration noise.
     oms_noise_level : float
         The level of OMS noise.
     base_model : string
         The base model of sensitivity curve, chosen from "semi" or "SciRD".
     duration : float
         The duration of observation, between 0 and 10, in the unit of years.
+
     Returns
     -------
     fseries : FrequencySeries
         The sky and polarization angle averaged
         LISA's sensitivity curve with Galactic confusion noise.
     Notes
     -----
         Pease see Eq.(85-86) in <LISA-LCST-SGS-TN-001> for more details.
     """
     if base_model == "semi":
         base_curve = sensitivity_curve_lisa_semi_analytical(
             length, delta_f, low_freq_cutoff,
             len_arm, acc_noise_level, oms_noise_level)
-    elif base_curve == "SciRD":
+    elif base_model == "SciRD":
         base_curve = sensitivity_curve_lisa_SciRD(
             length, delta_f, low_freq_cutoff)
     else:
         raise Exception("Must choose from 'semi' or 'SciRD'.")
     if duration < 0 or duration > 10:
         raise Exception("Must between 0 and 10.")
     fr = np.linspace(low_freq_cutoff, (length-1)*2*delta_f, length)
@@ -561,14 +589,15 @@
 def sh_transformed_psd_lisa_tdi_XYZ(length, delta_f, low_freq_cutoff,
                                     len_arm=2.5e9, acc_noise_level=3e-15,
                                     oms_noise_level=15e-12,
                                     base_model="semi", duration=1.0,
                                     tdi="1.5"):
     """ The TDI-1.5/2.0 PSD (X,Y,Z channel) for LISA
     with Galactic confusion noise, transformed from LISA sensitivity curve.
+
     Parameters
     ----------
     length : int
         Length of output Frequencyseries.
     delta_f : float
         Frequency step for output FrequencySeries.
     low_freq_cutoff : float
@@ -581,14 +610,15 @@
         The level of OMS noise.
     base_model : string
         The base model of sensitivity curve, chosen from "semi" or "SciRD".
     duration : float
         The duration of observation, between 0 and 10, in the unit of years.
     tdi : string
         The version of TDI, currently only for 1.5 or 2.0.
+
     Returns
     -------
     fseries : FrequencySeries
         The TDI-1.5/2.0 PSD (X,Y,Z channel) for LISA with Galactic confusion
         noise, transformed from LISA sensitivity curve.
     Notes
     -----
@@ -608,7 +638,106 @@
                                           oms_noise_level, base_model,
                                           duration)
     psd = 2*sh.data * fseries_response.data
     fseries = from_numpy_arrays(sh.sample_frequencies, psd,
                                 length, delta_f, low_freq_cutoff)
 
     return fseries
+
+
+def semi_analytical_psd_lisa_confusion_noise(length, delta_f, low_freq_cutoff,
+                                             len_arm=2.5e9, duration=1.0,
+                                             tdi="1.5"):
+    """ The TDI-1.5/2.0 PSD (X,Y,Z channel) for LISA Galactic confusion noise,
+    no instrumental noise.
+
+    Parameters
+    ----------
+    length : int
+        Length of output Frequencyseries.
+    delta_f : float
+        Frequency step for output FrequencySeries.
+    low_freq_cutoff : float
+        Low-frequency cutoff for output FrequencySeries.
+    len_arm : float
+        The arm length of LISA, in the unit of "m".
+    duration : float
+        The duration of observation, between 0 and 10, in the unit of years.
+    tdi : string
+        The version of TDI, currently only for 1.5 or 2.0.
+
+    Returns
+    -------
+    fseries : FrequencySeries
+        The TDI-1.5/2.0 PSD (X,Y,Z channel) for LISA Galactic confusion
+        noise, no instrumental noise.
+    """
+    fr = np.linspace(low_freq_cutoff, (length-1)*2*delta_f, length)
+    if tdi == "1.5":
+        response = averaged_response_lisa_tdi_1p5(fr, len_arm)
+    elif tdi == "2.0":
+        response = averaged_response_lisa_tdi_2p0(fr, len_arm)
+    else:
+        raise Exception("The version of TDI, currently only for 1.5 or 2.0.")
+    fseries_response = from_numpy_arrays(fr, np.array(response),
+                                         length, delta_f, low_freq_cutoff)
+    sh_confusion = []
+    f1 = 10**(-0.25*np.log10(duration)-2.7)
+    fk = 10**(-0.27*np.log10(duration)-2.47)
+    for f in fr:
+        sh_confusion.append(0.5*1.14e-44*f**(-7/3)*np.exp(-(f/f1)**1.8) *
+                            (1.0+np.tanh((fk-f)/(0.31e-3))))
+    fseries_confusion = from_numpy_arrays(fr, np.array(sh_confusion),
+                                          length, delta_f, low_freq_cutoff)
+    psd_confusion = 2*fseries_confusion.data * fseries_response.data
+    fseries = from_numpy_arrays(fseries_confusion.sample_frequencies,
+                                psd_confusion, length, delta_f,
+                                low_freq_cutoff)
+
+    return fseries
+
+
+def analytical_psd_lisa_tdi_AE_confusion(length, delta_f, low_freq_cutoff,
+                                         len_arm=2.5e9, acc_noise_level=3e-15,
+                                         oms_noise_level=15e-12,
+                                         duration=1.0, tdi="1.5"):
+    """ The TDI-1.5 PSD (A,E channel) for LISA
+    with Galactic confusion noise.
+
+    Parameters
+    ----------
+    length : int
+        Length of output Frequencyseries.
+    delta_f : float
+        Frequency step for output FrequencySeries.
+    low_freq_cutoff : float
+        Low-frequency cutoff for output FrequencySeries.
+    len_arm : float
+        The arm length of LISA, in the unit of "m".
+    acc_noise_level : float
+        The level of acceleration noise.
+    oms_noise_level : float
+        The level of OMS noise.
+    duration : float
+        The duration of observation, between 0 and 10, in the unit of years.
+    tdi : string
+        The version of TDI, currently only for 1.5.
+
+    Returns
+    -------
+    fseries : FrequencySeries
+        The TDI-1.5 PSD (A,E channel) for LISA with Galactic confusion
+        noise.
+    """
+    if tdi != "1.5":
+        raise Exception("The version of TDI, currently only for 1.5.")
+    psd_AE = analytical_psd_lisa_tdi_1p5_AE(length, delta_f, low_freq_cutoff,
+                                            len_arm, acc_noise_level,
+                                            oms_noise_level)
+    psd_X_confusion = semi_analytical_psd_lisa_confusion_noise(
+                        length, delta_f, low_freq_cutoff,
+                        len_arm, duration, tdi)
+    # Here we assume the confusion noise's contribution to the CSD Sxy is
+    # negligible for low-frequency part. So Sxy doesn't change.
+    fseries = psd_AE + psd_X_confusion
+
+    return fseries
```

### Comparing `PyCBC-2.2.0/pycbc/psd/estimate.py` & `PyCBC-2.2.1/pycbc/psd/estimate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/psd/read.py` & `PyCBC-2.2.1/pycbc/psd/read.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/psd/variation.py` & `PyCBC-2.2.1/pycbc/psd/variation.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/rate.py` & `PyCBC-2.2.1/pycbc/rate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/color.py` & `PyCBC-2.2.1/pycbc/results/color.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/dq.py` & `PyCBC-2.2.1/pycbc/results/dq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/followup.py` & `PyCBC-2.2.1/pycbc/results/followup.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/layout.py` & `PyCBC-2.2.1/pycbc/results/layout.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/metadata.py` & `PyCBC-2.2.1/pycbc/results/metadata.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/mpld3_utils.py` & `PyCBC-2.2.1/pycbc/results/mpld3_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/plot.py` & `PyCBC-2.2.1/pycbc/results/plot.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/psd.py` & `PyCBC-2.2.1/pycbc/results/psd.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/pygrb_postprocessing_utils.py` & `PyCBC-2.2.1/pycbc/results/pygrb_postprocessing_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -19,35 +19,32 @@
 # Preamble
 # =============================================================================
 
 """
 Module to generate PyGRB figures: scatter plots and timeseries.
 """
 
-import glob
 import os
 import logging
 import argparse
 import copy
 import numpy
 import h5py
 from scipy import stats
 from pycbc.detector import Detector
-import pycbc.workflow as _workflow
-from pycbc.workflow.core import resolve_url_to_file
 # All/most of these final imports will become obsolete with hdf5 switch
 try:
     from ligo import segments
     from ligo.lw import utils, lsctables
     from ligo.lw.table import Table
     from ligo.segments.utils import fromsegwizard
-    # Handle MultiInspiral xml-talbes with glue,
+    # Handle MultiInspiral xml-tables with glue,
     # as ligo.lw no longer supports them
     from glue.ligolw import lsctables as glsctables
-    from glue.ligolw.ilwd import ilwdchar as gilwdchar
+    # from glue.ligolw.ilwd import ilwdchar as gilwdchar
     from glue.ligolw.ligolw import LIGOLWContentHandler
 except ImportError:
     pass
 
 
 # =============================================================================
 # Arguments functions:
@@ -75,17 +72,14 @@
                         help="Comma separated minimum and maximum values " +
                         "for the vertical axis. When using negative values " +
                         "an equal sign after --y-lims is necessary.")
     parser.add_argument("--use-logs", default=False, action="store_true",
                         help="Produce a log-log plot")
     parser.add_argument("-i", "--ifo", default=None, help="IFO used for IFO " +
                         "specific plots")
-    parser.add_argument("-f", "--found-file", action="store",
-                        default=None,
-                        help="Location of the found injections file.")
     parser.add_argument("-a", "--seg-files", nargs="+", action="store",
                         default=None, help="The location of the buffer, " +
                         "onsource and offsource segment files.")
     parser.add_argument("-V", "--veto-files", nargs="+", action="store",
                         default=None, help="The location of the CATX veto " +
                         "files provided as a list of space-separated values.")
     parser.add_argument("-b", "--veto-category", action="store", type=int,
@@ -134,80 +128,45 @@
 
 
 def pygrb_add_bestnr_opts(parser):
     """Add to the parser object the arguments used for BestNR calculation."""
     if parser is None:
         parser = argparse.ArgumentParser()
     parser.add_argument("-Q", "--chisq-index", action="store", type=float,
-                        default=4.0, help="chisq_index for newSNR calculation")
+                        default=6.0, help="chisq_index for newSNR calculation")
     parser.add_argument("-N", "--chisq-nhigh", action="store", type=float,
-                        default=3.0, help="nhigh for newSNR calculation")
+                        default=2.0, help="nhigh for newSNR calculation")
     parser.add_argument("-B", "--sngl-snr-threshold", action="store",
                         type=float, default=4.0, help="Single detector SNR " +
                         "threshold, the two most sensitive detectors " +
                         "should have SNR above this.")
     parser.add_argument("-d", "--snr-threshold", action="store", type=float,
                         default=6.0, help="SNR threshold for recording " +
                         "triggers.")
     parser.add_argument("-c", "--newsnr-threshold", action="store", type=float,
-                        default=None, help="NewSNR threshold for " +
+                        default=6.0, help="NewSNR threshold for " +
                         "calculating the chisq of triggers (based on value " +
                         "of auto and bank chisq  values. By default will " +
                         "take the same value as snr-threshold.")
     parser.add_argument("-A", "--null-snr-threshold", action="store",
-                        default="4.25,6",
+                        default="3.5,5.25",
                         help="Comma separated lower,higher null SNR " +
                         "threshold for null SNR cut")
     parser.add_argument("-T", "--null-grad-thresh", action="store", type=float,
                         default=20., help="Threshold above which to " +
                         "increase the values of the null SNR cut")
     parser.add_argument("-D", "--null-grad-val", action="store", type=float,
                         default=0.2, help="Rate the null SNR cut will " +
                         "increase above the threshold")
 
 
-def pygrb_add_missed_injs_input_opt(parser):
-    """Add to parser object the arguments for missed injection file."""
-    if parser is None:
-        parser = argparse.ArgumentParser()
-    parser.add_argument("-m", "--missed-file", action="store",
-                        default=None,
-                        help="Location of the missed injections file.")
-
-
-# =============================================================================
-# Functions to create appropriate FileLists of veto/segment files
-# =============================================================================
-def build_veto_filelist(workflow):
-    """Construct a FileList instance containing all veto xml files"""
-
-    veto_dir = workflow.cp.get('workflow', 'veto-directory')
-    veto_files = glob.glob(veto_dir + '/*CAT*.xml')
-    veto_files = [resolve_url_to_file(vf) for vf in veto_files]
-    veto_files = _workflow.FileList(veto_files)
-
-    return veto_files
-
-
-def build_segment_filelist(workflow):
-    """Construct a FileList instance containing all segments txt files"""
-
-    seg_dir = workflow.cp.get('workflow', 'segment-dir')
-    file_names = ["bufferSeg.txt", "offSourceSeg.txt", "onSourceSeg.txt"]
-    seg_files = [os.path.join(seg_dir, file_name) for file_name in file_names]
-    seg_files = [resolve_url_to_file(sf) for sf in seg_files]
-    seg_files = _workflow.FileList(seg_files)
-
-    return seg_files
-
-
 # =============================================================================
 # Wrapper to read segments files
 # =============================================================================
-def read_seg_files(seg_files):
+def _read_seg_files(seg_files):
     """Read segments txt files"""
 
     if len(seg_files) != 3:
         err_msg = "The location of three segment files is necessary."
         err_msg += "[bufferSeg.txt, offSourceSeg.txt, onSourceSeg.txt]"
         raise RuntimeError(err_msg)
 
@@ -237,15 +196,15 @@
     )
     return Table.get_table(xml_doc, table_name)
 
 
 # ==============================================================================
 # Function to load segments from an xml file
 # ==============================================================================
-def load_segments_from_xml(xml_doc, return_dict=False, select_id=None):
+def _load_segments_from_xml(xml_doc, return_dict=False, select_id=None):
     """Read a ligo.segments.segmentlist from the file object file containing an
     xml segment table.
 
     Parameters
     ----------
         xml_doc: name of segment xml file
 
@@ -294,15 +253,15 @@
 
     return segs
 
 
 # =============================================================================
 # Function to extract vetoes
 # =============================================================================
-def extract_vetoes(all_veto_files, ifos, veto_cat):
+def _extract_vetoes(all_veto_files, ifos, veto_cat):
     """Extracts vetoes from veto filelist"""
 
     if all_veto_files and (veto_cat is None):
         err_msg = "Must supply veto category to apply vetoes."
         raise RuntimeError(err_msg)
 
     # Initialize veto containers
@@ -323,38 +282,38 @@
 
     # Construct veto list from veto filelist
     if veto_files:
         for veto_file in veto_files:
             ifo = os.path.basename(veto_file)[:2]
             if ifo in ifos:
                 # This returns a coalesced list of the vetoes
-                tmp_veto_segs = load_segments_from_xml(veto_file)
+                tmp_veto_segs = _load_segments_from_xml(veto_file)
                 for entry in tmp_veto_segs:
                     vetoes[ifo].append(entry)
     for ifo in ifos:
         vetoes[ifo].coalesce()
 
     return vetoes
 
 
 # =============================================================================
 # Function to get the ID numbers from a LIGO-LW table
 # =============================================================================
-def get_id_numbers(ligolw_table, column):
+def _get_id_numbers(ligolw_table, column):
     """Grab the IDs of a LIGO-LW table"""
 
     ids = [int(getattr(row, column)) for row in ligolw_table]
 
     return ids
 
 
 # =============================================================================
 # Function to build a dictionary (indexed by ifo) of time-slid vetoes
 # =============================================================================
-def slide_vetoes(vetoes, slide_dict_or_list, slide_id):
+def _slide_vetoes(vetoes, slide_dict_or_list, slide_id):
     """Build a dictionary (indexed by ifo) of time-slid vetoes"""
 
     # Copy vetoes
     slid_vetoes = copy.deepcopy(vetoes)
 
     # Slide them
     ifos = vetoes.keys()
@@ -381,52 +340,38 @@
         raise NotImplementedError
 
     return trigs
 
 
 # =============================================================================
 # Detector utils:
-# * Function to calculate the antenna factors F+ and Fx
 # * Function to calculate the antenna response F+^2 + Fx^2
 # * Function to calculate the antenna distance factor
 # =============================================================================
-# The call, e.g., Detector("H1", reference_time=None), will not always work in
-# Python 2.7 as is needs an old version of astropy which cannot download
-# recent enough IERS tables. TEMPORARILY use the default time (GW150914) as
-# reference, thus approximating the sidereal time.
-def get_antenna_factors(antenna, ra, dec, geocent_time):
-    """Returns the antenna responses F+ and Fx of an IFO (passed as pycbc
-    Detector type) at a given sky location and time."""
-
-    f_plus, f_cross = antenna.antenna_pattern(ra, dec, 0, geocent_time)
-
-    return f_plus, f_cross
-
-
-def get_antenna_single_response(antenna, ra, dec, geocent_time):
+def _get_antenna_single_response(antenna, ra, dec, geocent_time):
     """Returns the antenna response F+^2 + Fx^2 of an IFO (passed as pycbc
     Detector type) at a given sky location and time."""
 
-    fp, fc = get_antenna_factors(antenna, ra, dec, geocent_time)
+    fp, fc = antenna.antenna_pattern(ra, dec, 0, geocent_time)
 
     return fp**2 + fc**2
 
 
 # Vectorize the function above on all but the first argument
-get_antenna_responses = numpy.vectorize(get_antenna_single_response,
+get_antenna_responses = numpy.vectorize(_get_antenna_single_response,
                                         otypes=[float])
 get_antenna_responses.excluded.add(0)
 
 
 def get_antenna_dist_factor(antenna, ra, dec, geocent_time, inc=0.0):
     """Returns the antenna factors (defined as eq. 4.3 on page 57 of
     Duncan Brown's Ph.D.) for an IFO (passed as pycbc Detector type) at
     a given sky location and time."""
 
-    fp, fc = get_antenna_factors(antenna, ra, dec, geocent_time)
+    fp, fc = antenna.antenna_pattern(ra, dec, 0, geocent_time)
 
     return numpy.sqrt(fp ** 2 * (1 + numpy.cos(inc)) ** 2 / 4 + fc ** 2)
 
 
 # =============================================================================
 # Function to calculate the detection statistic of a list of triggers
 # =============================================================================
@@ -595,26 +540,14 @@
                                             null_grad_val=null_grad_val)
     logging.info("Time, SNR, and BestNR of triggers extracted.")
 
     return trig_time, trig_snr, trig_bestnr
 
 
 # =============================================================================
-# Find GRB trigger time
-# =============================================================================
-def get_grb_time(seg_files):
-    """Determine GRB trigger time"""
-
-    segs = read_seg_files(seg_files)
-    grb_time = segs['on'][1] - 1
-
-    return grb_time
-
-
-# =============================================================================
 # Function to extract ifos from hdfs
 # =============================================================================
 def extract_ifos(trig_file):
     """Extracts IFOs from hdf file"""
 
     # Load hdf file
     hdf_file = h5py.File(trig_file, 'r')
@@ -638,15 +571,15 @@
     logging.info("Extracting IFOs and vetoes.")
 
     # Extract IFOs
     ifos = extract_ifos(trig_file)
 
     # Extract vetoes
     if veto_files is not None:
-        vetoes = extract_vetoes(veto_files, ifos, veto_cat)
+        vetoes = _extract_vetoes(veto_files, ifos, veto_cat)
     else:
         vetoes = None
 
     return ifos, vetoes
 
 
 # =============================================================================
@@ -689,16 +622,16 @@
     time_slide = load_xml_table(xml_file, glsctables.TimeSlideTable.tableName)
     # Get a list of unique timeslide dictionaries
     time_slide_list = [dict(i) for i in time_slide.as_dict().values()]
     # Turn it into a dictionary indexed by the timeslide ID
     time_slide_dict = {int(time_slide.get_time_slide_id(ov)): ov
                        for ov in time_slide_list}
     # Check time_slide_ids are ordered correctly.
-    ids = get_id_numbers(time_slide,
-                         "time_slide_id")[::len(time_slide_dict[0].keys())]
+    ids = _get_id_numbers(time_slide,
+                          "time_slide_id")[::len(time_slide_dict[0].keys())]
     if not (numpy.all(ids[1:] == numpy.array(ids[:-1])+1) and ids[0] == 0):
         err_msg = "time_slide_ids list should start at zero and increase by "
         err_msg += "one for every element"
         raise RuntimeError(err_msg)
     # Check that the zero-lag slide has time_slide_id == 0.
     if not numpy.all(numpy.array(list(time_slide_dict[0].values())) == 0):
         err_msg = "The slide with time_slide_id == 0 should be the "
@@ -744,15 +677,15 @@
 # =============================================================================
 def construct_trials(seg_files, seg_dict, ifos, slide_dict, vetoes):
     """Constructs trials from triggers, timeslides, segments and vetoes"""
 
     trial_dict = {}
 
     # Get segments
-    segs = read_seg_files(seg_files)
+    segs = _read_seg_files(seg_files)
 
     # Separate segments
     trial_time = abs(segs['on'])
 
     for slide_id in slide_dict:
         # These can only *reduce* the analysis time
         curr_seg_list = seg_dict[slide_id]
@@ -764,15 +697,15 @@
             seg_buffer.append(segments.segment(segs['buffer'][0] -
                                                slide_offset,
                                                segs['buffer'][1] -
                                                slide_offset))
         seg_buffer.coalesce()
 
         # Construct the ifo-indexed dictionary of slid veteoes
-        slid_vetoes = slide_vetoes(vetoes, slide_dict, slide_id)
+        slid_vetoes = _slide_vetoes(vetoes, slide_dict, slide_id)
 
         # Construct trial list and check against buffer
         trial_dict[slide_id] = segments.segmentlist()
         for curr_seg in curr_seg_list:
             iter_int = 1
             while 1:
                 trial_end = curr_seg[0] + trial_time*iter_int
@@ -845,85 +778,31 @@
         inj_dist_mc[i+1, :] = inj_dists / (max_dc_cal_err *
                                            (1 + cal_dist_red) *
                                            (1 + wf_dist_red))
 
     return inj_dist_mc
 
 
-def read_multiinspiral_timeslides_from_files(file_list):
-    """
-    Read time-slid multiInspiral tables from a list of files
-    """
-
-    multis = None
-    time_slides = []
-
-    contenthandler = glsctables.use_in(LIGOLWContentHandler)
-    for this_file in file_list:
-        doc = utils.load_filename(this_file, compress='auto',
-                                  contenthandler=contenthandler)
-
-        # Extract the time slide table
-        time_slide_table = \
-            Table.get_table(doc, lsctables.TimeSlideTable.tableName)
-        slide_mapping = {}
-        curr_slides = {}
-        for slide in time_slide_table:
-            curr_id = int(slide.time_slide_id)
-            if curr_id not in curr_slides:
-                curr_slides[curr_id] = {}
-                curr_slides[curr_id][slide.instrument] = slide.offset
-            elif slide.instrument not in curr_slides[curr_id]:
-                curr_slides[curr_id][slide.instrument] = slide.offset
-
-        for slide_id, offset_dict in curr_slides.items():
-            try:
-                # Is the slide already in the list and where?
-                offset_index = time_slides.index(offset_dict)
-                slide_mapping[slide_id] = offset_index
-            except ValueError:
-                # If not then add it
-                time_slides.append(offset_dict)
-                slide_mapping[slide_id] = len(time_slides) - 1
-
-        # Extract the multi inspiral table
-        try:
-            multi_inspiral_table = Table.get_table(doc, 'multi_inspiral')
-            # Remap the time slide IDs
-            for multi in multi_inspiral_table:
-                new_id = slide_mapping[int(multi.time_slide_id)]
-                multi.time_slide_id = gilwdchar(
-                                      f"time_slide:time_slide_id:{new_id}")
-            if multis:
-                multis.extend(multi_inspiral_table)
-            else:
-                multis = multi_inspiral_table
-        except Exception as exc:
-            err_msg = "Unable to read a time-slid multiInspiral table "
-            err_msg += f"from {this_file}."
-            raise RuntimeError(err_msg) from exc
-
-    return multis, time_slides
-
-
 # =============================================================================
 # Function to calculate the coincident SNR
 # =============================================================================
 def get_coinc_snr(trigs_or_injs, ifos):
     """ Calculate coincident SNR using single IFO SNRs"""
 
     num_trigs_or_injs = len(trigs_or_injs['network/end_time_gc'][:])
 
     # Calculate coincident SNR
     single_snr_sq = dict((ifo, None) for ifo in ifos)
     snr_sum_square = numpy.zeros(num_trigs_or_injs)
     for ifo in ifos:
-        att = ifo[0].lower()
+        key = ifo + '/snr_' + ifo.lower()
+        if ifo.lower() != 'h1':
+            key = key[:-1]
         # Square the individual SNRs
         single_snr_sq[ifo] = numpy.square(
-            trigs_or_injs['%s/snr_%s' % (ifo, att)][:])
+            trigs_or_injs[key][:])
         # Add them
         snr_sum_square = numpy.add(snr_sum_square, single_snr_sq[ifo])
     # Obtain the square root
     coinc_snr = numpy.sqrt(snr_sum_square)
 
     return coinc_snr
```

### Comparing `PyCBC-2.2.0/pycbc/results/render.py` & `PyCBC-2.2.1/pycbc/results/render.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/scatter_histograms.py` & `PyCBC-2.2.1/pycbc/results/scatter_histograms.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/css/bootstrap/3.3.2/bootstrap.min.css` & `PyCBC-2.2.1/pycbc/results/static/css/bootstrap/3.3.2/bootstrap.min.css`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/css/bootstrap.min.css` & `PyCBC-2.2.1/pycbc/results/static/css/bootstrap.min.css`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/css/fancybox/2.1.5/jquery.fancybox.css` & `PyCBC-2.2.1/pycbc/results/static/css/fancybox/2.1.5/jquery.fancybox.css`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/css/pycbc/orange.css` & `PyCBC-2.2.1/pycbc/results/static/css/pycbc/orange.css`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/css/pycbc/red.css` & `PyCBC-2.2.1/pycbc/results/static/css/pycbc/red.css`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.eot` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.eot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.html` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.ttf` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.ttf`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Light.woff` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Light.woff`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.eot` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.eot`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.html` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.ttf` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.ttf`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/fonts/Lato-Semibold.woff` & `PyCBC-2.2.1/pycbc/results/static/fonts/Lato-Semibold.woff`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/bootstrap/3.3.2/bootstrap.min.js` & `PyCBC-2.2.1/pycbc/results/static/js/bootstrap/3.3.2/bootstrap.min.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/bootstrap.min.js` & `PyCBC-2.2.1/pycbc/results/static/js/bootstrap.min.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/fancybox-orange.js` & `PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/fancybox-orange.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.js` & `PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.pack.js` & `PyCBC-2.2.1/pycbc/results/static/js/fancybox/2.1.5/jquery.fancybox.pack.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/jquery/1.10.2/jquery-1.10.2.min.js` & `PyCBC-2.2.1/pycbc/results/static/js/jquery/1.10.2/jquery-1.10.2.min.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/jquery/1.10.2/jquery.mousewheel-3.0.6.pack.js` & `PyCBC-2.2.1/pycbc/results/static/js/jquery/1.10.2/jquery.mousewheel-3.0.6.pack.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/static/js/jquery.min.js` & `PyCBC-2.2.1/pycbc/results/static/js/jquery.min.js`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/str_utils.py` & `PyCBC-2.2.1/pycbc/results/str_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/table_utils.py` & `PyCBC-2.2.1/pycbc/results/table_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -79,15 +79,15 @@
     else:
         page = 'enable'
 
     div_id = uuid.uuid4()
 
     column_descriptions = []
     for column, name in zip(columns, names):
-        if column.dtype.kind == 'S':
+        if column.dtype.kind == 'S' or column.dtype.kind == 'U':
             ctype = 'string'
         else:
             ctype = 'number'
         column_descriptions.append((ctype, name))
 
     data = []
     for item in zip(*columns):
```

### Comparing `PyCBC-2.2.0/pycbc/results/templates/files/file_default.html` & `PyCBC-2.2.1/pycbc/results/templates/files/file_default.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/templates/files/file_glitchgram.html` & `PyCBC-2.2.1/pycbc/results/templates/files/file_glitchgram.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/templates/files/file_pre.html` & `PyCBC-2.2.1/pycbc/results/templates/files/file_pre.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/templates/files/file_tmplt.html` & `PyCBC-2.2.1/pycbc/results/templates/files/file_tmplt.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/templates/orange.html` & `PyCBC-2.2.1/pycbc/results/templates/orange.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/templates/red.html` & `PyCBC-2.2.1/pycbc/results/templates/red.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/templates/wells/two_column.html` & `PyCBC-2.2.1/pycbc/results/templates/wells/two_column.html`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/results/versioning.py` & `PyCBC-2.2.1/pycbc/results/versioning.py`

 * *Files 24% similar despite different names*

```diff
@@ -13,18 +13,16 @@
 # Public License for more details.
 #
 # You should have received a copy of the GNU General Public License along
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 
 import logging
-import os
 import subprocess
 import urllib.parse
-from pycbc.results import save_fig_with_metadata, html_escape
 
 import lal, lalframe
 import pycbc.version, glue.git_version
 
 def get_library_version_info():
     """This will return a list of dictionaries containing versioning
     information about the various LIGO libraries that PyCBC will use in an
@@ -114,84 +112,43 @@
     pycbcinfo['Branch'] = pycbc.version.git_branch
     pycbcinfo['Committer'] = pycbc.version.git_committer
     pycbcinfo['Date'] = pycbc.version.git_build_date
     library_list.append(pycbcinfo)
 
     return library_list
 
-def write_library_information(path):
-    library_list = get_library_version_info()
-    for curr_lib in library_list:
-        lib_name = curr_lib['Name']
-        text = ''
-        for key, value in curr_lib.items():
-            text+='<li> %s : %s </li>\n' %(key,value)
-        kwds = {'render-function' : 'render_text',
-                'title' : '%s Version Information'%lib_name,
-        }
-
-        save_fig_with_metadata(html_escape(text),
-          os.path.join(path,'%s_version_information.html' %(lib_name)), **kwds)
-
-def get_code_version_numbers(cp):
+def get_code_version_numbers(executable_names, executable_files):
     """Will extract the version information from the executables listed in
     the executable section of the supplied ConfigParser object.
 
     Returns
     --------
     dict
         A dictionary keyed by the executable name with values giving the
         version string for each executable.
     """
     code_version_dict = {}
-    for _, value in cp.items('executables'):
+    for exe_name, value in zip(executable_names, executable_files):
         value = urllib.parse.urlparse(value)
-        _, exe_name = os.path.split(value.path)
+        logging.info("Getting version info for %s", exe_name)
         version_string = None
         if value.scheme in ['gsiftp', 'http', 'https']:
             code_version_dict[exe_name] = "Using bundle downloaded from %s" % value
         elif value.scheme == 'singularity':
             txt = (
                 "Executable run from a singularity image. See config file "
                 "and site catalog for details of what image was used."
             )
             code_version_dict[exe_name] = txt
         else:
             try:
                 version_string = subprocess.check_output(
                     [value.path, '--version'],
                     stderr=subprocess.STDOUT
-                )
+                ).decode()
             except subprocess.CalledProcessError:
                 version_string = "Executable fails on {} --version"
                 version_string = version_string.format(value.path)
             except OSError:
                 version_string = "Executable doesn't seem to exist(!)"
             code_version_dict[exe_name] = version_string
     return code_version_dict
-
-def write_code_versions(path, cp):
-    code_version_dict = get_code_version_numbers(cp)
-    html_text = ''
-    for key,value in code_version_dict.items():
-        # value might be a str or a bytes object in python3. python2 is happy
-        # to combine these objects (or uniocde and str, their equivalents)
-        # but python3 is not.
-        try:
-            value = value.decode()
-        except AttributeError:
-            pass
-        html_text+= '<li><b>%s</b>:<br><pre>%s</pre></li><hr><br><br>\n' \
-            % (key, str(value).replace('@', '&#64;'))
-    kwds = {'render-function' : 'render_text',
-            'title' : 'Version Information from Executables',
-    }
-    save_fig_with_metadata(html_escape(html_text),
-        os.path.join(path,'version_information_from_executables.html'), **kwds)
-
-def create_versioning_page(path, cp):
-    logging.info("Entering versioning module")
-    if not os.path.exists(path):
-        os.mkdir(path)
-    write_library_information(path)
-    write_code_versions(path, cp)
-    logging.info("Leaving versioning module")
```

### Comparing `PyCBC-2.2.0/pycbc/scheme.py` & `PyCBC-2.2.1/pycbc/scheme.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,21 +28,14 @@
 """
 import os
 import pycbc
 from functools import wraps
 import logging
 from .libutils import get_ctypes_library
 
-try:
-    _libgomp = get_ctypes_library("gomp", ['gomp'])
-except:
-    # Should we fail or give a warning if we cannot import
-    # libgomp? Seems to work even for MKL scheme, but
-    # not entirely sure why...
-    _libgomp = None
 
 class _SchemeManager(object):
     _single = None
 
     def __init__(self):
 
         if _SchemeManager._single is not None:
@@ -125,25 +118,35 @@
         if isinstance(num_threads, int):
             self.num_threads=num_threads
         elif num_threads == 'env' and "PYCBC_NUM_THREADS" in os.environ:
             self.num_threads = int(os.environ["PYCBC_NUM_THREADS"])
         else:
             import multiprocessing
             self.num_threads = multiprocessing.cpu_count()
+        self._libgomp = None
 
     def __enter__(self):
         Scheme.__enter__(self)
+        try:
+            self._libgomp = get_ctypes_library("gomp", ['gomp'],
+                                               mode=ctypes.RTLD_GLOBAL)
+        except:
+            # Should we fail or give a warning if we cannot import
+            # libgomp? Seems to work even for MKL scheme, but
+            # not entirely sure why...
+            pass
+
         os.environ["OMP_NUM_THREADS"] = str(self.num_threads)
-        if _libgomp is not None:
-            _libgomp.omp_set_num_threads( int(self.num_threads) )
+        if self._libgomp is not None:
+            self._libgomp.omp_set_num_threads( int(self.num_threads) )
 
     def __exit__(self, type, value, traceback):
         os.environ["OMP_NUM_THREADS"] = "1"
-        if _libgomp is not None:
-            _libgomp.omp_set_num_threads(1)
+        if self._libgomp is not None:
+            self._libgomp.omp_set_num_threads(1)
         Scheme.__exit__(self, type, value, traceback)
 
 class MKLScheme(CPUScheme):
     def __init__(self, num_threads=1):
         CPUScheme.__init__(self, num_threads)
         if not pycbc.HAVE_MKL:
             raise RuntimeError("Can't find MKL libraries")
```

### Comparing `PyCBC-2.2.0/pycbc/sensitivity.py` & `PyCBC-2.2.1/pycbc/sensitivity.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/strain/__init__.py` & `PyCBC-2.2.1/pycbc/strain/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/strain/calibration.py` & `PyCBC-2.2.1/pycbc/strain/calibration.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/strain/gate.py` & `PyCBC-2.2.1/pycbc/strain/gate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/strain/lines.py` & `PyCBC-2.2.1/pycbc/strain/lines.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/strain/recalibrate.py` & `PyCBC-2.2.1/pycbc/strain/recalibrate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/strain/strain.py` & `PyCBC-2.2.1/pycbc/strain/strain.py`

 * *Files 2% similar despite different names*

```diff
@@ -1429,14 +1429,17 @@
                  psd_inverse_length=3.5,
                  trim_padding=0.25,
                  autogating_threshold=None,
                  autogating_cluster=None,
                  autogating_pad=None,
                  autogating_width=None,
                  autogating_taper=None,
+                 autogating_duration=None,
+                 autogating_psd_segment_length=None,
+                 autogating_psd_stride=None,
                  state_channel=None,
                  data_quality_channel=None,
                  idq_channel=None,
                  idq_state_channel=None,
                  idq_threshold=None,
                  dyn_range_fac=pycbc.DYN_RANGE_FAC,
                  psd_abort_difference=None,
@@ -1486,14 +1489,20 @@
             Seconds to cluster possible gating locations.
         autogating_pad: float, Optional
             Seconds of corrupted whitened strain to ignore when generating a gate.
         autogating_width: float, Optional
             Half-duration of the zeroed-out portion of autogates.
         autogating_taper: float, Optional
             Duration of taper on either side of the gating window in seconds.
+        autogating_duration: float, Optional
+            Amount of data in seconds to apply autogating on.
+        autogating_psd_segment_length: float, Optional
+            The length in seconds of each segment used to estimate the PSD with Welch's method.
+        autogating_psd_stride: float, Optional
+            The overlap in seconds between each segment used to estimate the PSD with Welch's method.
         state_channel: {str, None}, Optional
             Channel to use for state information about the strain
         data_quality_channel: {str, None}, Optional
             Channel to use for data quality information about the strain
         idq_channel: {str, None}, Optional
             Channel to use for idq timeseries
         idq_state_channel : {str, None}, Optional
@@ -1561,16 +1570,19 @@
                     and self.data_quality_flags[0] == 'veto_nonzero':
                 sb_kwargs['valid_on_zero'] = True
                 logging.info('DQ channel %s interpreted as zero = good',
                              data_quality_channel)
             else:
                 sb_kwargs['valid_mask'] = pycbc.frame.flag_names_to_bitmask(
                         self.data_quality_flags)
-                logging.info('DQ channel %s interpreted as bitmask %s = good',
-                             data_quality_channel, bin(valid_mask))
+                logging.info(
+                    'DQ channel %s interpreted as bitmask %s = good',
+                    data_quality_channel,
+                    bin(sb_kwargs['valid_mask'])
+                )
             self.dq = pycbc.frame.StatusBuffer(frame_src, data_quality_channel,
                                                start_time, **sb_kwargs)
 
         if idq_channel is not None:
             if idq_state_channel is None:
                 raise ValueError(
                     'Each detector with an iDQ channel requires an iDQ state channel as well')
@@ -1591,14 +1603,17 @@
         self.highpass_bandwidth = highpass_bandwidth
 
         self.autogating_threshold = autogating_threshold
         self.autogating_cluster = autogating_cluster
         self.autogating_pad = autogating_pad
         self.autogating_width = autogating_width
         self.autogating_taper = autogating_taper
+        self.autogating_duration = autogating_duration
+        self.autogating_psd_segment_length = autogating_psd_segment_length
+        self.autogating_psd_stride = autogating_psd_stride
         self.gate_params = []
 
         self.sample_rate = sample_rate
         self.dyn_range_fac = dyn_range_fac
 
         self.psd_abort_difference = psd_abort_difference
         self.psd_recalculate_difference = psd_recalculate_difference
@@ -1692,14 +1707,35 @@
 
         # If the new estimate replaces the current one, invalide the ineterpolate PSDs
         self.psd = psd
         self.psds = {}
         logging.info("Recalculating %s PSD, %s", self.detector, psd.dist)
         return True
 
+    def check_psd_dist(self, min_dist, max_dist):
+        """Check that the horizon distance of a detector is within a required
+        range. If so, return True, otherwise log a warning and return False.
+        """
+        if self.psd is None:
+            # ignore check
+            return True
+        # Note that the distance can in principle be inf or nan, e.g. if h(t)
+        # is identically zero. The check must fail in those cases.  Be careful
+        # with how the logic works out when comparing inf's or nan's!
+        good = self.psd.dist >= min_dist and self.psd.dist <= max_dist
+        if not good:
+            logging.info(
+                "%s PSD dist %s outside acceptable range [%s, %s]",
+                self.detector,
+                self.psd.dist,
+                min_dist,
+                max_dist
+            )
+        return good
+
     def overwhitened_data(self, delta_f):
         """ Return overwhitened data
 
         Parameters
         ----------
         delta_f: float
             The sample step to generate overwhitened frequency domain data for
@@ -1887,17 +1923,19 @@
         # Stitch into continuous stream
         self.strain.roll(-sample_step)
         self.strain[len(self.strain) - csize + self.corruption:] = strain[:]
         self.strain.start_time += blocksize
 
         # apply gating if needed
         if self.autogating_threshold is not None:
+            autogating_duration_length = self.autogating_duration * self.sample_rate
+            autogating_start_sample = int(len(self.strain) - autogating_duration_length)
             glitch_times = detect_loud_glitches(
-                    strain[:-self.corruption],
-                    psd_duration=2., psd_stride=1.,
+                    self.strain[autogating_start_sample:-self.corruption],
+                    psd_duration=self.autogating_psd_segment_length, psd_stride=self.autogating_psd_stride,
                     threshold=self.autogating_threshold,
                     cluster_window=self.autogating_cluster,
                     low_freq_cutoff=self.highpass_frequency,
                     corrupt_time=self.autogating_pad)
             if len(glitch_times) > 0:
                 logging.info('Autogating %s at %s', self.detector,
                              ', '.join(['%.3f' % gt for gt in glitch_times]))
@@ -1961,14 +1999,17 @@
                    psd_segment_length=args.psd_segment_length,
                    psd_inverse_length=args.psd_inverse_length,
                    autogating_threshold=args.autogating_threshold,
                    autogating_cluster=args.autogating_cluster,
                    autogating_pad=args.autogating_pad,
                    autogating_width=args.autogating_width,
                    autogating_taper=args.autogating_taper,
+                   autogating_duration=args.autogating_duration,
+                   autogating_psd_segment_length=args.autogating_psd_segment_length,
+                   autogating_psd_stride=args.autogating_psd_stride,
                    psd_abort_difference=args.psd_abort_difference,
                    psd_recalculate_difference=args.psd_recalculate_difference,
                    force_update_cache=args.force_update_cache,
                    increment_update_cache=args.increment_update_cache[ifo],
                    analyze_flags=analyze_flags,
                    data_quality_flags=dq_flags,
                    dq_padding=args.data_quality_padding)
```

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/bank_output_utils.py` & `PyCBC-2.2.1/pycbc/tmpltbank/bank_output_utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 import numpy
+import h5py
 from lal import PI, MTSUN_SI, TWOPI, GAMMA
 from ligo.lw import ligolw, lsctables, utils as ligolw_utils
 from pycbc import pnutils
 from pycbc.tmpltbank.lambda_mapping import ethinca_order_from_string
 from pycbc.io.ligolw import (
     return_empty_sngl, return_search_summary, create_process_table
 )
 
+from pycbc.waveform import get_waveform_filter_length_in_time as gwflit
 
 def convert_to_sngl_inspiral_table(params, proc_id):
     '''
     Convert a list of m1,m2,spin1z,spin2z values into a basic sngl_inspiral
     table with mass and spin parameters populated and event IDs assigned
 
     Parameters
@@ -194,45 +196,38 @@
 
     gammaVals[3] = g[0,0]
     gammaVals[4] = g[0,1]
     gammaVals[5] = g[1,1]
 
     return fMax_theor, gammaVals
 
-def output_sngl_inspiral_table(outputFile, tempBank, metricParams,
-                               ethincaParams, programName="", optDict = None,
-                               outdoc=None):
+def output_sngl_inspiral_table(outputFile, tempBank, programName="",
+                               optDict = None, outdoc=None,
+                               **kwargs): # pylint:disable=unused-argument
     """
     Function that converts the information produced by the various PyCBC bank
     generation codes into a valid LIGOLW XML file containing a sngl_inspiral
     table and outputs to file.
 
     Parameters
     -----------
     outputFile : string
         Name of the file that the bank will be written to
     tempBank : iterable
         Each entry in the tempBank iterable should be a sequence of
         [mass1,mass2,spin1z,spin2z] in that order.
-    metricParams : metricParameters instance
-        Structure holding all the options for construction of the metric
-        and the eigenvalues, eigenvectors and covariance matrix
-        needed to manipulate the space.
-    ethincaParams: {ethincaParameters instance, None}
-        Structure holding options relevant to the ethinca metric computation
-        including the upper frequency cutoff to be used for filtering.
-        NOTE: The computation is currently only valid for non-spinning systems
-        and uses the TaylorF2 approximant.
     programName (key-word-argument) : string
         Name of the executable that has been run
     optDict (key-word argument) : dictionary
         Dictionary of the command line arguments passed to the program
     outdoc (key-word argument) : ligolw xml document
         If given add template bank to this representation of a xml document and
         write to disk. If not given create a new document.
+    kwargs : optional key-word arguments
+        Allows unused options to be passed to this function (for modularity)
     """
     if optDict is None:
         optDict = {}
     if outdoc is None:
         outdoc = ligolw.Document()
         outdoc.appendChild(ligolw.LIGO_LW())
 
@@ -246,37 +241,14 @@
         outdoc,
         program_name=programName,
         detectors=ifos,
         options=optDict
     )
     proc_id = proc.process_id
     sngl_inspiral_table = convert_to_sngl_inspiral_table(tempBank, proc_id)
-    # Calculate Gamma components if needed
-    if ethincaParams is not None:
-        if ethincaParams.doEthinca:
-            for sngl in sngl_inspiral_table:
-                # Set tau_0 and tau_3 values needed for the calculation of
-                # ethinca metric distances
-                (sngl.tau0,sngl.tau3) = pnutils.mass1_mass2_to_tau0_tau3(
-                    sngl.mass1, sngl.mass2, metricParams.f0)
-                fMax_theor, GammaVals = calculate_ethinca_metric_comps(
-                    metricParams, ethincaParams,
-                    sngl.mass1, sngl.mass2, spin1z=sngl.spin1z,
-                    spin2z=sngl.spin2z, full_ethinca=ethincaParams.full_ethinca)
-                # assign the upper frequency cutoff and Gamma0-5 values
-                sngl.f_final = fMax_theor
-                for i in range(len(GammaVals)):
-                    setattr(sngl, "Gamma"+str(i), GammaVals[i])
-        # If Gamma metric components are not wanted, assign f_final from an
-        # upper frequency cutoff specified in ethincaParams
-        elif ethincaParams.cutoff is not None:
-            for sngl in sngl_inspiral_table:
-                sngl.f_final = pnutils.frequency_cutoff_from_name(
-                    ethincaParams.cutoff,
-                    sngl.mass1, sngl.mass2, sngl.spin1z, sngl.spin2z)
 
     # set per-template low-frequency cutoff
     if 'f_low_column' in optDict and 'f_low' in optDict and \
             optDict['f_low_column'] is not None:
         for sngl in sngl_inspiral_table:
             setattr(sngl, optDict['f_low_column'], optDict['f_low'])
 
@@ -295,7 +267,94 @@
         start_time, end_time, len(sngl_inspiral_table), ifos
     )
     search_summary_table.append(search_summary)
     outdoc.childNodes[0].appendChild(search_summary_table)
 
     # write the xml doc to disk
     ligolw_utils.write_filename(outdoc, outputFile)
+
+
+def output_bank_to_hdf(outputFile, tempBank, optDict=None, programName='',
+                       approximant=None, output_duration=False,
+                       **kwargs): # pylint:disable=unused-argument
+    """
+    Function that converts the information produced by the various PyCBC bank
+    generation codes into a hdf5 file.
+
+    Parameters
+    -----------
+    outputFile : string
+        Name of the file that the bank will be written to
+    tempBank : iterable
+        Each entry in the tempBank iterable should be a sequence of
+        [mass1,mass2,spin1z,spin2z] in that order.
+    programName (key-word-argument) : string
+        Name of the executable that has been run
+    optDict (key-word argument) : dictionary
+        Dictionary of the command line arguments passed to the program
+    approximant : string
+        The approximant to be outputted to the file,
+        if output_duration is True, this is also used for that calculation.
+    output_duration : boolean
+        Output the duration of the template, calculated using
+        get_waveform_filter_length_in_time, to the file.
+    kwargs : optional key-word arguments
+        Allows unused options to be passed to this function (for modularity)
+    """
+    bank_dict = {}
+    mass1, mass2, spin1z, spin2z = list(zip(*tempBank))
+    bank_dict['mass1'] = mass1
+    bank_dict['mass2'] = mass2
+    bank_dict['spin1z'] = spin1z
+    bank_dict['spin2z'] = spin2z
+
+    # Add other values to the bank dictionary as appropriate
+    if optDict is not None:
+        bank_dict['f_lower'] = numpy.ones_like(mass1) * \
+            optDict['f_low']
+        argument_string = [f'{k}:{v}' for k, v in optDict.items()]
+
+    if optDict is not None and optDict['output_f_final']:
+        bank_dict['f_final'] = numpy.ones_like(mass1) * \
+            optDict['f_upper']
+
+    if approximant:
+        if not isinstance(approximant, bytes):
+            appx = approximant.encode()
+        bank_dict['approximant'] = numpy.repeat(appx, len(mass1))
+
+    if output_duration:
+        appx = approximant if approximant else 'SPAtmplt'
+        tmplt_durations = numpy.zeros_like(mass1)
+        for i in range(len(mass1)):
+            wfrm_length = gwflit(appx,
+                                 mass1=mass1[i],
+                                 mass2=mass2[i],
+                                 f_lower=optDict['f_low'],
+                                 phase_order=7)
+            tmplt_durations[i] = wfrm_length
+        bank_dict['template_duration'] = tmplt_durations
+
+    with h5py.File(outputFile, 'w') as bankf_out:
+        bankf_out.attrs['program'] = programName
+        if optDict is not None:
+            bankf_out.attrs['arguments'] = argument_string
+        for k, v in bank_dict.items():
+            bankf_out[k] = v
+
+
+def output_bank_to_file(outputFile, tempBank, **kwargs):
+    if outputFile.endswith(('.xml','.xml.gz','.xmlgz')):
+        output_sngl_inspiral_table(
+            outputFile,
+            tempBank,
+            **kwargs
+        )
+    elif outputFile.endswith(('.h5','.hdf','.hdf5')):
+        output_bank_to_hdf(
+            outputFile,
+            tempBank,
+            **kwargs
+        )
+    else:
+        err_msg = f"Unrecognized extension for file {outputFile}."
+        raise ValueError(err_msg)
```

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/brute_force_methods.py` & `PyCBC-2.2.1/pycbc/tmpltbank/brute_force_methods.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/calc_moments.py` & `PyCBC-2.2.1/pycbc/tmpltbank/calc_moments.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/coord_utils.py` & `PyCBC-2.2.1/pycbc/tmpltbank/coord_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/lambda_mapping.py` & `PyCBC-2.2.1/pycbc/tmpltbank/lambda_mapping.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/lattice_utils.py` & `PyCBC-2.2.1/pycbc/tmpltbank/lattice_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/option_utils.py` & `PyCBC-2.2.1/pycbc/tmpltbank/option_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -103,15 +103,15 @@
     for option in option_list:
         option_strings = option.option_strings
         for string in option_strings:
             if string.startswith('--'):
                 command_lines.append(string)
     return command_lines
 
-def insert_base_bank_options(parser):
+def insert_base_bank_options(parser, match_req=True):
     """
     Adds essential common options for template bank generation to an
     ArgumentParser instance.
     """
 
     def match_type(s):
         err_msg = "must be a number between 0 and 1 excluded, not %r" % s
@@ -120,22 +120,25 @@
         except ValueError:
             raise argparse.ArgumentTypeError(err_msg)
         if value <= 0 or value >= 1:
             raise argparse.ArgumentTypeError(err_msg)
         return value
 
     parser.add_argument(
-            '-m', '--min-match', type=match_type, required=True,
+            '-m', '--min-match', type=match_type, required=match_req,
             help="Generate bank with specified minimum match. Required.")
     parser.add_argument(
             '-O', '--output-file', required=True,
             help="Output file name. Required.")
     parser.add_argument('--f-low-column', type=str, metavar='NAME',
                         help='If given, store the lower frequency cutoff into '
-                             'column NAME of the single-inspiral table.')
+                             'column NAME of the single-inspiral table. '
+                             '(Requires an output file ending in .xml)')
+    parser.add_argument('--output-f-final', action='store_true',
+            help="Include 'f_final' in the output hdf file.")
 
 def insert_metric_calculation_options(parser):
     """
     Adds the options used to obtain a metric in the bank generation codes to an
     argparser as an OptionGroup. This should be used if you want to use these
     options in your code.
     """
```

### Comparing `PyCBC-2.2.0/pycbc/tmpltbank/partitioned_bank.py` & `PyCBC-2.2.1/pycbc/tmpltbank/partitioned_bank.py`

 * *Files 0% similar despite different names*

```diff
@@ -545,15 +545,15 @@
         # Get mus and best fupper for this point, if needed
         if vary_fupper:
             mass_dict = {}
             mass_dict['m1'] = numpy.array([mass1])
             mass_dict['m2'] = numpy.array([mass2])
             mass_dict['s1z'] = numpy.array([spin1z])
             mass_dict['s2z'] = numpy.array([spin2z])
-            freqs = numpy.array([self.frequency_map.keys()], dtype=float)
+            freqs = numpy.array(list(self.frequency_map.keys()), dtype=float)
             freq_cutoff = coord_utils.return_nearest_cutoff(\
                                      self.upper_freq_formula, mass_dict, freqs)
             freq_cutoff = freq_cutoff[0]
             lambdas = coord_utils.get_chirp_params\
                 (mass1, mass2, spin1z, spin2z, self.metric_params.f0,
                  self.metric_params.pnOrder)
             mus = []
```

### Comparing `PyCBC-2.2.0/pycbc/transforms.py` & `PyCBC-2.2.1/pycbc/transforms.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/aligned.py` & `PyCBC-2.2.1/pycbc/types/aligned.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/array.py` & `PyCBC-2.2.1/pycbc/types/array.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/array_cpu.pyx` & `PyCBC-2.2.1/pycbc/types/array_cpu.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/array_cuda.py` & `PyCBC-2.2.1/pycbc/types/array_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/config.py` & `PyCBC-2.2.1/pycbc/types/config.py`

 * *Files 7% similar despite different names*

```diff
@@ -233,19 +233,20 @@
     def get_subsections(self, section_name):
         """Return a list of subsections for the given section name"""
         # Keep only subsection names
         subsections = [
             sec[len(section_name) + 1:]
             for sec in self.sections()
             if sec.startswith(section_name + "-")
+            and not sec.endswith('defaultvalues')
         ]
 
         for sec in subsections:
             sp = sec.split("-")
-            # This is unusual, but a format [section-subsection-tag] is okay. Just
+            # The format [section-subsection-tag] is okay. Just
             # check that [section-subsection] section exists. If not it is possible
             # the user is trying to use an subsection name with '-' in it
             if (len(sp) > 1) and not self.has_section(
                 "%s-%s" % (section_name, sp[0])
             ):
                 raise ValueError(
                     "Workflow uses the '-' as a delimiter so "
@@ -438,15 +439,15 @@
             add to the section of the ini file
         overwrite_options : Boolean, optional
             By default this function will throw a ValueError if an option exists
             in both the original section in the ConfigParser *and* in the
             provided items.
             This will override so that the options+values given in items
             will replace the original values if the value is set to True.
-            Default = True
+            Default = False
         """
         # Sanity checking
         if not self.has_section(section):
             raise ValueError(
                 "Section %s not present in ConfigParser." % (section,)
             )
 
@@ -459,30 +460,42 @@
                         + "ConfigParser section [%s] and " % (section,)
                         + "input list: %s" % (option,)
                     )
             self.set(section, option, value)
 
     def sanity_check_subsections(self):
         """
-        This function goes through the ConfigParset and checks that any options
+        This function goes through the ConfigParser and checks that any options
         given in the [SECTION_NAME] section are not also given in any
         [SECTION_NAME-SUBSECTION] sections.
 
         """
         # Loop over the sections in the ini file
         for section in self.sections():
-            # [pegasus_profile] specially is allowed to be overriden by
+            # [pegasus_profile] is specially allowed to be overriden by
             # sub-sections
             if section == "pegasus_profile":
                 continue
 
+            if section.endswith('-defaultvalues') and \
+                    not len(section.split('-')) == 2:
+                # Only allow defaultvalues for top-level sections
+                raise NotImplementedError(
+                    "-defaultvalues subsections are only allowed for "
+                    "top-level sections; given %s" % section
+                )
+
             # Loop over the sections again
             for section2 in self.sections():
                 # Check if any are subsections of section
                 if section2.startswith(section + "-"):
+                    if section2.endswith("defaultvalues"):
+                        # defaultvalues is storage for defaults, and will
+                        # be over-written by anything in the sections-proper
+                        continue
                     # Check for duplicate options whenever this exists
                     self.check_duplicate_options(
                         section, section2, raise_error=True
                     )
 
     def check_duplicate_options(self, section1, section2, raise_error=False):
         """
@@ -509,25 +522,41 @@
                 "Section %s not present in ConfigParser." % (section1,)
             )
         if not self.has_section(section2):
             raise ValueError(
                 "Section %s not present in ConfigParser." % (section2,)
             )
 
+        # Are section1 and section2 a section-and-defaultvalues pair?
+        section_and_default = (section1 == f"{section2}-defaultvalues" or
+                               section2 == f"{section1}-defaultvalues")
+
+        # Is one the sections defaultvalues, but the other is not the
+        # top-level section? This is to catch the case where we are
+        # comparing section-defaultvalues with section-subsection
+        if section1.endswith("-defaultvalues") or \
+                section2.endswith("-defaultvalues"):
+            if not section_and_default:
+                # Override the raise_error variable not to error when
+                # defaultvalues are given and the sections are not
+                # otherwise the same
+                raise_error = False
+
         items1 = self.options(section1)
         items2 = self.options(section2)
 
         # The list comprehension here creates a list of all duplicate items
         duplicates = [x for x in items1 if x in items2]
 
         if duplicates and raise_error:
-            raise ValueError(
-                "The following options appear in both section "
-                + "%s and %s: %s" % (section1, section2, " ".join(duplicates))
-            )
+            err_msg = ("The following options appear in both section "
+                       f"{section1} and {section2}: " + ", ".join(duplicates))
+            if section_and_default:
+                err_msg += ". Default values are unused in this case."
+            raise ValueError(err_msg)
 
         return duplicates
 
     def get_opt_tag(self, section, option, tag):
         """
         Convenience function accessing get_opt_tags() for a single tag: see
         documentation for that function.
@@ -552,16 +581,17 @@
         """
         return self.get_opt_tags(section, option, [tag])
 
     def get_opt_tags(self, section, option, tags):
         """
         Supplement to ConfigParser.ConfigParser.get(). This will search for an
         option in [section] and if it doesn't find it will also try in
-        [section-tag] for every value of tag in tags.
-        Will raise a ConfigParser.Error if it cannot find a value.
+        [section-defaultvalues], and [section-tag] for every value of tag
+        in tags. [section-tag] will be preferred to [section-defaultvalues]
+        values. Will raise a ConfigParser.Error if it cannot find a value.
 
         Parameters
         -----------
         self : ConfigParser object
             The ConfigParser object (automatically passed when this is appended
             to the ConfigParser class)
         section : string
@@ -583,14 +613,22 @@
         try:
             return self.get(section, option)
         except ConfigParser.Error:
             err_string = "No option '%s' in section [%s] " % (option, section)
             if not tags:
                 raise ConfigParser.Error(err_string + ".")
             return_vals = []
+            # First, check if there are any default values set:
+            has_defaultvalue = False
+            if self.has_section(f"{section}-defaultvalues"):
+                return_vals.append(
+                    self.get(f"{section}-defaultvalues", option)
+                )
+                has_defaultvalue = True
+
             sub_section_list = []
             for sec_len in range(1, len(tags) + 1):
                 for tag_permutation in itertools.permutations(tags, sec_len):
                     joined_name = "-".join(tag_permutation)
                     sub_section_list.append(joined_name)
             section_list = ["%s-%s" % (section, sb) for sb in sub_section_list]
             err_section_list = []
@@ -598,16 +636,20 @@
                 if self.has_section("%s-%s" % (section, sub)):
                     if self.has_option("%s-%s" % (section, sub), option):
                         err_section_list.append("%s-%s" % (section, sub))
                         return_vals.append(
                             self.get("%s-%s" % (section, sub), option)
                         )
 
-            # We also want to recursively go into sections
+            if has_defaultvalue and len(return_vals) > 1:
+                # option supplied which should overwrite the default;
+                # default will be first in the list, so remove it
+                return_vals = return_vals[1:]
 
+            # We also want to recursively go into sections
             if not return_vals:
                 err_string += "or in sections [%s]." % (
                     "] [".join(section_list)
                 )
                 raise ConfigParser.Error(err_string)
             if len(return_vals) > 1:
                 err_string += (
```

### Comparing `PyCBC-2.2.0/pycbc/types/frequencyseries.py` & `PyCBC-2.2.1/pycbc/types/frequencyseries.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/optparse.py` & `PyCBC-2.2.1/pycbc/types/optparse.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/types/timeseries.py` & `PyCBC-2.2.1/pycbc/types/timeseries.py`

 * *Files 2% similar despite different names*

```diff
@@ -236,70 +236,84 @@
         if self._epoch is None:
             return Array(range(len(self))) * self._delta_t
         else:
             return Array(range(len(self))) * self._delta_t + float(self._epoch)
     sample_times = property(get_sample_times,
                             doc="Array containing the sample times.")
 
-    def at_time(self, time, nearest_sample=False, interpolate=None):
+    def at_time(self, time, nearest_sample=False,
+                interpolate=None, extrapolate=None):
         """ Return the value at the specified gps time
 
         Parameters
         ----------
         nearest_sample: bool
             Return the sample at the time nearest to the chosen time rather
             than rounded down.
         interpolate: str, None
             Return the interpolated value of the time series. Choices
             are simple linear or quadratic interpolation.
+        extrapolate: str or float, None
+            Value to return if time is outsidde the range of the vector or
+            method of extrapolating the value.
         """
+        if nearest_sample:
+            time = time + self.delta_t / 2.0
+        vtime = _numpy.array(time, ndmin=1)
+
+        fill_value = None
+        keep_idx = None
+        size = len(vtime)
+        if extrapolate is not None:
+            if _numpy.isscalar(extrapolate) and _numpy.isreal(extrapolate):
+                fill_value = extrapolate
+                facl = facr = 0
+                if interpolate == 'quadratic':
+                    facl = facr = 1.1
+                elif interpolate == 'linear':
+                    facl, facr = 0.1, 1.1
+
+                left = (vtime >= self.start_time + self.delta_t * facl)
+                right = (vtime < self.end_time - self.delta_t * facr)
+                keep_idx = _numpy.where(left & right)[0]
+                vtime = vtime[keep_idx]
+            else:
+                raise ValueError("Unsuported extrapolate: %s" % extrapolate)
+
+        fi = (vtime - float(self.start_time))*self.sample_rate
+        i = _numpy.asarray(_numpy.floor(fi)).astype(int)
+        di = fi - i
+
         if interpolate == 'linear':
-            i = (time - float(self.start_time))*self.sample_rate
-            di = i - int(i)
-            i = int(i)
             a = self[i]
             b = self[i+1]
-            return a + (b - a) * di
+            ans = a + (b - a) * di
         elif interpolate == 'quadratic':
-            ir = (time - float(self.start_time))*self.sample_rate
-            i = _numpy.floor(_numpy.asarray(ir)).astype(int)
-            di = ir - i
             c = self.data[i]
             xr = self.data[i + 1] - c
             xl = self.data[i - 1] - c
             a = 0.5 * (xr + xl)
             b = 0.5 * (xr - xl)
             ans = a * di**2.0 + b * di + c
-            return ans
-
-        if nearest_sample:
-            time += self.delta_t / 2.0
-        return self[int((time-self.start_time)*self.sample_rate)]
-
-    def at_times(self, times, nearest_sample = False):
-        """ Return an array of values at the specified gps times
+        else:
+            ans = self[i]
 
-        Parameters
-        ----------
-        times: array of floats
-            The times whose values are needed
-        nearest_sample: bool
-            Return the samples at the times nearest to the chosen times rather
-            than rounded down.
+        ans = _numpy.array(ans, ndmin=1)
+        if fill_value is not None:
+            old = ans
+            ans = _numpy.zeros(size) + fill_value
+            ans[keep_idx] = old
+            ans = _numpy.array(ans, ndmin=1)
 
-        Returns
-        -------
-        values: array of floats
-            The values of the timeseries at the given times
-        """
+        if _numpy.isscalar(time):
+            return ans[0]
+        else:
+            return ans
 
-        if nearest_sample:
-            times += self.delta_t / 2.0
-        elapsed_times = times - self.start_time
-        return self[(elapsed_times * self.sample_rate).astype('int')]
+    at_times = at_time
 
     def __eq__(self,other):
         """
         This is the Python special method invoked whenever the '=='
         comparison is used.  It will return true if the data of two
         time series are identical, and all of the numeric meta-data
         are identical, irrespective of whether or not the two
```

### Comparing `PyCBC-2.2.0/pycbc/vetoes/autochisq.py` & `PyCBC-2.2.1/pycbc/vetoes/autochisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/vetoes/bank_chisq.py` & `PyCBC-2.2.1/pycbc/vetoes/bank_chisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/vetoes/chisq.py` & `PyCBC-2.2.1/pycbc/vetoes/chisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/vetoes/chisq_cpu.pyx` & `PyCBC-2.2.1/pycbc/vetoes/chisq_cpu.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/vetoes/chisq_cuda.py` & `PyCBC-2.2.1/pycbc/vetoes/chisq_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/vetoes/sgchisq.py` & `PyCBC-2.2.1/pycbc/vetoes/sgchisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/SpinTaylorF2.py` & `PyCBC-2.2.1/pycbc/waveform/SpinTaylorF2.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/__init__.py` & `PyCBC-2.2.1/pycbc/waveform/__init__.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/bank.py` & `PyCBC-2.2.1/pycbc/waveform/bank.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/compress.py` & `PyCBC-2.2.1/pycbc/waveform/compress.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/decompress_cpu.py` & `PyCBC-2.2.1/pycbc/waveform/decompress_cpu.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/decompress_cpu_ccode.cpp` & `PyCBC-2.2.1/pycbc/waveform/decompress_cpu_ccode.cpp`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/decompress_cpu_cython.pyx` & `PyCBC-2.2.1/pycbc/waveform/decompress_cpu_cython.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/decompress_cuda.py` & `PyCBC-2.2.1/pycbc/waveform/decompress_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/generator.py` & `PyCBC-2.2.1/pycbc/waveform/generator.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/multiband.py` & `PyCBC-2.2.1/pycbc/waveform/multiband.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/nltides.py` & `PyCBC-2.2.1/pycbc/waveform/nltides.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/parameters.py` & `PyCBC-2.2.1/pycbc/waveform/parameters.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/plugin.py` & `PyCBC-2.2.1/pycbc/waveform/plugin.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/premerger.py` & `PyCBC-2.2.1/pycbc/waveform/premerger.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/pycbc_phenomC_tmplt.py` & `PyCBC-2.2.1/pycbc/waveform/pycbc_phenomC_tmplt.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/ringdown.py` & `PyCBC-2.2.1/pycbc/waveform/ringdown.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/sinegauss.py` & `PyCBC-2.2.1/pycbc/waveform/sinegauss.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/spa_tmplt.py` & `PyCBC-2.2.1/pycbc/waveform/spa_tmplt.py`

 * *Files 5% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 #  Free Software Foundation, Inc., 59 Temple Place, Suite 330, Boston,
 #  MA  02111-1307  USA
 
 """This module contains functions for generating common SPA template precalculated
    vectors.
 """
 from math import sqrt, log
+import warnings
 import numpy, lal, pycbc.pnutils
 from pycbc.scheme import schemed
 from pycbc.types import FrequencySeries, Array, complex64, float32, zeros
 from pycbc.waveform.utils import ceilpow2
 
 lalsimulation = pycbc.libutils.import_optional('lalsimulation')
 
@@ -67,21 +68,23 @@
     x3T = xT * x2T
     x4T = x2T * x2T
     x5T = x2T * x3T
     x6T = x3T * x3T
     x7T = x3T * x4T
     x8T = x4T * x4T
 
-    # Computes the chirp time as tC = t(v_low)
+    # Computes the chirp time as tC = t(v_low);
     # tC = t(v_low) - t(v_upper) would be more
-    # correct, but the difference is negligble.
+    # correct, but the difference is negligible.
 
     # This formula works for any PN order, because
     # higher order coeffs will be set to zero.
-    return c0T * (1 + c2T * x2T + c3T * x3T + c4T * x4T + c5T * x5T + (c6T + c6LogT * numpy.log(xT)) * x6T + c7T * x7T) / x8T
+    return c0T * (1 + c2T * x2T + c3T * x3T + c4T * x4T + c5T * x5T +
+                  (c6T + c6LogT * numpy.log(xT)) * x6T + c7T * x7T) / x8T
+
 
 def spa_length_in_time(**kwds):
     """
     Returns the length in time of the template,
     based on the masses, PN order, and low-frequency
     cut-off.
     """
@@ -91,82 +94,88 @@
     porder = int(kwds['phase_order'])
 
     # For now, we call the swig-wrapped function below in
     # lalinspiral.  Eventually would be nice to replace this
     # with a function using PN coeffs from lalsimulation.
     return findchirp_chirptime(m1, m2, flow, porder)
 
+
 def spa_amplitude_factor(**kwds):
     m1 = kwds['mass1']
     m2 = kwds['mass2']
 
     _, eta = pycbc.pnutils.mass1_mass2_to_mchirp_eta(m1, m2)
 
-    FTaN = 32.0 * eta*eta / 5.0
-    dETaN = 2 * -eta/2.0
+    FTaN = 32. * eta * eta / 5.
+    dETaN = 2. * -eta / 2.
 
     M = m1 + m2
 
     m_sec = M * lal.MTSUN_SI
     piM = lal.PI * m_sec
 
-    amp0 = 4. * m1 * m2 / (1e6 * lal.PC_SI ) * lal.MRSUN_SI * lal.MTSUN_SI * sqrt(lal.PI/12.0)
+    amp0 = 4. * m1 * m2 / (1e6 * lal.PC_SI) * lal.MRSUN_SI * lal.MTSUN_SI * sqrt(lal.PI / 12.)
 
-    fac = numpy.sqrt( -dETaN / FTaN) * amp0 * (piM ** (-7.0/6.0))
+    fac = numpy.sqrt(-dETaN / FTaN) * amp0 * (piM ** (-7./6.))
     return -fac
 
+
 _prec = None
 def spa_tmplt_precondition(length, delta_f, kmin=0):
     """Return the amplitude portion of the TaylorF2 approximant, used to precondition
-    the strain data. The result is cached, and so should not be modified only read.
+    the strain data. The result is cached, and so should not be modified, only read.
     """
     global _prec
     if _prec is None or _prec.delta_f != delta_f or len(_prec) < length:
-        v = numpy.arange(0, (kmin+length*2), 1.0) * delta_f
-        v = numpy.power(v[1:len(v)], -7.0/6.0)
+        v = numpy.arange(0, (kmin + length*2), 1.) * delta_f
+        v = numpy.power(v[1:len(v)], -7./6.)
         _prec = FrequencySeries(v, delta_f=delta_f, dtype=float32)
     return _prec[kmin:kmin + length]
 
+
 def spa_tmplt_norm(psd, length, delta_f, f_lower):
     amp = spa_tmplt_precondition(length, delta_f)
     k_min = int(f_lower / delta_f)
-    sigma = (amp[k_min:length].numpy() ** 2.0 / psd[k_min:length].numpy())
+    sigma = (amp[k_min:length].numpy() ** 2. / psd[k_min:length].numpy())
     norm_vec = numpy.zeros(length)
-    norm_vec[k_min:length] = sigma.cumsum() * 4 * delta_f
+    norm_vec[k_min:length] = sigma.cumsum() * 4. * delta_f
     return norm_vec
 
+
 def spa_tmplt_end(**kwds):
-    return pycbc.pnutils.f_SchwarzISCO(kwds['mass1']+kwds['mass2'])
+    return pycbc.pnutils.f_SchwarzISCO(kwds['mass1'] + kwds['mass2'])
+
 
 def spa_distance(psd, mass1, mass2, lower_frequency_cutoff, snr=8):
     """ Return the distance at a given snr (default=8) of the SPA TaylorF2
     template.
     """
     kend = int(spa_tmplt_end(mass1=mass1, mass2=mass2) / psd.delta_f)
     norm1 = spa_tmplt_norm(psd, len(psd), psd.delta_f, lower_frequency_cutoff)
-    norm2 = (spa_amplitude_factor(mass1=mass1, mass2=mass2)) ** 2.0
+    norm2 = spa_amplitude_factor(mass1=mass1, mass2=mass2) ** 2.0
 
     if kend >= len(psd):
         kend = len(psd) - 2
     return sqrt(norm1[kend] * norm2) / snr
 
+
 @schemed("pycbc.waveform.spa_tmplt_")
 def spa_tmplt_engine(htilde, kmin, phase_order, delta_f, piM, pfaN,
                      pfa2, pfa3, pfa4, pfa5, pfl5,
                      pfa6, pfl6, pfa7, amp_factor):
     """ Calculate the spa tmplt phase
     """
     err_msg = "This function is a stub that should be overridden using the "
     err_msg += "scheme. You shouldn't be seeing this error!"
     raise ValueError(err_msg)
 
+
 def spa_tmplt(**kwds):
     """ Generate a minimal TaylorF2 approximant with optimizations for the sin/cos
     """
-    # Pull out the input arguments
     distance = kwds['distance']
     mass1 = kwds['mass1']
     mass2 = kwds['mass2']
     s1z = kwds['spin1z']
     s2z = kwds['spin2z']
     phase_order = int(kwds['phase_order'])
     #amplitude_order = int(kwds['amplitude_order'])
@@ -184,15 +193,15 @@
         lalsimulation.SimInspiralWaveformParamsInsertPNPhaseOrder(
             lal_pars, phase_order)
 
     if spin_order != -1:
         lalsimulation.SimInspiralWaveformParamsInsertPNSpinOrder(
             lal_pars, spin_order)
 
-    #Calculate the PN terms
+    # Calculate the PN terms
     phasing = lalsimulation.SimInspiralTaylorF2AlignedPhasing(
                                     float(mass1), float(mass2),
                                     float(s1z), float(s2z),
                                     lal_pars)
 
     pfaN = phasing.v[0]
     pfa2 = phasing.v[2] / pfaN
@@ -207,22 +216,33 @@
 
     piM = lal.PI * (mass1 + mass2) * lal.MTSUN_SI
 
     if 'sample_points' not in kwds:
         f_lower = kwds['f_lower']
         delta_f = kwds['delta_f']
         kmin = int(f_lower / float(delta_f))
-        vISCO = 1. / sqrt(6.)
-        fISCO = vISCO * vISCO * vISCO / piM
 
-        if 'f_upper' in kwds:
-            fISCO = kwds['f_upper']
+        # Get max frequency one way or another
+        # f_final is assigned default value 0 in parameters.py
+        if 'f_final' in kwds and kwds['f_final'] > 0.:
+            fstop = kwds['f_final']
+        elif 'f_upper' in kwds:
+            fstop = kwds['f_upper']
+            warnings.warn('f_upper is deprecated in favour of f_final!',
+                          DeprecationWarning)
+        else:
+            # Schwarzschild ISCO frequency
+            vISCO = 1. / sqrt(6.)
+            fstop = vISCO * vISCO * vISCO / piM
+        if fstop <= f_lower:
+            raise ValueError("cannot generate waveform! f_lower >= f_final"
+                             f" ({f_lower}, {fstop})")
 
-        kmax = int(fISCO / delta_f)
-        f_max = ceilpow2(fISCO)
+        kmax = int(fstop / delta_f)
+        f_max = ceilpow2(fstop)
         n = int(f_max / delta_f) + 1
 
         if not out:
             htilde = FrequencySeries(zeros(n, dtype=numpy.complex64), delta_f=delta_f, copy=False)
         else:
             if type(out) is not Array:
                 raise TypeError("Output must be an instance of Array")
@@ -238,9 +258,9 @@
                          pfa6, pfl6, pfa7, amp_factor)
     else:
         from .spa_tmplt_cpu import spa_tmplt_inline_sequence
         htilde = numpy.empty(len(kwds['sample_points']), dtype=numpy.complex64)
         spa_tmplt_inline_sequence(
             piM, pfaN, pfa2, pfa3, pfa4, pfa5, pfl5, pfa6, pfl6, pfa7,
             amp_factor, kwds['sample_points'], htilde)
-    return htilde
 
+    return htilde
```

### Comparing `PyCBC-2.2.0/pycbc/waveform/spa_tmplt_cpu.pyx` & `PyCBC-2.2.1/pycbc/waveform/spa_tmplt_cpu.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/spa_tmplt_cuda.py` & `PyCBC-2.2.1/pycbc/waveform/spa_tmplt_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/supernovae.py` & `PyCBC-2.2.1/pycbc/waveform/supernovae.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/utils.py` & `PyCBC-2.2.1/pycbc/waveform/utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/utils_cpu.pyx` & `PyCBC-2.2.1/pycbc/waveform/utils_cpu.pyx`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/utils_cuda.py` & `PyCBC-2.2.1/pycbc/waveform/utils_cuda.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/waveform/waveform.py` & `PyCBC-2.2.1/pycbc/waveform/waveform.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,14 +32,15 @@
 from pycbc.types import real_same_precision_as, complex_same_precision_as
 import pycbc.scheme as _scheme
 import inspect
 from pycbc.fft import fft
 from pycbc import pnutils, libutils
 from pycbc.waveform import utils as wfutils
 from pycbc.waveform import parameters
+from pycbc.conversions import get_final_from_initial, tau_from_final_mass_spin
 from pycbc.filter import interpolate_complex_frequency, resample_to_delta_t
 import pycbc
 from .spa_tmplt import spa_tmplt, spa_tmplt_norm, spa_tmplt_end, \
                       spa_tmplt_precondition, spa_amplitude_factor, \
                       spa_length_in_time
 
 class NoWaveformError(Exception):
@@ -729,16 +730,16 @@
     Returns
     -------
     dict
         The detector-frame waveform (with detector response) in frequency
         domain. Keys are requested data channels, values are FrequencySeries.
     """
     input_params = props(template, **kwargs)
-    input_params['delta_f'] = -1
-    input_params['f_lower'] = -1
+    if 'f_lower' not in input_params:
+        input_params['f_lower'] = -1
     if input_params['approximant'] not in fd_det:
         raise ValueError("Approximant %s not available" %
                             (input_params['approximant']))
     wav_gen = fd_det[input_params['approximant']]
     if hasattr(wav_gen, 'required'):
         required = wav_gen.required
     else:
@@ -746,46 +747,50 @@
     check_args(input_params, required)
     return wav_gen(**input_params)
 
 get_fd_det_waveform.__doc__ = get_fd_det_waveform.__doc__.format(
     params=parameters.fd_waveform_params.docstr(prefix="    ",
            include_label=False).lstrip(' '))
 
-def _base_get_td_waveform_from_fd(template=None, rwrap=0.2, **params):
+def _base_get_td_waveform_from_fd(template=None, rwrap=None, **params):
     """ The base function to calculate time domain version of fourier
     domain approximant which not include or includes detector response.
     Called by `get_td_waveform_from_fd` and `get_td_det_waveform_from_fd_det`.
     """
     kwds = props(template, **params)
     nparams = kwds.copy()
-    if nparams['approximant'] not in fd_det:
-        # determine the duration to use
-        full_duration = duration = \
-            get_waveform_filter_length_in_time(**params)
-    else:
-        if nparams['approximant'] not in _filter_time_lengths:
-            raise ValueError("Approximant %s _filter_time_lengths function \
-                             not available" % (nparams['approximant']))
-        full_duration = duration = \
-            _filter_time_lengths[nparams['approximant']](
-                m1=kwds['mass1'], m2=kwds['mass2'],
-                s1z=kwds['spin1z'], s2z=kwds['spin1z'],
-                f_lower=kwds['f_lower']
-            )
+
+    if rwrap is None:
+        # In the `pycbc.waveform.parameters` module, spin1z and
+        # spin2z have the default value 0. Users must have input
+        # masses, so no else is needed.
+        mass_spin_params = set(['mass1', 'mass2', 'spin1z', 'spin2z'])
+        if mass_spin_params.issubset(set(nparams.keys())):
+            m_final, spin_final = get_final_from_initial(
+                mass1=nparams['mass1'], mass2=nparams['mass2'],
+                spin1z=nparams['spin1z'], spin2z=nparams['spin2z'])
+            rwrap = tau_from_final_mass_spin(m_final, spin_final) * 10
+            if rwrap < 5:
+                # Long enough for very massive BBHs in XG detectors,
+                # up to (3000, 3000) solar mass, while still not a
+                # computational burden for 2G cases.
+                rwrap = 5
+
+    if nparams['approximant'] not in _filter_time_lengths:
+        raise ValueError("Approximant %s _filter_time_lengths function \
+                         not available" % (nparams['approximant']))
+    # determine the duration to use
+    full_duration = duration = get_waveform_filter_length_in_time(**nparams)
 
     while full_duration < duration * 1.5:
-        if nparams['approximant'] not in fd_det:
-            full_duration = get_waveform_filter_length_in_time(**nparams)
-        else:
-            full_duration = _filter_time_lengths[nparams['approximant']](
-                m1=kwds['mass1'], m2=kwds['mass2'],
-                s1z=kwds['spin1z'], s2z=kwds['spin1z'],
-                f_lower=nparams['f_lower']
-            )
+        full_duration = get_waveform_filter_length_in_time(**nparams)
         nparams['f_lower'] *= 0.99
+        if 't_obs_start' in nparams and \
+           full_duration >= nparams['t_obs_start']:
+            break
 
     if 'f_ref' not in nparams:
         nparams['f_ref'] = params['f_lower']
 
     # factor to ensure the vectors are all large enough. We don't need to
     # completely trust our duration estimator in this case, at a small
     # increase in computational cost
@@ -822,15 +827,15 @@
             # avoid wraparound
             wfs[ifo] = wfs[ifo].cyclic_time_shift(-rwrap)
             wfs[ifo] = wfutils.fd_to_td(wfs[ifo], delta_t=kwds['delta_t'],
                                         left_window=(nparams['f_lower'],
                                         kwds['f_lower']))
         return wfs
 
-def get_td_waveform_from_fd(rwrap=0.2, **params):
+def get_td_waveform_from_fd(rwrap=None, **params):
     """ Return time domain version of fourier domain approximant.
 
     This returns a time domain version of a fourier domain approximant, with
     padding and tapering at the start of the waveform.
 
     Parameters
     ----------
@@ -847,15 +852,15 @@
     hp: pycbc.types.TimeSeries
         Plus polarization time series
     hc: pycbc.types.TimeSeries
         Cross polarization time series
     """
     return _base_get_td_waveform_from_fd(None, rwrap, **params)
 
-def get_td_det_waveform_from_fd_det(template=None, rwrap=0.2, **params):
+def get_td_det_waveform_from_fd_det(template=None, rwrap=None, **params):
     """ Return time domain version of fourier domain approximant which
     includes detector response, with padding and tapering at the start
     of the waveform.
 
     Parameters
     ----------
     rwrap: float
```

### Comparing `PyCBC-2.2.0/pycbc/waveform/waveform_modes.py` & `PyCBC-2.2.1/pycbc/waveform/waveform_modes.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 
 """Provides functions and utilities for generating waveforms mode-by-mode.
 """
 
 from string import Formatter
 import lal
 
-from pycbc import libutils
+from pycbc import libutils, pnutils
 from pycbc.types import (TimeSeries, FrequencySeries)
 from .waveform import (props, _check_lal_pars, check_args)
 from . import parameters
 
 lalsimulation = libutils.import_optional('lalsimulation')
 
 def _formatdocstr(docstr):
@@ -178,70 +178,56 @@
         ret = ret.next
     return hlms
 
 
 get_nrsur_modes.__doc__ = _formatdocstr(get_nrsur_modes.__doc__)
 
 
-def _get_imrphenomx_modes(return_posneg=False, **params):
-    """Generates ``IMRPhenomX[P]HM`` waveforms mode-by-mode.
-
-    Currently does not work; just raises a ``NotImplementedError``.
-    """
-    # FIXME: raising not implemented error because this currently does not
-    # work. The issue is the OneMode function adds the +/- m modes together
-    # automatically. Remove once this is fixed in lalsimulation, and/or I
-    # figure out a reliable way to separate the +/-m modes.
-    raise NotImplementedError("Currently not implemented")
+def get_imrphenomxh_modes(**params):
+    """Generates ``IMRPhenomXHM`` waveforms mode-by-mode. """
     approx = params['approximant']
     if not approx.startswith('IMRPhenomX'):
         raise ValueError("unsupported approximant")
     mode_array = params.pop('mode_array', None)
     if mode_array is None:
         mode_array = default_modes(approx)
     if 'f_final' not in params:
         # setting to 0 will default to ringdown frequency
         params['f_final'] = 0.
     hlms = {}
     for (l, m) in mode_array:
         params['mode_array'] = [(l, m)]
         laldict = _check_lal_pars(params)
-        hpos, hneg = lalsimulation.SimIMRPhenomXPHMOneMode(
-            l, m,
-            params['mass1']*lal.MSUN_SI,
-            params['mass2']*lal.MSUN_SI,
-            params['spin1x'], params['spin1y'], params['spin1z'],
-            params['spin2x'], params['spin2y'], params['spin2z'],
-            params['distance']*1e6*lal.PC_SI, params['coa_phase'],
-            params['delta_f'], params['f_lower'], params['f_final'],
-            params['f_ref'],
+        hlm = lalsimulation.SimIMRPhenomXHMGenerateFDOneMode(
+            float(pnutils.solar_mass_to_kg(params['mass1'])),
+            float(pnutils.solar_mass_to_kg(params['mass2'])),
+            float(params['spin1z']),
+            float(params['spin2z']), l, m,
+            pnutils.megaparsecs_to_meters(float(params['distance'])),
+            params['f_lower'], params['f_final'], params['delta_f'],
+            params['coa_phase'], params['f_ref'],
             laldict)
-        hpos = FrequencySeries(hpos.data.data, delta_f=hpos.deltaF,
-                               epoch=hpos.epoch)
-        hneg = FrequencySeries(hneg.data.data, delta_f=hneg.deltaF,
-                               epoch=hneg.epoch)
-        if return_posneg:
-            hlms[l, m] = (hpos, hneg)
-        else:
-            # convert to ulm, vlm
-            ulm = 0.5 * (hpos + hneg.conj())
-            vlm = 0.5j * (hneg.conj() - hpos)
-            hlms[l, m] = (ulm, vlm)
+        hlm = FrequencySeries(hlm.data.data, delta_f=hlm.deltaF,
+                              epoch=hlm.epoch)
+        # Plus, cross strains without Y_lm.
+        # (-1)**(l) factor ALREADY included in FDOneMode
+        hplm = 0.5 * hlm  # Plus strain
+        hclm = 0.5j * hlm  # Cross strain
+        if m > 0:
+            hclm *= -1
+        hlms[l, m] = (hplm, hclm)
     return hlms
 
 
 _mode_waveform_td = {'NRSur7dq4': get_nrsur_modes,
                      }
-
-
-# Remove commented out once IMRPhenomX one mode is fixed
-_mode_waveform_fd = {#'IMRPhenomXHM': get_imrphenomhm_modes,
-                     #'IMRPhenomXPHM' : get_imrphenomhm_modes,
-                    }
-
+_mode_waveform_fd = {'IMRPhenomXHM': get_imrphenomxh_modes,
+                     }
+# 'IMRPhenomXPHM':get_imrphenomhm_modes needs to be implemented
+# LAL function do not split strain mode by mode
 
 def fd_waveform_mode_approximants():
     """Frequency domain approximants that will return separate modes."""
     return sorted(_mode_waveform_fd.keys())
 
 
 def td_waveform_mode_approximants():
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/__init__.py` & `PyCBC-2.2.1/pycbc/workflow/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -40,13 +40,14 @@
 from pycbc.workflow.psdfiles import *
 from pycbc.workflow.splittable import *
 from pycbc.workflow.coincidence import *
 from pycbc.workflow.injection import *
 from pycbc.workflow.plotting import *
 from pycbc.workflow.minifollowups import *
 from pycbc.workflow.dq import *
+from pycbc.workflow.versioning import *
 
 # Set the pycbc workflow specific pegasus configuration and planning files
 from pycbc.workflow.pegasus_workflow import PEGASUS_FILE_DIRECTORY
 
 # Set the configuration file base directory
 INI_FILE_DIRECTORY = os.path.join(os.path.dirname(__file__), 'ini_files')
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/coincidence.py` & `PyCBC-2.2.1/pycbc/workflow/coincidence.py`

 * *Files 14% similar despite different names*

```diff
@@ -104,53 +104,108 @@
             node.add_opt('--segment-name', veto_name)
         node.add_opt('--pivot-ifo', pivot_ifo)
         node.add_opt('--fixed-ifo', fixed_ifo)
         node.add_opt('--template-fraction-range', template_str)
         node.new_output_file_opt(seg, '.hdf', '--output-file', tags=tags)
         return node
 
+class PyCBCFindSnglsExecutable(Executable):
+    """Calculate single-detector ranking statistic for triggers"""
+
+    current_retention_level = Executable.ALL_TRIGGERS
+    file_input_options = ['--statistic-files']
+    def create_node(self, trig_files, bank_file, stat_files, veto_file,
+                    veto_name, template_str, tags=None):
+        if tags is None:
+            tags = []
+        segs = trig_files.get_times_covered_by_files()
+        seg = segments.segment(segs[0][0], segs[-1][1])
+        node = Node(self)
+        node.add_input_opt('--template-bank', bank_file)
+        node.add_input_list_opt('--trigger-files', trig_files)
+        if len(stat_files) > 0:
+            node.add_input_list_opt('--statistic-files', stat_files)
+        if veto_file is not None:
+            node.add_input_opt('--veto-files', veto_file)
+            node.add_opt('--segment-name', veto_name)
+        node.add_opt('--template-fraction-range', template_str)
+        node.new_output_file_opt(seg, '.hdf', '--output-file', tags=tags)
+        return node
 
 class PyCBCStatMapExecutable(Executable):
-    """Calculate FAP, IFAR, etc"""
+    """Calculate FAP, IFAR, etc for coincs"""
 
     current_retention_level = Executable.MERGED_TRIGGERS
     def create_node(self, coinc_files, ifos, tags=None):
         if tags is None:
             tags = []
         segs = coinc_files.get_times_covered_by_files()
         seg = segments.segment(segs[0][0], segs[-1][1])
 
         node = Node(self)
         node.add_input_list_opt('--coinc-files', coinc_files)
         node.add_opt('--ifos', ifos)
         node.new_output_file_opt(seg, '.hdf', '--output-file', tags=tags)
         return node
 
+class PyCBCSnglsStatMapExecutable(Executable):
+    """Calculate FAP, IFAR, etc for singles"""
+
+    current_retention_level = Executable.MERGED_TRIGGERS
+    def create_node(self, sngls_files, ifo, tags=None):
+        if tags is None:
+            tags = []
+        segs = sngls_files.get_times_covered_by_files()
+        seg = segments.segment(segs[0][0], segs[-1][1])
+
+        node = Node(self)
+        node.add_input_list_opt('--sngls-files', sngls_files)
+        node.add_opt('--ifos', ifo)
+        node.new_output_file_opt(seg, '.hdf', '--output-file', tags=tags)
+        return node
+
 
 class PyCBCStatMapInjExecutable(Executable):
-    """Calculate FAP, IFAR, etc"""
+    """Calculate FAP, IFAR, etc for coincs for injections"""
 
     current_retention_level = Executable.MERGED_TRIGGERS
-    def create_node(self, zerolag, full_data,
-                    injfull, fullinj, ifos, tags=None):
+    def create_node(self, coinc_files, full_data,
+                    ifos, tags=None):
         if tags is None:
             tags = []
-        segs = zerolag.get_times_covered_by_files()
+        segs = coinc_files.get_times_covered_by_files()
         seg = segments.segment(segs[0][0], segs[-1][1])
 
         node = Node(self)
-        node.add_input_list_opt('--zero-lag-coincs', zerolag)
+        node.add_input_list_opt('--zero-lag-coincs', coinc_files)
 
         if isinstance(full_data, list):
             node.add_input_list_opt('--full-data-background', full_data)
         else:
             node.add_input_opt('--full-data-background', full_data)
 
-        node.add_input_list_opt('--mixed-coincs-inj-full', injfull)
-        node.add_input_list_opt('--mixed-coincs-full-inj', fullinj)
+        node.add_opt('--ifos', ifos)
+        node.new_output_file_opt(seg, '.hdf', '--output-file', tags=tags)
+        return node
+
+class PyCBCSnglsStatMapInjExecutable(Executable):
+    """Calculate FAP, IFAR, etc for singles for injections"""
+
+    current_retention_level = Executable.MERGED_TRIGGERS
+    def create_node(self, sngls_files, background_file,
+                    ifos, tags=None):
+        if tags is None:
+            tags = []
+        segs = sngls_files.get_times_covered_by_files()
+        seg = segments.segment(segs[0][0], segs[-1][1])
+
+        node = Node(self)
+        node.add_input_list_opt('--sngls-files', sngls_files)
+        node.add_input_opt('--full-data-background', background_file)
+
         node.add_opt('--ifos', ifos)
         node.new_output_file_opt(seg, '.hdf', '--output-file', tags=tags)
         return node
 
 
 class PyCBCHDFInjFindExecutable(Executable):
     """Find injections in the hdf files output"""
@@ -369,34 +424,62 @@
 
     ifolist = ' '.join(ifos)
     stat_node = statmap_exe.create_node(coinc_files, ifolist)
     workflow.add_node(stat_node)
     return stat_node.output_file
 
 
+def setup_sngls_statmap(workflow, ifo, sngls_files, out_dir, tags=None):
+    tags = [] if tags is None else tags
+
+    statmap_exe = PyCBCSnglsStatMapExecutable(workflow.cp, 'sngls_statmap',
+                                              ifos=ifo,
+                                              tags=tags, out_dir=out_dir)
+
+    stat_node = statmap_exe.create_node(sngls_files, ifo)
+    workflow.add_node(stat_node)
+    return stat_node.output_file
+
+
 def setup_statmap_inj(workflow, ifos, coinc_files, background_file,
                       out_dir, tags=None):
     tags = [] if tags is None else tags
 
     statmap_exe = PyCBCStatMapInjExecutable(workflow.cp,
                                             'statmap_inj',
                                             ifos=ifos,
                                             tags=tags, out_dir=out_dir)
 
     ifolist = ' '.join(ifos)
-    stat_node = statmap_exe.create_node(FileList(coinc_files['injinj']),
+    stat_node = statmap_exe.create_node(FileList(coinc_files),
                                         background_file,
-                                        FileList(coinc_files['injfull']),
-                                        FileList(coinc_files['fullinj']),
                                         ifolist)
     workflow.add_node(stat_node)
     return stat_node.output_files[0]
 
 
-def setup_interval_coinc_inj(workflow, hdfbank, full_data_trig_files,
+def setup_sngls_statmap_inj(workflow, ifo, sngls_inj_files, background_file,
+                            out_dir, tags=None):
+    tags = [] if tags is None else tags
+
+    statmap_exe = PyCBCSnglsStatMapInjExecutable(workflow.cp,
+                                                 'sngls_statmap_inj',
+                                                 ifos=ifo,
+                                                 tags=tags,
+                                                 out_dir=out_dir)
+
+    stat_node = statmap_exe.create_node(sngls_inj_files,
+                                        background_file,
+                                        ifo)
+
+    workflow.add_node(stat_node)
+    return stat_node.output_files[0]
+
+
+def setup_interval_coinc_inj(workflow, hdfbank,
                              inj_trig_files, stat_files,
                              background_file, veto_file, veto_name,
                              out_dir, pivot_ifo, fixed_ifo, tags=None):
     """
     This function sets up exact match coincidence for injections
     """
     if tags is None:
@@ -404,60 +487,39 @@
     make_analysis_dir(out_dir)
     logging.info('Setting up coincidence for injections')
 
     # Wall time knob and memory knob
     factor = int(workflow.cp.get_opt_tags('workflow-coincidence',
                                           'parallelization-factor', tags))
 
-    ffiles = {}
     ifiles = {}
-    for ifo, ffi in zip(*full_data_trig_files.categorize_by_attr('ifo')):
-        ffiles[ifo] = ffi[0]
     for ifo, ifi in zip(*inj_trig_files.categorize_by_attr('ifo')):
         ifiles[ifo] = ifi[0]
 
     injinj_files = FileList()
-    injfull_files = FileList()
-    fullinj_files = FileList()
-    # For the injfull and fullinj separation we take the pivot_ifo on one side,
-    # and the rest that are attached to the fixed_ifo on the other side
     for ifo in ifiles:  # ifiles is keyed on ifo
-        if ifo == pivot_ifo:
-            injinj_files.append(ifiles[ifo])
-            injfull_files.append(ifiles[ifo])
-            fullinj_files.append(ffiles[ifo])
-        else:
-            injinj_files.append(ifiles[ifo])
-            injfull_files.append(ffiles[ifo])
-            fullinj_files.append(ifiles[ifo])
-
-    combo = [(injinj_files, "injinj"),
-             (injfull_files, "injfull"),
-             (fullinj_files, "fullinj"),
-            ]
-    bg_files = {'injinj':[], 'injfull':[], 'fullinj':[]}
-
-    for trig_files, ctag in combo:
-        findcoinc_exe = PyCBCFindCoincExecutable(workflow.cp,
-                                                 'coinc',
-                                                 ifos=ifiles.keys(),
-                                                 tags=tags + [ctag],
-                                                 out_dir=out_dir)
-        for i in range(factor):
-            group_str = '%s/%s' % (i, factor)
-            coinc_node = findcoinc_exe.create_node(trig_files, hdfbank,
-                                                   stat_files,
-                                                   veto_file, veto_name,
-                                                   group_str,
-                                                   pivot_ifo,
-                                                   fixed_ifo,
-                                                   tags=['JOB'+str(i)])
+        injinj_files.append(ifiles[ifo])
 
-            bg_files[ctag] += coinc_node.output_files
-            workflow.add_node(coinc_node)
+    findcoinc_exe = PyCBCFindCoincExecutable(workflow.cp,
+                                             'coinc',
+                                             ifos=ifiles.keys(),
+                                             tags=tags + ['injinj'],
+                                             out_dir=out_dir)
+    bg_files = []
+    for i in range(factor):
+        group_str = '%s/%s' % (i, factor)
+        coinc_node = findcoinc_exe.create_node(injinj_files, hdfbank,
+                                               stat_files,
+                                               veto_file, veto_name,
+                                               group_str,
+                                               pivot_ifo,
+                                               fixed_ifo,
+                                               tags=['JOB'+str(i)])
+        bg_files += coinc_node.output_files
+        workflow.add_node(coinc_node)
 
     logging.info('...leaving coincidence for injections')
 
     return setup_statmap_inj(workflow, ifiles.keys(), bg_files,
                              background_file, out_dir,
                              tags=tags + [veto_name])
 
@@ -500,14 +562,82 @@
     statmap_files = setup_statmap(workflow, ifos, bg_files,
                                   out_dir, tags=tags)
 
     logging.info('...leaving coincidence ')
     return statmap_files
 
 
+def setup_sngls(workflow, hdfbank, trig_files, stat_files,
+                veto_file, veto_name, out_dir, tags=None):
+    """
+    This function sets up getting statistic values for single-detector triggers
+    """
+    ifos, _ = trig_files.categorize_by_attr('ifo')
+    findsngls_exe = PyCBCFindSnglsExecutable(workflow.cp, 'sngls', ifos=ifos,
+                                             tags=tags, out_dir=out_dir)
+    # Wall time knob and memory knob
+    factor = int(workflow.cp.get_opt_tags('workflow-coincidence',
+                                          'parallelization-factor',
+                                          [findsngls_exe.ifo_string] + tags))
+
+    statmap_files = []
+    bg_files = FileList()
+    for i in range(factor):
+        group_str = '%s/%s' % (i, factor)
+        sngls_node = findsngls_exe.create_node(trig_files, hdfbank,
+                                               stat_files,
+                                               veto_file, veto_name,
+                                               group_str,
+                                               tags=['JOB'+str(i)])
+        bg_files += sngls_node.output_files
+        workflow.add_node(sngls_node)
+
+    statmap_files = setup_sngls_statmap(workflow, ifos[0], bg_files,
+                                        out_dir, tags=tags)
+
+    logging.info('...leaving coincidence ')
+    return statmap_files
+
+
+def setup_sngls_inj(workflow, hdfbank, inj_trig_files,
+                    stat_files, background_file, veto_file, veto_name,
+                    out_dir, tags=None):
+    """
+    This function sets up getting statistic values for single-detector triggers
+    from injections
+    """
+    ifos, _ = inj_trig_files.categorize_by_attr('ifo')
+    findsnglsinj_exe = PyCBCFindSnglsExecutable(workflow.cp, 'sngls', ifos=ifos,
+                                                tags=tags, out_dir=out_dir)
+    # Wall time knob and memory knob
+    exe_str_tags = [findsnglsinj_exe.ifo_string] + tags
+    factor = int(workflow.cp.get_opt_tags('workflow-coincidence',
+                                          'parallelization-factor',
+                                          exe_str_tags))
+
+    statmap_files = []
+    bg_files = FileList()
+    for i in range(factor):
+        group_str = '%s/%s' % (i, factor)
+        sngls_node = findsnglsinj_exe.create_node(inj_trig_files, hdfbank,
+                                                  stat_files,
+                                                  veto_file, veto_name,
+                                                  group_str,
+                                                  tags=['JOB'+str(i)])
+        bg_files += sngls_node.output_files
+        workflow.add_node(sngls_node)
+
+    statmap_files = setup_sngls_statmap_inj(workflow, ifos[0], bg_files,
+                                            background_file,
+                                            out_dir, tags=tags)
+
+    logging.info('...leaving coincidence ')
+    return statmap_files
+
+
 def select_files_by_ifo_combination(ifocomb, insps):
     """
     This function selects single-detector files ('insps') for a given ifo combination
     """
     inspcomb = FileList()
     for ifo, ifile in zip(*insps.categorize_by_attr('ifo')):
         if ifo in ifocomb:
@@ -518,14 +648,18 @@
 
 def get_ordered_ifo_list(ifocomb, ifo_ids):
     """
     This function sorts the combination of ifos (ifocomb) based on the given
     precedence list (ifo_ids dictionary) and returns the first ifo as pivot
     the second ifo as fixed, and the ordered list joined as a string.
     """
+    if len(ifocomb) == 1:
+        # Single-detector combinations don't have fixed/pivot IFOs
+        return None, None, ifocomb[0]
+
     # combination_prec stores precedence info for the detectors in the combination
     combination_prec = {ifo: ifo_ids[ifo] for ifo in ifocomb}
     ordered_ifo_list = sorted(combination_prec, key = combination_prec.get)
     pivot_ifo = ordered_ifo_list[0]
     fixed_ifo = ordered_ifo_list[1]
 
     return pivot_ifo, fixed_ifo, ''.join(ordered_ifo_list)
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/configparser_test.py` & `PyCBC-2.2.1/pycbc/workflow/configparser_test.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/configuration.py` & `PyCBC-2.2.1/pycbc/workflow/configuration.py`

 * *Files 0% similar despite different names*

```diff
@@ -86,16 +86,15 @@
             r.raise_for_status()
 
         output_fp = open(filename, "wb")
         output_fp.write(r.content)
         output_fp.close()
 
     else:
-        # TODO: We could support other schemes such as gsiftp by
-        # calling out to globus-url-copy
+        # TODO: We could support other schemes as needed
         errmsg = "Unknown URL scheme: %s\n" % (u.scheme)
         errmsg += "Currently supported are: file, http, and https."
         raise ValueError(errmsg)
 
     if not os.path.isfile(filename):
         errmsg = "Error trying to create file %s from %s" % (filename, url)
         raise ValueError(errmsg)
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/core.py` & `PyCBC-2.2.1/pycbc/workflow/core.py`

 * *Files 1% similar despite different names*

```diff
@@ -193,15 +193,15 @@
                                 "at %s on site %s" % (name, exe_path,
                                 exe_site))
         elif exe_url.scheme == 'singularity':
             # Will use an executable within a singularity container. Don't
             # need to do anything here, as I cannot easily check it exists.
             exe_path = exe_url.path
         else:
-            # Could be http, gsiftp, etc. so it needs fetching if run now
+            # Could be http, https, etc. so it needs fetching if run now
             self.needs_fetching = True
             if self.needs_fetching and not self.installed:
                 err_msg = "Non-file path URLs cannot be used unless the "
                 err_msg += "executable is a bundled standalone executable. "
                 err_msg += "If this is the case, then add the "
                 err_msg += "pycbc.installed=True property."
                 raise ValueError(err_msg)
@@ -311,30 +311,38 @@
             if namespace == 'pycbc' or namespace == 'container':
                 continue
 
             value = cp.get(sec, opt).strip()
             key = opt.split('|')[1]
             self.add_profile(namespace, key, value)
 
-    def add_ini_opts(self, cp, sec):
+    def _add_ini_opts(self, cp, sec, ignore_existing=False):
         """Add job-specific options from configuration file.
 
         Parameters
         -----------
         cp : ConfigParser object
-            The ConfigParser object holding the workflow configuration settings
+            The ConfigParser object holding the workflow configuration
+            settings
         sec : string
             The section containing options for this job.
         """
         for opt in cp.options(sec):
+            if opt in self.all_added_options:
+                if ignore_existing:
+                    continue
+                else:
+                    raise ValueError("Option %s has already been added" % opt)
+            self.all_added_options.add(opt)
+
             value = cp.get(sec, opt).strip()
-            opt = '--%s' %(opt,)
+            opt = f'--{opt}'
             if opt in self.file_input_options:
                 # This now expects the option to be a file
-                # Check is we have a list of files
+                # Check if we have a list of files
                 values = [path for path in value.split(' ') if path]
 
                 self.common_raw_options.append(opt)
                 self.common_raw_options.append(' ')
 
                 # Get LFN and PFN
                 for path in values:
@@ -376,14 +384,15 @@
                 # There is a possibility of time-dependent, file options.
                 # For now we will avoid supporting that complication unless
                 # it is needed. This would require resolving the file first
                 # in this function, and then dealing with the time-dependent
                 # stuff later.
                 self.unresolved_td_options[opt] = value
             else:
+                # This option comes from the config file(s)
                 self.common_options += [opt, value]
 
     def add_opt(self, opt, value=None):
         """Add option to job.
 
         Parameters
         -----------
@@ -532,15 +541,14 @@
             self.tagged_name = "{0}-{1}".format(self.name, '_'.join(tags))
         else:
             self.tagged_name = self.name
         if self.ifo_string is not None:
             self.tagged_name = "{0}-{1}".format(self.tagged_name,
                                                 self.ifo_string)
 
-
         # Determine the sections from the ini file that will configure
         # this executable
         sections = [self.name]
         if self.ifo_list is not None:
             if len(self.ifo_list) > 1:
                 sec_tags = tags + self.ifo_list + [self.ifo_string]
             else:
@@ -558,26 +566,32 @@
 
         # Do some basic sanity checking on the options
         for sec1, sec2 in combinations(sections, 2):
             self.cp.check_duplicate_options(sec1, sec2, raise_error=True)
 
         # collect the options and profile information
         # from the ini file section(s)
+        self.all_added_options = set()
         self.common_options = []
         self.common_raw_options = []
         self.unresolved_td_options = {}
         self.common_input_files = []
         for sec in sections:
             if self.cp.has_section(sec):
-                self.add_ini_opts(self.cp, sec)
+                self._add_ini_opts(self.cp, sec)
             else:
                 warn_string = "warning: config file is missing section "
                 warn_string += "[{0}]".format(sec)
                 logging.warn(warn_string)
 
+        # get uppermost section
+        if self.cp.has_section(f'{self.name}-defaultvalues'):
+            self._add_ini_opts(self.cp, f'{self.name}-defaultvalues',
+                               ignore_existing=True)
+
     def update_output_directory(self, out_dir=None):
         """Update the default output directory for output files.
 
         Parameters
         -----------
         out_dir : string (optional, default=None)
             If provided use this as the output directory. Else choose this
@@ -657,14 +671,15 @@
         ifos = []
         if self.cp.has_section('workflow-ifos'):
             for ifo in self.cp.options('workflow-ifos'):
                 ifos.append(ifo.upper())
 
         self.ifos = ifos
         self.ifos.sort(key=str.lower)
+        self.get_ifo_combinations()
         self.ifo_string = ''.join(self.ifos)
 
         # Set up input and output file lists for workflow
         self._inputs = FileList([])
         self._outputs = FileList([])
 
     # FIXME: Should this be in pegasus_workflow?
@@ -841,14 +856,24 @@
                         file_url="file://" + ini_file_path)
         # set the physical file name
         ini_file.add_pfn(ini_file_path, "local")
         # set the storage path to be the same
         ini_file.storage_path = ini_file_path
         return FileList([ini_file])
 
+    def get_ifo_combinations(self):
+        """
+        Get a list of strings for all possible combinations of IFOs
+        in the workflow
+        """
+        self.ifo_combinations = []
+        for n in range(len(self.ifos)):
+            self.ifo_combinations += [''.join(ifos).lower() for ifos in
+                                      combinations(self.ifos, n + 1)]
+
 
 class Node(pegasus_workflow.Node):
     def __init__(self, executable, valid_seg=None):
         super(Node, self).__init__(executable.get_transformation())
         self.executable = executable
         self.executed = False
         self.set_category(executable.name)
@@ -2070,14 +2095,39 @@
             curr_file.add_pfn(pfn_local, 'local')
         # Store the file to avoid later duplication
         tuple_val = (local_file_path, curr_file, curr_pfn)
         file_input_from_config_dict[curr_lfn] = tuple_val
     return curr_file
 
 
+def configparser_value_to_file(cp, sec, opt, attrs=None):
+    """
+    Fetch a file given its url location via the section
+    and option in the workflow configuration parser.
+
+    Parameters
+    -----------
+    cp : ConfigParser object
+         The ConfigParser object holding the workflow configuration settings
+    sec : string
+         The section containing options for this job.
+    opt : string
+         Name of option (e.g. --output-file)
+    attrs : list to specify the 4 attributes of the file.
+
+    Returns
+    --------
+    fileobj_from_path : workflow.File object obtained from the path
+        specified by opt, within sec, in cp.
+    """
+    path = cp.get(sec, opt)
+    fileobj_from_path = resolve_url_to_file(path, attrs=attrs)
+    return fileobj_from_path
+
+
 def get_full_analysis_chunk(science_segs):
     """
     Function to find the first and last time point contained in the science segments
     and return a single segment spanning that full time.
 
     Parameters
     -----------
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/datafind.py` & `PyCBC-2.2.1/pycbc/workflow/datafind.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,16 +30,16 @@
 """
 
 import os, copy
 import logging
 from ligo import segments
 from ligo.lw import utils, table
 from glue import lal
+from gwdatafind import find_urls as find_frame_urls
 from pycbc.workflow.core import SegFile, File, FileList, make_analysis_dir
-from pycbc.frame import datafind_connection
 from pycbc.io.ligolw import LIGOLWContentHandler
 
 
 def setup_datafind_workflow(workflow, scienceSegs, outputDir, seg_file=None,
                             tags=None):
     """
     Setup datafind section of the workflow. This section is responsible for
@@ -392,19 +392,14 @@
     datafindOuts : pycbc.workflow.core.FileList
         List of all the datafind output files for use later in the pipeline.
 
     """
     if tags is None:
         tags = []
 
-    # First job is to do setup for the datafind jobs
-    # First get the server name
-    logging.info("Setting up connection to datafind server.")
-    connection = setup_datafind_server_connection(cp, tags=tags)
-
     # Now ready to loop over the input segments
     datafindouts = []
     datafindcaches = []
     logging.info("Querying datafind server for all science segments.")
     for ifo, scienceSegsIfo in scienceSegs.items():
         observatory = ifo[0].upper()
         frameType = cp.get_opt_tags("workflow-datafind",
@@ -415,22 +410,35 @@
             logging.info(msg)
             # WARNING: For now the workflow will expect times to be in integer seconds
             startTime = int(seg[0])
             endTime = int(seg[1])
 
             # Sometimes the connection can drop, so try a backup here
             try:
-                cache, cache_file = run_datafind_instance(cp, outputDir,
-                                           connection, observatory, frameType,
-                                           startTime, endTime, ifo, tags=tags)
+                cache, cache_file = run_datafind_instance(
+                    cp,
+                    outputDir,
+                    observatory,
+                    frameType,
+                    startTime,
+                    endTime,
+                    ifo,
+                    tags=tags
+                )
             except:
-                connection = setup_datafind_server_connection(cp, tags=tags)
-                cache, cache_file = run_datafind_instance(cp, outputDir,
-                                           connection, observatory, frameType,
-                                           startTime, endTime, ifo, tags=tags)
+                cache, cache_file = run_datafind_instance(
+                    cp,
+                    outputDir,
+                    observatory,
+                    frameType,
+                    startTime,
+                    endTime,
+                    ifo,
+                    tags=tags
+                )
             datafindouts.append(cache_file)
             datafindcaches.append(cache)
     return datafindcaches, datafindouts
 
 def setup_datafind_runtime_cache_single_call_perifo(cp, scienceSegs, outputDir,
                                               tags=None):
     """
@@ -472,19 +480,14 @@
     datafindOuts : pycbc.workflow.core.FileList
         List of all the datafind output files for use later in the pipeline.
 
     """
     if tags is None:
         tags = []
 
-    # First job is to do setup for the datafind jobs
-    # First get the server name
-    logging.info("Setting up connection to datafind server.")
-    connection = setup_datafind_server_connection(cp, tags=tags)
-
     # We want to ignore gaps as the detectors go up and down and calling this
     # way will give gaps. See the setup_datafind_runtime_generated function
     # for datafind calls that only query for data that will exist
     cp.set("datafind","on_gaps","ignore")
 
     # Now ready to loop over the input segments
     datafindouts = []
@@ -526,28 +529,25 @@
             checked_times.append(curr_times)
 
             # Ask datafind where the frames are
             try:
                 cache, cache_file = run_datafind_instance(
                     cp,
                     outputDir,
-                    connection,
                     observatory,
                     ftype,
                     start,
                     end,
                     ifo,
                     tags=tags
                 )
             except:
-                connection = setup_datafind_server_connection(cp, tags=tags)
                 cache, cache_file = run_datafind_instance(
                     cp,
                     outputDir,
-                    connection,
                     observatory,
                     ftype,
                     start,
                     end,
                     ifo,
                     tags=tags
                 )
@@ -741,36 +741,23 @@
                 currFile = File(curr_ifo, frame.description,
                     frame.segment, file_url=frame.url, use_tmp_subdirs=True)
                 datafind_filelist.append(currFile)
                 prev_file = currFile
 
             # Populate the PFNs for the File() we just created
             if frame.url.startswith('file://'):
-                currFile.add_pfn(frame.url, site='local')
-                if frame.url.startswith(
-                    'file:///cvmfs/oasis.opensciencegrid.org/ligo/frames'):
-                    # Datafind returned a URL valid on the osg as well
-                    # so add the additional PFNs to allow OSG access.
-                    currFile.add_pfn(frame.url, site='osg')
-                    currFile.add_pfn(frame.url.replace(
-                        'file:///cvmfs/oasis.opensciencegrid.org/',
-                        'root://xrootd-local.unl.edu/user/'), site='osg')
-                    currFile.add_pfn(frame.url.replace(
-                        'file:///cvmfs/oasis.opensciencegrid.org/',
-                        'gsiftp://red-gridftp.unl.edu/user/'), site='osg')
-                    currFile.add_pfn(frame.url.replace(
-                        'file:///cvmfs/oasis.opensciencegrid.org/',
-                        'gsiftp://ldas-grid.ligo.caltech.edu/hdfs/'), site='osg')
-                elif frame.url.startswith(
-                    'file:///cvmfs/gwosc.osgstorage.org/'):
-                    # Datafind returned a URL valid on the osg as well
-                    # so add the additional PFNs to allow OSG access.
-                    for s in ['osg', 'orangegrid', 'osgconnect']:
-                        currFile.add_pfn(frame.url, site=s)
-                        currFile.add_pfn(frame.url, site="{}-scratch".format(s))
+                if frame.url.startswith('file:///cvmfs/'):
+                    # Frame is on CVMFS, so let all sites read it directly.
+                    currFile.add_pfn(frame.url, site='all')
+                else:
+                    # Frame not on CVMFS, so may need transferring.
+                    # Be careful here! If all your frames files are on site
+                    # = local and you try to run on OSG, it will likely
+                    # overwhelm the condor file transfer process!
+                    currFile.add_pfn(frame.url, site='local')
             else:
                 currFile.add_pfn(frame.url, site='notlocal')
 
     return datafind_filelist
 
 
 def get_science_segs_from_datafind_outs(datafindcaches):
@@ -842,39 +829,14 @@
                 missingFrameSegs[ifo].extend(missingSegs)
                 # NOTE: This .coalesce probably isn't needed as the segments
                 # should be disjoint. If speed becomes an issue maybe remove it?
                 missingFrameSegs[ifo].coalesce()
                 missingFrames[ifo].extend(currMissingFrames)
     return missingFrameSegs, missingFrames
 
-def setup_datafind_server_connection(cp, tags=None):
-    """
-    This function is resposible for setting up the connection with the datafind
-    server.
-
-    Parameters
-    -----------
-    cp : pycbc.workflow.configuration.WorkflowConfigParser
-        The memory representation of the ConfigParser
-    Returns
-    --------
-    connection
-        The open connection to the datafind server.
-    """
-    if tags is None:
-        tags = []
-
-    if cp.has_option_tags("workflow-datafind",
-                          "datafind-ligo-datafind-server", tags):
-        datafind_server = cp.get_opt_tags("workflow-datafind",
-                                        "datafind-ligo-datafind-server", tags)
-    else:
-        datafind_server = None
-
-    return datafind_connection(datafind_server)
 
 def get_segment_summary_times(scienceFile, segmentName):
     """
     This function will find the times for which the segment_summary is set
     for the flag given by segmentName.
 
     Parameters
@@ -924,30 +886,27 @@
         if entry.segment_def_id == segDefID:
             segment = segments.segment(entry.start_time, entry.end_time)
             summSegList.append(segment)
     summSegList.coalesce()
 
     return summSegList
 
-def run_datafind_instance(cp, outputDir, connection, observatory, frameType,
+def run_datafind_instance(cp, outputDir, observatory, frameType,
                           startTime, endTime, ifo, tags=None):
     """
     This function will query the datafind server once to find frames between
     the specified times for the specified frame type and observatory.
 
     Parameters
     ----------
     cp : ConfigParser instance
         Source for any kwargs that should be sent to the datafind module
     outputDir : Output cache files will be written here. We also write the
         commands for reproducing what is done in this function to this
         directory.
-    connection : datafind connection object
-        Initialized through the `gwdatafind` module, this is the open
-        connection to the datafind server.
     observatory : string
         The observatory to query frames for. Ex. 'H', 'L' or 'V'.  NB: not
         'H1', 'L1', 'V1' which denote interferometers.
     frameType : string
         The frame type to query for.
     startTime : int
         Integer start time to query the datafind server for frames.
@@ -973,14 +932,25 @@
     cacheFile : pycbc.workflow.core.File
         Cache file listing all of the datafind output files for use later in the pipeline.
 
     """
     if tags is None:
         tags = []
 
+    # Determine if we should override the default datafind server
+    if cp.has_option_tags("workflow-datafind",
+                          "datafind-ligo-datafind-server", tags):
+        datafind_server = cp.get_opt_tags(
+            "workflow-datafind",
+            "datafind-ligo-datafind-server",
+            tags
+        )
+    else:
+        datafind_server = None
+
     seg = segments.segment([startTime, endTime])
     # Take the datafind kwargs from config (usually urltype=file is
     # given).
     dfKwargs = {}
     # By default ignore missing frames, this case is dealt with outside of here
     dfKwargs['on_gaps'] = 'ignore'
     if cp.has_section("datafind"):
@@ -993,16 +963,22 @@
 
     # It is useful to print the corresponding command to the logs
     # directory to check if this was expected.
     log_datafind_command(observatory, frameType, startTime, endTime,
                          os.path.join(outputDir,'logs'), **dfKwargs)
     logging.debug("Asking datafind server for frames.")
     dfCache = lal.Cache.from_urls(
-        connection.find_frame_urls(observatory, frameType,
-                                   startTime, endTime, **dfKwargs),
+        find_frame_urls(
+            observatory,
+            frameType,
+            startTime,
+            endTime,
+            host=datafind_server,
+            **dfKwargs
+        ),
     )
     logging.debug("Frames returned")
     # workflow format output file
     cache_file = File(ifo, 'DATAFIND', seg, extension='lcf',
                       directory=outputDir, tags=tags)
     cache_file.add_pfn(cache_file.cache_entry.path, site='local')
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/dq.py` & `PyCBC-2.2.1/pycbc/workflow/dq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/inference_followups.py` & `PyCBC-2.2.1/pycbc/workflow/inference_followups.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/injection.py` & `PyCBC-2.2.1/pycbc/workflow/injection.py`

 * *Files 16% similar despite different names*

```diff
@@ -31,17 +31,15 @@
 
 import logging
 import os.path
 import configparser as ConfigParser
 from pycbc.workflow.core import FileList, make_analysis_dir, Node
 from pycbc.workflow.core import Executable, resolve_url_to_file
 from pycbc.workflow.jobsetup import (LalappsInspinjExecutable,
-        LigolwCBCJitterSkylocExecutable, LigolwCBCAlignTotalSpinExecutable,
-        PycbcDarkVsBrightInjectionsExecutable, select_generic_executable,
-        PycbcCreateInjectionsExecutable)
+        PycbcCreateInjectionsExecutable, select_generic_executable)
 
 def veto_injections(workflow, inj_file, veto_file, veto_name, out_dir, tags=None):
     tags = [] if tags is None else tags
     make_analysis_dir(out_dir)
 
     node = Executable(workflow.cp, 'strip_injections', ifos=workflow.ifos,
                           out_dir=out_dir, tags=tags).create_node()
@@ -162,16 +160,15 @@
                       out_dir=out_dir, tags=tags).create_node()
     node.add_input_opt('--injection-file', inj_file)
     node.new_output_file_opt(workflow.analysis_time, '.hdf', '--output-file')
     workflow += node
     return node.output_file
 
 def setup_injection_workflow(workflow, output_dir=None,
-                             inj_section_name='injections', exttrig_file=None,
-                             tags =None):
+                             inj_section_name='injections', tags=None):
     """
     This function is the gateway for setting up injection-generation jobs in a
     workflow. It should be possible for this function to support a number
     of different ways/codes that could be used for doing this, however as this
     will presumably stay as a single call to a single code (which need not be
     inspinj) there are currently no subfunctions in this moudle.
 
@@ -202,15 +199,14 @@
     if tags is None:
         tags = []
     logging.info("Entering injection module.")
     make_analysis_dir(output_dir)
 
     # Get full analysis segment for output file naming
     full_segment = workflow.analysis_time
-    ifos = workflow.ifos
 
     # Identify which injections to do by presence of sub-sections in
     # the configuration file
     inj_tags = []
     inj_files = FileList([])
 
     for section in  workflow.cp.get_subsections(inj_section_name):
@@ -249,69 +245,14 @@
             injection_path = workflow.cp.get_opt_tags(
                 "workflow-injections",
                 "injections-pregenerated-file",
                 curr_tags
             )
             curr_file = resolve_url_to_file(injection_path, attrs=file_attrs)
             inj_files.append(curr_file)
-        elif injection_method in ["IN_COH_PTF_WORKFLOW", "AT_COH_PTF_RUNTIME"]:
-            inj_job = LalappsInspinjExecutable(workflow.cp, inj_section_name,
-                                               out_dir=output_dir, ifos=ifos,
-                                               tags=curr_tags)
-            node = inj_job.create_node(full_segment, exttrig_file)
-            if injection_method == "AT_COH_PTF_RUNTIME":
-                workflow.execute_node(node)
-            else:
-                workflow.add_node(node)
-            inj_file = node.output_files[0]
-
-            if workflow.cp.has_option("workflow-injections",
-                                      "em-bright-only"):
-                em_filter_job = PycbcDarkVsBrightInjectionsExecutable(
-                                                 workflow.cp,
-                                                 'em_bright_filter',
-                                                 tags=curr_tags,
-                                                 out_dir=output_dir,
-                                                 ifos=ifos)
-                node = em_filter_job.create_node(inj_file, full_segment,
-                                                 curr_tags)
-                if injection_method == "AT_COH_PTF_RUNTIME":
-                    workflow.execute_node(node)
-                else:
-                    workflow.add_node(node)
-                inj_file = node.output_files[0]
-
-            if workflow.cp.has_option("workflow-injections",
-                                      "do-jitter-skyloc"):
-                jitter_job = LigolwCBCJitterSkylocExecutable(workflow.cp,
-                                                             'jitter_skyloc',
-                                                             tags=curr_tags,
-                                                             out_dir=output_dir,
-                                                             ifos=ifos)
-                node = jitter_job.create_node(inj_file, full_segment, curr_tags)
-                if injection_method == "AT_COH_PTF_RUNTIME":
-                    workflow.execute_node(node)
-                else:
-                    workflow.add_node(node)
-                inj_file = node.output_files[0]
-
-            if workflow.cp.has_option("workflow-injections",
-                                      "do-align-total-spin"):
-                align_job = LigolwCBCAlignTotalSpinExecutable(workflow.cp,
-                        'align_total_spin', tags=curr_tags, out_dir=output_dir,
-                        ifos=ifos)
-                node = align_job.create_node(inj_file, full_segment, curr_tags)
-
-                if injection_method == "AT_COH_PTF_RUNTIME":
-                    workflow.execute_node(node)
-                else:
-                    workflow.add_node(node)
-                inj_file = node.output_files[0]
-
-            inj_files.append(inj_file)
         else:
             err = "Injection method must be one of IN_WORKFLOW, "
             err += "AT_RUNTIME or PREGENERATED. Got %s." % (injection_method)
             raise ValueError(err)
 
         inj_tags.append(inj_tag)
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/jobsetup.py` & `PyCBC-2.2.1/pycbc/workflow/jobsetup.py`

 * *Files 8% similar despite different names*

```diff
@@ -124,15 +124,14 @@
     """
     exe_path = workflow.cp.get("executables", exe_tag)
     exe_name = os.path.basename(exe_path)
     exe_to_class_map = {
         'ligolw_add'               : LigolwAddExecutable,
         'lalapps_inspinj'          : LalappsInspinjExecutable,
         'pycbc_create_injections'  : PycbcCreateInjectionsExecutable,
-        'pycbc_dark_vs_bright_injections' : PycbcDarkVsBrightInjectionsExecutable,
         'pycbc_condition_strain'         : PycbcConditionStrainExecutable
     }
     try:
         return exe_to_class_map[exe_name]
     except KeyError:
         # Should we try some sort of default class??
         raise NotImplementedError(
@@ -659,14 +658,19 @@
 #        very different between GRB and all-sky/all-time
 class PyCBCMultiInspiralExecutable(Executable):
     """
     The class responsible for setting up jobs for the
     pycbc_multi_inspiral executable.
     """
     current_retention_level = Executable.ALL_TRIGGERS
+
+    # bank-veto-bank-file is a file input option for pycbc_multi_inspiral
+    file_input_options = Executable.file_input_options + \
+        ['--bank-veto-bank-file']
+
     def __init__(self, cp, name, ifo=None, injection_file=None,
                  gate_files=None, out_dir=None, tags=None):
         if tags is None:
             tags = []
         super().__init__(cp, name, ifo, out_dir=out_dir, tags=tags)
         self.injection_file = injection_file
         self.data_seg = segments.segment(int(cp.get('workflow', 'start-time')),
@@ -987,90 +991,14 @@
                                  store_file=self.retain_files)
 
         node.add_opt('--gps-start-time', int_gps_time_to_str(segment[0]))
         node.add_opt('--gps-end-time', int_gps_time_to_str(segment[1]))
         return node
 
 
-class PycbcDarkVsBrightInjectionsExecutable(Executable):
-    """
-    The clase used to create jobs for the pycbc_dark_vs_bright_injections Executable.
-    """
-    current_retention_level = Executable.FINAL_RESULT
-
-    def create_node(self, parent, segment, tags=None):
-        if tags is None:
-            tags = []
-        node = Node(self)
-        if not parent:
-            raise ValueError("Must provide an input file.")
-
-        node = Node(self)
-        # Standard injection file produced by lalapps_inspinj
-        # becomes the input here
-        node.add_input_opt('-i', parent)
-        if self.has_opt('write-compress'):
-            ext = '.xml.gz'
-        else:
-            ext = '.xml'
-        # The output are two files:
-        # 1) the list of potentially EM bright injections
-        tag=['POTENTIALLY_BRIGHT']
-        node.new_output_file_opt(segment, ext, '--output-bright',
-                                 store_file=self.retain_files, tags=tags+tag)
-        # 2) the list of EM dim injections
-        tag=['DIM_ONLY']
-        node.new_output_file_opt(segment,
-                                 ext, '--output-dim',
-                                 store_file=self.retain_files, tags=tags+tag)
-        return node
-
-class LigolwCBCJitterSkylocExecutable(Executable):
-    """
-    The class used to create jobs for the ligolw_cbc_skyloc_jitter executable.
-    """
-    current_retention_level = Executable.MERGED_TRIGGERS
-
-    def create_node(self, parent, segment, tags=None):
-        if tags is None:
-            tags = []
-        if not parent:
-            raise ValueError("Must provide an input file.")
-
-        node = Node(self)
-        node.add_input_opt('--input-file', parent)
-        output_file = File(parent.ifo_list, self.name,
-                           segment, extension='.xml', store_file=True,
-                           directory=self.out_dir, tags=tags)
-        node.add_output_opt('--output-file', output_file)
-
-        return node
-
-
-class LigolwCBCAlignTotalSpinExecutable(Executable):
-    """
-    The class used to create jobs for the ligolw_cbc_skyloc_jitter executable.
-    """
-    current_retention_level = Executable.MERGED_TRIGGERS
-
-    def create_node(self, parent, segment, tags=None):
-        if tags is None:
-            tags = []
-        if not parent:
-            raise ValueError("Must provide an input file.")
-
-        node = Node(self)
-        output_file = File(parent.ifo_list, self.name, segment,
-                           extension='.xml', store_file=self.retain_files,
-                           directory=self.out_dir, tags=tags)
-        node.add_output_opt('--output-file', output_file)
-        node.add_input_arg(parent)
-        return node
-
-
 class PycbcSplitBankExecutable(Executable):
     """ The class responsible for creating jobs for pycbc_hdf5_splitbank. """
 
     extension = '.hdf'
     current_retention_level = Executable.ALL_TRIGGERS
     def __init__(self, cp, exe_name, num_banks,
                  ifo=None, out_dir=None):
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/matched_filter.py` & `PyCBC-2.2.1/pycbc/workflow/matched_filter.py`

 * *Files 2% similar despite different names*

```diff
@@ -99,15 +99,15 @@
         logging.info("Adding matched-filter jobs to workflow.")
         inspiral_outs = setup_matchedfltr_dax_generated_multi(workflow,
                                       science_segs, datafind_outs, tmplt_banks,
                                       output_dir, injection_file=injection_file,
                                       tags=tags)
     else:
         errMsg = "Matched filter method not recognized. Must be one of "
-        errMsg += "WORKFLOW_INDEPENDENT_IFOS (currently only one option)."
+        errMsg += "WORKFLOW_INDEPENDENT_IFOS or WORKFLOW_MULTIPLE_IFOS."
         raise ValueError(errMsg)
 
     logging.info("Leaving matched-filtering setup module.")
     return inspiral_outs
 
 def setup_matchedfltr_dax_generated(workflow, science_segs, datafind_outs,
                                     tmplt_banks, output_dir,
@@ -183,19 +183,17 @@
 def setup_matchedfltr_dax_generated_multi(workflow, science_segs, datafind_outs,
                                           tmplt_banks, output_dir,
                                           injection_file=None,
                                           tags=None):
     '''
     Setup matched-filter jobs that are generated as part of the workflow in
     which a single job reads in and generates triggers over multiple ifos.
-    This
-    module can support any matched-filter code that is similar in principle to
-    pycbc_multi_inspiral or lalapps_coh_PTF_inspiral, but for new codes some
-    additions are needed to define Executable and Job sub-classes
-    (see jobutils.py).
+    This module can support any matched-filter code that is similar in
+    principle to pycbc_multi_inspiral, but for new codes some additions are
+    needed to define Executable and Job sub-classes (see jobutils.py).
 
     Parameters
     -----------
     workflow : pycbc.workflow.core.Workflow
         The Workflow instance that the coincidence jobs will be added to.
     science_segs : ifo-keyed dictionary of ligo.segments.segmentlist instances
         The list of times that are being analysed in this workflow.
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/minifollowups.py` & `PyCBC-2.2.1/pycbc/workflow/minifollowups.py`

 * *Files 0% similar despite different names*

```diff
@@ -398,14 +398,15 @@
         The list of workflow.Files created in this function.
     """
     tags = [] if tags is None else tags
     makedir(out_dir)
     name = 'single_template_plot'
     secs = requirestr(workflow.cp.get_subsections(name), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         for ifo in workflow.ifos:
             if params['%s_end_time' % ifo] == -1.0:
                 continue
             # Reanalyze the time around the trigger in each detector
             curr_exe = SingleTemplateExecutable(workflow.cp, 'single_template',
@@ -481,14 +482,15 @@
     """ Add plot_waveform jobs to the workflow.
     """
     tags = [] if tags is None else tags
     makedir(out_dir)
     name = 'single_template_plot'
     secs = requirestr(workflow.cp.get_subsections(name), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         node = PlotExecutable(workflow.cp, 'plot_waveform', ifos=ifos,
                               out_dir=out_dir, tags=[tag] + tags).create_node()
         node.add_opt('--mass1', "%.6f" % params['mass1'])
         node.add_opt('--mass2', "%.6f" % params['mass2'])
         node.add_opt('--spin1z',"%.6f" % params['spin1z'])
@@ -573,14 +575,15 @@
 def make_trigger_timeseries(workflow, singles, ifo_times, out_dir, special_tids=None,
                             exclude=None, require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     name = 'plot_trigger_timeseries'
     secs = requirestr(workflow.cp.get_subsections(name), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         node = PlotExecutable(workflow.cp, name, ifos=workflow.ifos,
                               out_dir=out_dir, tags=[tag] + tags).create_node()
         node.add_multiifo_input_list_opt('--single-trigger-files', singles)
         node.add_opt('--times', ifo_times)
         node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/pegasus_files/pegasus-properties.conf` & `PyCBC-2.2.1/pycbc/workflow/pegasus_files/pegasus-properties.conf`

 * *Files 3% similar despite different names*

```diff
@@ -13,22 +13,20 @@
 
 # Also tell Pegasus to use the Replica Catalog for file locations
 pegasus.dir.storage.mapper=Replica
 pegasus.dir.storage.mapper.replica=File
 pegasus.dir.storage.mapper.replica.file=output.map
 
 # Add Replica selection options so that it will try URLs first, then 
-# XrootD for OSG, then gridftp, then anything else
+# XrootD for OSG, then anything else
 # FIXME: This feels like a *site* property, not a global
 pegasus.selector.replica=Regex
 pegasus.selector.replica.regex.rank.1=file://(?!.*(cvmfs)).*
 pegasus.selector.replica.regex.rank.2=file:///cvmfs/.*
 pegasus.selector.replica.regex.rank.3=root://.*
-pegasus.selector.replica.regex.rank.4=gsiftp://red-gridftp.unl.edu.*
-pegasus.selector.replica.regex.rank.5=gridftp://.*
 pegasus.selector.replica.regex.rank.6=.\*
 
 dagman.maxpre=1
 # Override default value of 1800s
 condor.periodic_remove=(JobStatus == 5) && ((CurrentTime - EnteredCurrentStatus) > 43200)
 
 # Use --cache file as a supplement to the in-dax replica catalog
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/pegasus_sites.py` & `PyCBC-2.2.1/pycbc/workflow/pegasus_sites.py`

 * *Files 1% similar despite different names*

```diff
@@ -198,37 +198,37 @@
     site.add_profiles(Namespace.PEGASUS, key="data.configuration",
                       value="condorio")
     site.add_profiles(Namespace.PEGASUS, key='transfer.bypass.input.staging',
                       value="true")
     site.add_profiles(Namespace.CONDOR, key="should_transfer_files",
                       value="Yes")
     site.add_profiles(Namespace.CONDOR, key="when_to_transfer_output",
-                      value="ON_EXIT_OR_EVICT")
+                      value="ON_SUCCESS")
+    site.add_profiles(Namespace.CONDOR, key="success_exit_code",
+                      value="0")
     site.add_profiles(Namespace.CONDOR, key="+OpenScienceGrid",
                       value="True")
     site.add_profiles(Namespace.CONDOR, key="getenv",
                       value="False")
     site.add_profiles(Namespace.CONDOR, key="+InitializeModulesEnv",
                       value="False")
     site.add_profiles(Namespace.CONDOR, key="+SingularityCleanEnv",
                       value="True")
-    site.add_profiles(Namespace.CONDOR, key="use_x509userproxy",
-                      value="True")
     site.add_profiles(Namespace.CONDOR, key="Requirements",
                       value="(HAS_SINGULARITY =?= TRUE) && "
                             "(HAS_LIGO_FRAMES =?= True) && "
                             "(IS_GLIDEIN =?= True)")
     cvmfs_loc = '"/cvmfs/singularity.opensciencegrid.org/pycbc/pycbc-el8:v'
     cvmfs_loc += last_release + '"'
     site.add_profiles(Namespace.CONDOR, key="+SingularityImage",
                       value=cvmfs_loc)
     # On OSG failure rate is high
     site.add_profiles(Namespace.DAGMAN, key="retry", value="4")
     site.add_profiles(Namespace.ENV, key="LAL_DATA_PATH",
-                      value="/cvmfs/oasis.opensciencegrid.org/ligo/sw/pycbc/lalsuite-extra/current/share/lalsimulation")
+                      value="/cvmfs/software.igwn.org/pycbc/lalsuite-extra/current/share/lalsimulation")
     # Add MKL location to LD_LIBRARY_PATH for OSG
     site.add_profiles(Namespace.ENV, key="LD_LIBRARY_PATH",
                       value="/usr/local/lib:/.singularity.d/libs")
     sitecat.add_sites(site)
 
 
 def add_site(sitecat, sitename, cp, out_dir=None):
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/pegasus_workflow.py` & `PyCBC-2.2.1/pycbc/workflow/pegasus_workflow.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,52 +25,23 @@
 #
 """ This module provides thin wrappers around Pegasus.DAX3 functionality that
 provides additional abstraction and argument handling.
 """
 import os
 import shutil
 import tempfile
+import subprocess
+from packaging import version
 from urllib.request import pathname2url
 from urllib.parse import urljoin, urlsplit
 import Pegasus.api as dax
 
 PEGASUS_FILE_DIRECTORY = os.path.join(os.path.dirname(__file__),
                                       'pegasus_files')
 
-GRID_START_TEMPLATE = '''#!/bin/bash
-
-if [ -f /tmp/x509up_u`id -u` ] ; then
-  unset X509_USER_PROXY
-else
-  if [ ! -z ${X509_USER_PROXY} ] ; then
-    if [ -f ${X509_USER_PROXY} ] ; then
-      cp -a ${X509_USER_PROXY} /tmp/x509up_u`id -u`
-    fi
-  fi
-  unset X509_USER_PROXY
-fi
-
-# Check that the proxy is valid
-grid-proxy-info -exists
-RESULT=${?}
-if [ ${RESULT} -eq 0 ] ; then
-  PROXY_TYPE=`grid-proxy-info -type | tr -d ' '`
-  if [ x${PROXY_TYPE} == 'xRFC3820compliantimpersonationproxy' ] ; then
-    grid-proxy-info
-  else
-    cp /tmp/x509up_u`id -u` /tmp/x509up_u`id -u`.orig
-    grid-proxy-init -cert /tmp/x509up_u`id -u`.orig -key /tmp/x509up_u`id -u`.orig
-    rm -f /tmp/x509up_u`id -u`.orig
-    grid-proxy-info
-  fi
-else
-  echo "Error: Could not find a valid grid proxy to submit workflow."
-  exit 1
-fi
-'''
 
 class ProfileShortcuts(object):
     """ Container of common methods for setting pegasus profile information
     on Executables and nodes. This class expects to be inherited from
     and for a add_profile method to be implemented.
     """
     def set_memory(self, size):
@@ -725,17 +696,14 @@
             fp.write('pegasus-analyzer -r ')
             fp.write('-v {}/work $@'.format(submitdir))
 
         with open('stop', 'w') as fp:
             fp.write('pegasus-remove {}/work $@'.format(submitdir))
 
         with open('start', 'w') as fp:
-            if self.cp.has_option('pegasus_profile', 'pycbc|check_grid'):
-                fp.write(GRID_START_TEMPLATE)
-                fp.write('\n')
             fp.write('pegasus-run {}/work $@'.format(submitdir))
 
         os.chmod('status', 0o755)
         os.chmod('debug', 0o755)
         os.chmod('stop', 0o755)
         os.chmod('start', 0o755)
 
@@ -784,14 +752,23 @@
 
     def set_subworkflow_properties(self, output_map_file,
                                    staging_site,
                                    cache_file):
 
         self.add_planner_arg('pegasus.dir.storage.mapper.replica.file',
                              os.path.basename(output_map_file.name))
+        # Ensure output_map_file has the for_planning flag set. There's no
+        # API way to set this after the File is initialized, so we have to
+        # change the attribute here.
+        # WORSE, we only want to set this if the pegasus *planner* is version
+        # 5.0.4 or larger
+        sproc_out = subprocess.check_output(['pegasus-version']).strip()
+        sproc_out = sproc_out.decode()
+        if version.parse(sproc_out) >= version.parse('5.0.4'):
+            output_map_file.for_planning=True
         self.add_inputs(output_map_file)
 
         # I think this is needed to deal with cases where the subworkflow file
         # does not exist at submission time.
         bname = os.path.splitext(os.path.basename(self.file))[0]
         self.add_planner_arg('basename',  bname)
         self.add_planner_arg('output_sites', ['local'])
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/plotting.py` & `PyCBC-2.2.1/pycbc/workflow/plotting.py`

 * *Files 4% similar despite different names*

```diff
@@ -74,14 +74,15 @@
 
 def make_range_plot(workflow, psd_files, out_dir, exclude=None, require=None,
                    tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_range'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         node = PlotExecutable(workflow.cp, 'plot_range', ifos=workflow.ifos,
                               out_dir=out_dir, tags=[tag] + tags).create_node()
         node.add_input_list_opt('--psd-files', psd_files)
         node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')
         workflow += node
@@ -161,14 +162,15 @@
 
 def make_sensitivity_plot(workflow, inj_file, out_dir, exclude=None,
                          require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_sensitivity'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         node = PlotExecutable(workflow.cp, 'plot_sensitivity', ifos=workflow.ifos,
                     out_dir=out_dir, tags=[tag] + tags).create_node()
         node.add_input_opt('--injection-file', inj_file)
         node.new_output_file_opt(inj_file.segment, '.png', '--output-file')
         workflow += node
@@ -177,14 +179,15 @@
 
 def make_coinc_snrchi_plot(workflow, inj_file, inj_trig, stat_file, trig_file,
                           out_dir, exclude=None, require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_coinc_snrchi'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         exe = PlotExecutable(workflow.cp, 'plot_coinc_snrchi',
                              ifos=inj_trig.ifo_list,
                              out_dir=out_dir, tags=[tag] + tags)
         node = exe.create_node()
         node.add_input_opt('--found-injection-file', inj_file)
@@ -311,14 +314,15 @@
 
 def make_snrchi_plot(workflow, trig_files, veto_file, veto_name,
                      out_dir, exclude=None, require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_snrchi'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         for trig_file in trig_files:
             exe = PlotExecutable(workflow.cp, 'plot_snrchi',
                                  ifos=trig_file.ifo_list,
                                  out_dir=out_dir,
                                  tags=[tag] + tags)
@@ -337,14 +341,15 @@
 def make_foundmissed_plot(workflow, inj_file, out_dir, exclude=None,
                          require=None, tags=None):
     if tags is None:
         tags = []
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_foundmissed'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         exe = PlotExecutable(workflow.cp, 'plot_foundmissed', ifos=workflow.ifos,
                     out_dir=out_dir, tags=[tag] + tags)
         node = exe.create_node()
         ext = '.html' if exe.has_opt('dynamic') else '.png'
         node.add_input_opt('--injection-file', inj_file)
@@ -427,14 +432,15 @@
 def make_single_hist(workflow, trig_file, veto_file, veto_name,
                      out_dir, bank_file=None, exclude=None,
                      require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_hist'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         node = PlotExecutable(workflow.cp, 'plot_hist',
                     ifos=trig_file.ifo,
                     out_dir=out_dir,
                     tags=[tag] + tags).create_node()
         if veto_file is not None:
@@ -451,14 +457,15 @@
 def make_binned_hist(workflow, trig_file, veto_file, veto_name,
                      out_dir, bank_file, exclude=None,
                      require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_binnedhist'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         node = PlotExecutable(workflow.cp, 'plot_binnedhist',
                     ifos=trig_file.ifo,
                     out_dir=out_dir,
                     tags=[tag] + tags).create_node()
         node.add_opt('--ifo', trig_file.ifo)
@@ -474,14 +481,15 @@
 
 def make_singles_plot(workflow, trig_files, bank_file, veto_file, veto_name,
                      out_dir, exclude=None, require=None, tags=None):
     tags = [] if tags is None else tags
     makedir(out_dir)
     secs = requirestr(workflow.cp.get_subsections('plot_singles'), require)
     secs = excludestr(secs, exclude)
+    secs = excludestr(secs, workflow.ifo_combinations)
     files = FileList([])
     for tag in secs:
         for trig_file in trig_files:
             node = PlotExecutable(workflow.cp, 'plot_singles',
                         ifos=trig_file.ifo,
                         out_dir=out_dir,
                         tags=[tag] + tags).create_node()
```

### Comparing `PyCBC-2.2.0/pycbc/workflow/psd.py` & `PyCBC-2.2.1/pycbc/workflow/psd.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/psdfiles.py` & `PyCBC-2.2.1/pycbc/workflow/psdfiles.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/segment.py` & `PyCBC-2.2.1/pycbc/workflow/segment.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/splittable.py` & `PyCBC-2.2.1/pycbc/workflow/splittable.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pycbc/workflow/tmpltbank.py` & `PyCBC-2.2.1/pycbc/workflow/tmpltbank.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/pyproject.toml` & `PyCBC-2.2.1/pyproject.toml`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/setup.py` & `PyCBC-2.2.1/setup.py`

 * *Files 16% similar despite different names*

```diff
@@ -15,51 +15,49 @@
 # with this program; if not, write to the Free Software Foundation, Inc.,
 # 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 
 """
 setup.py file for PyCBC package
 """
 
-from __future__ import print_function
-
 import sys
 import os, subprocess, shutil
 import platform
 
-from distutils.errors import DistutilsError
 from distutils.command.clean import clean as _clean
 
-from setuptools.command.install import install as _install
 from setuptools import Extension, setup, Command
 from setuptools.command.build_ext import build_ext as _build_ext
 from setuptools import find_packages
 
+
 requires = []
 setup_requires = ['numpy>=1.16.0']
-install_requires =  setup_requires + [
-                      'cython>=0.29',
-                      'numpy>=1.16.0,!=1.19.0',
-                      'scipy>=0.16.0',
-                      'astropy>=2.0.3,!=4.2.1,!=4.0.5',
-                      'matplotlib>=1.5.1',
-                      'mpld3>=0.3',
-                      'pillow',
-                      'h5py>=3.0.0,!=3.7.0',
-                      'jinja2',
-                      'Mako>=1.0.1',
-                      'beautifulsoup4>=4.6.0',
-                      'tqdm',
-                      'setuptools',
-                      'gwdatafind',
-                      'pegasus-wms.api >= 5.0.3',
-                      'python-ligo-lw >= 1.7.0',
-                      'ligo-segments',
-                      'lalsuite!=7.2',
-                      'lscsoft-glue>=1.59.3',
-                      ]
+install_requires = setup_requires + [
+    'cython>=0.29',
+    'numpy>=1.16.0,!=1.19.0',
+    'scipy>=0.16.0',
+    'astropy>=2.0.3,!=4.2.1,!=4.0.5',
+    'matplotlib>=1.5.1',
+    'mpld3>=0.3',
+    'pillow',
+    'h5py>=3.0.0,!=3.7.0',
+    'jinja2',
+    'Mako>=1.0.1',
+    'beautifulsoup4>=4.6.0',
+    'tqdm',
+    'setuptools',
+    'gwdatafind',
+    'pegasus-wms.api >= 5.0.3',
+    'python-ligo-lw >= 1.7.0',
+    'ligo-segments',
+    'lalsuite!=7.2',
+    'lscsoft-glue>=1.59.3',
+    'pykerr',
+]
 
 def find_files(dirname, relpath=None):
     def find_paths(dirname):
         items = []
         for fname in os.listdir(dirname):
             path = os.path.join(dirname, fname)
             if os.path.isdir(path):
@@ -95,111 +93,113 @@
         self.clean_files = []
         self.clean_folders = ['docs/_build']
     def run(self):
         _clean.run(self)
         for f in self.clean_files:
             try:
                 os.unlink(f)
-                print('removed {0}'.format(f))
+                print('removed ' + f)
             except:
                 pass
 
         for fol in self.clean_folders:
             shutil.rmtree(fol, ignore_errors=True)
-            print('removed {0}'.format(fol))
+            print('removed ' + fol)
 
-# write versioning info
 def get_version_info():
-    """Get VCS info and write version info to version.py
+    """Get VCS info and write version info to version.py.
     """
     from pycbc import _version_helper
 
     class vdummy(object):
         def __getattr__(self, attr):
             return ''
 
     # If this is a pycbc git repo always populate version information using GIT
     try:
         vinfo = _version_helper.generate_git_version_info()
     except:
         vinfo = vdummy()
-        vinfo.version = '2.2.0'
+        vinfo.version = '2.2.1'
         vinfo.release = 'True'
 
+    version_script = f"""# coding: utf-8
+# Generated by setup.py for PyCBC on {vinfo.build_date}.
+
+# general info
+version = '{vinfo.version}'
+date = '{vinfo.date}'
+release = '{vinfo.release}'
+last_release = '{vinfo.last_release}'
+
+# git info
+git_hash = '{vinfo.hash}'
+git_branch = '{vinfo.branch}'
+git_tag = '{vinfo.tag}'
+git_author = '{vinfo.author}'
+git_committer = '{vinfo.committer}'
+git_status = '{vinfo.status}'
+git_builder = '{vinfo.builder}'
+git_build_date = '{vinfo.build_date}'
+git_verbose_msg = \"\"\"Version: {vinfo.version}
+Branch: {vinfo.branch}
+Tag: {vinfo.tag}
+Id: {vinfo.hash}
+Builder: {vinfo.builder}
+Build date: {vinfo.build_date}
+Repository status is {vinfo.status}\"\"\"
+
+from pycbc._version import *
+"""
     with open('pycbc/version.py', 'wb') as f:
-        f.write("# coding: utf-8\n".encode('utf-8'))
-        f.write(("# Generated by setup.py for PyCBC on %s.\n\n"
-                % vinfo.build_date).encode('utf-8'))
-
-        # print general info
-        f.write(('version = \'%s\'\n' % vinfo.version).encode('utf-8'))
-        f.write(('date = \'%s\'\n' % vinfo.date).encode('utf-8'))
-        f.write(('release = %s\n' % vinfo.release).encode('utf-8'))
-        f.write(('last_release = \'%s\'\n' % vinfo.last_release).encode('utf-8'))
-
-        # print git info
-        f.write(('\ngit_hash = \'%s\'\n' % vinfo.hash).encode('utf-8'))
-        f.write(('git_branch = \'%s\'\n' % vinfo.branch).encode('utf-8'))
-        f.write(('git_tag = \'%s\'\n' % vinfo.tag).encode('utf-8'))
-        f.write(('git_author = \'%s\'\n' % vinfo.author).encode('utf-8'))
-        f.write(('git_committer = \'%s\'\n' % vinfo.committer).encode('utf-8'))
-        f.write(('git_status = \'%s\'\n' % vinfo.status).encode('utf-8'))
-        f.write(('git_builder = \'%s\'\n' % vinfo.builder).encode('utf-8'))
-        f.write(('git_build_date = \'%s\'\n' % vinfo.build_date).encode('utf-8'))
-        f.write(('git_verbose_msg = """Version: %s\n'
-                 'Branch: %s\n'
-                 'Tag: %s\n'
-                 'Id: %s\n'
-                 'Builder: %s\n'
-                 'Build date: %s\n'
-                 'Repository status is %s"""\n' %(
-                                                vinfo.version,
-                                                vinfo.branch,
-                                                vinfo.tag,
-                                                vinfo.hash,
-                                                vinfo.builder,
-                                                vinfo.build_date,
-                                                vinfo.status)).encode('utf-8'))
-        f.write('from pycbc._version import *\n'.encode('utf-8'))
-        version = vinfo.version
+        f.write(version_script.encode('utf-8'))
 
     from pycbc import version
-    version = version.version
-    return version
+    return version.version
 
 class build_docs(Command):
     user_options = []
     description = "Build the documentation pages"
     def initialize_options(self):
         pass
     def finalize_options(self):
         pass
     def run(self):
-        subprocess.check_call("cd docs; cp Makefile.std Makefile; sphinx-apidoc "
-                              " -o ./ -f -A 'PyCBC dev team' -V '0.1' ../pycbc && make html",
-                            stderr=subprocess.STDOUT, shell=True)
+        cmd = (
+            "cd docs; "
+            "cp Makefile.std Makefile; "
+            "sphinx-apidoc -o ./ -f -A 'PyCBC dev team' -V '0.1' ../pycbc "
+            "&& make html"
+        )
+        subprocess.check_call(cmd, stderr=subprocess.STDOUT, shell=True)
 
 class build_gh_pages(Command):
     user_options = []
     description = "Build the documentation pages for GitHub"
     def initialize_options(self):
         pass
     def finalize_options(self):
         pass
     def run(self):
-        subprocess.check_call("mkdir -p _gh-pages/latest && touch _gh-pages/.nojekyll && "
-                              "cd docs; cp Makefile.gh_pages Makefile; sphinx-apidoc "
-                              " -o ./ -f -A 'PyCBC dev team' -V '0.1' ../pycbc && make html",
-                            stderr=subprocess.STDOUT, shell=True)
-
-cmdclass = { 'build_docs' : build_docs,
-             'build_gh_pages' : build_gh_pages,
-             'clean' : clean,
-             'build_ext':cbuild_ext
-            }
+        cmd = (
+            "mkdir -p _gh-pages/latest "
+            "&& touch _gh-pages/.nojekyll "
+            "&& cd docs; "
+            "cp Makefile.gh_pages Makefile; "
+            "sphinx-apidoc -o ./ -f -A 'PyCBC dev team' -V '0.1' ../pycbc "
+            "&& make html"
+        )
+        subprocess.check_call(cmd, stderr=subprocess.STDOUT, shell=True)
+
+cmdclass = {
+    'build_docs': build_docs,
+    'build_gh_pages': build_gh_pages,
+    'clean': clean,
+    'build_ext': cbuild_ext
+}
 
 extras_require = {
     'cuda': [
         'pycuda>=2015.1',
         'scikit-cuda',
     ],
     'igwn': [
@@ -281,34 +281,42 @@
               language='c++',
               extra_compile_args=cython_compile_args,
               extra_link_args=cython_link_args,
               compiler_directives={'embedsignature': True})
 ext.append(e)
 
 
-setup (
+setup(
     name = 'PyCBC',
     version = VERSION,
     description = 'Core library to analyze gravitational-wave data, find signals, and study their parameters.',
     long_description = open('README.md').read(),
     long_description_content_type='text/markdown',
     author = 'The PyCBC team',
     author_email = 'alex.nitz@gmail.org',
     url = 'http://www.pycbc.org/',
-    download_url = 'https://github.com/gwastro/pycbc/tarball/v%s' % VERSION,
-    keywords = ['ligo', 'physics', 'gravity', 'signal processing', 'gravitational waves'],
+    download_url = f'https://github.com/gwastro/pycbc/tarball/v{VERSION}',
+    keywords = [
+        'ligo',
+        'physics',
+        'gravity',
+        'signal processing',
+        'gravitational waves'
+    ],
     cmdclass = cmdclass,
     setup_requires = setup_requires,
     extras_require = extras_require,
     install_requires = install_requires,
     scripts  = find_files('bin', relpath='./'),
     packages = find_packages(),
-    package_data = {'pycbc.workflow': find_files('pycbc/workflow'),
-                    'pycbc.results': find_files('pycbc/results'),
-                    'pycbc.neutron_stars': find_files('pycbc/neutron_stars')},
+    package_data = {
+        'pycbc.workflow': find_files('pycbc/workflow'),
+        'pycbc.results': find_files('pycbc/results'),
+        'pycbc.neutron_stars': find_files('pycbc/neutron_stars')
+    },
     ext_modules = ext,
     python_requires='>=3.7',
     classifiers=[
         'Programming Language :: Python',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
         'Programming Language :: Python :: 3.9',
```

### Comparing `PyCBC-2.2.0/test/bankvetotest.py` & `PyCBC-2.2.1/test/bankvetotest.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/fft_base.py` & `PyCBC-2.2.1/test/fft_base.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/lalsim.py` & `PyCBC-2.2.1/test/lalsim.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_array.py` & `PyCBC-2.2.1/test/test_array.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_array_lal.py` & `PyCBC-2.2.1/test/test_array_lal.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_autochisq.py` & `PyCBC-2.2.1/test/test_autochisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_calibration.py` & `PyCBC-2.2.1/test/test_calibration.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_chisq.py` & `PyCBC-2.2.1/test/test_chisq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_coinc_stat.py` & `PyCBC-2.2.1/test/test_coinc_stat.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_conversions.py` & `PyCBC-2.2.1/test/test_conversions.py`

 * *Files 1% similar despite different names*

```diff
@@ -122,16 +122,18 @@
             (conversions.mass2_from_mchirp_eta, (self.mchirp, self.eta), 'ms'),
             (conversions.mass2_from_mchirp_mass1, (self.mchirp, self.mp), 'ms'),
             (conversions.mass2_from_mass1_eta, (self.mp, self.eta), 'ms'),
             (conversions.mass1_from_mass2_eta, (self.ms, self.eta), 'mp'),
             (conversions.eta_from_q, (self.q,), 'eta'),
             (conversions.mass1_from_mchirp_q, (self.mchirp, self.q), 'mp'),
             (conversions.mass2_from_mchirp_q, (self.mchirp, self.q), 'ms'),
+            (conversions.tau0_from_mchirp, (self.mchirp, self.f_lower), 'tau0'),
             (conversions.tau0_from_mass1_mass2, (self.m1, self.m2, self.f_lower), 'tau0'),
             (conversions.tau3_from_mass1_mass2, (self.m1, self.m2, self.f_lower), 'tau3'),
+            (conversions.mchirp_from_tau0, (self.tau0, self.f_lower), 'mchirp'),
             (conversions.mtotal_from_tau0_tau3, (self.tau0, self.tau3, self.f_lower), 'mtotal'),
             (conversions.eta_from_tau0_tau3, (self.tau0, self.tau3, self.f_lower), 'eta'),
             (conversions.mass1_from_tau0_tau3, (self.tau0, self.tau3, self.f_lower), 'mp'),
             (conversions.mass2_from_tau0_tau3, (self.tau0, self.tau3, self.f_lower), 'ms'),
             (conversions.chi_eff_from_spherical,
                 (self.m1, self.m2, self.spin1_amp, self.spin1_polar,
                  self.spin2_amp, self.spin2_polar), 'effective_spin'),
```

### Comparing `PyCBC-2.2.0/test/test_correlate.py` & `PyCBC-2.2.1/test/test_correlate.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_cuts.py` & `PyCBC-2.2.1/test/test_cuts.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_detector.py` & `PyCBC-2.2.1/test/test_detector.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,31 +18,31 @@
 # =============================================================================
 #
 #                                   Preamble
 #
 # =============================================================================
 #
 """
-These are the unittests for the pycbc.waveform module
+These are the unittests for the pycbc.detector module
 """
-from __future__ import print_function
+
 import pycbc.detector as det
 import unittest, numpy
 from numpy.random import uniform, seed
 seed(0)
 
 # We require lal as a reference comparison
 import lal
 
 from utils import simple_exit
 
 class TestDetector(unittest.TestCase):
     def setUp(self):
         self.d = [det.Detector(ifo)
-                  for ifo, name in det.get_available_detectors()]
+                  for ifo in det.get_available_detectors()]
 
         # not distributed sanely, but should provide some good coverage
         N = 1000
         self.ra = uniform(0, numpy.pi * 2, size=N)
         self.dec = uniform(-numpy.pi, numpy.pi, size=N)
         self.pol = uniform(0, numpy.pi * 2, size=N)
         self.time = uniform(1126000000.0, 1336096017.0, size=N)
```

### Comparing `PyCBC-2.2.0/test/test_distributions.py` & `PyCBC-2.2.1/test/test_distributions.py`

 * *Files 0% similar despite different names*

```diff
@@ -28,14 +28,15 @@
 from utils import simple_exit
 from pycbc.workflow import WorkflowConfigParser
 
 # distributions to exclude from one-dimensional distribution unit tests
 # some of these distributons have their own specific unit test
 EXCLUDE_DIST_NAMES = ["fromfile", "arbitrary",
                       "external", "external_func_fromfile",
+                      "fisher_sky",
                       "uniform_solidangle", "uniform_sky",
                       "independent_chip_chieff",
                       "uniform_f0_tau", "fixed_samples"]
 
 # tests only need to happen on the CPU
 parse_args_cpu_only("Distributions")
```

### Comparing `PyCBC-2.2.0/test/test_dq.py` & `PyCBC-2.2.1/test/test_dq.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_fft_mkl_threaded.py` & `PyCBC-2.2.1/test/test_fft_mkl_threaded.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_fft_unthreaded.py` & `PyCBC-2.2.1/test/test_fft_unthreaded.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_fftw_openmp.py` & `PyCBC-2.2.1/test/test_fftw_openmp.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_fftw_pthreads.py` & `PyCBC-2.2.1/test/test_fftw_pthreads.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_frame.py` & `PyCBC-2.2.1/test/test_frame.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_frequencyseries.py` & `PyCBC-2.2.1/test/test_frequencyseries.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_infmodel.py` & `PyCBC-2.2.1/test/test_infmodel.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_injection.py` & `PyCBC-2.2.1/test/test_injection.py`

 * *Files 5% similar despite different names*

```diff
@@ -81,15 +81,14 @@
         row.numrel_mode_max = 0
         row.numrel_data = None
         row.source = 'ANTANI'
 
 class TestInjection(unittest.TestCase):
     def setUp(self):
         available_detectors = get_available_detectors()
-        available_detectors = [a[0] for a in available_detectors]
         self.assertTrue('H1' in available_detectors)
         self.assertTrue('L1' in available_detectors)
         self.assertTrue('V1' in available_detectors)
         self.detectors = [Detector(d) for d in ['H1', 'L1', 'V1']]
         self.sample_rate = 4096.
         self.earth_time = lal.REARTH_SI / lal.C_SI
```

### Comparing `PyCBC-2.2.0/test/test_io_live.py` & `PyCBC-2.2.1/test/test_io_live.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_live_coinc_compare.py` & `PyCBC-2.2.1/test/test_live_coinc_compare.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_matchedfilter.py` & `PyCBC-2.2.1/test/test_matchedfilter.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_noise.py` & `PyCBC-2.2.1/test/test_noise.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_pnutils.py` & `PyCBC-2.2.1/test/test_pnutils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_psd.py` & `PyCBC-2.2.1/test/test_psd.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 #                                   Preamble
 #
 # =============================================================================
 #
 '''
 These are the unittests for the pycbc PSD module.
 '''
-from __future__ import division
+
 import os
 import tempfile
 import pycbc
 import pycbc.psd
 from pycbc.types import TimeSeries, FrequencySeries
 from pycbc.fft import ifft
 from pycbc.fft.fftw import set_measure_level
@@ -47,17 +47,18 @@
         self.psd_delta_f = 0.1
         self.psd_low_freq_cutoff = 10.
         # generate 1/f noise for testing PSD estimation
         noise_size = 524288
         sample_freq = 4096.
         delta_f = sample_freq / noise_size
         numpy.random.seed(132435)
-        noise = numpy.random.normal(loc=0, scale=1, size=noise_size//2+1) + \
-            1j * numpy.random.normal(loc=0, scale=1, size=noise_size//2+1)
-        noise_model = 1. / numpy.linspace(1., 100., noise_size // 2 + 1)
+        fd_size = noise_size // 2 + 1
+        noise = numpy.random.normal(loc=0, scale=1, size=fd_size) + \
+            1j * numpy.random.normal(loc=0, scale=1, size=fd_size)
+        noise_model = 1. / numpy.linspace(1., 100., fd_size)
         noise *= noise_model / numpy.sqrt(delta_f) / 2
         noise[0] = noise[0].real
         noise_fs = FrequencySeries(noise, delta_f=delta_f)
         self.noise = TimeSeries(numpy.zeros(noise_size), delta_t=1./sample_freq)
         ifft(noise_fs, self.noise)
 
     def test_analytical(self):
```

### Comparing `PyCBC-2.2.0/test/test_resample.py` & `PyCBC-2.2.1/test/test_resample.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_schemes.py` & `PyCBC-2.2.1/test/test_schemes.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_significance_module.py` & `PyCBC-2.2.1/test/test_significance_module.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_skymax.py` & `PyCBC-2.2.1/test/test_skymax.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_spatmplt.py` & `PyCBC-2.2.1/test/test_spatmplt.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_threshold.py` & `PyCBC-2.2.1/test/test_threshold.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_timeseries.py` & `PyCBC-2.2.1/test/test_timeseries.py`

 * *Files 3% similar despite different names*

```diff
@@ -473,14 +473,41 @@
             self.a*=1
             self.b*=1
             self.bad3*=1
             self.assertAlmostEqual(self.a.duration, 0.3)
             self.assertAlmostEqual(self.b.duration, 0.3)
             self.assertAlmostEqual(self.bad3.duration, 0.6)
 
+    def test_at_time(self):
+        a = TimeSeries([0, 1, 2, 3, 4, 5, 6, 7], delta_t=1.0)
+
+        self.assertAlmostEqual(a.at_time(0.5), 0.0)
+        self.assertAlmostEqual(a.at_time(0.6,  nearest_sample=True), 1.0)
+        self.assertAlmostEqual(a.at_time(0.5, interpolate='linear'), 0.5)
+        self.assertAlmostEqual(a.at_time([2.5],
+                               interpolate='quadratic'), 2.5)
+
+        i = numpy.array([-0.2, 0.5, 1.5, 7.0])
+
+        x = a.at_time(i, extrapolate=0)
+        n = numpy.array([0, 0.0, 1.0, 7.0])
+        self.assertAlmostEqual((x-n).sum(), 0)
+
+        x = a.at_time(i, extrapolate=0, nearest_sample=True)
+        n = numpy.array([0, 1.0, 2.0, 7.0])
+        self.assertAlmostEqual((x-n).sum(), 0)
+
+        x = a.at_time(i, extrapolate=0, interpolate='linear')
+        n = numpy.array([0, 0.5, 1.5, 0.0])
+        self.assertAlmostEqual((x-n).sum(), 0)
+
+        x = a.at_time(i, extrapolate=0, interpolate='quadratic')
+        n = numpy.array([0, 0.0, 1.5, 0.0])
+        self.assertAlmostEqual((x-n).sum(), 0)
+
     def test_inject(self):
         a = TimeSeries(numpy.zeros(2**20, dtype=numpy.float32),
                                    delta_t=1.0)
         a[2**19] = 1
 
         # Check that the obvious case reduces to an add operation
         r = a.inject(a)
```

### Comparing `PyCBC-2.2.0/test/test_tmpltbank.py` & `PyCBC-2.2.1/test/test_tmpltbank.py`

 * *Files 0% similar despite different names*

```diff
@@ -21,16 +21,14 @@
 #
 # =============================================================================
 #
 """
 These are the unittests for the pycbc.tmpltbank module
 """
 
-from __future__ import division
-import os
 import math
 import numpy
 from astropy.utils.data import download_file
 import pycbc.tmpltbank
 # Old LigoLW output functions are not imported at tmpltbank level
 import pycbc.tmpltbank.bank_output_utils as llw_output
 import pycbc.psd
```

### Comparing `PyCBC-2.2.0/test/test_transforms.py` & `PyCBC-2.2.1/test/test_transforms.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_waveform.py` & `PyCBC-2.2.1/test/test_waveform.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/test_waveform_utils.py` & `PyCBC-2.2.1/test/test_waveform_utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/test/utils.py` & `PyCBC-2.2.1/test/utils.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/benchmarking/absolute_times.py` & `PyCBC-2.2.1/tools/benchmarking/absolute_times.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/einsteinathome/check_GW150914_detection.py` & `PyCBC-2.2.1/tools/einsteinathome/check_GW150914_detection.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/static/hooks/hook-pycbc.py` & `PyCBC-2.2.1/tools/static/hooks/hook-pycbc.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/static/runtime-scipy.py` & `PyCBC-2.2.1/tools/static/runtime-scipy.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/timing/arr_perf.py` & `PyCBC-2.2.1/tools/timing/arr_perf.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/timing/banksim/banksim.py` & `PyCBC-2.2.1/tools/timing/banksim/banksim.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/timing/correlate_perf.py` & `PyCBC-2.2.1/tools/timing/correlate_perf.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/timing/fft_perf.py` & `PyCBC-2.2.1/tools/timing/fft_perf.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/timing/match_perf.py` & `PyCBC-2.2.1/tools/timing/match_perf.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tools/timing/wav_perf.py` & `PyCBC-2.2.1/tools/timing/wav_perf.py`

 * *Files identical despite different names*

### Comparing `PyCBC-2.2.0/tox.ini` & `PyCBC-2.2.1/tox.ini`

 * *Files 8% similar despite different names*

```diff
@@ -53,14 +53,16 @@
 deps =
     {[base]deps}
     ; Needed for `BBHx` package to work with PyCBC
     git+https://github.com/mikekatz04/BBHx.git; sys_platform == 'linux'
     git+https://github.com/ConWea/BBHX-waveform-model.git; sys_platform == 'linux'
 conda_deps=
     mysqlclient
-    gcc_linux-64
-    gxx_linux-64
+    gcc_linux-64>=12.2.0
+    gxx_linux-64>=12.2.0
+    binutils_linux-64>=2.39
     gsl
     lapack==3.6.1
+    openmpi
 conda_channels=conda-forge
 setenv = PYCBC_TEST_TYPE=docs
 commands = bash tools/pycbc_test_suite.sh
```


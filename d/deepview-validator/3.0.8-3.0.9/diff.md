# Comparing `tmp/deepview_validator-3.0.8-py3-none-any.whl.zip` & `tmp/deepview_validator-3.0.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,46 +1,47 @@
-Zip file size: 99292 bytes, number of entries: 44
--rw-rw-r--  2.0 unx      760 b- defN 23-May-31 23:39 deepview/validator/__init__.py
--rw-rw-r--  2.0 unx    16195 b- defN 23-May-31 23:39 deepview/validator/__main__.py
--rw-rw-r--  2.0 unx    12847 b- defN 23-May-31 23:39 deepview/validator/exceptions.py
--rw-rw-r--  2.0 unx      497 b- defN 23-May-31 23:39 deepview/validator/datasets/__init__.py
--rw-rw-r--  2.0 unx    30031 b- defN 23-May-31 23:39 deepview/validator/datasets/core.py
--rw-rw-r--  2.0 unx    14688 b- defN 23-May-31 23:39 deepview/validator/datasets/darknet.py
--rw-rw-r--  2.0 unx     7029 b- defN 23-May-31 23:39 deepview/validator/datasets/tfrecord.py
--rw-rw-r--  2.0 unx     3786 b- defN 23-May-31 23:39 deepview/validator/datasets/utils.py
--rw-rw-r--  2.0 unx      530 b- defN 23-May-31 23:39 deepview/validator/evaluators/__init__.py
--rw-rw-r--  2.0 unx     6406 b- defN 23-May-31 23:39 deepview/validator/evaluators/core.py
--rw-rw-r--  2.0 unx    25209 b- defN 23-May-31 23:39 deepview/validator/evaluators/detectionevaluator.py
--rw-rw-r--  2.0 unx    19326 b- defN 23-May-31 23:39 deepview/validator/evaluators/segmentationevaluator.py
--rw-rw-r--  2.0 unx      830 b- defN 23-May-31 23:39 deepview/validator/metrics/__init__.py
--rw-rw-r--  2.0 unx    11367 b- defN 23-May-31 23:39 deepview/validator/metrics/core.py
--rw-rw-r--  2.0 unx    32340 b- defN 23-May-31 23:39 deepview/validator/metrics/detectiondata.py
--rw-rw-r--  2.0 unx    26274 b- defN 23-May-31 23:39 deepview/validator/metrics/detectionmetrics.py
--rw-rw-r--  2.0 unx     4548 b- defN 23-May-31 23:39 deepview/validator/metrics/detectionutils.py
--rw-rw-r--  2.0 unx    17192 b- defN 23-May-31 23:39 deepview/validator/metrics/segmentationdata.py
--rw-rw-r--  2.0 unx     6514 b- defN 23-May-31 23:39 deepview/validator/metrics/segmentationmetrics.py
--rw-rw-r--  2.0 unx    11690 b- defN 23-May-31 23:39 deepview/validator/metrics/segmentationutils.py
--rw-rw-r--  2.0 unx      735 b- defN 23-May-31 23:39 deepview/validator/runners/__init__.py
--rw-rw-r--  2.0 unx     5144 b- defN 23-May-31 23:39 deepview/validator/runners/core.py
--rw-rw-r--  2.0 unx    13848 b- defN 23-May-31 23:39 deepview/validator/runners/deepviewrt.py
--rw-rw-r--  2.0 unx    17781 b- defN 23-May-31 23:39 deepview/validator/runners/keras.py
--rw-rw-r--  2.0 unx     6147 b- defN 23-May-31 23:39 deepview/validator/runners/offline.py
--rw-rw-r--  2.0 unx    16285 b- defN 23-May-31 23:39 deepview/validator/runners/tensorrt.py
--rw-rw-r--  2.0 unx    14466 b- defN 23-May-31 23:39 deepview/validator/runners/tflite.py
--rw-rw-r--  2.0 unx      612 b- defN 23-May-31 23:39 deepview/validator/runners/modelclient/__init__.py
--rw-rw-r--  2.0 unx    27957 b- defN 23-May-31 23:39 deepview/validator/runners/modelclient/boxes.py
--rw-rw-r--  2.0 unx     3116 b- defN 23-May-31 23:39 deepview/validator/runners/modelclient/core.py
--rw-rw-r--  2.0 unx    11168 b- defN 23-May-31 23:39 deepview/validator/runners/modelclient/segmentation.py
--rw-rw-r--  2.0 unx      521 b- defN 23-May-31 23:39 deepview/validator/visualize/__init__.py
--rw-rw-r--  2.0 unx    11896 b- defN 23-May-31 23:39 deepview/validator/visualize/core.py
--rw-rw-r--  2.0 unx     7277 b- defN 23-May-31 23:39 deepview/validator/visualize/detectiondrawer.py
--rw-rw-r--  2.0 unx    14370 b- defN 23-May-31 23:39 deepview/validator/visualize/segmentationdrawer.py
--rw-rw-r--  2.0 unx      574 b- defN 23-May-31 23:39 deepview/validator/writers/__init__.py
--rw-rw-r--  2.0 unx     7611 b- defN 23-May-31 23:39 deepview/validator/writers/console.py
--rw-rw-r--  2.0 unx    19839 b- defN 23-May-31 23:39 deepview/validator/writers/core.py
--rw-rw-r--  2.0 unx     9065 b- defN 23-May-31 23:39 deepview/validator/writers/tensorboard.py
--rw-rw-r--  2.0 unx      468 b- defN 23-May-31 23:40 deepview_validator-3.0.8.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-31 23:40 deepview_validator-3.0.8.dist-info/WHEEL
--rw-rw-r--  2.0 unx       73 b- defN 23-May-31 23:40 deepview_validator-3.0.8.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-May-31 23:40 deepview_validator-3.0.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4268 b- defN 23-May-31 23:40 deepview_validator-3.0.8.dist-info/RECORD
-44 files, 441381 bytes uncompressed, 92306 bytes compressed:  79.1%
+Zip file size: 104186 bytes, number of entries: 45
+-rw-rw-r--  2.0 unx      760 b- defN 23-Jul-27 17:06 deepview/validator/__init__.py
+-rw-rw-r--  2.0 unx    17902 b- defN 23-Jul-27 17:06 deepview/validator/__main__.py
+-rw-rw-r--  2.0 unx    12716 b- defN 23-Jul-27 17:06 deepview/validator/exceptions.py
+-rw-rw-r--  2.0 unx      497 b- defN 23-Jul-27 17:06 deepview/validator/datasets/__init__.py
+-rw-rw-r--  2.0 unx    30711 b- defN 23-Jul-27 17:06 deepview/validator/datasets/core.py
+-rw-rw-r--  2.0 unx    14726 b- defN 23-Jul-27 17:06 deepview/validator/datasets/darknet.py
+-rw-rw-r--  2.0 unx     7026 b- defN 23-Jul-27 17:06 deepview/validator/datasets/tfrecord.py
+-rw-rw-r--  2.0 unx     3959 b- defN 23-Jul-27 17:06 deepview/validator/datasets/utils.py
+-rw-rw-r--  2.0 unx      536 b- defN 23-Jul-27 17:06 deepview/validator/evaluators/__init__.py
+-rw-rw-r--  2.0 unx     6672 b- defN 23-Jul-27 17:06 deepview/validator/evaluators/core.py
+-rw-rw-r--  2.0 unx    29717 b- defN 23-Jul-27 17:06 deepview/validator/evaluators/detectionevaluator.py
+-rw-rw-r--  2.0 unx    16758 b- defN 23-Jul-27 17:06 deepview/validator/evaluators/segmentationevaluator.py
+-rw-rw-r--  2.0 unx      836 b- defN 23-Jul-27 17:06 deepview/validator/metrics/__init__.py
+-rw-rw-r--  2.0 unx    11356 b- defN 23-Jul-27 17:06 deepview/validator/metrics/core.py
+-rw-rw-r--  2.0 unx    36080 b- defN 23-Jul-27 17:06 deepview/validator/metrics/detectiondata.py
+-rw-rw-r--  2.0 unx    29321 b- defN 23-Jul-27 17:06 deepview/validator/metrics/detectionmetrics.py
+-rw-rw-r--  2.0 unx    24126 b- defN 23-Jul-27 17:06 deepview/validator/metrics/detectionutils.py
+-rw-rw-r--  2.0 unx    16661 b- defN 23-Jul-27 17:06 deepview/validator/metrics/segmentationdata.py
+-rw-rw-r--  2.0 unx     6512 b- defN 23-Jul-27 17:06 deepview/validator/metrics/segmentationmetrics.py
+-rw-rw-r--  2.0 unx    11683 b- defN 23-Jul-27 17:06 deepview/validator/metrics/segmentationutils.py
+-rw-rw-r--  2.0 unx      735 b- defN 23-Jul-27 17:06 deepview/validator/runners/__init__.py
+-rw-rw-r--  2.0 unx     6173 b- defN 23-Jul-27 17:06 deepview/validator/runners/core.py
+-rw-rw-r--  2.0 unx    17964 b- defN 23-Jul-27 17:06 deepview/validator/runners/deepviewrt.py
+-rw-rw-r--  2.0 unx    17680 b- defN 23-Jul-27 17:06 deepview/validator/runners/keras.py
+-rw-rw-r--  2.0 unx     6203 b- defN 23-Jul-27 17:06 deepview/validator/runners/offline.py
+-rw-rw-r--  2.0 unx    16289 b- defN 23-Jul-27 17:06 deepview/validator/runners/tensorrt.py
+-rw-rw-r--  2.0 unx    14459 b- defN 23-Jul-27 17:06 deepview/validator/runners/tflite.py
+-rw-rw-r--  2.0 unx      612 b- defN 23-Jul-27 17:06 deepview/validator/runners/modelclient/__init__.py
+-rw-rw-r--  2.0 unx    28003 b- defN 23-Jul-27 17:06 deepview/validator/runners/modelclient/boxes.py
+-rw-rw-r--  2.0 unx     3115 b- defN 23-Jul-27 17:06 deepview/validator/runners/modelclient/core.py
+-rw-rw-r--  2.0 unx    11160 b- defN 23-Jul-27 17:06 deepview/validator/runners/modelclient/segmentation.py
+-rw-rw-r--  2.0 unx      521 b- defN 23-Jul-27 17:06 deepview/validator/visualize/__init__.py
+-rw-rw-r--  2.0 unx      649 b- defN 23-Jul-27 17:06 deepview/validator/visualize/core.py
+-rw-rw-r--  2.0 unx     6955 b- defN 23-Jul-27 17:06 deepview/validator/visualize/detectiondrawer.py
+-rw-rw-r--  2.0 unx    12968 b- defN 23-Jul-27 17:06 deepview/validator/visualize/segmentationdrawer.py
+-rw-rw-r--  2.0 unx    11442 b- defN 23-Jul-27 17:06 deepview/validator/visualize/utils.py
+-rw-rw-r--  2.0 unx      574 b- defN 23-Jul-27 17:06 deepview/validator/writers/__init__.py
+-rw-rw-r--  2.0 unx     2402 b- defN 23-Jul-27 17:06 deepview/validator/writers/console.py
+-rw-rw-r--  2.0 unx    14283 b- defN 23-Jul-27 17:06 deepview/validator/writers/core.py
+-rw-rw-r--  2.0 unx     5234 b- defN 23-Jul-27 17:06 deepview/validator/writers/tensorboard.py
+-rw-rw-r--  2.0 unx      468 b- defN 23-Jul-27 17:06 deepview_validator-3.0.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jul-27 17:06 deepview_validator-3.0.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       73 b- defN 23-Jul-27 17:06 deepview_validator-3.0.9.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Jul-27 17:06 deepview_validator-3.0.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     4362 b- defN 23-Jul-27 17:06 deepview_validator-3.0.9.dist-info/RECORD
+45 files, 460980 bytes uncompressed, 97050 bytes compressed:  78.9%
```

## zipnote {}

```diff
@@ -99,35 +99,38 @@
 
 Filename: deepview/validator/visualize/detectiondrawer.py
 Comment: 
 
 Filename: deepview/validator/visualize/segmentationdrawer.py
 Comment: 
 
+Filename: deepview/validator/visualize/utils.py
+Comment: 
+
 Filename: deepview/validator/writers/__init__.py
 Comment: 
 
 Filename: deepview/validator/writers/console.py
 Comment: 
 
 Filename: deepview/validator/writers/core.py
 Comment: 
 
 Filename: deepview/validator/writers/tensorboard.py
 Comment: 
 
-Filename: deepview_validator-3.0.8.dist-info/METADATA
+Filename: deepview_validator-3.0.9.dist-info/METADATA
 Comment: 
 
-Filename: deepview_validator-3.0.8.dist-info/WHEEL
+Filename: deepview_validator-3.0.9.dist-info/WHEEL
 Comment: 
 
-Filename: deepview_validator-3.0.8.dist-info/entry_points.txt
+Filename: deepview_validator-3.0.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: deepview_validator-3.0.8.dist-info/top_level.txt
+Filename: deepview_validator-3.0.9.dist-info/top_level.txt
 Comment: 
 
-Filename: deepview_validator-3.0.8.dist-info/RECORD
+Filename: deepview_validator-3.0.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deepview/validator/__main__.py

```diff
@@ -25,15 +25,19 @@
 
 
 def main():
     parser = argparse.ArgumentParser(
         description=('Standalone DeepView Validator.'),
         formatter_class=argparse.RawTextHelpFormatter
     )
-    parser.add_argument('-V', '--version', action='version', version=version())
+    parser.add_argument('-V', '--version', 
+                        help="Print the package version",
+                        action='version', 
+                        version=version()
+                        )
     parser.add_argument('--validate',
                         help=("Type of validation to perform: "
                               "'detection' or 'segmentation'."),
                         choices=['detection', 'segmentation'],
                         default='detection',
                         type=str
                         )
@@ -41,21 +45,22 @@
                         help=("Type of application to use: "
                               "'vaal' or 'modelclient'."),
                         choices=['vaal', 'modelclient'],
                         default='vaal',
                         type=str
                         )
     parser.add_argument('-d', '--dataset',
-                        help=("absolute or relative path"
+                        help=("absolute or relative path "
                               "to the dataset folder or yaml file."),
                         required=True,
                         type=str
                         )
     parser.add_argument('--labels_file',
-                        help='absolute or relative path to the labels file.',
+                        help=("absolute or relative path "
+                              "to the labels.txt file."),
                         type=str
                         )
     parser.add_argument('--annotation_format',
                         help=("Specify the format of the annotations: "
                               "'yolo', 'coco', 'pascalvoc'"),
                         choices=['yolo', 'coco', 'pascalvoc'],
                         default='yolo',
@@ -75,14 +80,33 @@
                         action='store_true'
                         )
     parser.add_argument('--show_missing_annotations',
                         help=("This shows the image names without "
                               "annotations on the terminal"),
                         action='store_true'
                         )
+    parser.add_argument('--clamp_box',
+                        help=("The value to clamp the minimum width or height "
+                              "of the bounding boxes for ground truth and "
+                              "predictions in pixels."),
+                        type=int
+                        )
+    parser.add_argument('--ignore_box',
+                        help=("Ignore bounding boxes "
+                              "for detections and ground truth with height "
+                              "or width less than this value in pixels."),
+                        type=int
+                        )
+    parser.add_argument('--metric', 
+                        help=("Specify the metric to use when "
+                              "matching model predictions to ground truth."),
+                        choices=['iou', 'centerpoint'],
+                        default='iou',
+                        type=str
+                        )
     parser.add_argument('-e', '--engine',
                         help=("Compute engine for inference."
                               "'npu', 'gpu', 'cpu'"),
                         choices=['cpu', 'npu', 'gpu'],
                         default='npu',
                         type=str
                         )
@@ -90,80 +114,74 @@
                         help='NMS threshold for valid scores.',
                         default=0.01,
                         type=float
                         )
     parser.add_argument('--validation_threshold',
                         help=("Validation score threshold "
                               "to filter predictions."),
-                        default=0.5,
+                        default=0.25,
                         type=float
                         )
     parser.add_argument('--detection_iou',
                         help='IoU threshold for NMS.',
                         default=0.45,
                         type=float
                         )
     parser.add_argument('--validation_iou',
                         help=("Validation IoU threshold "
                               "to consider true positives."),
-                        default=0.5,
+                        default=0.50,
                         type=float
                         )
     parser.add_argument('-l', '--label_offset',
-                        help='Label offset when matching index to label name.',
+                        help="Label offset when matching index to label name.",
                         default=0,
                         type=int
                         )
     parser.add_argument('-b', '--box_format',
                         help=("box format to reorient the prediction "
                               "coordinates: 'xywh', 'xyxy', 'yxyx', etc).\n"),
                         choices=['xywh', 'xyxy', 'yxyx'],
                         default='xyxy',
                         type=str
                         )
     parser.add_argument(
-        '-n',
-        '--norm',
-        help=(
-            'Normalization method applied to input images.'
-            '- raw (default, no processing)\n'
-            '- unsigned (0...1)\n'
-            '- signed (-1...1)\n'
-            '- whitening (per-image standardization/whitening)\n'
-            '- imagenet (standardization using imagenet)\n'),
-        choices=[
-            'raw',
-            'unsigned',
-            'signed',
-            'whitening',
-            'imagenet'],
-        default='raw',
-        type=str)
+                    '-n', '--norm',
+                    help=(
+                        'Normalization method applied to input images.'
+                        '- raw (default, no processing)\n'
+                        '- unsigned (0...1)\n'
+                        '- signed (-1...1)\n'
+                        '- whitening (per-image standardization/whitening)\n'
+                        '- imagenet (standardization using imagenet)\n'),
+                    choices=['raw','unsigned','signed','whitening','imagenet'],
+                    default='raw',
+                    type=str
+                    )
     parser.add_argument('-m', '--max_detection',
                         help='Number of maximum predictions (bounding boxes).',
-                        default=100,
                         type=int
                         )
     parser.add_argument('-w', '--warmup',
                         help='The warmup iterations before processing images.',
                         default=0,
                         type=int
                         )
     parser.add_argument('-s', '--nms_type',
                         help=("NMS type to perform validation: "
-                              "'standard', 'fast', 'matrix'."),
-                        choices=['standard', 'fast', 'matrix'],
+                              "'standard', 'fast', 'matrix', 'tensorflow'. "
+                              "For Keras models, only tensorflow is allowed."),
+                        choices=['standard', 'fast', 'matrix', 'tensorflow'],
                         type=str
                         )
-    parser.add_argument(
-        '--decoder',
-        help=(
-            "If the model does not have embedded decoder, "
-            "then apply this parameter in the command line."),
-        action='store_true')
+    parser.add_argument('--decoder',
+                        help=("If the model does not have embedded decoder, "
+                            "then apply this parameter in the command line."),
+                            action='store_true'
+                        )
     parser.add_argument('--model_box_type',
                         help="Type of the box model: 'modelpack', 'yolo'.",
                         choices=['modelpack', 'yolo'],
                         default='modelpack',
                         type=str
                         )
     parser.add_argument('--model_segmentation_type',
@@ -182,77 +200,73 @@
     parser.add_argument('--display',
                         help=("How many images to display into tensorboard. "
                               "By default it is (-1) all the images, "
                               "but an integer can be passed."),
                         default=-1,
                         type=int
                         )
-    parser.add_argument(
-        '--visualize',
-        help=(
-            "Path to store visualizations "
-            "(images with bounding boxes or segmentation masks)."),
-        default=None,
-        type=str
-    )
+    parser.add_argument('--visualize',
+                        help=("Path to store visualizations "
+                            "(images with bounding boxes "
+                            "or segmentation masks)."),
+                        default=None,
+                        type=str
+                        )
     parser.add_argument('--tensorboard',
                         help=("Path to store *.tfevents files "
                               "needed for tensorboard."),
                         default=None,
                         type=str
                         )
-    parser.add_argument(
-        '--json_out',
-        help=("Path to save the validation summary as a json file on disk."),
-        default=None,
-        type=str
-    )
-    parser.add_argument(
-        'model',
-        help=(
-            "Model path to the DeepViewRT (rtm), "
-            "Keras (H5), TFlite, TensorRT (trt) model to load."),
-        metavar='model.rtm',
-        type=str
-    )
+    parser.add_argument('--json_out',
+                        help=("Path to save the validation summary "
+                              "as a json file on disk."),
+                        default=None,
+                        type=str
+                        )
+    parser.add_argument('model',
+                        help=("Model path to the DeepViewRT (rtm), "
+                            "Keras (H5), TFlite, TensorRT (trt) "
+                            "model to load."),
+                        metavar='model.rtm',
+                        type=str
+                        )
     args = parser.parse_args()
 
     parameters = {
         "validation-iou": args.validation_iou,
         "detection-iou": args.detection_iou,
         "validation-threshold": args.validation_threshold,
         "detection-threshold": args.detection_threshold,
         "nms": args.nms_type,
         "normalization": args.norm,
-        "maximum_detections": args.max_detection,
         "warmup": args.warmup,
-        "label offset": args.label_offset
+        "label offset": args.label_offset,
+        "metric": args.metric,
+        "clamp boxes": args.clamp_box,
+        "ignore boxes": args.ignore_box 
     }
 
-    base_dataset = Dataset()
-    info_dataset = base_dataset.get_detection_dataset(
+    info_dataset = Dataset().get_detection_dataset(
         source=args.dataset,
         labels_path=args.labels_file
         )
     dataset = instantiate_dataset(
         info_dataset=info_dataset,
         source=args.dataset,
         gformat=args.annotation_format,
         absolute=args.absolute_annotations,
         validate_type=args.validate,
         show_missing_annotations=args.show_missing_annotations
     )
 
     # DEEPVIEWRT EVALUATION
     if path.splitext(args.model)[1].lower() == ".rtm":
-
         if args.validate.lower() == "detection":
-
             if args.application.lower() == "vaal":
-
                 runner = DeepViewRTRunner(
                     model_path=args.model,
                     labels=dataset.labels,
                     engine=args.engine,
                     detection_score_threshold=args.detection_threshold,
                     detection_iou_threshold=args.detection_iou,
                     label_offset=args.label_offset,
@@ -260,97 +274,86 @@
                     norm=args.norm,
                     max_detections=args.max_detection,
                     warmup=args.warmup,
                     nms_type=args.nms_type
                 )
 
             elif args.application.lower() == "modelclient":
-
                 if args.model_box_type.lower() == 'modelpack':
-
                     runner = BoxesModelPack(
                         model_path=args.model,
                         target=f"http://{args.target}/v1",
                         labels=dataset.labels,
                         detection_iou_threshold=args.detection_iou,
                         detection_score_threshold=args.detection_threshold,
                         norm=args.norm,
                         decoder=args.decoder,
                         label_offset=args.label_offset
                     )
 
                 elif args.model_box_type.lower() == 'yolo':
-
                     runner = BoxesYolo(
                         model_path=args.model,
                         target=f"http://{args.target}/v1",
                         labels=dataset.labels,
                         detection_iou_threshold=args.detection_iou,
                         detection_score_threshold=args.detection_threshold,
                         norm=args.norm,
                         decoder=args.decoder,
                         label_offset=args.label_offset
                     )
 
                 else:
                     raise UnsupportedModelTypeException(args.model_box_type)
-
             else:
                 raise UnsupportedApplicationException(args.application)
 
         elif args.validate.lower() == "segmentation":
-
             if args.model_segmentation_type.lower() == 'modelpack':
-
                 runner = SegmentationModelPack(
                     model_path=args.model,
                     target=f'http://{args.target}/v1'
                 )
                 runner.seg_type = 'modelpack'
                 dataset.shape = runner.get_input_shape()
 
             elif args.model_segmentation_type.lower() == 'deeplab':
-
                 runner = SegmentationDeepLab(
                     model_path=args.model,
                     target=f'http://{args.target}/v1'
                 )
                 runner.seg_type = 'deeplab'
                 dataset.shape = runner.get_input_shape()
-
         else:
             raise UnsupportedValidationTypeException(args.validate)
 
     # KERAS EVALUATION
     elif path.splitext(args.model)[1].lower() == ".h5":
-
         if args.validate.lower() == "detection":
-
             runner = DetectionKerasRunner(
                 model_path=args.model,
                 labels=dataset.labels,
                 detection_iou_threshold=args.detection_iou,
                 detection_score_threshold=args.detection_threshold,
                 norm=args.norm,
                 label_offset=args.label_offset
             )
         
         elif args.validate.lower() == "segmentation":
-
             runner = SegmentationKerasRunner(
                 model=args.model
             )
 
     # TFLITE EVALUATION
     elif path.splitext(args.model)[1].lower() == ".tflite":
-        raise NotImplementedError()
+        raise NotImplementedError("TFLite validation is not currently " +
+                                  "implemented.")
 
     # TensorRT Engine Evaluation
     elif path.splitext(args.model)[1].lower() == ".trt":
-
         runner = TensorRTRunner(
             engine_path=args.model,
             labels=dataset.labels,
             detection_score_threshold=args.detection_threshold,
             label_offset=args.label_offset
         )
 
@@ -376,39 +379,35 @@
 
     else:
         raise UnsupportedModelExtensionException(
             path.splitext(args.model)[1].lower()
         )
 
     if args.validate.lower() == 'detection':
-
         evaluator = DetectionEval(
             runner=runner,
             dataset=dataset,
             visualize=args.visualize,
             tensorboard=args.tensorboard,
             json_out=args.json_out,
             display=args.display,
             parameters=parameters
         )
-
         truth_values = evaluator.group_evaluation()
         evaluator.conclude(truth_values)
 
     elif args.validate.lower() == 'segmentation':
-
         evaluator = SegmentationEval(
             runner=runner,
             dataset=dataset,
             visualize=args.visualize,
             tensorboard=args.tensorboard,
             display=args.display,
             parameters=parameters
         )
-
         truth_values = evaluator.group_evaluation()
         evaluator.conclude(truth_values)
 
     else:
         raise UnsupportedValidationTypeException(args.validate)
```

## deepview/validator/exceptions.py

```diff
@@ -388,22 +388,19 @@
     """
     This exception will be raised if
     the matching algorithm encountered
     invalid values for both IoU
     and ground truth index.
     """
 
-    def __init__(self, iou, gti):
+    def __init__(self, message):
         sys.tracebacklimit = 0
         super(
             MatchingAlgorithmException,
-            self).__init__(
-            "The matching algorithm encountered invalid " +
-            "IoU {} and a ground truth index {}.".format(iou, gti)
-        )
+            self).__init__(message)
 
 
 class InvalidIoUException(Exception):
     """
     This exception will be raised if
     the IoU value is invalid such as
     less than 0 or greater than 1.
@@ -500,8 +497,8 @@
             This explains why the exception was raised.
     """
 
     def __init__(self, message):
         sys.tracebacklimit = 0
         super(MissingLibraryException, self).__init__(
             "Missing library: {}".format(message)
-        )
+        )
```

## deepview/validator/datasets/core.py

```diff
@@ -27,15 +27,14 @@
     on the annotation formats by following this link:
     https://support.deepviewml.com/hc/en-us/articles/10869801702029-Darknet-Ground-Truth-Annotations-Schema
     Unit-test for this class is defined under:
         file: test/deepview/validator/datasets/test_core.py
 
     Parameters
     ----------
-
         gformat: str
             The annotation format (yolo, pascalvoc, coco).
 
         absolute: bool
             If true, then the annotations are not normalized.
 
         validate_type: str
@@ -113,24 +112,51 @@
             if validate_type.lower() == 'detection':
                 self.normalizer = self.normalize
         else:
             if validate_type.lower() == 'segmentation':
                 self.denormalizer = self.denormalize_polygon
 
     @staticmethod
+    def clamp_dim(dim1, dim2, min):
+        """
+        This method clamps the bounding box dimensions to have a minimum 
+        width or height set by default to 42.
+
+        Parameters
+        ----------
+            dim1: float
+                This can mean xmin or ymin.
+
+            dim2: float
+                This can mean xmax or ymax.
+
+            min: int
+                The minimum acceptable dimension of the bounding box.
+
+        Returns
+        -------
+            dim1, dim2: float
+                This is the clamped dimensions.
+
+        Raises
+        ------
+            None
+        """
+        return (dim1, dim1+min) if dim2-dim1 < min else (dim1, dim2)
+
+    @staticmethod
     def validate_input_source(source):
         """
         This method validates the existance of the source path.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_validate_input_source
 
         Parameters
         ----------
-
             source: str
                 The path to the dataset.
 
         Returns
         -------
             source: str
                 The validated path to the dataset.
@@ -170,15 +196,14 @@
         dataset information.
         Unit-test for this function is defined under:
             file: test/test_datasets.py
             function: test_read_yaml_file
 
         Parameters
         ----------
-
             file_path: str
                 The path to the yaml file.
 
         Returns
         -------
             info_dataset: dict
                 This contains the yaml file contents.
@@ -226,15 +251,14 @@
         annotations (Darknet) or tfrecord files (TFRecord Dataset).
         Unit-test for this method is defined under:
             file: test/test_datasets.py
             function: test_get_detection_dataset
 
         Parameters
         ----------
-
             source: str
                 The validated path to the dataset.
                 This can point to a yaml file or a directory containing
                 tfrecords or images and text annotations.
 
             labels_path: str
                 The path to the labels.txt (if provided).
@@ -364,15 +388,14 @@
         integers and cannot be less than or equal to 0.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_validate_dimension
 
         Parameters
         ----------
-
             dimension: int
                 The dimension to validate.
 
         Returns
         -------
             dimension: int
                 The validated dimension that is an integer
@@ -400,15 +423,14 @@
         This method resizes the images depending on the size.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_resize
 
         Parameters
         ----------
-
             image: (height, width, 3) np.ndarray
                 The image represented as a numpy array.
 
             size: (height, width) tuple
                 Specify the size to resize.
 
         Returns
@@ -454,15 +476,14 @@
         This method converts BGR image to RGB image.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_bgr2rgb
 
         Parameters
         ----------
-
             image: (height, width, 3) np.ndarray
                 The image as a BGR numpy array.
 
         Returns
         -------
             image: (height, width, 3) np.ndarray
                 RGB image.
@@ -481,15 +502,14 @@
         This method converts RGB image to BGR image.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_rgb2bgr
 
         Parameters
         ----------
-
             image: (height, width, 3) np.ndarray
                 The image as a RGB numpy array.
 
         Returns
         -------
             image: (height, width, 3) np.ndarray
                 BGR image.
@@ -509,15 +529,14 @@
         and height of the image or model input resolution.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_normalize
 
         Parameters
         ----------
-
             boxes: np.ndarray
                 List of lists of floats [[boxes1], [boxes2]].
                 Contains boxes to normalize.
 
             height: int
                 The dimension to normalize the y-coordinates.
 
@@ -569,15 +588,14 @@
         and height of the image or model input resolution.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_denormalize
 
         Parameters
         ----------
-
             boxes: np.ndarray
                 List of lists of floats [[boxes1], [boxes2]].
                 Contains boxes to denormalize.
 
             height: int
                 The dimension to denormalize the y-coordinates.
 
@@ -626,15 +644,14 @@
         coordinate of a polygon.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_normalize_polygon
 
         Parameters
         ----------
-
             vertex: list
                 This contains [x, y] coordinate.
 
             height: int
                 The dimension to normalize the y-coordinates.
 
             width: int
@@ -676,15 +693,14 @@
         coordinate of a polygon.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_denormalize_polygon
 
         Parameters
         ----------
-
             vertex: list
                 This contains [x, y] coordinate.
 
             height: int
                 The dimension to denormalize the y-coordinates.
 
             width: int
@@ -725,15 +741,14 @@
         This method converts yolo format into pascalvoc format.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_yolo2xyxy
 
         Parameters
         ----------
-
             boxes: np.ndarray
                 Contains lists for each boxes in
                 yolo format [[boxes1], [boxes2]]
 
         Returns
         -------
             boxes: np.ndarray
@@ -767,15 +782,14 @@
         This method converts coco format to pascalvoc format.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_xywh2xyxy
 
         Parameters
         ----------
-
             boxes: np.ndarray
                 Contains lists for each boxes in
                 coco format [[boxes1], [boxes2]]
 
         Returns
         -------
             boxes: np.ndarray
@@ -813,15 +827,14 @@
         TFRecord datasets.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_core.py
             function: test_read_all_samples
 
         Parameters
         ----------
-
             info: str
                 The description of why image instances are being read.
                 By default it is to run validation,
                 hence "Validation Progress".
 
         Returns
         -------
```

## deepview/validator/datasets/darknet.py

```diff
@@ -261,16 +261,16 @@
             boxes = load.get('boxes')
             labels = load.get('labels')
         else:
             return {
                 'image': image,
                 'height': height,
                 'width': width,
-                'boxes': [],
-                'labels': [],
+                'boxes': np.array([]),
+                'labels': np.array([]),
                 'image_path': image_path
             }
 
         return {
             'image': image,
             'height': height,
             'width': width,
@@ -284,15 +284,14 @@
         This method reads from the text file annotation.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_darknet.py
             function: test_txt_load_boxes
 
         Parameters
         ----------
-
             annotation_path: str
                 This is the path to the text file annotation.
 
         Returns
         -------
             annotation info: dict
                 This contains information such as:
@@ -328,27 +327,26 @@
 
         labels = annotation[:, 0:1].flatten().astype(np.int32)
         if len(self.labels):
             labels = [self.labels[int(label)].lower() for label in labels]
 
         return {
             'boxes': boxes,
-            'labels': labels
+            'labels': np.array(labels)
         }
 
     def json_load_boxes(self, annotation_path):
         """
         This method reads from the JSON annotation.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_darknet.py
             function: test_json_load_boxes
 
         Parameters
         ----------
-
             annotation_path: str
                 This is the path to the JSON annotation
 
         Returns
         -------
             annotation info: dict
                 This contains information such as:
@@ -390,15 +388,15 @@
                 except KeyError:
                     return None
         except FileNotFoundError:
             return None
 
         return {
             'boxes': boxes,
-            'labels': labels
+            'labels': np.array(labels)
         }
     # TODO: Make this method single with the method above.
     def json_load_polygon_instance(self, annotation_path, height, width):
         """
         This method loads a single instance from
         a JSON file in the image dataset to grab the segments.
         Unit-test for this method is defined under:
```

## deepview/validator/datasets/tfrecord.py

```diff
@@ -23,15 +23,14 @@
     This class reads TFRecord Datasets.
 
     Unit-test for this class is defined under:
         file: test/deepview/validator/datasets/test_tfrecord.py
 
     Parameters
     ----------
-
         source: str
             The path to the source dataset.
 
         info_dataset: dict
             Contains information such as:
 
             .. code-block:: python
@@ -111,15 +110,14 @@
         to extract information.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_tfrecord.py
             function: test_py_read_data
 
         Parameters
         ----------
-
             example:
 
         Returns
         -------
 
         Raises
         ------
@@ -219,15 +217,14 @@
         This method reads one sample from the dataset (one annotation file).
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_tfrecord.py
             function: test_read_sample
 
         Parameters
         ----------
-
             instance
 
         Returns
         -------
 
         Raises
         ------
```

## deepview/validator/datasets/utils.py

```diff
@@ -23,15 +23,14 @@
     format dataset objects.
     Unit-test for this function is defined under:
         file: test/test_datasets.py
         function: test_instantiate_dataset
 
     Parameters
     ----------
-
         info_dataset: dict
             If the dataset is darknet, this 
             contains information such as:
 
             .. code-block:: python
 
                 {
@@ -73,14 +72,20 @@
             'segmentation'.
 
         show_missing_annotations: bool
             If this is True, then print on the terminal all
             missing annotations. Else, it will only
             print the number of missing annotations.
 
+    Returns
+    -------
+        Dataset object: DarknetDataset or TFRecordDataset
+            This object is returned depending on the type of dataset
+            provided.
+
     Raises
     ------
         InvalidDatasetSourceException
             This exception will be raised if the path
             to the images or annotations is None.
 
         DatasetNotFoundException
```

## deepview/validator/evaluators/__init__.py

```diff
@@ -3,10 +3,11 @@
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
 
-from deepview.validator.evaluators.segmentationevaluator import SegmentationEval
+from deepview.validator.evaluators.segmentationevaluator import \
+    SegmentationEval
 from deepview.validator.evaluators.detectionevaluator import DetectionEval
 from deepview.validator.evaluators.core import Evaluator
```

## deepview/validator/evaluators/core.py

```diff
@@ -16,25 +16,24 @@
     This class is an abstract class that provides a template for the
     validation evaluations (detection or segmentation).
     Unit-test for this class is defined under:
         file: test/deepview/validator/evaluators/test_core.py
 
     Parameters
     ----------
-
         runner: Runner object depending on the model.
             This object provides methods to run the detection model.
 
         dataset: Dataset object depending on the dataset.
             This object provides methods to read and parse the dataset.
 
         datacollection: DataCollection object depending on detection
         or segmentation. This object stores the number of true positives,
         false positives, and false negatives per label
-        that allows the computation of metrics.
+        that allows the computation of metrics. 
 
         visualize: str
             This is the path to store the images with visualizations. Can
             optionally be None to exclude.
 
         tensorboard: str
             This is the path to store the tensorboard tfevents file. Can
@@ -57,15 +56,19 @@
                     "validation-iou": args.validation_iou,
                     "detection-iou": args.detection_iou,
                     "validation-threshold": args.validation_threshold,
                     "detection-threshold": args.detection_threshold,
                     "nms": args.nms_type,
                     "normalization": args.norm,
                     "maximum_detections": args.max_detection,
-                    "warmup": args.warmup
+                    "warmup": args.warmup,
+                    "label offset": args.label_offset,
+                    "metric": args.metric,
+                    "clamp boxes": args.clamp_box,
+                    "ignore boxes": args.ignore_box 
                 }
 
     Raises
     ------
         None
 
     """
@@ -106,24 +109,26 @@
             else:
                 raise ValueError("--json_out parameter can only accept " + 
                                  "json files, but received {}".format(json_out)
                                 )
 
         if visualize:
             self.save_path = path.join(
-                visualize, f"{path.basename(path.normpath(self.runner.source))}_{today}"
+                visualize, 
+                f"{path.basename(path.normpath(self.runner.source))}_{today}"
             )
             if not path.exists(self.save_path):
                 makedirs(self.save_path)
             self.consolewriter = ConsoleWriter()
             self.tensorboardwriter = None
 
         elif tensorboard:
             self.save_path = path.join(
-                tensorboard, f"{path.basename(path.normpath(self.runner.source))}_{today}"
+                tensorboard, 
+                f"{path.basename(path.normpath(self.runner.source))}_{today}"
             )
             if not path.exists(self.save_path):
                 makedirs(self.save_path)
             self.tensorboardwriter = TensorBoardWriter(self.save_path)
 
         else:
             self.consolewriter = ConsoleWriter()
@@ -151,15 +156,14 @@
         application is trying to serialize into a JSON file.
         Unit-test for this class is defined under:
             file: test/deepview/validator/evaluators/test_core.py
             function: test_print_types
 
         Parameters
         ----------
-
             d: dict or list
                 This is the datastructure to debug for the types.
 
             tabs: int
                 This allows for better formatting showing the nested
                 structures.
         
@@ -178,9 +182,10 @@
                 print(f"{t} {key=}: type: {type(value)}")
                 if type(value) == type(dict()) or type(value) == type(list()):
                     self.print_types(value, tabs+1)
         elif type(d) == type(list()):
             for index in range(min(len(d), 4)):
                 t = '\t'*tabs
                 print(f"{t} {index=}: type: {type(d[index])}")
-                if type(d[index]) == type(dict()) or type(d[index]) == type(list()):
+                if type(d[index]) == type(dict()) or \
+                            type(d[index]) == type(list()):
                     self.print_types(d[index], tabs+1)
```

## deepview/validator/evaluators/detectionevaluator.py

```diff
@@ -2,18 +2,23 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
-from deepview.validator.metrics import DetectionMetrics, DetectionDataCollection
-from deepview.validator.metrics.detectionutils import match_dt_gt
-from deepview.validator.visualize import Drawer, DetectionDrawer
+from deepview.validator.metrics import DetectionMetrics, \
+    DetectionDataCollection
+from deepview.validator.metrics.detectionutils import match_gt_dt, filter_dt, \
+    clamp_boxes, ignore_boxes
+from deepview.validator.visualize.utils import plot_pr, plot_classification, \
+    figure2numpy, close_figures
+from deepview.validator.visualize import DetectionDrawer
 from deepview.validator.evaluators.core import Evaluator
+from deepview.validator.writers.core import Writer
 from copy import deepcopy
 from os import path
 import numpy as np
 
 
 class DetectionEval(Evaluator):
     """
@@ -28,15 +33,14 @@
         5) Calculate the metrics.
 
     Unit-test for this class is defined under:
         file: test/deepview/validator/evaluators/test_detectionevaluator.py
 
     Parameters
     ----------
-
         runner: Runner object depending on the model.
             This object provides methods to run the detection model.
 
         dataset: Dataset object depending on the dataset.
             This object provides methods to read and parse the dataset.
 
         visualize: str
@@ -79,45 +83,50 @@
             or the parameters are out of bounds. i.e. The thresholds provided
             are greater than 1 or less than 0.
 
     """
 
     def __init__(
         self,
-        runner,
+        runner=None,
         dataset=None,
         visualize=None,
         tensorboard=None,
         json_out=None,
         display=-1,
-        parameters=None
+        parameters=None,
+        full_summary=False
     ):
         super(DetectionEval, self).__init__(
             runner=runner,
             dataset=dataset,
             datacollection=DetectionDataCollection(),
             visualize=visualize,
             tensorboard=tensorboard,
             json_out=json_out,
             display=display,
             parameters=parameters
         )
 
+        self.drawer = DetectionDrawer()
+        self.clamp_box = self.parameters.get("clamp boxes")
+        self.ignore_box = self.parameters.get("ignore boxes")
+        self.full_summary = full_summary
+
     def __call__(self, val_messenger):
         """
         This method allows tensorboardformatter object in trainer
         to operate here and resets the metrics after each epoch.
         Default validation has no use of this method.
         Unit-test for this method is defined under:
             file: test/deepview/validator/evaluators/test_detectionevaluator.py
             function: test__call__.py
 
         Parameters
         ----------
-
             val_messenger: TrainingTensorBoardWriter
                 This object is internal for modelpack that was instantiated
                 specifically for training a model.
 
         Returns
         -------
             None
@@ -140,36 +149,18 @@
 
         Parameters
         ----------
             None
 
         Returns
         -------
-
             instances: dict
                 This yields one image instance from the ground
                 truth and the model predictions.
-
-                .. code-block:: python
-
-                    {
-                        'gt_instance': {
-                            'image': image numpy array,
-                            'height': height,
-                            'width': width,
-                            'boxes': list bounding boxes,
-                            'labels': list of labels,
-                            'image_path': image_path
-                        },
-                        'dt_instance': {
-                            'boxes': list of prediction bounding boxes,
-                            'labels': list of prediction labels,
-                            'scores': list of confidence scores
-                        }
-                    }
+                See README.md (Method Parameters Format) for more information.
 
         Raises
         ------
             ValueError
                 This method will raise an exception if provided image path
                 does not exist and the provided image array
                 is not a numpy.ndarray.
@@ -188,17 +179,23 @@
                 raise ValueError(
                     "The provided image_path does not exist at: " +
                     gt_instance.get('image_path') +
                     "The provided image array is not a numpy.ndarray: " +
                     type(gt_instance.get('image'))
                 )
 
-            dt_boxes, dt_classes, dt_scores = \
-                self.runner.run_single_instance(image)
-
+            output = self.runner.run_single_instance(image)
+            if output is not None:
+                dt_boxes, dt_classes, dt_scores = output
+            else:
+                yield {
+                'gt_instance': None,
+                'dt_instance': None
+            }
+            
             # Convert any synonym of the predicted label into the standard coco
             # label.
             if "clock" in self.dataset.labels:
                 dt_classes_modified = list()
                 for label in dt_classes:
                     for key in self.runner.sync_dict.keys():
                         if label == key:
@@ -213,44 +210,26 @@
             }
 
             yield {
                 'gt_instance': gt_instance,
                 'dt_instance': dt_instance
             }
 
-    def single_evaluation(self, instance, epoch=0, add_image=True):
+    def single_evaluation(self, instance, epoch=0, add_image=False):
         """
         This method runs validation on a single instance.
         Unit-test for this method is defined under:
             file: test/deepview/validator/evaluators/test_detectionevaluator.py
             function: test_single_evaluation
 
         Parameters
         ----------
-
             instance: dict
-                The ground truth and the predictions instances:
-
-                .. code-block:: python
-
-                        instance = {
-                            gt_instance = {
-                                'image':image,
-                                'height': height,
-                                'width': width,
-                                'boxes': [],
-                                'labels': [],
-                                'image_path': image_path
-                            },
-                            dt_instance = {
-                                'boxes': dt_boxes,
-                                'labels': dt_classes,
-                                'scores': dt_scores
-                            }
-                        }
+                The ground truth and the predictions instances.
+                See README.md (Method Parameters Format) for more information.
 
             epoch: int
                 This is the training epoch number. This
                 parameter is internal for modelpack usage.
                 Default validation has no use of this parameter.
 
             add_image: bool
@@ -268,50 +247,83 @@
                 This will raise the exception if the provided parameters
                 in certain methods does not conform to the specified data
                 type or the parameters are out of bounds. i.e.
                 The thresholds provided
                 are greater than 1 or less than 0.
         """
 
-        iou_threshold = self.parameters.get("validation-iou")
-        score_threshold = self.parameters.get("validation-threshold")
-        self.drawer = DetectionDrawer()
-
-        gt_boxes = instance.get('gt_instance').get('boxes')
-        dt_boxes = instance.get('dt_instance').get('boxes')
-        gt_labels = instance.get('gt_instance').get('labels')
-        dt_labels = instance.get('dt_instance').get('labels')
-        scores = instance.get('dt_instance').get('scores')
-
-        self.datacollection.capture_class(dt_labels)
-        self.datacollection.capture_class(gt_labels)
-        stats = match_dt_gt(gt_boxes=gt_boxes, dt_boxes=dt_boxes)
+        iou_t = self.parameters.get("validation-iou")
+        score_t= self.parameters.get("validation-threshold")
+        gt_instance = instance.get("gt_instance")
+        dt_instance = instance.get("dt_instance")
+
+        # Store all unfiltered detections. 
+        key = path.basename(gt_instance.get('image_path'))
+        original_instance = {
+            'gt_instance': {
+                "height": gt_instance.get("height"),
+                "width": gt_instance.get("width"),
+                "boxes": gt_instance.get("boxes"),
+                "labels": gt_instance.get("labels")
+            },
+            'dt_instance': {
+                "boxes": dt_instance.get("boxes"),
+                "labels": dt_instance.get("labels"),
+                "scores": dt_instance.get("scores")
+            }
+        }
+        self.datacollection.capture_all_class(
+            original_instance.get("gt_instance").get("labels"))
+        self.datacollection.capture_all_class(
+            original_instance.get("dt_instance").get("labels"))
+        self.datacollection.append_instance(key, original_instance)
+
+        # Filter detections only for valid scores.
+        dt_boxes, dt_labels, scores = filter_dt(
+            dt_instance.get("boxes"), 
+            dt_instance.get("labels"), 
+            dt_instance.get("scores"), 
+            score_t)
+        instance['dt_instance']['boxes'] = dt_boxes
+        instance['dt_instance']['labels'] = dt_labels
+        instance['dt_instance']['scores'] = scores
+
+        if self.clamp_box:
+            instance = clamp_boxes(instance, self.clamp_box)
+        if self.ignore_box:
+            instance = ignore_boxes(instance, self.ignore_box)
+
+        self.datacollection.capture_class(dt_instance.get("labels"))
+        self.datacollection.capture_class(gt_instance.get("labels"))
+        stats = match_gt_dt(
+            gt_instance.get("boxes"), 
+            dt_instance.get("boxes"), 
+            self.parameters.get("metric"))
         self.datacollection.categorize(
             *stats,
-            gt_labels=gt_labels,
-            dt_labels=dt_labels,
-            scores=scores)
+            gt_labels=gt_instance.get("labels"),
+            dt_labels=dt_instance.get("labels"),
+            scores=dt_instance.get("scores"))
 
         if add_image:
             if self.visualize or self.tensorboardwriter:
                 original_image = instance.get('gt_instance').get('image')
                 image = self.drawer.draw_with_pillow(
-                    iou_threshold, score_threshold,
+                    iou_t, score_t,
                     original_image, instance, *stats
                 )
 
             if self.visualize:
                 image.save(path.join(self.save_path, path.basename(
                     instance.get('gt_instance').get('image_path'))))
             elif self.tensorboardwriter:
                 nimage = np.asarray(image)
                 self.tensorboardwriter(nimage, instance.get(
                     'gt_instance').get('image_path'), step=epoch)
-
-        return self.datacollection.sum_outcomes(iou_threshold, score_threshold)
+        return self.datacollection.sum_outcomes(iou_t, score_t)
 
     def group_evaluation(self):
         """
         This method performs the bounding box evaluation on all images.
         Unit-test for this method is defined under:
             file: test/deepview/validator/evaluators/test_detectionevaluator.py
             function: test_group_evaluation
@@ -334,40 +346,80 @@
                 i.e. The thresholds provided
                 are greater than 1 or less than 0.
         """
 
         iou_threshold = self.parameters.get("validation-iou")
         score_threshold = self.parameters.get("validation-threshold")
         self.counter = 0
-        self.drawer = DetectionDrawer()
 
         for instances in self.instance_collector():
 
-            gt_boxes = instances.get('gt_instance').get('boxes')
-            dt_boxes = instances.get('dt_instance').get('boxes')
-            gt_labels = instances.get('gt_instance').get('labels')
-            dt_labels = instances.get('dt_instance').get('labels')
-            scores = instances.get('dt_instance').get('scores')
-
-            self.datacollection.capture_class(dt_labels)
-            self.datacollection.capture_class(gt_labels)
-            stats = match_dt_gt(gt_boxes=gt_boxes, dt_boxes=dt_boxes)
+            gt_instance = instances.get("gt_instance")
+            dt_instance = instances.get("dt_instance")
 
+            if None in [gt_instance, dt_instance]:
+                Writer.logger(
+                    "VisionPack Trial Expired. Please use a licensed version" + 
+                    " for complete validation. Contact support@au-zone.com" +
+                    " for more information.", code="WARNING")
+                break
+
+            # Store all unfiltered detections. 
+            key = path.basename(gt_instance.get('image_path'))
+            original_instance = {
+                'gt_instance': {
+                    "height": gt_instance.get("height"),
+                    "width": gt_instance.get("width"),
+                    "boxes": gt_instance.get("boxes"),
+                    "labels": gt_instance.get("labels")
+                },
+                'dt_instance': {
+                    "boxes": dt_instance.get("boxes"),
+                    "labels": dt_instance.get("labels"),
+                    "scores": dt_instance.get("scores")
+                }
+            }
+            self.datacollection.capture_all_class(
+                original_instance.get("gt_instance").get("labels"))
+            self.datacollection.capture_all_class(
+                original_instance.get("dt_instance").get("labels"))
+            self.datacollection.append_instance(key, original_instance)
+            
+            # Filter detections only for valid scores.
+            dt_boxes, dt_labels, scores = filter_dt(
+                dt_instance.get("boxes"), 
+                dt_instance.get("labels"), 
+                dt_instance.get("scores"), 
+                score_threshold)
+            instances['dt_instance']['boxes'] = dt_boxes
+            instances['dt_instance']['labels'] = dt_labels
+            instances['dt_instance']['scores'] = scores
+
+            if self.clamp_box:
+                instances = clamp_boxes(instances, self.clamp_box)
+            if self.ignore_box:
+                instances = ignore_boxes(instances, self.ignore_box)
+
+            self.datacollection.capture_class(dt_instance.get("labels"))
+            self.datacollection.capture_class(gt_instance.get("labels"))
+            stats = match_gt_dt(
+                gt_instance.get("boxes"), 
+                dt_instance.get("boxes"), 
+                self.parameters.get("metric"))
             self.datacollection.categorize(
                 *stats,
-                gt_labels=gt_labels,
-                dt_labels=dt_labels,
-                scores=scores)
+                gt_labels=gt_instance.get("labels"),
+                dt_labels=dt_instance.get("labels"),
+                scores=dt_instance.get("scores"))
 
             if self.visualize or self.tensorboardwriter:
                 original_image = instances.get('gt_instance').get('image')
                 image = self.drawer.draw_with_pillow(
                     iou_threshold, score_threshold,
-                    original_image, instances, *stats
-                )
+                    original_image, instances, *stats)
 
                 if self.display >= 0:
                     if self.counter < self.display:
                         self.counter += 1
                     else:
                         continue
 
@@ -376,117 +428,177 @@
                     instances.get('gt_instance').get('image_path'))))
             elif self.tensorboardwriter:
                 nimage = np.asarray(image)
                 self.tensorboardwriter(nimage, instances.get(
                     'gt_instance').get('image_path'))
             else:
                 continue
-
         return self.datacollection.sum_outcomes(iou_threshold, score_threshold)
 
     def conclude(self, truth_values, epoch=0):
         """
         This method computes the final metrics, draws the final plots, and
         saves the results either to tensorboard or to the local machine.
         Unit-test for this method is defined under:
             file: test/deepview/validator/evaluators/test_detectionevaluator.py
             function: test_conclude
 
         Parameters
         ----------
-
-            truth values: int
+            truth values: list
                 total_tp, total_fn, total_class_fp, total_local_fp
 
             epoch: int
                 This is the training epoch number. This
                 parameter is internal for modelpack usage.
                 Default validation has no use of this parameter.
 
         Returns
         -------
             summary: dict
+                The validation summary.
+                See README.md (Method Parameters Format) for more information.
+                
+        Raises
+        ------
+            None
+        """
+        
+        metrics = DetectionMetrics(self.datacollection)
+        if self.runner is not None:
+            timings = self.runner.summarize()
+            self.parameters["engine"] = self.runner.device
+            self.parameters["maximum_detections"] = self.runner.max_detections
+            self.parameters["nms"] = self.runner.nms_type
+        else:
+            timings = None
+
+        # Grab the validation metrics
+        overall_metrics = metrics.compute_overall_metrics(*truth_values)
+        truth_values.append(self.datacollection.total_gt)
+        mean_metrics, class_histogram_data = metrics.compute_detection_metrics(
+            self.parameters.get("validation-threshold"))
+        false_positive_ratios = metrics.get_fp_error(
+            self.parameters.get("validation-threshold"))
+        precision_recall_data = metrics.get_pr_data(
+            self.parameters.get("validation-threshold"), 
+            self.parameters.get("validation-iou")
+        )
+        
+        # Contain the metrics inside a dictionary
+        summary = self.create_summary(
+            truth_values=truth_values,
+            overall_metrics=overall_metrics,
+            mean_metrics=mean_metrics,
+            false_positive_ratios=false_positive_ratios,
+            timings=timings)
+
+        # Get, save, or publish plots
+        if self.visualize or self.tensorboardwriter:
+            plots = self.get_plots(
+                summary.get("model"), 
+                class_histogram_data, 
+                precision_recall_data)
+            
+        if self.visualize:
+            self.save_plots_disk(plots)
+        elif self.tensorboardwriter:
+            self.publish_plots(plots, summary.get("model"), epoch)
+
+        if self.full_summary or self.json_out:
+            summary["class_histogram_data"] = class_histogram_data
+            summary["precision_recall_data"] = precision_recall_data
+            if self.json_out:
+                self.save_jsonsummary(summary)
+            
+        if self.tensorboardwriter:
+            self.publish_metrics(summary, epoch)
+        else:
+            header, format_summary, format_timings = self.consolewriter(
+                message=deepcopy(summary),
+                parameters=self.parameters,
+                validation_type="detection")
+            
+            if self.visualize:
+                self.save_metrics_disk(header, format_summary, format_timings)    
+        return summary
+    
+    def create_summary(
+            self, 
+            truth_values, 
+            overall_metrics, 
+            mean_metrics, 
+            false_positive_ratios, 
+            timings
+        ):
+        """
+        This method creates a summary dictionary containing all the collected
+        validation metrics.
+
+        Parameters
+        ----------
+            truth_values: list
+                total_tp, total_fn, total_class_fp, total_local_fp, total_gt
+            
+            overall_metrics: list
+                This contains overall precision, overall recall,
+                    and overall accuracy.
+
+            mean_metrics: list
+                This contains mean average precision, recall, and accuracy
+                at IoU threshold 0.5, 0.75, and 0.5-0.95
+
+            false_positive_ratio: list
+                This contains false positive ratios for
+                IoU thresholds (0.5, 0.75, 0.5-0.95).
+
+            timings: dict
 
             .. code-block:: python
 
                 {
-                    "model": Name of the model,
-                    "engine": Engine used (npu, cpu, gpu),
-                    "dataset": Name of the dataset,
-                    "numgt": Number of ground truths in the dataset,
-                    "Total TP": Total true positives,
-                    "Total FN": Total false negatives,
-                    "Total Class FP": Total classification false positives,
-                    "Total Loc FP": Total localization false positives,
-                    "OP": Overall precision,
-                    "mAP": {'0.5': mAP at 0.5 IoU threshold,
-                            '0.75': mAP at 0.75 IoU threshold,
-                            '0.5:0.95': mAP across 0.5-0.95 IoU thresholds
-                        },
-                    "OR": Overall recall,
-                    "mAR": {'0.5': mAR at 0.5 IoU threshold,
-                            '0.75': mAR at 0.75 IoU threshold,
-                            '0.5:0.95': mAR across 0.5-0.95 IoU thresholds
-                        },
-                    "OA": Overall Accuracy,
-                    "mACC": {'0.5': mACC at 0.5 IoU threshold,
-                            '0.75': mACC at 0.75 IoU threshold,
-                            '0.5:0.95': mACC across 0.5-0.95 IoU thresholds
-                            },
-                    "LocFPErr": {'0.5': localization false positive
-                                        ratio at 0.5 IoU threshold,
-                                '0.75': localization false positive
-                                        ratio at 0.75 IoU threshold,
-                                '0.5:0.95': localization false positive
-                                        ratio across 0.5-0.95 IoU thresholds
-                                },
-                    "ClassFPErr": { '0.5': classification false positive
-                                        ratio at 0.5 IoU threshold,
-                                    '0.75': classification false positive
-                                        ratio at 0.75 IoU threshold,
-                                    '0.5:0.95': classification false positive
-                                        ratio across 0.5-0.95 IoU thresholds
-                                },
-                    "timings": timings
-                            (input, inference, decode at min/max/avg)
+                 'min_inference_time': minimum time to produce bounding boxes,
+                 'max_inference_time': maximum time to produce bounding boxes,
+                 'min_input_time': minimum time to load an image,
+                 'max_input_time': maximum time to load an image,
+                 'min_decoding_time': minimum time to process model
+                                    predictions,
+                 'max_decoding_time': maximum time to process model
+                                    predictions,
+                 'avg_decoding': average time to process model predictions,
+                 'avg_input': average time to load an image,
+                 'avg_inference': average time to produce bounding boxes,
                 }
 
+        Returns
+        -------
+            summary: dict
+                The validation summary.
+                See README.md (Method Parameters Format) for more information.
+
         Raises
         ------
             None
+            
         """
-        summary=dict()
-        iou_threshold = self.parameters.get("validation-iou")
-        score_threshold = self.parameters.get("validation-threshold")
-
-        metrics = DetectionMetrics(detectiondatacollection=self.datacollection)
-        timings = self.runner.summarize()
-        self.parameters["engine"] = self.runner.device
 
         try:
             model_name = path.basename(path.normpath(self.runner.source))
         except AttributeError:
             model_name = "Training Model"
 
         try:
             dataset_name = path.basename(path.normpath(self.dataset.source))
         except AttributeError:
             dataset_name = "Validation Dataset"
 
-        overall_metrics = metrics.compute_overall_metrics(*truth_values)
-        mean_metrics, class_histogram_data = metrics.compute_detection_metrics(
-            score_threshold)
-        false_positive_ratios = metrics.get_fp_error(score_threshold)
-        precision_recall_data = metrics.get_pr_data(
-            score_threshold=score_threshold, iou_threshold=iou_threshold)
-
         summary = {
             "model": model_name,
             "dataset": dataset_name,
-            "numgt": self.datacollection.total_gt,
+            "numgt": truth_values[-1],
             "Total TP": truth_values[0],
             "Total FN": truth_values[1],
             "Total Class FP": truth_values[2],
             "Total Loc FP": truth_values[3],
             "OP": overall_metrics[0],
             "mAP": {'0.5': mean_metrics[0][0],
                     '0.75': mean_metrics[0][1],
@@ -508,142 +620,222 @@
                          },
             "ClassFPErr": {'0.5': false_positive_ratios[1],
                            '0.75': false_positive_ratios[3],
                            '0.5:0.95': false_positive_ratios[5]
                            },
             "timings": timings
         }
+        return summary
+    
+    @staticmethod
+    def get_plots(model_name, class_histogram_data, precision_recall_data):
+        """
+        This method gets the validation plots based on the data recieved.
 
-        if self.visualize or self.tensorboardwriter:
-            fig_class_metrics = Drawer.plot_classification(
-                class_histogram_data, model=model_name)
-            fig_prec_rec_curve = Drawer.plot_pr_curve(
-                precision_recall_data.get("recall"),
-                precision_recall_data.get("precision"),
-                precision_recall_data.get("average-precision"),
-                names=precision_recall_data.get("names"),
-                model=model_name
-            )
-            fig_f1_curve = Drawer.plot_mc_curve(
-                precision_recall_data.get("x-data"),
-                precision_recall_data.get("f1"),
-                names=precision_recall_data.get("names"),
-                ylabel='F1',
-                model=model_name
-            )
-            fig_prec_confidence_curve = Drawer.plot_mc_curve(
-                precision_recall_data.get("x-data"),
-                precision_recall_data.get("precision-confidence"),
-                names=precision_recall_data.get("names"),
-                ylabel='Precision',
-                model=model_name
-            )
-            fig_rec_confidence_curve = Drawer.plot_mc_curve(
-                precision_recall_data.get("x-data"),
-                precision_recall_data.get("recall-confidence"),
-                names=precision_recall_data.get("names"),
-                ylabel='Recall',
-                model=model_name
-            )
+        Parameters
+        ----------
+            model_name: str
+                The name of the model being validated.
 
-        if self.json_out:
-            import json
-            summary["class_histogram_data"] = class_histogram_data
-            reformat_pr_data = dict()
-            for key, value in precision_recall_data.items():
-                if isinstance(value, (list, np.ndarray)):
-                    tmp_value = list()
-                    for x in value:
-                        if isinstance(x, np.ndarray):
-                            tmp_value.append(x.tolist())
-                        else:
-                            tmp_value.append(x)
-                elif isinstance(value, (np.int32)):
-                    tmp_value = int(value)
-                else:
-                    pass
+            class_histogram_data: dict
+                This contains the number of true positives,
+                false positives, and false negatives and
+                aswell as precision, recall, and accuracy at
+                IoU threshold 0.5 to plot as a histogram.
 
-                reformat_pr_data[key] = tmp_value
-            summary["precision_recall_data"] = reformat_pr_data
-            
-            # This is for debugging purposes...
-            # self.print_types(summary)
-            with open(self.json_out, 'w', encoding='utf-8') as fp:
-                json.dump(summary, fp, ensure_ascii=False, indent=4)
+            precision_recall_data: dict
+                .. code-block:: python
 
-        if self.visualize:
-            fig_class_metrics.savefig(
-                f'{self.save_path}/class_scores.png',
-                bbox_inches="tight")
-            fig_prec_rec_curve.savefig(
-                f'{self.save_path}/prec_rec_curve.png',
-                bbox_inches='tight')
-            fig_f1_curve.savefig(
-                f'{self.save_path}/f1_curve.png',
-                bbox_inches='tight')
-            fig_prec_confidence_curve.savefig(
-                f'{self.save_path}/precision_confidence_curve.png',
-                bbox_inches='tight')
-            fig_rec_confidence_curve.savefig(
-                f'{self.save_path}/rec_confidence_curve.png',
-                bbox_inches='tight')
-            
-            self.drawer.close_figures([fig_class_metrics, fig_prec_rec_curve,
-                            fig_f1_curve, fig_prec_confidence_curve,
-                            fig_rec_confidence_curve])
+                    {
+                        "precision"
+                        "recall"
+                        "average-precision"
+                        "x-data"
+                        "f1"
+                        "precision-confidence"
+                        "recall-confidence"
+                        "names"
+                    }
 
-        elif self.tensorboardwriter:
-            nimage_class = Drawer.figure2numpy(fig_class_metrics)
-            nimage_precision_recall = Drawer.figure2numpy(fig_prec_rec_curve)
-            nimage_precision_confidence = Drawer.figure2numpy(
-                fig_prec_confidence_curve)
-            nimage_recall_confidence = Drawer.figure2numpy(
-                fig_rec_confidence_curve)
-            nimage_f1_curve = Drawer.figure2numpy(fig_f1_curve)
-
-            self.tensorboardwriter(
-                nimage_class,
-                f"{summary.get('model')}_scores.png",
-                step=epoch)
-            self.tensorboardwriter(
-                nimage_precision_recall,
-                f"{summary.get('model')}_precision_recall.png",
-                step=epoch)
-            self.tensorboardwriter(
-                nimage_precision_confidence,
-                f"{summary.get('model')}_precision_confidence.png",
-                step=epoch)
-            self.tensorboardwriter(
-                nimage_recall_confidence,
-                f"{summary.get('model')}_recall_confidence.png",
-                step=epoch)
-            self.tensorboardwriter(
-                nimage_f1_curve,
-                f"{summary.get('model')}_f1.png",
-                step=epoch)
-            
-            self.drawer.close_figures([fig_class_metrics, fig_prec_rec_curve,
-                            fig_f1_curve, fig_prec_confidence_curve,
-                            fig_rec_confidence_curve])
+        Returns
+        -------
+            plots: list 
+                This contains [fig_class_metrics, fig_prec_rec_curve, 
+                fig_f1_curve, fig_prec_confidence_curve, 
+                fig_rec_confidence_curve]
 
-        if self.tensorboardwriter:
-            self.tensorboardwriter.publish_metrics(
-                message=deepcopy(summary),
-                parameters=self.parameters,
-                step=epoch,
-                validation_type="detection")
-        else:
-            header, format_summary, format_timings = self.consolewriter(
-                message=deepcopy(summary),
-                parameters=self.parameters,
-                validation_type="detection"
-            )
+        Raises
+        ------
+            None
+        """
 
-            if self.visualize:
-                with open(self.save_path + '/metrics.txt', 'w') as fp:
-                    fp.write(header + '\n')
-                    fp.write(format_summary + '\n')
-                    if timings is not None:
-                        fp.write(format_timings)
-                fp.close()
+        fig_class_metrics = plot_classification(
+            class_histogram_data, model=model_name)
+        fig_prec_rec_curve = plot_pr(
+            precision_recall_data.get("precision"),
+            precision_recall_data.get("recall"),
+            precision_recall_data.get("average precision"),
+            precision_recall_data.get("names"))
+        return [fig_class_metrics, fig_prec_rec_curve]
+
+    def save_plots_disk(self, plots):
+        """
+        This method saves the plots locally on disk.
+
+        Parameters
+        ----------
+            plots: list
+                [fig_class_metrics, fig_prec_rec_curve, fig_f1_curve, 
+                fig_prec_confidence_curve, fig_rec_confidence_curve]
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None
+        """
+
+        plots[0].savefig(
+            f'{self.save_path}/class_scores.png',
+            bbox_inches="tight")
+        plots[1].savefig(
+            f'{self.save_path}/prec_rec_curve.png',
+            bbox_inches='tight')
+        close_figures(plots)
+
+    def publish_plots(self, plots, model_name="Training Model", epoch=0):
+        """
+        This method publishes the plots onto tensorboard.
+
+        Parameters
+        ----------
+            plots: list
+                [fig_class_metrics, fig_prec_rec_curve, fig_f1_curve, 
+                    fig_prec_confidence_curve, fig_rec_confidence_curve]
+
+            model_name: str
+                The name of the model being validated.
+
+            epoch: int
+                The epoch number if it is a training model. 
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None
+        """
+
+        nimage_class = figure2numpy(plots[0])
+        nimage_precision_recall = figure2numpy(plots[1])
+
+        self.tensorboardwriter(
+            nimage_class,
+            f"{model_name}_scores.png",
+            step=epoch)
+        self.tensorboardwriter(
+            nimage_precision_recall,
+            f"{model_name}_precision_recall.png",
+            step=epoch)
+        close_figures(plots)
+
+    def save_metrics_disk(self, header, format_summary, format_timings):
+        """
+        This method saves the validation metrics onto a text file.
+
+        Parameters
+        ----------
+            header: str
+                The title of the validation metrics.
+
+            format_summary: str
+                The formatted validation showing the metrics.
+
+            format_timings: str
+                The formatted timings summary.
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None
+        """
+
+        with open(self.save_path + '/metrics.txt', 'w') as fp:
+            fp.write(header + '\n')
+            fp.write(format_summary + '\n')
+            if format_timings is not None:
+                fp.write(format_timings)
+            fp.close()
+
+    def publish_metrics(self, summary, epoch=0):
+        """
+        This method publishes the metrics onto tensorboard.
+
+        Parameters
+        ----------
+            summary: dict
+                The validation summary.
+                See README.md (Method Parameters Format) for more information.
+
+            epoch: int
+                The epoch number when training a model.
 
-        return summary
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None 
+        """
+        self.tensorboardwriter.publish_metrics(
+            message=deepcopy(summary),
+            parameters=self.parameters,
+            step=epoch,
+            validation_type="detection")
+
+    def save_jsonsummary(self, summary):
+        """
+        This method saves the summary dictionary as a JSON file.
+
+        Parameters
+        ----------
+            summary: dict
+                The validation summary.
+                See README.md (Method Parameters Format) for more information.
+        
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None
+        """
+        import json
+        reformat_pr_data = dict()
+        for key, value in summary["precision_recall_data"].items():
+            if isinstance(value, (list, np.ndarray)):
+                tmp_value = list()
+                for x in value:
+                    if isinstance(x, np.ndarray):
+                        tmp_value.append(x.tolist())
+                    else:
+                        tmp_value.append(x)
+            elif isinstance(value, (np.int32)):
+                tmp_value = int(value)
+            else:
+                pass
+            reformat_pr_data[key] = tmp_value
+        summary["precision_recall_data"] = reformat_pr_data
+        # This is for debugging purposes...
+        # self.print_types(summary)
+        with open(self.json_out, 'w', encoding='utf-8') as fp:
+            json.dump(summary, fp, ensure_ascii=False, indent=4)
+            fp.close()
```

## deepview/validator/evaluators/segmentationevaluator.py

```diff
@@ -3,16 +3,18 @@
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
 from deepview.validator.metrics.segmentationutils import create_mask_image, \
-    create_mask_class, create_mask_background, \
-    classify_mask, create_mask_classes
+    create_mask_class, create_mask_background, classify_mask, \
+    create_mask_classes
+from deepview.validator.visualize.utils import plot_pr, plot_classification, \
+    figure2numpy, close_figures
 from deepview.validator.visualize import Drawer, SegmentationDrawer
 from deepview.validator.metrics import SegmentationDataCollection, \
     SegmentationMetrics
 from deepview.validator.evaluators.core import Evaluator
 from deepview.validator.datasets.core import Dataset
 from copy import deepcopy
 from os import path
@@ -32,15 +34,14 @@
         5) Calculate the metrics.
 
     Unit-test for this class is defined under:
         file: test/deepview/validator/evaluators/test_segmentationevaluator.py
 
     Parameters
     ----------
-
         runner: Runner object depending on the model.
             This object provides methods to run the detection model.
 
         dataset: Dataset object depending on the dataset.
             This object provides methods to read and parse the dataset.
 
         datacollection: DetectionDataCollection
@@ -61,29 +62,16 @@
             summary.
 
         display: int
             This is the number of images to save. By default it is -1
             specifying all the images.
 
         parameters: dict
-            The model parameters:
-
-            .. code-block:: python
-
-                {
-                    "validation-iou": args.validation_iou,
-                    "detection-iou": args.detection_iou,
-                    "validation-threshold": args.validation_threshold,
-                    "detection-threshold": args.detection_threshold,
-                    "nms": args.nms_type,
-                    "normalization": args.norm,
-                    "maximum_detections": args.max_detection,
-                    "warmup": args.warmup
-                }
-
+            The model parameters.
+            See README.md (Method Parameters Format) for more information.
             This does not apply to segmentation, therefore it is set as None.
 
     Raises
     -------
         ValueError
             This will raise the exception if the provided parameters
             in certain methods does not conform to the specified data type
@@ -114,20 +102,19 @@
 
     def __call__(self, val_messenger):
         """
         This method allows tensorboardformatter object in trainer
         to operate here and resets the metrics after each epoch.
         Default validation has no use of this method.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/evaluators/test_segmentationevaluator.py
+            file: test/test_segmentationevaluator.py
             function: test__call__.py
 
         Parameters
         ----------
-
             val_messenger: TrainingTensorBoardWriter
                 This object is internal for modelpack that was instantiated
                 specifically for training a model.
 
         Returns
         -------
             None
@@ -149,44 +136,27 @@
         model predictions (dt_mask, dt_labels)
         for each image.
 
 
         This method collects the instances from the ground truth
         and the model segmentations.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/evaluators/test_segmentationevaluator.py
+            file: test/test_segmentationevaluator.py
             function: test_instance_collector
 
         Parameters
         ----------
             None
 
         Returns
         -------
-
             instances: dict
                 This yields one image instance from the ground
                 truth and the model predictions.
-
-                .. code-block:: python
-
-                    {
-                        'gt_instance': {
-                            'image': image numpy array,
-                            'height': height,
-                            'width': width,
-                            'gt_mask': ground truth mask of the image,
-                            'labels': list of labels,
-                            'image_path': image_path
-                        },
-                        'dt_instance': {
-                            'dt_mask': model mask of the image,
-                            'labels': list of prediction labels,
-                        }
-                    }
+                See README.md (Method Parameters Format) for more information.
 
         Raises
         ------
             None
         """
 
         for gt_instance in self.dataset.read_all_samples():
@@ -212,39 +182,22 @@
                 'gt_instance': gt_instance
             }
 
     def single_evaluation(self, instance, labels, epoch=0, add_image=True):
         """
         This method runs validation on a single instance.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/evaluators/test_segmentationevaluator.py
+            file: test/test_segmentationevaluator.py
             function: test_single_evaluation
 
         Parameters
         ----------
-
             instance: dict
-                The ground truth and the predictions instances:
-
-                .. code-block:: python
-
-                    instance = {
-                            'gt_instance': {
-                                'image': image numpy array,
-                                'height': height,
-                                'width': width,
-                                'gt_mask': ground truth mask of the image,
-                                'labels': list of labels,
-                                'image_path': image_path
-                            },
-                            'dt_instance': {
-                                'dt_mask': model mask of the image,
-                                'labels': list of prediction labels,
-                            }
-                        }
+                The ground truth and the predictions instances.
+                See README.md (Method Parameters Format) for more information.
 
             epoch: int
                 This is the training epoch number. This
                 parameter is internal for modelpack usage.
                 Default validation has no use of this parameter.
 
             add_image: bool
@@ -319,15 +272,15 @@
         return self.datacollection.sum_outcomes()
 
     def group_evaluation(self):
         """
         This method performs the semantic segmentation
         evaluation of all images.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/evaluators/test_segmentationevaluator.py
+            file: test/test_segmentationevaluator.py
             function: test_group_evaluation
 
         Parameters
         ----------
             None
 
         Returns
@@ -337,15 +290,14 @@
 
         Raises
         ------
             None
         """
 
         self.counter = 0
-        #height, width = self.dataset.shape[0], self.dataset.shape[1]
         self.drawer = SegmentationDrawer()
 
         for instance in self.instance_collector():
 
             gt_labels = instance.get('gt_instance').get('labels')
             dt_labels = instance.get('dt_instance').get('labels')
             class_labels = np.unique(gt_labels + dt_labels)
@@ -413,51 +365,32 @@
         return self.datacollection.sum_outcomes()
 
     def conclude(self, truth_values, epoch=0):
         """
         This method computes the final metrics, draws the class histogram, and
         saves the results either to tensorboard or to the local machine.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/evaluators/test_segmentationevaluator.py
+            file: test/test_segmentationevaluator.py
             function: test_conclude
 
         Parameters
         ----------
-
             truth values: int
                 total_tp, total_fn, total_fp
 
             epoch: int
                 This is the training epoch number. This
                 parameter is internal for modelpack usage.
                 Default validation has no use of this parameter.
 
         Returns
         -------
             summary: dict
-
-            .. code-block:: python
-
-                {
-                    "model": Name of the model,
-                    "engine": Engine used (npu, cpu, gpu),
-                    "dataset": Name of the dataset,
-                    "numgt": Number of ground truths in the dataset,
-                    "Total TP": Total true positives,
-                    "Total FN": Total false negatives,
-                    "Total FP": Total false positives,
-                    "OP": Overall precision,
-                    "OR": Overall recall,
-                    "OA": Overall Accuracy,
-                    "mAP": Mean average precision,
-                    "mAR": Mean average recall,
-                    "mACC": Mean average accuracy,
-                    "timings": timings
-                    (input, inference, decode at min/max/avg)
-                }
+                The validation summary
+                See README.md (Method Parameters Format) for more information.
 
         Raises
         ------
             None
         """
 
         metrics = SegmentationMetrics(
@@ -472,15 +405,16 @@
             model_name = "Training Model"
 
         try:
             dataset_name = path.basename(path.normpath(self.dataset.source))
         except AttributeError:
             dataset_name = "Validation Dataset"
 
-        mean_metrics, class_histogram_data = metrics.compute_segmentation_metrics()
+        mean_metrics, class_histogram_data =\
+              metrics.compute_segmentation_metrics()
         overall_metrics = metrics.compute_overall_metrics(*truth_values)
 
         summary = {
             "model": model_name,
             "engine": self.runner.device,
             "dataset": dataset_name,
             "numgt": int(self.datacollection.total_gt),
@@ -493,29 +427,29 @@
             "mAP": mean_metrics[0],
             "mAR": mean_metrics[1],
             "mACC": mean_metrics[2],
             "timings": timings
         }
 
         if self.visualize or self.tensorboardwriter:
-            fig_class_metrics = Drawer.plot_classification(
+            fig_class_metrics = plot_classification(
                 class_histogram_data, model=model_name)
 
         if self.visualize:
             fig_class_metrics.savefig(
                 f'{self.save_path}/class_scores.png',
                 bbox_inches="tight")
-            self.drawer.close_figures([fig_class_metrics])
+            close_figures([fig_class_metrics])
         elif self.tensorboardwriter:
-            nimage_class = Drawer.figure2numpy(fig_class_metrics)
+            nimage_class = figure2numpy(fig_class_metrics)
             self.tensorboardwriter(
                 nimage_class,
                 f"{summary.get('model')}_scores.png",
                 step=epoch)
-            self.drawer.close_figures([fig_class_metrics])
+            close_figures([fig_class_metrics])
             
         if self.json_out:
             import json
             summary["class_histogram_data"] = class_histogram_data
             
             with open(self.json_out, 'w') as fp:
                 json.dump(summary, fp)
@@ -531,9 +465,8 @@
             if self.visualize:
                 with open(self.save_path + '/metrics.txt', 'w') as fp:
                     fp.write(header + '\n')
                     fp.write(format_summary + '\n')
                     if timings is not None:
                         fp.write(format_timings)
                 fp.close()
-
-        return summary
+        return summary
```

## deepview/validator/metrics/__init__.py

```diff
@@ -2,14 +2,15 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
-from deepview.validator.metrics.segmentationdata import SegmentationDataCollection
+from deepview.validator.metrics.segmentationdata import \
+    SegmentationDataCollection
 from deepview.validator.metrics.segmentationmetrics import SegmentationMetrics
 from deepview.validator.metrics.segmentationdata import SegmentationLabelData
 from deepview.validator.metrics.detectiondata import DetectionDataCollection
 from deepview.validator.metrics.detectionmetrics import DetectionMetrics
 from deepview.validator.metrics.detectiondata import DetectionLabelData
 from deepview.validator.metrics.core import Metrics
```

## deepview/validator/metrics/core.py

```diff
@@ -15,15 +15,15 @@
     This class is an abstract class that provides a template for the
     metric computations.
     Unit-test for this class is defined under:
         file: test/test_metrics.py
 
     Parameters
     ----------
-    None
+        None
 
     Raises
     ------
         DivisionByZeroException
             This will raise the exception if a ZeroDivisionError is caught.
 
         ValueError
@@ -35,73 +35,72 @@
 
     def __init__(
         self
     ):
         pass
 
     @staticmethod
-    def validate_threshold(threshold, eps=1e-10):
+    def validate_threshold(threshold, min=0., max=1.):
         """
         The method validates the threshold to be a floating type
         and does not exceed defined bounds (0...1).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_validate_threshold
 
         Parameters
         ----------
-
             threshold: float
                 The threshold to validate.
 
+            min: float
+                The minimum acceptable threshold.
+
+            max: float
+                The maximum acceptable threshold.
+
         Returns
         -------
             threshold: float
                 The validated threshold.
 
         Raises
         ------
             ValueError
                 This method will raise the exception if the provided threshold
-                is not floating point type or it is out bounds
-                (less than 0 or greater than 1).
+                is not floating point type.
         """
 
         if not (isinstance(threshold, (float, np.float32))):
             raise ValueError(
                 "The provided threshold is not of numeric type: float. " +
                 "Provided with: {}".format(
                     type(threshold)))
-        if (threshold <= 0 or threshold >= 1. + eps):
-            raise ValueError(
-                "The provided threshold is out of bounds: {}. ".format(
-                    threshold) + "This value can only be between 0 and 1")
-        else:
-            return threshold
+        return \
+            min if threshold < min else max if threshold > max else threshold
 
     @staticmethod
     def divisor(num, denom, info='Basic Division'):
         """
         This method performs basic division operations for ratio metrics.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_divisor
 
         Parameters
         ----------
+            num: int, float, complex
+                This is the numerator in the division.
 
-        num: int, float, complex
-            This is the numerator in the division.
+            denom: int, float, complex
+                This is the denominator in the division.
 
-        denom: int, float, complex
-            This is the denominator in the division.
-
-        info: str
-            This is the description of the operation.
-            i.e what is being calculated?
+            info: str
+                This is the description of the operation.
+                i.e what is being calculated?
 
         Returns
         -------
             The division result: int, float, complex
                 Resulting value is the result when num/denom is performed.
 
         Raises
@@ -155,20 +154,19 @@
         This method calculates the precision = tp/(tp+fp).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_compute_precision
 
         Parameters
         ----------
+            tp: int
+                The number of true positives.
 
-        tp: int
-            The number of true positives.
-
-        fp: int
-            The number of false positives.
+            fp: int
+                The number of false positives.
 
         Returns
         -------
             Precision score: float
                 Resulting value is the result of tp/(tp+fp).
 
         Raises
@@ -217,20 +215,19 @@
         This method calculates the recall = tp/(tp+fn).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_compute_recall
 
         Parameters
         ----------
+            tp: int
+                The number of true positives.
 
-        tp: int
-            The number of true positives.
-
-        fn: int
-            The number of false negatives.
+            fn: int
+                The number of false negatives.
 
         Returns
         -------
             Recall score: float
                 Resulting value is the result of tp/(tp+fn).
 
         Raises
@@ -279,23 +276,22 @@
         This method calculates the accuracy = tp/(tp+fp+fn).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_compute_accuracy
 
         Parameters
         ----------
+            tp: int
+                The number of true positives.
 
-        tp: int
-            The number of true positives.
-
-        fp: int
-            The number of false positives.
+            fp: int
+                The number of false positives.
 
-        fn: int
-            The number of false negatives.
+            fn: int
+                The number of false negatives.
 
         Returns
         -------
             Accuracy score: float
                 Resulting value is the result of tp/(tp+fp+fn).
 
         Raises
```

## deepview/validator/metrics/detectiondata.py

```diff
@@ -35,115 +35,158 @@
 
         # A list containing the DetectionLabelData objects for each label.
         self.label_data_list = list()
         # Total number of ground truths in the dataset.
         self.total_gt = 0
         # A list containing the strings of unique labels.
         self.labels = list()
+        # All labels captured with threshold set low
+        self.all_labels=list()
+        # container for all ground truth and detections.
+        self.instances = dict()
+
+    def append_instance(self, key, instance):
+        """
+        This method appends an instance inside the instances container.
+
+        Parameters
+        ----------
+            key: str
+                The name to point to the instance.
+
+            instance: dict
+
+                .. code-block:: python
+
+                    {
+                        'gt_instance': {
+                            'height': height,
+                            'width': width,
+                            'boxes': list bounding boxes,
+                            'labels': list of labels
+                        },
+                        'dt_instance': {
+                            'boxes': list of prediction bounding boxes,
+                            'labels': list of prediction labels,
+                            'scores': list of confidence scores
+                        }
+                    }
+        
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None
+        """
+
+        self.instances[key] = instance
+
+    def get_instances(self):
+        """
+        This method returns the instances for all captured detections and
+        all captured ground truth.
+        """
+        return self.instances
 
     @staticmethod
     def validate_score(score, min=0., max=1.):
         """
         The method validates the confidence score or the
         score threshold to be a floating type and does not
         exceed defined bounds (0...1).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_validate_score_iou
 
         Parameters
         ----------
-
             score: float
                 The score to validate.
 
-        Returns
-        -------
-            score: float
-                The validated score.
-
             min: float
                 The minimum acceptable score.
 
             max: float
                 The maximum acceptable score.
 
+        Returns
+        -------
+            score: float
+                The validated score.
+
         Raises
         ------
             ValueError
                 This method will raise the exception if the provided score
                 is not floating point type or it is out bounds
                 (less than 0 or greater than 1).
         """
 
         if not isinstance(score, (float, np.float32)):
             raise ValueError(
                 "The provided score is not of numeric type: float. " +
                 "Provided with type: {}".format(
                     type(score)))
-        
         return min if score < min else max if score > max else score
 
     @staticmethod
-    def validate_iou(iou, eps=1e-10):
+    def validate_iou(iou, min=0., max=1.):
         """
         The method validates the IoU score or the
         IoU threshold to be a floating type and does not
         exceed defined bounds (0...1).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_validate_score_iou
 
         Parameters
         ----------
-
             iou: float
                 The iou to validate.
 
+            min: float
+                The minimum acceptable iou.
+
+            max: float
+                The maximum acceptable iou.
+
         Returns
         -------
             iou: float
                 The validated IoU.
 
-            eps: float
-                Invalid IoU leniency.
-
         Raises
         ------
             ValueError
                 This method will raise the exception if the provided IoU
                 is not floating point type or it is out bounds
                 (less than 0 or greater than 1).
         """
 
         if not isinstance(iou, (float, np.float32)):
             raise ValueError(
                 "The provided iou is not of numeric type: float. " +
                 "Provided with iou: {}".format(
-                    type(iou)))
-        if (iou < 0 or iou > 1. + eps):
-            raise ValueError(
-                "The provided iou is out of bounds: {}. ".format(iou) +
-                "This value can only be between 0 and 1")
-        else:
-            return iou
+                    type(iou))) 
+        return min if iou < min else max if iou > max else iou
 
     def add_gt(self, num=1):
         """
         This method adds the number of recorded ground truths
         of the dataset.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_add_get_gt
 
         Parameters
         ----------
-
-        num: int
-            The number of ground truths to add.
+            num: int
+                The number of ground truths to add.
 
         Returns
         -------
             None
 
         Raises
         ------
@@ -193,18 +236,17 @@
         This method adds DetectionLabelData object per label.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_add_get_label_data
 
         Parameters
         ----------
-
-        label: str or int
-            The string label or the integer index
-            to place as a data container.
+            label: str or int
+                The string label or the integer index
+                to place as a data container.
 
         Returns
         -------
             None
 
         Raises
         ------
@@ -223,17 +265,16 @@
         This method grabs the DetectionLabelData object by label.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_add_get_label_data
 
         Parameters
         ----------
-
-        label: str or int
-            A unique string label or integer index from the dataset.
+            label: str or int
+                A unique string label or integer index from the dataset.
 
         Returns
         -------
             None if the object does not exist.
 
             label_data: DetectionLabelData
                 The data container of the label specified.
@@ -315,27 +356,57 @@
         Raises
         ------
             None
         """
 
         self.label_data_list = []
         self.labels = []
+        self.total_gt = 0
+
+    def capture_all_class(self, labels):
+        """
+        This method records the unique labels encountered
+        in the prediction and ground truth and creates a
+        container (DetectionLabelData) for each unique label found.
+        Unit-test for this method is defined under:
+            file: test/test_metrics.py
+            function: test_capture_all_class
+
+        Parameters
+        ----------
+            labels: list() or np.ndarray
+                This list contains labels for one image.
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            None
+        """
+
+        for label in labels:
+            if isinstance(label, str):
+                if label.lower() in ["background", " ", ""]:
+                    continue
+            if label not in self.all_labels:
+                self.all_labels.append(label)
 
     def capture_class(self, labels):
         """
         This method records the unique labels encountered
         in the prediction and ground truth and creates a
         container (DetectionLabelData) for each unique label found.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_capture_class
 
         Parameters
         ----------
-
             labels: list() or np.ndarray
                 This list contains labels for one image.
 
         Returns
         -------
             None
 
@@ -367,15 +438,14 @@
         false positives, or false negatives.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function test_categorize_sum_outcomes
 
         Parameters
         ----------
-
             index_matches: list or np.ndarray
                 This contains the indices of each match
                 [[index_dt, index_gt]...].
 
             index_extra_dt: list or np.ndarray
                 This contains indices of model
                 predictions captured as local fp.
@@ -406,19 +476,16 @@
             ValueError
                 This method will raise the exception if the provided iou
                 or score does not conform to the specified data type
                 or the parameters are out of bounds. i.e. The provided iou
                 or score is greater than 1 or less than 0.
         """
 
-        self.add_gt(len(gt_labels))
-
         # Matched Predictions
         for match in index_matches:
-
             dt_label, gt_label = dt_labels[match[0]], gt_labels[match[1]]
             iou = iou_list[match[0]]
             score = scores[match[0]]
 
             if dt_label != gt_label:
                 label_data = self.get_label_data(dt_label)
                 if label_data is not None:
@@ -427,52 +494,51 @@
             label_data = self.get_label_data(gt_label)
             if label_data is not None:
                 label_data.add_gt()
                 if dt_label == gt_label:
                     label_data.add_tp(iou, score)
 
         # Extra Predictions.
-        for extra in index_extra_dt:
+        for extra in index_extra_dt: 
             dt_label = dt_labels[extra]
             score = scores[extra]
-
             label_data = self.get_label_data(dt_label)
             if label_data is not None:
                 label_data.add_local_fp(score)
 
         # Missed Predictions.
         for missed in index_missed_gt:
             gt_label = gt_labels[missed]
-
             label_data = self.get_label_data(gt_label)
             if label_data is not None:
                 label_data.add_gt()
 
+        self.add_gt(len(gt_labels))
+       
     def sum_outcomes(self, iou_threshold, score_threshold):
         """
         This method adds the total number of
         true positives, false positives (localization and classification)
         and false negatives in the dataset at the specified validation
         IoU and score thresholds.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_categorize_sum_outcomes
 
         Parameters
         ----------
-
             iou_threshold: float
                 Validation IoU threshold to consider true positives.
 
             score_threshold: float
                 Validation score threshold to filter predictions.
 
         Returns
         -------
-            outcomes: int
+            outcomes: list
                 The total number of true positives,
                 false positives (local and class), and false negatives.
 
         Raises
         ------
             ValueError
                 This method will raise the exception if the provided thresholds
@@ -488,28 +554,27 @@
             total_tp += label_data.get_tp_count(iou_threshold, score_threshold)
             total_class_fp += label_data.get_class_fp_count(
                 iou_threshold, score_threshold)
             total_local_fp += label_data.get_local_fp_count(
                 iou_threshold, score_threshold)
 
         total_fn = self.total_gt - (total_tp + total_class_fp)
-        return total_tp, total_fn, total_class_fp, total_local_fp
+        return [total_tp, total_fn, total_class_fp, total_local_fp]
 
 
 class DetectionLabelData:
     """
     This class acts a container that stores the total number
     of true positives,  false positives, false negatives per label
     and provides methods to calculate the mean average metrics.
     Unit-test for this class is defined under:
         file: test/test_metrics.py
 
     Parameters
     ----------
-
         label: str or int
             The unique string or integer index label to base the container.
 
     Raises
     ------
         ValueError
             The methods will raise an exception if the provided parameters
@@ -533,14 +598,18 @@
         self.tps = list()
         # Contains (IoU, score) values for predictions marked as classification
         # false positives
         self.class_fps = list()
         # Contains score values for predictions captured as localization false
         # positives
         self.local_fps = list()
+        
+        # The number of true positives that became localization false positives
+        # due to the IoU less than the set threshold
+        self.tp2fp = 0
 
     def get_label(self):
         """
         This method grabs the class label being evaluated.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_set_label
@@ -566,15 +635,14 @@
         This method sets the label to a different label.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_set_label
 
         Parameters
         ----------
-
             new_label: str or int
                 The label to change.
 
         Returns
         -------
             None
 
@@ -584,29 +652,28 @@
                 This method will raise the exception if the provided label is
                 neither string nor integer.
         """
 
         if not (isinstance(new_label, (str, int, np.int32, np.int64))):
             raise ValueError(
                 "The provided label has an incorrect type: {}. ".format(
-                    type(new_label)) + "Can only accept string or integer type.")
+                type(new_label)) + "Can only accept string or integer type.")
         else:
             self.label = new_label
 
     def add_gt(self, num=1):
         """
         This method adds the number of ground truth for the
         class label from the entire dataset.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_add_gt
 
         Parameters
         ----------
-
             num: int
                 The number of ground truths to add.
 
         Returns
         -------
             None
 
@@ -780,28 +847,51 @@
             None
         """
 
         if len(self.local_fps):
             return np.array(self.local_fps)
         else:
             return np.array([])
+        
+    def get_tp2fp(self):
+        """
+        This method gets the number of true positives treated as localization
+        false positives due to the IoU being less than the set threshold.
+        Unit-test for this method is defined under:
+            file: test/test_metrics.py
+            function: test_get_tp2fp
+
+        Parameters
+        ----------
+            None
+
+        Returns
+        -------
+            count: int
+                The number of these occurences
+
+        Raises
+        ------
+            None
+        """
+
+        return self.tp2fp
 
     def add_tp(self, iou, score):
         """
         This method adds the true positive prediction IoU and
         confidence score. A true positive is when the prediction
         and the ground truth label matches and the IoU is
         greater than the set IoU threshold.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_add_truth_values
 
         Parameters
         ----------
-
             iou: float
                 The IoU of the true positive prediction.
 
             score: float
                 The confidence score of the true positive prediction.
 
         Returns
@@ -828,15 +918,14 @@
         and the IoU is greater than the set IoU threshold.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_add_truth_values
 
         Parameters
         ----------
-
             iou: float
                 The IoU of the classification false positive prediction.
 
             score: float
                 The confidence score of the classification false
                 positive prediction.
 
@@ -863,15 +952,14 @@
         prediction but no ground truth.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_add_truth_values
 
         Parameters
         ----------
-
             score: float
                 The confidence score of the localization
                 false positive prediction.
 
         Returns
         -------
             None
@@ -883,25 +971,62 @@
                 is not a floating point type and is out bounds meaning it is
                 greater than 1 or less than 0.
         """
 
         score = DetectionDataCollection.validate_score(score)
         self.local_fps.append(score)
 
+    def add_tp2fp(self, iou_threshold, score_threshold):
+        """
+        The method adds the number of potential true positives that became 
+        localization false positives due to their IoU being less than the 
+        defined IoU threshold. 
+        Unit-test for this method is defined under:
+            file: test/test_metrics.py
+            function: test_add_tp2fp
+        
+        Parameters
+        ----------
+            iou_threshold: float
+                The IoU threshold set.
+
+            score_threshold: float
+                The score threshold set. 
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            ValueError
+                This method will raise the exception if the provided
+                iou_threshold and score_threshold are not floating
+                types or they are out of bounds
+                such as greater than 1 and less than 0.
+
+        """
+        
+        if len(self.tps):
+            fp_iou = np.array(self.tps)[:, 0] < iou_threshold
+            tp_score = np.array(self.tps)[:, 1] >= score_threshold
+            # These are the IoUs for those TP that are less than threshold.
+            #loc_iou = np.array(self.tps)[:, 0] * tp_score.astype(int)
+            self.tp2fp += np.count_nonzero(fp_iou * tp_score)
+
     def get_tp_count(self, iou_threshold, score_threshold):
         """
         This method gets the number of true positives at the
         specified IoU threshold and score threshold.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_truth_values
 
         Parameters
         ----------
-
             iou_threshold: float
                 The IoU threshold to consider the true positives.
 
             score_threshold: float
                 The score threshold to consider the predictions.
 
         Returns
@@ -915,15 +1040,16 @@
             ValueError
                 This method will raise the exception if the provided
                 iou_threshold and score_threshold are not floating
                 types or they are out of bounds
                 such as greater than 1 and less than 0.
         """
 
-        score_threshold = DetectionDataCollection.validate_score(score_threshold)
+        score_threshold = DetectionDataCollection.validate_score(
+            score_threshold)
         iou_threshold = DetectionDataCollection.validate_iou(iou_threshold)
         if len(self.tps):
             tp_iou = np.array(self.tps)[:, 0] >= iou_threshold
             tp_score = np.array(self.tps)[:, 1] >= score_threshold
             return np.count_nonzero(tp_iou * tp_score)
         else:
             return 0
@@ -934,15 +1060,14 @@
         the specified IoU and score threshold.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_truth_values
 
         Parameters
         ----------
-
             iou_threshold: float
                 The IoU threshold to consider classification false positives.
 
             score_threshold: float
                 The score threshold to consider predictions.
 
         Returns
@@ -956,15 +1081,16 @@
             ValueError
                 This method will raise the exception if the provided
                 iou_threshold and score_threshold are not floating
                 types or they are out of bounds
                 such as greater than 1 and less than 0.
         """
 
-        score_threshold = DetectionDataCollection.validate_score(score_threshold)
+        score_threshold = DetectionDataCollection.validate_score(
+            score_threshold)
         iou_threshold = DetectionDataCollection.validate_iou(iou_threshold)
         if len(self.class_fps):
             fp_iou = np.array(self.class_fps)[:, 0] >= iou_threshold
             fp_score = np.array(self.class_fps)[:, 1] >= score_threshold
             return np.count_nonzero(fp_iou * fp_score)
         else:
             return 0
@@ -978,15 +1104,14 @@
         localization false positives.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_truth_values
 
         Parameters
         ----------
-
             iou_threshold: float
                 The IoU threshold to consider true positives as local
                 false positives.
 
             score_threshold: float
                 The score threshold to consider predictions.
 
@@ -1001,15 +1126,16 @@
             ValueError
                 This method will raise the exception if the provided
                 iou_threshold and score_threshold are not floating
                 types or they are out of bounds
                 such as greater than 1 and less than 0.
         """
 
-        score_threshold = DetectionDataCollection.validate_score(score_threshold)
+        score_threshold = DetectionDataCollection.validate_score(
+            score_threshold)
         iou_threshold = DetectionDataCollection.validate_iou(iou_threshold)
         local_fp = 0
 
         if len(self.tps):
             # Any predictions that are below the IoU thresholds are
             # localization false positives.
             fp_iou = np.array(self.tps)[:, 0] < iou_threshold
@@ -1019,43 +1145,29 @@
         if len(self.class_fps):
             class_fp_iou = np.array(self.class_fps)[:, 0] < iou_threshold
             class_fp_score = np.array(self.class_fps)[:, 1] >= score_threshold
             local_fp += np.count_nonzero(class_fp_iou * class_fp_score)
 
         local_fp += np.count_nonzero(
             np.array(self.local_fps) >= score_threshold)
-        
         return local_fp
 
-        # if len(self.tps):
-        #     # Any predictions that are below the IoU thresholds are
-        #     # localization false positives.
-        #     fp_iou = np.array(self.tps)[:, 0] < iou_threshold
-        #     tp_score = np.array(self.tps)[:, 1] >= score_threshold
-        #     return np.count_nonzero(
-        #         fp_iou * tp_score) + np.count_nonzero(
-        #         np.array(self.local_fps) >= score_threshold) 
-        # else:
-        #     return np.count_nonzero(
-        #         np.array(self.local_fps) >= score_threshold)
-
     def get_fn_count(self, iou_threshold, score_threshold):
         """
         This method gets the number of false negatives
         at the specified IoU threshold and score threshold.
         Score threshold is needed because by principle fp = gt - tp,
         and score and IoU threshold is required to find the
         number of true positives.
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_get_truth_values
 
         Parameters
         ----------
-
             iou_threshold: float
                 The IoU threshold to consider true positives.
 
             score_threshold: float
                 The score threshold to consider predictions.
 
         Returns
@@ -1069,9 +1181,8 @@
             ValueError
                 This method will raise the exception if the provided
                 iou_threshold and score_threshold are not
                 floating types or they are out of bounds
                 such as greater than 1 and less than 0.
 
         """
-
-        return self.gt - self.get_tp_count(iou_threshold, score_threshold)
+        return self.gt - self.get_tp_count(iou_threshold, score_threshold)
```

## deepview/validator/metrics/detectionmetrics.py

```diff
@@ -2,16 +2,17 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
+from deepview.validator.metrics.detectionutils import match_gt_dt, filter_dt, \
+    nan_to_last_num
 from deepview.validator.exceptions import ZeroUniqueLabelsException
-from deepview.validator.exceptions import InvalidIoUException
 from deepview.validator.metrics.core import Metrics
 import numpy as np
 
 
 class DetectionMetrics(Metrics):
     """
     This class provides methods to calculate:
@@ -32,15 +33,14 @@
     Other calculations such as IoU, false positive ratios,
     precision vs recall data are also handled in this class.
     Unit-test for this class is defined under:
         test/test_metrics.py
 
     Parameters
     ----------
-
         detectiondatacollection: DetectionDataCollection
             This contains the number of ground truths in
             the dataset and tp, fp, and fn per class.
 
     Raises
     ------
         InvalidIoUException
@@ -63,140 +63,16 @@
     """
 
     def __init__(
         self,
         detectiondatacollection=None
     ):
         super(DetectionMetrics, self).__init__()
-
         self.detectiondatacollection = detectiondatacollection
-
-    @staticmethod
-    def bbox_iou(bboxes1, bboxes2, eps=1e-10):
-        """
-        This method computes the intersection over union.
-        Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_detectionmetrics.py
-            function: test_bbox_iou
-
-        Parameters
-        ----------
-
-            bboxes1: np.ndarray
-                (a, b, ..., 4)
-
-            bboxes2: np.ndarray
-                (A, B, ..., 4)
-                x:X is 1:n or n:n or n:1
-
-            eps: float
-                Invalid IoU leniency.
-
-        Returns
-        -------
-            IoU score: float
-                (max(a,A), max(b,B), ...)
-                ex) (4,):(3,4) -> (3,)
-                    (2,1,4):(2,3,4) -> (2,3)
-
-        Raises
-        ------
-            InvalidIoUException
-                This method will raise an exception if the calculated
-                IoU is invalid. i.e. less than 0 or greater than 1.
-        """
-
-        bboxes1_area = bboxes1[..., 2] * bboxes1[..., 3]
-        bboxes2_area = bboxes2[..., 2] * bboxes2[..., 3]
-
-        bboxes1_coor = np.concatenate(
-            [
-                bboxes1[..., :2] - bboxes1[..., 2:] * 0.5,
-                bboxes1[..., :2] + bboxes1[..., 2:] * 0.5,
-            ],
-            axis=-1,
-        )
-        bboxes2_coor = np.concatenate(
-            [
-                bboxes2[..., :2] - bboxes2[..., 2:] * 0.5,
-                bboxes2[..., :2] + bboxes2[..., 2:] * 0.5,
-            ],
-            axis=-1,
-        )
-
-        left_up = np.maximum(bboxes1_coor[..., :2], bboxes2_coor[..., :2])
-        right_down = np.minimum(bboxes1_coor[..., 2:], bboxes2_coor[..., 2:])
-        inter_section = np.maximum(right_down - left_up, 0.0)
-        inter_area = inter_section[..., 0] * inter_section[..., 1]
-        union_area = bboxes1_area + bboxes2_area - inter_area
-        iou = inter_area / union_area
-
-        if iou > 1. + eps or iou < 0.:
-            raise InvalidIoUException(iou)
-        return iou
-
-    @staticmethod
-    def compute_iou(gt_box, dt_box, width=1, height=1, eps=1e-10):
-        """
-        This method computes the intersection over union.
-        This computation was taken from the following source:
-        https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/
-
-        Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_detectionmetrics.py
-            function: test_compute_iou
-
-        Parameters
-        ----------
-
-            gt_box: list or np.ndarray
-                Ground truth box containing [x1, y1, x2, y2].
-
-            dt_box: list or np.ndarray
-                Model prediction box containing [x1, y1, x2, y2].
-
-            width: int
-                Width of the image to denormalize x coordinates.
-
-            height: int
-                Height of the image to denormalize y coordinates.
-
-            eps: float
-                Invalid IoU leniency.
-
-        Returns
-        -------
-            IoU score: float
-                The intersection over union between the prediction and the
-                ground truth bounding box.
-
-        Raises
-        ------
-            InvalidIoUException
-                This method will raise an exception if the calculated
-                IoU is invalid. i.e. less than 0 or greater than 1.
-        """
-
-        xa = max(float(gt_box[0]) * width, float(dt_box[0]) * width)
-        ya = max(float(gt_box[1]) * height, float(dt_box[1]) * height)
-        xb = min(float(gt_box[2]) * width, float(dt_box[2]) * width)
-        yb = min(float(gt_box[3]) * height, float(dt_box[3]) * height)
-
-        inter_area = max(0, xb - xa + 1) * max(0, yb - ya + 1)
-        boxa_area = (float(dt_box[2]) * width - float(dt_box[0]) * width + 1) * (
-            float(dt_box[3]) * height - float(dt_box[1]) * height + 1)
-        boxb_area = (float(gt_box[2]) * width - float(gt_box[0]) * width + 1) * (
-            float(gt_box[3]) * height - float(gt_box[1]) * height + 1)
-
-        iou = inter_area / float(boxa_area + boxb_area - inter_area)
-
-        if iou > 1. + eps or iou < 0.:
-            raise InvalidIoUException(iou)
-        return iou
-
+    
     def compute_overall_metrics(
             self,
             total_tp,
             total_fn,
             total_class_fp,
             total_local_fp):
         """
@@ -211,15 +87,14 @@
 
         Unit-test for this method is defined under:
             file: test/deepview/validator/metrics/test_detectionmetrics.py
             function: test_compute_overall_metrics
 
         Parameters
         ----------
-
             total_tp: int
                 Total number of true positives in the dataset.
 
             total_fn: int
                 Total number of false negatives in the dataset.
 
             total_class_fp: int
@@ -273,15 +148,14 @@
 
         Unit-test for this method is defined under:
             file: test/deepview/validator/metrics/test_detectionmetrics.py
             function: test_compute_detection_metrics
 
         Parameters
         ----------
-
             score_threshold: float
                 The score threshold to consider for predictions.
 
         Returns
         -------
             class metrics: list
                 This contains mean average precision, recall, and accuracy
@@ -380,19 +254,180 @@
             mmap[0], mmap[5], map_5095], [
             mar[0], mar[5], mar_5095], [
             macc[0], macc[5], macc_5095]
 
         return [metric_map, metric_mar, metric_maccuracy], class_histogram_data
 
     def get_pr_data(
+        self,
+        score_threshold=0.5,
+        iou_threshold=0.5,
+        eps=1e-16,
+        interval=0.01
+        ):
+        """
+        This method computes the precision and recall
+        based on varying score thresholds.
+        Unit-test for this method is defined under:
+            file: test/deepview/validator/metrics/test_detectionmetrics.py
+            function: test_get_pr_data
+
+        Parameters
+        ----------
+            score_threshold: float
+                The score threshold to consider for predictions.
+
+            iou_threshold: float
+                The IoU threshold to consider true positives.
+
+            eps: float
+                Minimum value to substitute for zero.
+
+            interval: float
+                Score threshold interval to test
+                from eps (min)...1. + interval (max)
+
+        Returns
+        -------
+            data for plots:
+                precision, recall, names
+
+        Raises
+        ------
+            ZeroUniqueLabelsException
+                This method will raise an exception if the number of unique
+                labels captured is zero.
+
+            DivisionByZeroException
+                This method will raise an exception if a division of zero
+                is encountered when calculating precision and recall.
+        """
+
+        score_threshold = self.validate_threshold(score_threshold)
+        iou_threshold = self.validate_threshold(iou_threshold)
+
+        # All unique classes found at 0.01 detection threshold
+        if len(self.detectiondatacollection.all_labels) == 0:
+            if len(self.detectiondatacollection.labels) == 0:
+                raise ZeroUniqueLabelsException()
+            else:
+                all_labels = self.detectiondatacollection.labels
+                nc = len(all_labels)
+        else:
+            all_labels = self.detectiondatacollection.all_labels
+            nc = len(all_labels)
+        
+        score_min, score_max = eps, 1. + interval
+        score_thresholds = np.arange(score_min, score_max, interval)
+
+        # Precision and recall, rows = classes, columns = range of thresholds
+        p = np.zeros((nc, len(score_thresholds)))
+        r = np.zeros((nc, len(score_thresholds)))
+        # Average Precision, rows = classes, columns = range of IoUs (0.5-0.95)
+        ap = np.zeros((nc, 10))
+        
+        # Iterate the range of thresholds
+        for ti, score_t in enumerate(score_thresholds):
+            # Reset the data
+            self.detectiondatacollection.reset_containers()
+            instances = self.detectiondatacollection.get_instances()
+
+            for _, instance in instances.items():
+                gt_boxes = instance.get('gt_instance').get('boxes')
+                dt_boxes = instance.get('dt_instance').get('boxes')
+                gt_labels = instance.get('gt_instance').get('labels')
+                dt_labels = instance.get('dt_instance').get('labels')
+                scores = instance.get('dt_instance').get('scores')
+
+                # Filter detections only for valid scores based on threshold.
+                dt_boxes, dt_labels, scores = filter_dt(
+                    dt_boxes, dt_labels, scores, score_t)
+                instance['dt_instance']['boxes'] = dt_boxes
+                instance['dt_instance']['labels'] = dt_labels
+                instance['dt_instance']['scores'] = scores
+
+                # Match ground truths to detections
+                self.detectiondatacollection.capture_class(dt_labels)
+                self.detectiondatacollection.capture_class(gt_labels)
+                stats = match_gt_dt(gt_boxes, dt_boxes)
+                # Evaluate
+                self.detectiondatacollection.categorize(
+                    *stats,
+                    gt_labels=gt_labels,
+                    dt_labels=dt_labels,
+                    scores=scores)
+            
+            # Precision for each class at this threshold.
+            class_precision, class_recall = np.zeros(nc), np.zeros(nc)
+           
+            # Iterate through each data and grab precision and recall
+            for label_data in self.detectiondatacollection.label_data_list:
+
+                tp = label_data.get_tp_count(iou_threshold, score_t)
+                class_fp = label_data.get_class_fp_count(
+                    iou_threshold, score_t)
+                local_fp = label_data.get_local_fp_count(
+                    iou_threshold, score_t)
+                fn = label_data.get_fn_count(iou_threshold, score_t)
+               
+                # Number of ground truth labels for this class
+                n_l = label_data.get_gt()
+                # Number of predictions for this class
+                n_p = tp + class_fp + local_fp
+                # The index to store the precision and recall based on class
+                ci = all_labels.index(label_data.get_label())
+
+                if n_p != 0 and n_l != 0:
+                    if (tp + class_fp + local_fp) == 0:
+                        # A division of 0/0 is not a number
+                        class_precision[ci] = np.nan
+                    else:
+                        precision = self.compute_precision(
+                            tp, class_fp + local_fp)
+                        class_precision[ci] = precision
+
+                    if (tp + fn) == 0:
+                        # A division of 0/0 is not a number
+                        class_recall[ci] = np.nan
+                    else:
+                        recall = self.compute_recall(tp, fn)
+                        class_recall[ci] = recall
+                else:
+                    class_precision[ci] = np.nan
+                    class_recall[ci] = np.nan
+
+                if round(score_t, 2) == round(score_threshold, 2):
+                    # AP from recall-precision curve
+                    ap[ci, :] = self.compute_ap_iou(
+                        label_data, score_threshold)
+            
+            p[:, ti] = class_precision
+            r[:, ti] = class_recall
+
+        # This portion replaces NaN values with the last acceptable values.
+        # This is necessary so that the lengths are the same for both 
+        # precision and recall. 
+        for ci in range(nc):
+            p[ci] = nan_to_last_num(p[ci])
+            r[ci] = nan_to_last_num(r[ci])
+
+        return {
+            "precision": p,
+            "recall": r,
+            "average precision": ap,
+            "names": all_labels
+        }
+
+    def get_plots_data(
             self,
             score_threshold=0.5,
             iou_threshold=0.5,
             eps=1e-16,
-            interval=0.01):
+            interval=0.01
+        ):
         """
         This method computes the precision and recall
         based on varying score thresholds to
         acquire data for the plots:
 
             - precision vs recall curve
             - precision vs confidence curve
@@ -401,15 +436,14 @@
 
         Unit-test for this method is defined under:
             file: test/deepview/validator/metrics/test_detectionmetrics.py
             function: test_get_pr_data
 
         Parameters
         ----------
-
             score_threshold: float
                 The score threshold to consider for predictions.
 
             iou_threshold: float
                 The IoU threshold to consider true positives.
 
             eps: float
@@ -443,40 +477,61 @@
                 labels captured is zero.
 
             DivisionByZeroException
                 This method will raise an exception if a division of zero
                 is encountered when calculating precision and recall.
         """
 
-        score_threshold = self.validate_threshold(score_threshold)
         iou_threshold = self.validate_threshold(iou_threshold)
 
         # number of unique classes
-        nc = len(self.detectiondatacollection.label_data_list)
+        all_labels = self.detectiondatacollection.all_labels
+        nc = len(all_labels)
         if nc == 0:
             raise ZeroUniqueLabelsException()
 
         score_min, score_max = eps, 1. + interval
         score_thresholds = np.arange(score_min, score_max, interval)
 
         px = np.linspace(0, 1, 1000)
         overall_precision, overall_recall = list(), list()
-        names, pred_scores = list(), list()
+        names, pred_scores = all_labels, list()
         ap, p, r = np.zeros(
             (nc, 10)), np.zeros(
             (nc, 1000)), np.zeros(
             (nc, 1000))
+        
+        self.detectiondatacollection.reset_containers()
+        instances = self.detectiondatacollection.get_instances()
+
+        # Rematch the unfiltered detections (low NMS score threshold) 
+        for _, instance in instances.items():
+            gt_boxes = instance.get('gt_instance').get('boxes')
+            dt_boxes = instance.get('dt_instance').get('boxes')
+            gt_labels = instance.get('gt_instance').get('labels')
+            dt_labels = instance.get('dt_instance').get('labels')
+            scores = instance.get('dt_instance').get('scores')
+
+            # Match
+            self.detectiondatacollection.capture_class(dt_labels)
+            self.detectiondatacollection.capture_class(gt_labels)
+            stats = match_gt_dt(gt_boxes, dt_boxes)
+            # Evaluate
+            self.detectiondatacollection.categorize(
+                *stats,
+                gt_labels=gt_labels,
+                dt_labels=dt_labels,
+                scores=scores)
 
-        for ci, label_data in enumerate(
-                self.detectiondatacollection.label_data_list):
+        for label_data in self.detectiondatacollection.label_data_list:
             class_precision, class_recall = list(), list()
-
-            # number of ground truth labels for this class
+            ci = all_labels.index(label_data.get_label())
+            # The number of ground truth labels for this class
             n_l = label_data.gt
-            # number of predictions for this class
+            # The number of predictions for this class
             n_p = label_data.get_tp_count(iou_threshold, score_min) + \
                 label_data.get_class_fp_count(iou_threshold, score_min) + \
                 label_data.get_local_fp_count(iou_threshold, score_min)
 
             if n_p == 0 or n_l == 0:
                 class_precision = np.zeros(int(score_max / interval))
                 class_recall = np.zeros(int(score_max / interval))
@@ -550,19 +605,14 @@
                 class_recall = np.flip(class_recall)
 
                 # AP from recall-precision curve
                 ap[ci, :] = self.compute_ap_iou(label_data, score_threshold)
 
             overall_precision.append(class_precision)
             overall_recall.append(class_recall)
-            name = label_data.get_label()
-            if isinstance(name, (np.int32, np.int64)):
-                names.append(int(name))
-            else:
-                names.append(name)    
             
         # Compute F1 (harmonic mean of precision and recall)
         f1 = 2 * p * r / (p + r + eps)
 
         return {
             "precision": overall_precision,
             "recall": overall_recall,
@@ -580,15 +630,14 @@
         at 10 different iou thresholds.
         Unit-test for this method is defined under:
             file: test/deepview/validator/metrics/test_detectionmetrics.py
             function: test_compute_ap_iou
 
         Parameters
         ----------
-
             label_data: DetectionLabelData
                 A container for the number of tp, fp, and fn for the label.
 
             score_threshold: float
                 The score threshold to consider for predictions.
 
         Returns
@@ -606,28 +655,25 @@
 
             DivisionByZeroException
                 This method will raise an exception if a division by
                 zero occurs when calculating the precision.
         """
 
         score_threshold = self.validate_threshold(score_threshold)
-        precision_list = list()
-        for iou_threshold in np.arange(0.5, 1, 0.05):
+        precision_list = np.zeros(10)
+        for i, iou_threshold in enumerate(np.arange(0.5, 1, 0.05)):
             tp = label_data.get_tp_count(iou_threshold, score_threshold)
             class_fp = label_data.get_class_fp_count(
                 iou_threshold, score_threshold)
             local_fp = label_data.get_local_fp_count(
                 iou_threshold, score_threshold)
-            if tp == 0:
-                precision_list.append(0)
-            else:
-                precision_list.append(
-                    self.compute_precision(
-                        tp, class_fp + local_fp))
-        return np.array(precision_list)
+            if tp != 0:
+                precision_list[i] = self.compute_precision(
+                        tp, class_fp + local_fp)
+        return precision_list
 
     def get_fp_error(self, score_threshold):
         """
         This method calculates the false positive error ratios.
 
         * Localization FP Error = Localization FP /
                             (Classification FP + Localization FP).
@@ -639,15 +685,14 @@
         false positives are predictions with non matching labels.*
         Unit-test for this method is defined under:
             file: test/deepview/validator/metrics/test_detectionmetrics.py
             function: test_get_fp_error
 
         Parameters
         ----------
-
             score_threshold: float
                 The score threshold to consider for predictions.
 
         Returns
         -------
             Error Ratios: list
                 This contains false positive ratios for
@@ -679,15 +724,16 @@
                 "The provided score_threshold is out of bounds: {}. ".format(
                     score_threshold) + 
                 "Can only accept values between 0 and 1.")
         else:
             local_fp_error, class_fp_error = np.zeros(10), np.zeros(10)
             for it, iou_threshold in enumerate(np.arange(0.5, 1, 0.05)):
                 _, _, \
-                    class_fp, local_fp = self.detectiondatacollection.sum_outcomes(
+                    class_fp, local_fp = \
+                        self.detectiondatacollection.sum_outcomes(
                         iou_threshold, score_threshold
                     )
                 if local_fp == 0:
                     local_fp_error[it] = 0
                 else:
                     local_fp_error[it] = self.divisor(
                         local_fp, local_fp + class_fp)
```

## deepview/validator/metrics/detectionutils.py

```diff
@@ -3,28 +3,582 @@
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
 from deepview.validator.exceptions import MatchingAlgorithmException
-from deepview.validator.metrics.detectionmetrics import DetectionMetrics
+from deepview.validator.exceptions import InvalidIoUException
+import numpy as np
 
 
-def match_dt_gt(gt_boxes, dt_boxes):
+def center_point_distance(boxA, boxB):
     """
+    This method finds the distance between the center of two 
+    bounding boxes using pythagoras. 
+
+    Parameters
+    ----------
+        boxA: list or np.ndarray
+            This contains [xmin, ymin, xmax, ymax] for detections.
+
+        boxB: list or np.ndarray
+            This contains [xmin, ymin, xmax, ymax] for ground truth.
+
+    Returns
+    -------
+        center point distance: float
+            This is the distance from center to center of the
+            bounding boxes.
+
+    Raises
+    ------
+        None
+    """
+
+    width_a = boxA[2] - boxA[0]
+    width_b = boxB[2] - boxB[0]
+    height_a = boxA[3] - boxA[1]
+    height_b = boxB[3] - boxB[1]
+
+    a = abs((boxA[0] + width_a/2) - (boxB[0] + width_b/2))
+    b = abs((boxA[1] + height_a/2) - (boxB[1] + height_b/2))
+    return (a**2 + b**2)**0.5
+
+def bb_intersection_over_union(boxA, boxB, eps=1e-10):
+    """
+    This method computes the IoU between ground truth and detection
+    bounding boxes.
+    IoU computation method retrieved from:
+    https://gist.github.com/meyerjo/dd3533edc97c81258898f60d8978eddc
+
+    Parameters
+    ----------
+        boxA: list
+            This is a bounding box [xmin, ymin, xmax, ymax]
+        
+        boxB: list
+            This is a bounding box [xmin, ymin, xmax, ymax]
+
+    Returns
+    -------
+        IoU: float
+            The IoU score between boxes.
+
+    Exceptions
+    ----------
+        InvalidIoUException
+            This method will raise an exception if the calculated
+            IoU is invalid. i.e. less than 0 or greater than 1.
+
+        ValueError
+            This method will raise an exception if the provided boxes for
+            ground truth and detection does not have a length of four.
+    """
+
+    if len(boxA) != 4 or len(boxB) != 4:
+        raise ValueError("The provided bounding boxes does not meet " \
+                            "expected lengths [xmin, ymin, xmax, ymax]")
+    
+    # determine the (x, y)-coordinates of the intersection rectangle
+    xA = max(boxA[0], boxB[0])
+    yA = max(boxA[1], boxB[1])
+    xB = min(boxA[2], boxB[2])
+    yB = min(boxA[3], boxB[3])
+
+    # compute the area of intersection rectangle
+    interArea = max((xB - xA, 0)) * max((yB - yA), 0)
+    if interArea == 0:
+        return 0.
+    # compute the area of both the prediction and ground-truth
+    # rectangles
+    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))
+    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))
+
+    # compute the intersection over union by taking the intersection
+    # area and dividing it by the sum of prediction + ground-truth
+    # areas - the interesection area
+    iou = interArea / float(boxAArea + boxBArea - interArea)
+
+    if iou > 1. + eps or iou < 0.:
+        raise InvalidIoUException(iou)
+    # return the intersection over union value
+    return iou   
+
+def bbox_iou(bboxes1, bboxes2, eps=1e-10):
+    """
+    This method computes the intersection over union.
+    Unit-test for this method is defined under:
+        file: test/deepview/validator/metrics/test_detectionmetrics.py
+        function: test_bbox_iou
+
+    Parameters
+    ----------
+        bboxes1: np.ndarray
+            (a, b, ..., 4)
+
+        bboxes2: np.ndarray
+            (A, B, ..., 4)
+            x:X is 1:n or n:n or n:1
+
+        eps: float
+            Invalid IoU leniency.
+
+    Returns
+    -------
+        IoU score: float
+            (max(a,A), max(b,B), ...)
+            ex) (4,):(3,4) -> (3,)
+                (2,1,4):(2,3,4) -> (2,3)
+
+    Raises
+    ------
+        InvalidIoUException
+            This method will raise an exception if the calculated
+            IoU is invalid. i.e. less than 0 or greater than 1.
+
+        ValueError
+            This method will raise an exception if the provided boxes for
+            ground truth and detection does not have a length of four.
+    """
+
+    if len(bboxes1) != 4 or len(bboxes2) != 4:
+        raise ValueError("The provided bounding boxes does not meet " \
+                            "expected lengths [xmin, ymin, xmax, ymax]")
+
+    bboxes1_area = bboxes1[..., 2] * bboxes1[..., 3]
+    bboxes2_area = bboxes2[..., 2] * bboxes2[..., 3]
+
+    bboxes1_coor = np.concatenate(
+        [
+            bboxes1[..., :2] - bboxes1[..., 2:] * 0.5,
+            bboxes1[..., :2] + bboxes1[..., 2:] * 0.5,
+        ],
+        axis=-1,
+    )
+    bboxes2_coor = np.concatenate(
+        [
+            bboxes2[..., :2] - bboxes2[..., 2:] * 0.5,
+            bboxes2[..., :2] + bboxes2[..., 2:] * 0.5,
+        ],
+        axis=-1,
+    )
+
+    left_up = np.maximum(bboxes1_coor[..., :2], bboxes2_coor[..., :2])
+    right_down = np.minimum(bboxes1_coor[..., 2:], bboxes2_coor[..., 2:])
+    inter_section = np.maximum(right_down - left_up, 0.0)
+    inter_area = inter_section[..., 0] * inter_section[..., 1]
+    union_area = bboxes1_area + bboxes2_area - inter_area
+    iou = inter_area / union_area
+
+    if iou > 1. + eps or iou < 0.:
+        raise InvalidIoUException(iou)
+    return iou
+
+def compute_iou(dt_box, gt_box, width=1, height=1, eps=1e-10):
+    """
+    This method computes the intersection over union.
+    This computation was taken from the following source:
+    https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/
+
+    Unit-test for this method is defined under:
+        file: test/deepview/validator/metrics/test_detectionmetrics.py
+        function: test_compute_iou
+
+    Parameters
+    ----------
+        dt_box: list or np.ndarray
+            Model prediction box containing [x1, y1, x2, y2].
+
+        gt_box: list or np.ndarray
+            Ground truth box containing [x1, y1, x2, y2].
+
+        width: int
+            Width of the image to denormalize x coordinates.
+
+        height: int
+            Height of the image to denormalize y coordinates.
+
+        eps: float
+            Invalid IoU leniency.
+
+    Returns
+    -------
+        IoU score: float
+            The intersection over union between the prediction and the
+            ground truth bounding box.
+
+    Raises
+    ------
+        InvalidIoUException
+            This method will raise an exception if the calculated
+            IoU is invalid. i.e. less than 0 or greater than 1.
+
+        ValueError
+            This method will raise an exception if the provided boxes for
+            ground truth and detection does not have a length of four.
+    """
+
+    if len(dt_box) != 4 or len(gt_box) != 4:
+        raise ValueError("The provided bounding boxes does not meet " \
+                            "expected lengths [xmin, ymin, xmax, ymax]")
+
+    xa = max(float(gt_box[0]) * width, float(dt_box[0]) * width)
+    ya = max(float(gt_box[1]) * height, float(dt_box[1]) * height)
+    xb = min(float(gt_box[2]) * width, float(dt_box[2]) * width)
+    yb = min(float(gt_box[3]) * height, float(dt_box[3]) * height)
+
+    inter_area = max(0, xb - xa + 1) * max(0, yb - ya + 1)
+    boxa_area = (float(dt_box[2]) * width - float(dt_box[0]) * width + 1)*(
+        float(dt_box[3]) * height - float(dt_box[1]) * height + 1)
+    boxb_area = (float(gt_box[2]) * width - float(gt_box[0]) * width + 1)*(
+        float(gt_box[3]) * height - float(gt_box[1]) * height + 1)
+
+    iou = inter_area / float(boxa_area + boxb_area - inter_area)
+
+    if iou > 1. + eps or iou < 0.:
+        raise InvalidIoUException(iou)
+    return iou
+
+def filter_dt(boxes, classes, scores, threshold):
+    """
+    This function filters the detections to include only scores 
+    greater than or equal to the validation threshold set.
+    
+
+    Parameters
+    ----------
+        boxes: np.ndarray
+            The prediction bounding boxes.. [[box1], [box2], ...]
+
+        classes: np.ndarray
+            The prediction labels.. [cl1, cl2, ...]
+
+        scores: np.ndarray
+            The prediction confidence scores.. [score, score, ...]
+            normalized between 0 and 1.
+
+        threshold: float
+            This is the validation score threshold to filter
+            the detections.
+
+    Returns
+    ------- 
+        boxes, classes, scores: np.ndarray
+            These contain only the detections whose scores are 
+            larger than or greater than the validation threshold set.
+
+    Raises
+    ------
+        None
+
+    """
+    filter_indices = np.argwhere(scores >= threshold).flatten()
+    boxes = np.take(boxes, filter_indices, axis=0)
+    scores = np.take(scores, filter_indices, axis=0)
+    classes = np.take(classes, filter_indices, axis=0)
+    return boxes, classes, scores
+
+def clamp_boxes(instances, clamp):
+    """
+    This function clamps bounding box less than the provided clamp value to
+    the clamp value in pixels. The minimum width and height of the bounding
+    is the clamp value in pixels. 
+
+    Parameters
+    ----------
+        instances: dict
+            This contains the ground truth and the detection data.
+            See README.md (Method Parameters Format) for more information.
+
+        clamp: int
+            The minimum acceptable dimensions of the bounding boxes for 
+            detections and ground truth. 
+
+    Returns
+    -------
+        instances: dict
+            This now contains the updated clamped bounding boxes.
+
+    Raises
+    ------ 
+        None
+    """
+    height = instances.get('gt_instance').get('height')
+    width = instances.get('gt_instance').get('width')
+    gt_boxes = instances.get('gt_instance').get('boxes')
+    dt_boxes = instances.get('dt_instance').get('boxes')
+
+    gt_widths = ((gt_boxes[..., 2:3] - gt_boxes[..., 0:1])*width).flatten()
+    gt_heights = ((gt_boxes[..., 3:4] - gt_boxes[..., 1:2])*height).flatten()
+    dt_widths = ((dt_boxes[..., 2:3] - dt_boxes[..., 0:1])*width).flatten()
+    dt_heights = ((dt_boxes[..., 3:4] - dt_boxes[..., 1:2])*height).flatten()
+
+    gt_modify = np.transpose(
+        np.nonzero(((gt_widths<clamp)+(gt_heights<clamp)))).flatten()
+    dt_modify = np.transpose(
+        np.nonzero(((dt_widths<clamp)+(dt_heights<clamp)))).flatten()
+
+    if len(gt_boxes):
+        gt_boxes[gt_modify, 2:3] = gt_boxes[gt_modify, 0:1] + clamp/width
+        gt_boxes[gt_modify, 3:4] = gt_boxes[gt_modify, 1:2] + clamp/height
+        instances['gt_instance']['boxes'] = gt_boxes
+    if len(dt_boxes):
+        dt_boxes[dt_modify, 2:3] = dt_boxes[dt_modify, 0:1] + clamp/width
+        dt_boxes[dt_modify, 3:4] = dt_boxes[dt_modify, 1:2] + clamp/height
+        instances['dt_instance']['boxes'] = dt_boxes
+    return instances
+
+def ignore_boxes(instances, ignore):
+    """
+    This function ignores the boxes with dimensions less than the ignore 
+    parameter provided. 
+
+    Parameters
+    ----------
+        instances: dict
+            This contains the ground truth and the detection data.
+            See README.md (Method Parameters Format) for more information.
+
+        ignore: int
+            The dimension pixels threshold to ignore. Any boxes with width 
+            and height less than this value will be ignored and filtered out.
+
+    Returns
+    -------
+        instances: dict
+            This is the updated instances data which filtered out the boxes.
+
+    Raises
+    ------
+        None
+    """
+    height = instances.get('gt_instance').get('height')
+    width = instances.get('gt_instance').get('width')
+    gt_boxes = instances.get('gt_instance').get('boxes')
+    gt_labels = instances.get('gt_instance').get('labels')  
+    dt_boxes = instances.get('dt_instance').get('boxes')
+    dt_labels = instances.get('dt_instance').get('labels')
+    scores = instances.get('dt_instance').get('scores')
+
+    gt_widths = ((gt_boxes[..., 2:3] - gt_boxes[..., 0:1])*width).flatten()
+    gt_heights = ((gt_boxes[..., 3:4] - gt_boxes[..., 1:2])*height).flatten()
+    dt_widths = ((dt_boxes[..., 2:3] - dt_boxes[..., 0:1])*width).flatten()
+    dt_heights = ((dt_boxes[..., 3:4] - dt_boxes[..., 1:2])*height).flatten()
+
+    gt_keep = np.transpose(
+        np.nonzero(((gt_widths>=ignore)*(gt_heights>=ignore)))).flatten()
+    dt_keep = np.transpose(
+        np.nonzero(((dt_widths>=ignore)*(dt_heights>=ignore)))).flatten()
+    gt_boxes = gt_boxes[gt_keep]
+    gt_labels =  gt_labels[gt_keep]
+    dt_boxes = dt_boxes[dt_keep]
+    dt_labels = dt_labels[dt_keep]
+    scores = scores[dt_keep]
+
+    instances['gt_instance']['boxes'] = gt_boxes
+    instances['gt_instance']['labels'] = gt_labels
+    instances['dt_instance']['boxes'] = dt_boxes
+    instances['dt_instance']['labels'] = dt_labels
+    instances['dt_instance']['scores'] = scores
+    return instances
+
+def nan_to_last_num(process_array):
+    """
+    This function replaces all NAN values with the last valid number. If all
+    values are NaN, then all elements are replaced with zeros.
+
+    Parameters
+    ----------
+        process_array: np.ndarray
+            This is the array to replace NaN values with the last 
+            acceptable value.
+
+    Returns
+    -------
+        process_array: np.ndarray
+            The same array but with NaN replaced with last acceptable values.
+            Otherwise, all elements are replaced with zeros if all elements are
+            NaN.
+
+    Raises
+    ------
+        None
+    """
+
+    try:
+        # Find the maximum index where the value is not a NaN.
+        precision_repeat_id = np.max(
+            np.argwhere(
+                np.logical_not(
+                    np.isnan(process_array))).flatten())
+        # NaN values should be replace with the last acceptable value.
+        process_array = np.nan_to_num(
+            process_array,
+            nan=process_array[int(precision_repeat_id)]
+        )
+
+    except ValueError:
+        # The whole array are nans just convert back to zero.
+        process_array[np.isnan(process_array)] = 0.
+
+    return process_array
+
+def match_gt_dt(gt_boxes, dt_boxes, metric='iou'):
+    """
+    This function is version 2 of the matching algorithm incorporates
+    recursive calls to perform rematching of ground truth that were 
+    unmatched due to duplicative matches, but the rematching is based on the
+    next best IoU. This function attempts a best-fit of
+    predictions to ground truths.
+    Unit-test for this method is defined under:
+        file: test/test_metrics.py
+        function: test_match_gt_dt
+
+    Parameters
+    ----------
+        gt_boxes: list or np.ndarray
+            A list of ground truth boxes [[x1, y1, x2, y2]...].
+
+        dt_boxes: list or np.ndarray
+            A list of prediction boxes [[x1, y1, x2, y2]...].
+
+    Returns
+    -------
+       indices : list
+            This contains indices of the matches, extra predictions,
+            missed ground truths, and
+            IoU values for each match.
+
+            * matches [[detection index, ground truth index],
+                    [detection index, ground truth index], ...]
+            * extras [detection index, detection index, ...]
+            * missed [ground truth index, ground truth index, ...]
+
+    Raises
+    ------
+        MatchingAlgorithmException
+            This function will raise an exception if the method finds
+            invalid values for IoU and ground truth index such as -1
+    """
+    
+    # This contains the IoUs of each detection.
+    iou_list = np.zeros(len(dt_boxes))
+    # Row is ground truth, columns is detection IoUs
+    iou_grid = np.zeros((len(gt_boxes), len(dt_boxes)))
+    index_matches = list()
+
+    def compare_matches(dti, gti, iou):
+        """
+        This function checks if duplicate matches exists. A duplicate match
+        is when the same detection is being matched to more than one 
+        ground truth. The IoUs are compared and the better IoU is the true
+        match and the ground truth of the other match is then rematch 
+        to the next best IoU, but it performs a recursive call to check
+        if the next best IoU also generates a duplicate match.
+
+        Parameters
+        ----------
+            dti: int
+                The detection index being matched to the current ground truth.
+            
+            gti: int
+                The current ground truth matched to the detection.
+
+            iou: float
+                The current best IoU that was computed for the current ground
+                truth against all detections.
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            MatchingAlgorithmException:
+                This function will raise an exception if a duplicate match
+                was left unchecked and was not rematched. 
+        """
+
+        twice_matched = [(d,g) for d, g in index_matches if d == dti]
+        if len(twice_matched) == 1:
+            # Compare the IoUs between duplicate matches.
+            dti, pre_gti = twice_matched[0]
+            if iou > iou_list[dti]:
+                index_matches.remove((dti, pre_gti))
+                iou_list[dti] = iou
+                index_matches.append((dti, gti))
+
+                # Rematch pre_gti
+                iou_grid[pre_gti][dti] = 0.
+                dti = np.argmax(iou_grid[pre_gti])
+                iou = max(iou_grid[pre_gti])
+                if iou > 0.:
+                    compare_matches(dti, pre_gti, iou)
+            else:
+                # Rematch gti
+                iou_grid[gti][dti] = 0.
+                dti = np.argmax(iou_grid[gti])
+                iou = max(iou_grid[gti])
+                if iou > 0.:
+                    compare_matches(dti, gti, iou)
+        elif len(twice_matched) == 0:
+            if iou > 0.:
+                iou_list[dti] = iou
+                index_matches.append((dti, gti))
+        else:
+            raise MatchingAlgorithmException(
+                "Duplicate matches were unchecked.") 
+
+    if len(gt_boxes) > 0:
+        for gti, gt in enumerate(gt_boxes):
+            if len(dt_boxes):
+                for dti, dt in enumerate(dt_boxes):
+                    # Find the IoUs of each prediction against the current gt.
+                    if metric.lower() == 'iou':
+                        iou_grid[gti][dti] = \
+                            bb_intersection_over_union(dt, gt)
+                    elif metric.lower() == 'centerpoint':
+                        iou_grid[gti][dti] = \
+                            1 - center_point_distance(dt, gt)
+                    else:
+                        raise MatchingAlgorithmException("Unknown matching " + 
+                                    "matching metric specified.")
+            else:
+                return [index_matches, [], list(range(0, len(gt_boxes))), []]
+                         
+            # A potential match is the detection that produced the highest IoU.
+            dti = np.argmax(iou_grid[gti])
+            iou = max(iou_grid[gti])
+            compare_matches(dti, gti, iou)
+               
+        # Find the unmatched predictions
+        index_unmatched_dt = list(range(0, len(dt_boxes)))
+        index_unmatched_gt = list(range(0, len(gt_boxes)))
+        for match in index_matches:
+            index_unmatched_dt.remove(match[0])
+            index_unmatched_gt.remove(match[1])                    
+    else:
+        index_unmatched_dt = list(range(0, len(dt_boxes)))
+        index_unmatched_gt = list()
+    return [index_matches, index_unmatched_dt, index_unmatched_gt, iou_list]
+
+def match_dt_gt(gt_boxes, dt_boxes, metric='iou'):
+    """
+    This function is version 1 of the matching algorithm. This function is 
+    lacking in the aspect of performing rematching of detections on the 
+    next best IoU.
     This function attempts a best-fit of
     predictions to ground truths.
     Unit-test for this method is defined under:
         file: test/test_metrics.py
         function: test_match_dt_gt
 
     Parameters
     ----------
-
         gt_boxes: list or np.ndarray
             A list of ground truth boxes [[x1, y1, x2, y2]...].
 
         dt_boxes: list or np.ndarray
             A list of prediction boxes [[x1, y1, x2, y2]...].
 
     Returns
@@ -52,16 +606,21 @@
 
     if len(gt_boxes) > 0:
         for dti, dt in enumerate(dt_boxes):
             iou, gti = -1, -1
             iou_row = list()
             # get the best IOU and its GT index for the detection
             for id, gt in enumerate(gt_boxes):
-                t_iou = DetectionMetrics.bbox_iou(dt, gt)
-                # t_iou = self.metrics.compute_iou(dt, gt, width, height)
+                if metric.lower() == 'iou':
+                    t_iou = bb_intersection_over_union(dt, gt)
+                elif metric.lower() == 'centerpoint':
+                    t_iou = 1 - center_point_distance(dt, gt)
+                else:
+                    raise MatchingAlgorithmException("Unknown matching " + 
+                                "matching metric specified.")
                 iou_row.append(t_iou)
                 if t_iou > iou:
                     iou = t_iou
                     gti = id
 
             iou_grid.append(iou_row)
 
@@ -116,8 +675,8 @@
         # [[1,2], [0,3], [4,8]].
         index_matches.sort(key=lambda x: x[1])
 
     else:
         index_extra_dt = list(range(0, len(dt_boxes)))
         index_missed_gt = list()
 
-    return [index_matches, index_extra_dt, index_missed_gt, iou_list]
+    return [index_matches, index_extra_dt, index_missed_gt, iou_list]
```

## deepview/validator/metrics/segmentationdata.py

```diff
@@ -13,15 +13,15 @@
     """
     This class acts as a container of SegmentationLabelData objects
     for each label and provides methods to capture the total
     number of ground truths, true positives, false positives,
     and false negatives for each pixel in the entire
     dataset that allows calculation of the overall metrics.
     Unit-test for this class is defined under:
-        file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+        file: test/test_segmentationdatacollection.py
 
     Parameters
     ----------
         None
 
     Raises
     ------
@@ -42,20 +42,19 @@
     @staticmethod
     def validate_num_parameter(num):
         """
         This method validates input parameter num to be of type integer only.
         This method is used to check when adding the number of ground truths,
         true positives, false positives, and false negatives to be integers.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_validate_num_parameter
 
         Parameters
         ----------
-
             num: int
                 The parameter to validate.
 
         Returns
         -------
             num: int
                 The validated integer type parameter
@@ -75,20 +74,19 @@
             return num
 
     def add_gt(self, num=1):
         """
         This method adds the number of recorded
         ground truths of the dataset.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_add_gt
 
         Parameters
         ----------
-
             num: int
                 The number of ground truths (pixels) to add.
 
         Returns
         -------
             None
 
@@ -106,15 +104,15 @@
         else:
             self.total_gt += num
 
     def get_gt(self):
         """
         This method gets the number of recorded ground truths.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_get_gt
 
         Parameters
         ----------
             None
 
         Returns
@@ -129,23 +127,22 @@
 
         return self.total_gt
 
     def add_label_data(self, label):
         """
         This method adds a SegmentationLabelData object for the label.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_add_label_data
 
         Parameters
         ----------
-
             label: str or int
-            The string label or the integer index
-            to place as a data container.
+                The string label or the integer index
+                to place as a data container.
 
         Returns
         -------
             None
 
         Raises
         ------
@@ -154,22 +151,21 @@
 
         self.label_data_list.append(SegmentationLabelData(label))
 
     def get_label_data(self, label):
         """
         This method grabs the SegmentationLabelData object by label.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_get_label_data
 
         Parameters
         ----------
-
-        label: str or int
-            A unique string label or integer index from the dataset.
+            label: str or int
+                A unique string label or integer index from the dataset.
 
         Returns
         -------
             None if the object does not exist.
 
             label_data: DetectionLabelData
                 The data container of the label specified.
@@ -184,15 +180,15 @@
                 return label_data
         return None
 
     def get_label_data_list(self):
         """
         This method gets the list containing the SegmentationLabelData objects.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_get_label_data_list
 
         Parameters
         ----------
             None
 
         Returns
@@ -208,15 +204,15 @@
         return self.label_data_list
 
     def get_labels(self):
         """
         This method gets the list containing the recorded
         unique string or integer index labels.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_get_labels
 
         Parameters
         ----------
             None
 
         Returns
@@ -235,20 +231,19 @@
     def capture_class(self, class_labels, labels):
         """
         This method records the unique labels encountered
         in the prediction and ground truth and creates a
         container (SegmentationLabelData) for the label
         found in the model predictions and ground truth.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_capture_class
 
         Parameters
         ----------
-
             class_labels: list of int.
                 All unique indices for the classes found from the ground
                 truth and the model predictions.
 
             labels: list or np.ndarray
                 This list contains unique string labels for the classes found.
 
@@ -281,15 +276,15 @@
 
     def sum_outcomes(self):
         """
         This method adds the total number of
         true positives, false positives, and
         false negatives in the dataset.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationdatacollection.py
+            file: test/test_segmentationdatacollection.py
             function: test_sum_outcomes
 
         Parameters
         ----------
             None
 
         Returns
@@ -323,15 +318,14 @@
     false positives, false negatives per label that allows calculation of
     mean average metrics.
     Unit-test for this class is defined under:
         file: test/deepview/validator/metrics/test_segmentationlabeldata.py
 
     Parameters
     ----------
-
         label: str or int
             The unique string or integer index label to base the container.
 
     Raises
     ------
         ValueError
             The methods will raise an exception if the provided parameters
@@ -350,15 +344,15 @@
         # positives, false negatives of the class.
         self.tps, self.fps, self.fns = 0, 0, 0
 
     def get_label(self):
         """
         This method grabs the class label being evaluated.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_get_label
 
         Parameters
         ----------
             None
 
         Returns
@@ -373,20 +367,19 @@
 
         return self.label
 
     def set_label(self, new_label):
         """
         This method sets the label to a different label.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_set_label
 
         Parameters
         ----------
-
             new_label: str or int
                 The label to change.
 
         Returns
         -------
             None
 
@@ -396,29 +389,28 @@
                 This method will raise the exception if the provided label is
                 neither string nor integer.
         """
 
         if not (isinstance(new_label, (str, int))):
             raise ValueError(
                 "The provided label has an incorrect type: {}. ".format(
-                    type(new_label)) + "Can only accept string or integer type.")
+                type(new_label)) + "Can only accept string or integer type.")
         else:
             self.label = new_label
 
     def add_gt(self, num=1):
         """
         This method adds the number of ground truth
         pixels for the class label from the entire dataset.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_add_gt
 
         Parameters
         ----------
-
             num: int
                 The number of ground truth pixels to add.
 
         Returns
         -------
             None
 
@@ -433,15 +425,15 @@
         self.gt += num
 
     def get_gt(self):
         """
         This method gets the number of ground truths that is currently
         recorded.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_get_gt
 
         Parameters
         ----------
             None
 
         Returns
@@ -457,15 +449,15 @@
         return self.gt
 
     def get_tp(self):
         """
         This method gets the total number of recorded
         true positives.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_get_tp
 
         Parameters
         ----------
             None
 
         Returns
@@ -481,15 +473,15 @@
         return self.tps
 
     def get_fn(self):
         """
         This method gets the total number of recorded
         false negatives.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_get_fn
 
         Parameters
         ----------
             None
 
         Returns
@@ -505,15 +497,15 @@
         return self.fns
 
     def get_fp(self):
         """
         This method gets the total number of recorded
         false positives.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_get_fp
 
         Parameters
         ----------
             None
 
         Returns
@@ -531,20 +523,19 @@
     def add_tp(self, num=1):
         """
         This method adds the number of true positives gathered.
         A true positive is when the prediction pixel is a 1 and the ground
         truth pixel is a 1. A true positive is also when the prediction pixel
         is a 0 and the ground truth pixel is a 0.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_add_tp
 
         Parameters
         ----------
-
             num: int
                 The number of true positives to add.
 
         Returns
         -------
             None
 
@@ -560,20 +551,19 @@
 
     def add_fn(self, num=1):
         """
         This method adds the number of false negatives gathered.
         A false negative is when the prediction pixel is a 0 but the ground
         truth pixel is a 1.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_add_fn
 
         Parameters
         ----------
-
             num: int
                 The number of false negatives to add.
 
         Returns
         -------
             None
 
@@ -590,20 +580,19 @@
     def add_fp(self, num=1):
         """
         This method adds the number of false positives gathered.
         A false positive is when the prediction pixel is a 1 but the ground
         truth pixel is a 0.
 
         Unit-test for this method is defined under:
-            file: test/deepview/validator/metrics/test_segmentationlabeldata.py
+            file: test/test_segmentationlabeldata.py
             function: test_add_fp
 
         Parameters
         ----------
-
             num: int
                 The number of false positives to add.
 
         Returns
         -------
             None
```

## deepview/validator/metrics/segmentationmetrics.py

```diff
@@ -28,15 +28,14 @@
                 -> mACC.
 
     Unit-test for this class is defined under:
         test/deepview/validator/metrics/test_segmentationmetrics.py
 
     Parameters
     ----------
-
         segmentationdatacollection: SegmentationDataCollection
             This contains the number of ground truths in the dataset
             and tp, fp, and fn per class.
 
     Raises
     ------
         DivisionByZeroException
@@ -69,15 +68,14 @@
 
         Unit-test for this method is defined under:
             file: test/deepview/validator/metrics/test_segmentationmetrics.py
             function: test_compute_overall_metrics
 
         Parameters
         ----------
-
             total_tp: int
                 Total number of true positives in the dataset.
 
             total_fn: int
                 Total number of false negatives in the dataset.
 
             total_fp: int
```

## deepview/validator/metrics/segmentationutils.py

```diff
@@ -15,15 +15,14 @@
     This function validates the input mask to be a numpy array.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_validate_input_mask
 
     Parameters
     ----------
-
         mask: np.ndarray
             The mask to validate as a numpy array.
 
     Returns
     -------
         mask: np.ndarray
             The validated mask being a numpy array.
@@ -46,15 +45,14 @@
     This function creates a numpy array of masks from a given polygon.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_create_mask_image
 
     Parameters
     ----------
-
         height: int
             The height of the image.
 
         width: int
             The width of the image.
 
         gt_instance: dict
@@ -122,15 +120,14 @@
     and background as 0.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_create_binary_mask
 
     Parameters
     ----------
-
         mask: (height, width) np.ndarray
             Mask of class labels unique to each object.
 
     Returns
     -------
         binary_mask: (height, width) np.ndarray
             Binary mask of 1's and 0's.
@@ -153,15 +150,14 @@
     specified class and 0 represents other classes.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_create_mask_class
 
     Parameters
     ----------
-
         mask: (height, width) np.ndarray
             Mask of class labels unique to each object.
 
         cls: int
             The integer representing the class in the mask
             to keep as a value of 1.
 
@@ -197,15 +193,14 @@
     the new mask with its class.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_create_mask_classes
 
     Parameters
     ----------
-
         new_mask: (height, width) np.ndarray
             The current binary mask.
 
         cls: int
             Class representing the 1's in the new mask.
 
         current_mask: (height, width) np.ndarray
@@ -245,15 +240,14 @@
     in the image and the rest of the objects will have values of zeroes.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_create_mask_background
 
     Parameters
     ----------
-
         mask: (height, width) np.ndarray
             Matrix array of classes representing the image pixels.
 
     Returns
     -------
         mask: (height, width) np.ndarray
             Binary mask of 1's and 0's, where 1's is background and
@@ -280,15 +274,14 @@
     false positive, or a false negative.
     Unit-test for this function is defined under:
         file: test/deepview/validator/metrics/test_segmentationutils.py
         function: test_classify_mask
 
     Parameters
     ----------
-
         height: int
             The height of the model input shape.
 
         gt_class_mask: (height, width) np.ndarray
             2D binary array representing pixels forming the image ground truth.
 
         dt_class_mask: (height, width) np.ndarray
```

## deepview/validator/runners/core.py

```diff
@@ -48,14 +48,16 @@
             self.source = self.validate_model_path(source)
         else:
             self.model = source
 
         self.model = None
         self.input_shape = None
         self.device = None
+        self.nms_type = None
+        self.max_detections = None
 
         self.box_timings = list()
         self.inference_timings = list()
         self.loading_input_timings = list()
 
         self.sync_dict = {
             "motorbike": "motorcycle",
@@ -90,26 +92,25 @@
         Raises
         ------
 
             FileNotFoundError
                 This exception will be raised if the path to the
                 model does not exist.
         """
-
+        
         if not exists(source):
             raise FileNotFoundError(
                 "Model file is expected to be at: {}".format(source))
-
         return source
 
     def load_model(self):
         """Abstract Method"""
         pass
 
-    def run_single_instance(self, image, image_path):
+    def run_single_instance(self, image):
         """Abstract Method"""
         pass
 
     def postprocessing(self, outputs):
         """Abstract Method"""
         pass
 
@@ -121,14 +122,47 @@
         """Abstract Method"""
         pass
 
     def get_input_shape(self):
         """Abstract Method"""
         pass
 
+    @staticmethod
+    def clamp(value, min=0, max=1):
+        """
+        This method clamps a given value between 0 and 1 by default. If
+        the value is in between the set min and max, then it is returned.
+        Otherwise it returns either min or max depending on
+        which is the closest.
+        Unit-test for this method is defined under:
+            file: test/deepview/validator/runners/test_deepviewrt.py
+            function: test_clamp
+
+        Parameters
+        ----------
+            value: float or int
+                Value to clamp between 0 and 1 (defaults).
+
+            min: int or float
+                Minimum acceptable value.
+
+            max: int or float
+                Maximum acceptable value.
+
+        Returns
+        -------
+            value: int or float
+                This is the clamped value.
+
+        Raises
+        ------
+            None
+        """
+        return min if value < min else max if value > max else value
+
     def summarize(self):
         """
         This method returns a summary of all the timings.
         (mean, avg, max) of (load, inference, box).
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_core.py
             function: test_summarize
```

## deepview/validator/runners/deepviewrt.py

```diff
@@ -23,15 +23,14 @@
     """
     This class runs DeepViewRT models using the VAAL API.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_deepviewrt.py
 
     Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         labels: list
             Unique string labels.
 
         max_detections: int
@@ -93,28 +92,29 @@
             does not exist and the provided image is not a numpy.ndarray.
     """
 
     def __init__(
         self,
         model_path,
         labels,
-        max_detections=100,
+        max_detections,
         warmup=0,
         engine='npu',
         norm='raw',
         nms_type=None,
         detection_score_threshold=0.5,
         detection_iou_threshold=0.5,
         label_offset=0,
         box_format='xyxy'
     ):
         super(DeepViewRTRunner, self).__init__(model_path)
 
         self.labels = labels
         self.label_offset = label_offset
+        self.max_detections = max_detections
 
         try:
             import deepview.vaal as vaal
         except ImportError:
             raise MissingLibraryException(
                 "vaal library is needed to run DeepViewRT models.")
 
@@ -126,32 +126,41 @@
         except AttributeError:
             raise EnvironmentError(
                 'Did not find Vaal Context. Try setting the environment \
                     variable VAAL_LIBRARY to the VAAL library.')
         self.device = self.ctx.device
 
         if nms_type is not None:
-            if nms_type.lower() not in ['standard', 'fast', 'matrix']:
+            if nms_type.lower() not in [
+                'standard', 'fast', 'matrix', 'tensorflow']:
                 raise UnsupportedNMSException(nms_type)
-            self.ctx['nms_type'] = nms_type
+            elif nms_type.lower() in ['standard', 'fast', 'matrix']:
+                self.ctx['nms_type'] = nms_type
+            self.nms_type = nms_type
+        else:
+            self.nms_type = 'fast'
 
         if norm.lower() not in [
             'raw',
             'signed',
             'unsigned',
             'whitening',
                 'imagenet']:
             raise UnsupportedNormalizationException(self.norm)
 
         if max_detections is not None:
             self.ctx['max_detection'] = max_detections
 
+        self.detection_threshold = detection_score_threshold
+        self.detection_iou = detection_iou_threshold
+
         self.ctx['box_format'] = box_format
         self.ctx['score_threshold'] = detection_score_threshold
         self.ctx['iou_threshold'] = detection_iou_threshold
+        #self.ctx['label_offset'] = label_offset
 
         if norm is not None:
             if norm == 'raw':
                 self.ctx['normalization'] = vaal.ImageProc.RAW.value
             elif norm == 'signed':
                 self.ctx['proc'] = vaal.ImageProc.SIGNED_NORM.value
             elif norm == 'unsigned':
@@ -178,24 +187,22 @@
         (load, inference, box).
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_deepviewrt.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str or np.ndarray
                 If the dataset is Darknet, then the image path is used which
                 is a string.
                 If the dataset is TFRecords, then the image is a
                 np.ndarray.
 
         Returns
         -------
-
             boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -232,38 +239,42 @@
 
         start = clock_now()
         self.ctx.run_model()
         infer_ns = clock_now() - start
         self.inference_timings.append(infer_ns * 1e-6)
 
         start = clock_now()
-        bxs = self.ctx.boxes()
-        boxes, classes, scores = self.postprocessing(bxs)
+        if self.nms_type.lower() == "tensorflow": 
+            outputs = list()
+            for i in range(4):
+                out = self.ctx.output(index=i)
+                if out is not None:
+                    outputs.append(out.array())
+        else:
+            outputs = self.ctx.boxes()
+        outputs = self.postprocessing(outputs)
         boxes_ns = clock_now() - start
         self.box_timings.append(boxes_ns * 1e-6)
-
-        return boxes, classes, scores
+        return outputs
 
     def postprocessing(self, outputs):
         """
         This method collects the bounding boxes, scores and
         labels for the image.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_deepviewrt.py
             function: test_postprocessing
 
         Parameters
         ----------
-
-            outputs:
+            outputs: list
                 This contains bounding boxes, scores, labels.
 
         Returns
         -------
-
             boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -274,80 +285,168 @@
         ------
             NonMatchingIndexException
                 This method will raise an exception if the model label
                 index is out of bounds to the input labels list
                 or the unique labels contained within the model.
         """
 
-        boxes, classes, scores = list(), list(), list()
+        if self.nms_type.lower() == "tensorflow":
+            boxes, scores = None, None
+            for out in outputs:
+                if len(out.shape) == 3:
+                    scores = out
+                    continue
+                if out.shape[-1] == 4 and out.shape[-2] == 1:
+                    boxes = out
+                    continue
+            boxes, classes, scores = self.process_tensorflow_nms(boxes, scores)
+        else:
+            boxes, classes, scores = list(), list(), list()
+            for box in outputs:
+                output = self.process_vaal_nms(box)
+                if output is not None:
+                    label, box, score = output
+                    classes.append(label)
+                    boxes.append(box)
+                    scores.append(score)
+                else:
+                    return output
+            boxes = np.array(boxes)
+            classes = np.array(classes)
+            scores = np.array(scores)
+        return boxes, classes, scores
+    
+    def process_vaal_nms(self, box):
+        """
+        This method processes model detections using the VAAL NMS.
+        Unit-test for this method is defined under:
+            file: test/test_deepviewrt.py
+            function: test_process_vaal_nms
+
+        Parameters
+        ----------
+            box: deepview.vaal.library.VAALBox
+                This contains the label, box, and score from the model.
 
-        for box in outputs:
-            label = box.label + self.label_offset
+        Returns
+        -------
+            The NMS processed label, box, and score.
 
+        Raises
+        ------
+            NonMatchingIndexException
+                This method will raise an exception if the label index 
+                returned by the model does not match any index position in
+                either context labels or the provided labels from the dataset.
+        """
+
+        label = box.label + self.label_offset
+        if label >= 0:
             if len(self.ctx.labels):
-                if label >= 0:
-                    try:
+                try:
+                    if self.ctx.labels[label] == "VisionPack Trial Expired":
+                        return None
+                    else:
                         dt_class = self.ctx.labels[label].lower().rstrip(
                             '\"').lstrip('\"')
-                    except IndexError:
-                        raise NonMatchingIndexException(label)
-                    classes.append(dt_class)
-                    boxes.append([box.xmin, box.ymin, box.xmax, box.ymax])
-                    scores.append(box.score)
+                except IndexError:
+                    raise NonMatchingIndexException(label)
             elif len(self.labels):
-                if label >= 0:
-                    try:
-                        dt_class = self.labels[label].lower().rstrip(
-                            '\"').lstrip('\"')
-                    except IndexError:
-                        raise NonMatchingIndexException(label)
-                    classes.append(dt_class)
-                    boxes.append([box.xmin, box.ymin, box.xmax, box.ymax])
-                    scores.append(box.score)
+                try:
+                    dt_class = self.labels[label].lower().rstrip(
+                        '\"').lstrip('\"')
+                except IndexError:
+                    raise NonMatchingIndexException(label)
             else:
-                if label >= 0:
-                    classes.append(label)
-                    boxes.append([box.xmin, box.ymin, box.xmax, box.ymax])
-                    scores.append(box.score)
-
-        boxes = np.array(boxes)
-        classes = np.array(classes)
-        scores = np.array(scores)
-
-        return boxes, classes, scores
-
-    def clamp(self, value, min=0, max=1):
+                dt_class = label
+            dt_box = [box.xmin, box.ymin, box.xmax, box.ymax]
+            dt_score = box.score
+            return dt_class, dt_box, dt_score
+            
+    def process_tensorflow_nms(self, boxes, scores):
         """
-        This method clamps a given value between 0 and 1 by default. If
-        the value is in between the set min and max, then it is returned.
-        Otherwise it returns either min or max depending on
-        which is the closest.
+        This method processes model detections using tensorflow NMS.
         Unit-test for this method is defined under:
-            file: test/deepview/validator/runners/test_deepviewrt.py
-            function: test_clamp
+            file: test/test_deepviewrt.py
+            function: test_process_tensorflow_nms
 
         Parameters
         ----------
-
-            value: float or int
-                Value to clamp between 0 and 1 (defaults).
-
-            min: int or float
-                Minimum acceptable value.
-
-            max: int or float
-                Maximum acceptable value.
+            boxes: np.ndarray
+                This array contains the bounding boxes for one image.
+            
+            scores: np.ndarray
+                This array contains the model score per bounding box.
 
         Returns
         -------
-            value: int or float
-                This is the clamped value.
+            labels, bounding boxes, scores: np.ndarray
+                These are the model detections after being processed using
+                tensorflow NMS.
+        
+        Raises
+        ------
+            NonMatchingIndexException
+                This exception will be raised if the model label index does
+                not match any index position in the provided labels from
+                the dataset. 
         """
 
-        return min if value < min else max if value > max else value
+        try:
+            import tensorflow as tf
+        except ImportError:
+            pass
+
+        # Two outputs
+        # scores = 0.0029488196596503258 * (scores - (-128 + 128))
+        # boxes = 0.005290760658681393 * (boxes - (-98 + 128))
+
+        scores = 0.003921568859368563 * scores
+        boxes = 0.003921568859368563 * boxes 
+
+        # specific to model 
+        # scores = 0.0029488196596503258 * (scores - (-128 + 128))
+        # boxes = 0.005336543545126915 * (boxes - (-98 + 128))
+        
+        if self.label_offset > 0:
+            scores = scores[..., self.label_offset:]
+
+        nmsed_boxes, nmsed_scores, nmsed_classes, valid_boxes = \
+                tf.image.combined_non_max_suppression(
+                boxes.astype(np.float32),
+                scores.astype(np.float32),
+                100,
+                100,
+                iou_threshold=self.detection_iou,
+                score_threshold=self.detection_threshold
+            )
+        nmsed_boxes = nmsed_boxes.numpy()
+        nmsed_classes = tf.cast(nmsed_classes, tf.int32)
+
+        nms_predicted_boxes = [nmsed_boxes[i, :valid_boxes[i], :]
+                            for i in range(nmsed_boxes.shape[0])][0]
+        nms_predicted_classes = [nmsed_classes.numpy()[i, :valid_boxes[i]]
+                                for i in range(nmsed_classes.shape[0])][0]
+        nms_predicted_scores = [nmsed_scores.numpy()[i, :valid_boxes[i]]
+                                for i in range(nmsed_scores.shape[0])][0]
+
+        if len(self.labels):
+            string_nms_predicted_classes = list()
+            format_nms_predicted_boxes = list()
+            for cls, box in zip(nms_predicted_classes, nms_predicted_boxes):
+                try:
+                    string_nms_predicted_classes.append(
+                        self.labels[int(cls)])
+                except IndexError:
+                    raise NonMatchingIndexException(cls)
+                format_nms_predicted_boxes.append(self.get_boxes(box))
+            nms_predicted_classes = np.array(string_nms_predicted_classes)
+            nms_predicted_boxes = np.array(format_nms_predicted_boxes)
+        
+        return nms_predicted_boxes, nms_predicted_classes, nms_predicted_scores
 
     def get_input_type(self):
         """
         This method returns the model input type.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_deepviewrt.py
             function: test_get_input_type
@@ -363,15 +462,14 @@
 
         Raises
         ------
             NotImplementedError
                 This method will raise an exception because it has not
                 been implemented yet.
         """
-
         raise NotImplementedError("This method has not been implemented.")
 
     def get_output_type(self):
         """
         This method returns the model output type.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_deepviewrt.py
@@ -388,15 +486,14 @@
 
         Raises
         ------
             NotImplementedError
                 This method will raise an exception because it has not
                 been implemented yet.
         """
-
         raise NotImplementedError("This method has not been implemented.")
 
     def get_input_shape(self):
         """
         This method gets the model input shape.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_deepviewrt.py
@@ -409,13 +506,10 @@
         Returns
         -------
             type: tuple or list
                 The model input shape.
 
         Raises
         ------
-            NotImplementedError
-                This method will raise an exception because it has not
-                been implemented yet.
+            None
         """
-
-        raise NotImplementedError("This method has not been implemented.")
+        return self.ctx.tensor('serving_default_input_1:0').shape
```

## deepview/validator/runners/keras.py

```diff
@@ -19,15 +19,14 @@
     """
     This class runs the Keras (h5) models using the tensorflow library.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_keras.py
 
      Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         labels: list
             Unique string labels.
 
         detection_score_threshold: float
@@ -40,14 +39,15 @@
             The type of image normalization to perform (raw, unsigned, signed).
 
         label_offset: int
             The index offset to match label index to the ground truth index.
 
         max_detections: int, default 100
             Maximum number of detections used by TensorFlow NMS
+
     Raises
     ------
         UnsupportedNormalizationException
             This exception will be raised if the passed image normalization is
             not recognized.
 
         NonMatchingIndexException
@@ -65,29 +65,30 @@
                 numpy.ndarray. Furthermore it will raise an exception if the
                 provided image path does not exist.
     """
 
     def __init__(
             self,
             model_path,
-            labels,
+            labels=None,
             detection_iou_threshold=0.5,
             detection_score_threshold=0.5,
             norm='unsigned',
             label_offset=0,
             max_detections=100
     ):
         super(DetectionKerasRunner, self).__init__(model_path)
 
         self.iou_threshold = detection_iou_threshold
         self.score_threshold = detection_score_threshold
         self.norm = norm.lower()
         self.labels = labels
         self.label_offset = label_offset
-        self.max_num_detections = max_detections
+        self.max_detections = max_detections
+        self.nms_type = "tensorflow"
 
         try:
             import tensorflow as tf
         except ImportError:
             raise MissingLibraryException(
                 "tensorflow is needed to load the model.")
         
@@ -95,15 +96,15 @@
             self.device = "gpu"
         elif len(tf.config.list_physical_devices('CPU')):
             self.device = "cpu"
         else:
             self.device = "unknown"
 
         if self.norm not in ['raw', 'signed', 'unsigned']:
-            raise UnsupportedNormalizationException(self.norm)
+            raise UnsupportedNormalizationException(norm)
 
     def load_model(self):
         """
         This method loads the Keras model using tensorflow.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_keras.py
             function: test_load_model
@@ -137,24 +138,22 @@
         records the timing information of the model.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_keras.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str or np.ndarray
                 If the dataset is Darknet, then the image path is used which
                 is a string.
                 If the dataset is TFRecords, then the image is a
                 np.ndarray.
 
         Returns
         -------
-
             boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -181,27 +180,25 @@
         infer_ns = clock_now() - start
         self.inference_timings.append(infer_ns * 1e-6)
 
         start = clock_now()
         boxes, classes, scores = self.postprocessing(outputs)
         boxes_ns = clock_now() - start
         self.box_timings.append(boxes_ns * 1e-6)
-
         return boxes, classes, scores
 
     def preprocess_input(self, image):
         """
         This method performs images normalizations (signed, unsigned, raw).
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_keras.py
             function: test_preprocess_input
 
         Parameters
         ----------
-
             image: np.ndarray
                     The image to perform normalization
 
         Returns
         -------
             image: np.ndarray
                 Depending on the normalization, the image will be returned.
@@ -223,21 +220,19 @@
         This method extracts the boxes, labels, and scores.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_keras.py
             function: test_postprocessing
 
         Parameters
         ----------
-
             outputs:
                 This contains bounding boxes, scores, labels.
 
         Returns
         -------
-
             boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -257,46 +252,43 @@
 
         try:
             import tensorflow as tf
         except ImportError:
             raise MissingLibraryException(
                 "tensorflow is needed to perform NMS operations.")
 
-        boxes = outputs[2]
+        boxes = outputs[-2]
 
         if self.label_offset > 0:
-            scores = outputs[3][..., self.label_offset:]
+            scores = outputs[-1][..., self.label_offset:]
         else:
-            scores = outputs[3]
+            scores = outputs[-1]
 
         nmsed_boxes, nmsed_scores, nmsed_classes, valid_boxes = \
             tf.image.combined_non_max_suppression(
                 boxes,
                 scores,
-                self.max_num_detections,
-                self.max_num_detections,
+                self.max_detections,
+                self.max_detections,
                 iou_threshold=self.iou_threshold,
                 score_threshold=self.score_threshold,
                 clip_boxes=False
             )
 
         nmsed_boxes = nmsed_boxes.numpy()
         nmsed_classes = tf.cast(nmsed_classes, tf.int32)
 
-        if self.label_offset > 0:
-            nmsed_classes = nmsed_classes + self.label_offset
-
         nms_predicted_boxes = [nmsed_boxes[i, :valid_boxes[i], :]
                                for i in range(nmsed_boxes.shape[0])][0]
         nms_predicted_classes = [nmsed_classes.numpy()[i, :valid_boxes[i]]
                                  for i in range(nmsed_classes.shape[0])][0]
         nms_predicted_scores = [nmsed_scores.numpy()[i, :valid_boxes[i]]
                                 for i in range(nmsed_scores.shape[0])][0]
 
-        if len(self.labels):
+        if self.labels:
             string_nms_predicted_classes = list()
             for cls in nms_predicted_classes:
                 try:
                     string_nms_predicted_classes.append(
                         self.labels[int(cls)])
                 except IndexError:
                     raise NonMatchingIndexException(cls)
@@ -343,15 +335,14 @@
             type: str
                 The model output type
 
         Raises
         ------
             None
         """
-
         return 'float32'
 
     def get_input_shape(self):
         """
         This method gets the model input shape.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_keras.py
@@ -378,19 +369,17 @@
     This class runs Keras models to produce 
     segmentation masks.
     Unit-test for this class is defined under:
         file: test/test_runners.py
     
     Parameters
     -----------
-
         model_path: str
             The path to the Keras model.
 
-    
     Raises
     ------
         MissingLibraryException:
             This exception will be raised if the the tensorflow library
             which is used to load and run a keras model is not installed.
     """
 
@@ -425,15 +414,14 @@
         image to produce a mask for the image.
         Unit-test for this method is defined under:
             file: test/test_runners.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str or np.ndarray
                 If the dataset is Darknet, then the image path is used which
                 is a string.
                 If the dataset is TFRecords, then the image is a
                 np.ndarray.
 
         Returns
@@ -469,15 +457,14 @@
         infer_ns = clock_now() - start
         self.inference_timings.append(infer_ns * 1e-6)
 
         start = clock_now()
         mask = tf.argmax(output, axis=-1)[0].numpy().astype(np.uint8)
         boxes_ns = clock_now() - start
         self.box_timings.append(boxes_ns * 1e-6)
-
         return mask
 
     def get_input_shape(self):
         """
         This method returns the input shape of the 
         Keras model.
         Unit-test for this method is defined under:
@@ -489,27 +476,26 @@
             None
         
         Returns
         -------
             input shape: tuple
                 This is the model input shape (height, width)
         """
-        _, mH, mW, num_classes = self.model.input.shape
+        _, mH, mW, _ = self.model.input.shape
         return (mH, mW)
 
 
 class InferenceKerasModel(DetectionKerasRunner):
     """
     This class provides inference to the Keras Runner during model training.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_keras.py
 
      Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         labels: list
             Unique string or integer index labels.
 
         detection_score_threshold: float
@@ -563,15 +549,14 @@
             model_path=model,
             labels=labels,
             detection_iou_threshold=detection_iou_threshold,
             detection_score_threshold=detection_score_threshold,
             norm=norm,
             label_offset=label_offset,
             max_detections=max_detections
-
         )
 
         self.model = model
         self.input_shape = self.model.input.shape[1:]
 
     def load_model(self):
         """Abstract Method"""
```

## deepview/validator/runners/offline.py

```diff
@@ -29,15 +29,14 @@
     are in Yolo format. This class will read the text files to be validated.
 
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_offline.py
 
     Parameters
     ----------
-
         annotation_source: str
             This is the path to the model prediction annotations
             stored in text files with yolo format annotations.
             [cls score xc yc width height].
 
         labels: list
             This contains the unique string labels in the dataset.
@@ -98,22 +97,20 @@
         image name and returns the bounding boxes and labels.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_offline.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str
                 The path to the image. This is used to match the
                 annotation to be read.
 
         Returns
         -------
-
             boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -147,30 +144,29 @@
                 "to the image. Provided with type: {}".format(type(image)))
 
         try:
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
                 annotation = np.genfromtxt(annotation_path)
         except FileNotFoundError:
-            return [], [], []
+            return np.array([]), np.array([]), np.array([])
 
         if len(annotation):
             annotation = annotation.reshape(-1, 6)
             boxes = annotation[:, 2:6]
             boxes = self.transformer(boxes) if self.transformer else boxes
         else:
-            return [], [], []
+            return np.array([]), np.array([]), np.array([])
 
         scores = annotation[:, 1:2].flatten().astype(np.float32)
         labels = annotation[:, 0:1].flatten().astype(
             np.int32) + self.label_offset
 
         if len(self.labels):
             string_labels = list()
             for label in labels:
                 try:
                     string_labels.append(self.labels[int(label)])
                 except IndexError:
                     raise NonMatchingIndexException(label)
             labels = string_labels
-
         return boxes, labels, scores
```

## deepview/validator/runners/tensorrt.py

```diff
@@ -19,15 +19,14 @@
     """
     This class runs TensorRT Engines (.trt).
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_tensorrt.py
 
     Parameters
     ----------
-
         engine_path: str
             The path to the TensorRT engine.
 
         labels: list
             Unique string labels.
 
         preprocessor: str
@@ -178,19 +177,19 @@
         This method executes inference on a batch of images.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tensorrt.py
             function: test_infer
 
         Parameters
         ----------
-
             batch: np.ndarray
 
         Returns
         -------
+            
 
         Raises
         ------
             MissingLibraryException
                 This method will raise an exception if the pycuda library
                 is not installed.
         """
@@ -218,21 +217,19 @@
         will be performed here.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tensorrt.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str
                 The path to the image to feed the engine.
 
         Returns
         -------
-
             boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -283,15 +280,14 @@
         the value provided.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tensorrt.py
             function: test_resize_pad
 
         Parameters
         ----------
-
             image: PIL object
                 image to resize.
 
             pad_color: tuple
                 The RGB values to use for the padded area.
                 Default: Black/Zeros.
 
@@ -338,21 +334,19 @@
         * EfficientDet: Resizes and pads the image to fit the input size.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tensorrt.py
             function: test_preprocess_input
 
         Parameters
         ----------
-
             image_path: str
                 The path to the image to load from disk.
 
         Returns
         -------
-
             image: np.ndarray
                 A numpy array holding the image sample
 
             scale: int
                 resize scale used.
 
             (width, height): tuple
@@ -391,28 +385,26 @@
         detection bounding boxes, classes and scores.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tensorrt.py
             function: test_postprocessing
 
         Parameters
         ----------
-
             outputs:
 
             img_scale:
 
             width: int
                 The width of the image
 
             height: int
                 The height of the image
 
         Returns
         -------
-
              boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             scores: np.ndarray
```

## deepview/validator/runners/tflite.py

```diff
@@ -19,15 +19,14 @@
     """
     This class runs TensorFlow Lite models.
     Unit-test for this method is defined under:
         file: test/deepview/validator/runners/test_tflite.py
 
     Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         warmup: int
             The number of warmup iterations to perform.
 
         anchors: str
@@ -168,21 +167,19 @@
         the timings.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tflite.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str
                 The path to the image.
 
         Returns
         -------
-
             nmsed_boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             nmsed_classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             nmsed_scores: np.ndarray
@@ -234,15 +231,14 @@
 
         Parameters
         ----------
             None
 
         Returns
         -------
-
             nmsed_boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             nmsed_classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             nmsed_scores: np.ndarray
@@ -326,15 +322,14 @@
         This method handles normalizations on the image.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tflite.py
             function: test_preprocess_input
 
         Parameters
         ----------
-
             image: np.ndarray
                 The image perform normalization.
 
         Returns
         -------
             image: np.ndarray
                 Depending on the normalization, the image will be returned
@@ -360,21 +355,19 @@
         This method retrieves the boxes, scores and labels.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_tflite.py
             function: test_postprocessing
 
         Parameters
         ----------
-
             outputs:
                 This contains bounding boxes, scores, labels.
 
         Returns
         -------
-
             nmsed_boxes: np.ndarray
                 The prediction bounding boxes.. [[box1], [box2], ...]
 
             nmsed_classes: np.ndarray
                 The prediction labels.. [cl1, cl2, ...]
 
             nmsed_scores: np.ndarray
```

## deepview/validator/runners/modelclient/boxes.py

```diff
@@ -18,15 +18,14 @@
     """
     This class runs Yolo DeepViewRT models using modelrunner.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/modelclient/test_boxes.py
 
     Parameters
     ----------
-
         model_path: str
             This is the path to the model.
 
         target: str
             This is the modelrunner target in the EVK. Ex. 10.10.40.205:10817.
 
         labels: list
@@ -106,15 +105,14 @@
         This method runs the model to produce predictions on a single image.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str
                 The path to the image.
 
         Returns
         -------
             boxes: np.ndarray
                 This contains the bounding boxes [[box1], [box2], ...]
@@ -177,22 +175,20 @@
         This method extracts the boxes, labels, and scores.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_postprocessing
 
         Parameters
         ----------
-
             Outputs: np.ndarray
                 This contains information regarding boxes,
                 labels, and scores
 
         Returns
         -------
-
             boxes: np.ndarray
                 This contains the bounding boxes [[box1], [box2], ...]
 
             classes: np.ndarray
                 This contains the labels [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -260,22 +256,20 @@
         This method builds the embeds decoder on the model.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_build_decoder
 
         Parameters
         ----------
-
             Outputs: np.ndarray
                 This contains information regarding boxes,
                 labels, and scores
 
         Returns
         -------
-
             boxes: np.ndarray
                 This contains the bounding boxes [[box1], [box2], ...]
 
             scores: np.ndarray
                 This contains the scores [score, score, ...]
 
         Raises
@@ -332,15 +326,14 @@
         This method adds resolution decoder to the model.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_add_resolution_decoder
 
         Parameters
         ----------
-
             output: np.ndarray
 
             anchors: np.ndarray
 
             stride: np.ndarray
 
             scale: np.ndarray
@@ -407,15 +400,14 @@
     """
     This class runs Yolo DeepViewRT models using modelrunner.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/modelclient/test_boxes.py
 
     Parameters
     ----------
-
         model_path: str
             This is the path to the model.
 
         target: str
             This is the modelrunner target in the EVK. Ex. 10.10.40.205:10817.
 
         labels: list
@@ -454,27 +446,29 @@
         self,
         model_path,
         target,
         labels,
         detection_iou_threshold,
         detection_score_threshold,
         norm,
-        decoder=False
+        decoder=False,
+        label_offset=0
     ):
 
         super(BoxesYolo, self).__init__(
             model_path=model_path,
             target=target
         )
 
         self.labels = labels
         self.iou_threshold = detection_iou_threshold
         self.score_threshold = detection_score_threshold
         self.norm = norm
         self.decoder = decoder
+        self.label_offset = label_offset
 
         self.num_classes = 2
         self.strides = [8, 16, 32]
         self.anchors = np.array([
             10, 13, 16, 30, 33, 23, 30, 61, 62, 45,
             59, 119, 116, 90, 156, 198, 373, 326
         ]).reshape(3, 1, 3, 1, 2)
@@ -484,15 +478,14 @@
         This method runs the model to produce predictions on a single image.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str
                 The path to the image.
 
         Returns
         -------
             boxes: np.ndarray
                 This contains the bounding boxes [[box1], [box2], ...]
@@ -544,22 +537,20 @@
         This method extracts the boxes, labels, and scores.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_postprocessing
 
         Parameters
         ----------
-
             Outputs: np.ndarray
                 This contains information regarding boxes,
                 labels, and scores
 
         Returns
         -------
-
             boxes: np.ndarray
                 This contains the bounding boxes [[box1], [box2], ...]
 
             classes: np.ndarray
                 This contains the labels [cl1, cl2, ...]
 
             scores: np.ndarray
@@ -619,15 +610,14 @@
         This method reads the model parameters from a given URL.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_read_quant_parameters
 
         Parameters
         ----------
-
             url: str
                 Ex: http://10.10.40.205:10818/v1
 
         Returns
         -------
             parameters: dict
 
@@ -664,22 +654,20 @@
         This method gets the boxes and the scores.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_get_boxes_scores
 
         Parameters
         ----------
-
             predictions:
 
             qunat:
 
         Returns
         -------
-
             boxes: np.ndarray
                 The model prediction bounding boxes.
 
             scores: np.ndarray
                 The model prediction confidence scores.
 
         Raises
@@ -706,24 +694,22 @@
         to the ground truth.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test__predict
 
         Parameters
         ----------
-
             predictions:
 
             num_classes: int
 
             num_fakes: int
 
         Returns
         -------
-
             boxes: np.ndarray
                 The model prediction bounding boxes.
 
             scores: np.ndarray
                 The model prediction confidence scores.
 
         Raises
@@ -780,15 +766,14 @@
         This method performs dequantization.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test_de_quantize
 
         Parameters
         ----------
-
             predictions:
 
             qunat_params: dict
 
         Returns
         -------
             pp: list
@@ -816,15 +801,14 @@
 
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test__make_grid
 
         Parameters
         ----------
-
             ny: int
 
             nx: int
 
         Returns
         -------
 
@@ -851,15 +835,14 @@
 
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_boxes.py
             function: test__xywh2xyxy
 
         Parameters
         ----------
-
             xywh:
 
         Returns
         -------
 
         Raises
         ------
```

## deepview/validator/runners/modelclient/core.py

```diff
@@ -15,15 +15,14 @@
     """
     This class uses the modelclient API to run DeepViewRT models.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/modelclient/test_core.py
 
     Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         target: str
             The modelrunner target in the EVK. Ex. 10.10.40.205:10817.
 
     Raises
```

## deepview/validator/runners/modelclient/segmentation.py

```diff
@@ -18,15 +18,14 @@
     This class uploads the model to the target and runs the model
     per image.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/modelclient/test_segmentation.py
 
     Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         target: str
             The modelrunner target in the EVK. Ex. 10.10.40.205:10817.
 
     Raises
@@ -91,15 +90,14 @@
         This method reads the image and grabs the segmentation mask output.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_segmentation.py
             function: test_run_single_instance
 
         Parameters
         ----------
-
             image: str or np.ndarray
                 This is either the path to the image or
                 a numpy array image.
 
         Returns
         -------
             dt_mask: np.ndarray
@@ -161,15 +159,14 @@
     models and decodes the output to generate
     the segmentation mask.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/modelclient/test_segmentation.py
 
     Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         target: str
             The modelrunner target in the EVK. Ex. 10.10.40.205:10817.
 
     Raises
@@ -198,15 +195,14 @@
         of the objects ids.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_segmentation.py
             function: test_decode
 
         Parameters
         ----------
-
             outputs: dict
                 This is the output of the model which
                 contains the detection mask.
 
         Returns
         -------
 
@@ -224,15 +220,14 @@
         This method expands the dimension of the array by one.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_segmentation.py
             function: test_preprocess_input
 
         Parameters
         ----------
-
             img: np.ndarray
                 This is the image to feed the model.
 
         Returns
         -------
             img: np.ndarray
                 Expanded image with extra dimension 1.
@@ -274,15 +269,14 @@
     preprocesses the input type of the Deeplab models and decodes
     the output to generate the segmentation mask.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/modelclient/test_segmentation.py
 
     Parameters
     ----------
-
         model_path: str
             The path to the model.
 
         target: str
             The modelrunner target in the EVK. Ex. 10.10.40.205:10817.
 
     Raises
@@ -312,15 +306,14 @@
         the objects ids.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_segmentation.py
             function: test_decode
 
         Parameters
         ----------
-
             outputs: dict
                 This is the output of the model which
                 contains the detection mask.
 
         Returns
         -------
 
@@ -352,15 +345,14 @@
         This method expands the dimension of the array by one.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/modelclient/test_segmentation.py
             function: test_preprocess_input
 
         Parameters
         ----------
-
             img: np.ndarray
                 This is the image to feed the model.
 
         Returns
         -------
             img: np.ndarray
                 Expanded image with extra dimension 1.
```

## deepview/validator/visualize/core.py

```diff
@@ -2,20 +2,14 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
-import matplotlib
-matplotlib.use('Agg')
-import matplotlib.pyplot as plt
-import numpy as np
-import io
-
 
 class Drawer:
     """
     This class is an abstract class that provides common methods
     between other visualizer classes.
     Unit-test for this class is defined under:
         file: test/test_drawer.py
@@ -27,339 +21,7 @@
     Raises
     ------
         None
     """
 
     def __init__(self):
         pass
-
-    @staticmethod
-    def figure2numpy(figure):
-        """
-        This method converts a matplotlib.pyplot figure into a numpy
-        array so that it can be posted to tensorboard.
-        Unit-test for this method is defined under:
-            file: test/test_drawer.py
-            function: test_figure2numpy
-
-        Parameters
-        ----------
-
-            figure: matplotlib.pyplot
-                This is the figure to convert to a numpy array.
-
-        Returns
-        -------
-            figure: np.ndarray
-                The figure that is represented as a numpy array
-
-        Raises
-        ------
-            None
-        """
-
-        io_buf = io.BytesIO()
-        figure.savefig(io_buf, format='raw')
-        io_buf.seek(0)
-        nimage = np.reshape(
-            np.frombuffer(io_buf.getvalue(), dtype=np.uint8),
-            newshape=(
-                int(figure.bbox.bounds[3]),
-                int(figure.bbox.bounds[2]),
-                -1)
-        )
-        io_buf.close()
-        return nimage
-
-    @staticmethod
-    def plot_classification(class_histogram_data, model="Training Model"):
-        """
-        This method plots the bar charts showing the
-        precision, recall, and accuracy per class.
-        It also shows the number of true positives,
-        false positives, and false negatives per class.
-        Unit-test for this method is defined under:
-            file: test/test_drawer.py
-            function: test_plot_classification
-
-        Parameters
-        ----------
-
-            class_histogram_data: dict.
-                This contains information about the metrics per class.
-
-                .. code-block:: python
-
-                    {
-                        'label_1': {
-                            'precision 0.5': The calculated precision at
-                                        IoU threshold 0.5 for the class,
-                            'recall 0.5': The calculated recall at
-                                        IoU threshold 0.5 for the class,
-                            'accuracy 0.5': The calculated accuracy at
-                                        IoU threshold 0.5 for the class,
-                            'tp 0.5': The number of true posituves
-                                    for the class,
-                            'fn 0.5': The number of false negatives
-                                    for the class,
-                            'class fp 0.5': The number of classification
-                                    false positives for the class,
-                            'local fp 0.5': The number of localization
-                                    false positives for the class,
-                            'gt': The number of grounds truths for the class
-                        },
-                        'label_2': ...
-                    }
-
-            model: str
-                The name of the model.
-
-        Returns
-        -------
-            fig: matplotlib.pyplot
-                This shows two histograms on the left that compares
-                the precision, recall, and accuracy
-                and on the right compares then number
-                of true positives, false positives,
-                and false negatives
-                for each class.
-
-        Raises
-        ------
-            None
-        """
-
-        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 10))
-        # Score = [[prec c1, prec c2, prec c3], [rec c1, rec c2, rec c3], [acc
-        # c1, acc c2, acc c3]]
-        X = np.arange(len(class_histogram_data))
-        labels, precision, recall, accuracy = list(), list(), list(), list()
-        tp, fp, fn = list(), list(), list()
-
-        for cls, value, in class_histogram_data.items():
-            labels.append(cls)
-            precision.append(round(value.get('precision') * 100, 2))
-            recall.append(round(value.get('recall') * 100, 2))
-            accuracy.append(round(value.get('accuracy') * 100, 2))
-            tp.append(value.get('tp'))
-            fn.append(value.get('fn'))
-            fp.append(value.get('fp'))
-
-        ax1.bar(X + 0.0, precision, color='m', width=0.25)
-        ax1.bar(X + 0.25, recall, color='y', width=0.25)
-        ax1.bar(X + 0.5, accuracy, color='c', width=0.25)
-
-        ax2.bar(X + 0.0, tp, color='LimeGreen', width=0.25)
-        ax2.bar(X + 0.25, fn, color='RoyalBlue', width=0.25)
-        ax2.bar(X + 0.5, fp, color='OrangeRed', width=0.25)
-
-        ax1.set_ylabel('Score (%)')
-        ax2.set_ylabel("Total Number")
-        fig.suptitle(f"{model} Evaluation Table")
-
-        ax1.xaxis.set_ticks(range(len(labels)), labels, rotation='vertical')
-        ax2.xaxis.set_ticks(range(len(labels)), labels, rotation='vertical')
-
-        colors = {'precision': 'm', 'recall': 'y', 'accuracy': 'c'}
-        labels = list(colors.keys())
-        handles = [plt.Rectangle((0, 0), 1, 1, color=colors[label])
-                   for label in labels]
-        ax1.legend(handles, labels)
-        colors = {
-            'true positives': 'green',
-            'false negatives': 'blue',
-            'false positives': 'red'}
-        labels = list(colors.keys())
-        handles = [plt.Rectangle((0, 0), 1, 1, color=colors[label])
-                   for label in labels]
-        ax2.legend(handles, labels)
-
-        return fig
-
-    @staticmethod
-    def plot_pr_curve(px, py, ap, names=(), model="Training Model"):
-        """
-        This method plots the precision-recall
-        curve based on the implementation from:
-        https://github.com/ultralytics/yolov5/blob/master/utils/metrics.py#L318
-        Unit-test for this method is defined under:
-            file: test/deepview/validator/test_visualize.py
-            function: test_plot_pr_curve
-
-        Parameters
-        ----------
-
-            px: (NxM) np.ndarray
-                N => number of classes and M is the number of recall values.
-
-            py: (NxM) np.ndarray
-                N => number of classes, M => number of precision values.
-
-            ap: (NxM) np.ndarray
-                N => number of classes, M => 10 denoting each IoU threshold
-                from (0.5 to 0.95 at 0.05 intervals).
-
-            names: list
-                This contains unique string labels captured.
-
-            model: str
-                The name of the model tested.
-
-        Returns
-        -------
-            fig: matplotlib.pyplot
-                The precision recall plot where recall is denoted
-                on the x-axis and precision is denoted
-                on the y-axis.
-
-        Raises
-        ------
-            None
-        """
-
-        fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)
-
-        if 0 < len(names) < 21:  # display per-class legend if < 21 classes
-            for i, y in enumerate(py):
-                # plot(recall, precision)
-                ax.plot(
-                    px[i],
-                    y,
-                    linewidth=1,
-                    label=f'{names[i]} {ap[i, 0]:.3f}'
-                )
-        else:
-            # plot(recall, precision)
-            ax.plot(px, py, linewidth=1, color='grey')
-
-        ax.plot(
-            np.mean(px, axis=0),
-            np.mean(py, axis=0),
-            linewidth=3,
-            color='blue',
-            label='all classes %.3f mAP@0.5' % ap[:, 0].mean()
-        )
-        ax.set_xlabel('Recall')
-        ax.set_ylabel('Precision')
-        ax.set_xlim(0, 1)
-        ax.set_ylim(0, 1.01)
-        ax.legend(bbox_to_anchor=(1.04, 1), loc="upper left")
-        ax.set_title(f'{model} Precision-Recall Curve')
-        return fig
-
-    @staticmethod
-    def plot_mc_curve(
-            px,
-            py,
-            names=(),
-            xlabel='Confidence',
-            ylabel='Metric',
-            model="Training Model"):
-        """
-        This method plots Metric-Confidence curve
-        based on the implementation from:
-        https://github.com/ultralytics/yolov5/blob/master/utils/metrics.py#L341
-        Unit-test for this method is defined under:
-            file: test/deepview/validator/test_visualize.py
-            function: test_plot_mc_curve
-
-        Parameters
-        ----------
-
-            px: (NxM) np.ndarray
-                N => number of classes and M is the number of
-                confidence values.
-
-            py: (NxM) np.ndarray
-                N => number of classes, M => number of
-                (f1, precision, recall) values.
-
-            x-label: str
-                The x-axis metric name to plot.
-
-            y-label: str
-                The y-axis metric name to plot.
-
-            names: list
-                This contains unique string labels captured.
-
-            model: str
-                The name of the model tested.
-
-        Returns
-        -------
-            fig: matplotlib.pyplot
-                This method is used to plot precision vs. confidence,
-                recall vs. confidence, and f1 vs. confidence curves
-                where confidence is situated on the x-axis.
-
-        Raises
-        ------
-            None
-        """
-
-        def smooth(y, f=0.05):
-            # Box filter of fraction f
-            # number of filter elements (must be odd)
-            nf = round(len(y) * f * 2) // 2 + 1
-            p = np.ones(nf // 2)  # ones padding
-            yp = np.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded
-            return np.convolve(
-                yp, np.ones(nf) / nf, mode='valid'
-            )  # y-smoothed
-
-        # Metric-confidence curve
-        fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)
-
-        if 0 < len(names) < 21:  # display per-class legend if < 21 classes
-            for i, y in enumerate(py):
-                # plot(confidence, metric)
-                ax.plot(px, y, linewidth=1, label=f'{str(names[i])}')
-        else:
-            # plot(confidence, metric)
-            ax.plot(px, py.T, linewidth=1, color='grey')
-
-        y = smooth(py.mean(0), 0.05)
-        ax.plot(
-            px,
-            y,
-            linewidth=3,
-            color='blue',
-            label=f'all classes {y.max():.2f} at {px[y.argmax()]:.3f}')
-        ax.set_xlabel(xlabel)
-        ax.set_ylabel(ylabel)
-        ax.set_xlim(0, 1)
-        ax.set_ylim(0, 1)
-        ax.legend(bbox_to_anchor=(1.04, 1), loc="upper left")
-        ax.set_title(f'{model} {ylabel}-Confidence Curve')
-        return fig
-    
-    @staticmethod
-    def close_figures(figures):
-        """
-        This method closes the matplotlib figures opened to prevent
-        errors such as "Fail to allocate bitmap."
-
-        Parameters
-        ----------
-
-            figures: list
-                Contains matplotlib.pyplot figures
-
-        Returns
-        -------
-            None
-
-        Raises
-        ------
-            ValueError
-                This method will raise an exception if the 
-                provided figures is an empty list.
-        """
-
-        import matplotlib.pyplot as plt
-        if len(figures) == 0:
-            raise ValueError("The provided figures does not contain any " +
-                             "matplotlib.pyplot figures.")
-        for figure in figures:
-            plt.close(figure)
-
```

## deepview/validator/visualize/detectiondrawer.py

```diff
@@ -25,25 +25,31 @@
     ------
         None
     """
 
     def __init__(self):
         super(DetectionDrawer, self).__init__()
 
+        self.texts = {
+            "Match": "%s %.2f%% %.2f",
+            "Match Loc": "LOC: %s %.2f%% %.2f",
+            "Loc": "LOC: %s %.2f%%",
+            "Clf": "CLF: %s %.2f%% %.2f"
+        }
+
     @staticmethod
     def __drawRect(image_draw, color, text, textpos, boxpos):
         """
         This function draws bouding boxes and texts on the image.
         Unit-test for this function is defined under:
             file: test/test_detectiondrawer.py
             function: test_draw_with_pillow
 
         Parameters
         ----------
-
             image_draw: PIL ImageDraw object
                 The image to draw on.
 
             color: str
                 Name to color the bounding box.
 
             text: str
@@ -68,22 +74,23 @@
         text_width, text_height = font.getsize(text)
         box_text_x1 = boxpos[0][0]
         box_text_x2 = box_text_x1 + text_width
 
         if textpos[1] > boxpos[0][1]:
             box_text_y1 = boxpos[1][1] - text_height
         else:
-            box_text_y1 = boxpos[0][1] - text_height
+            box_text_y1 = boxpos[0][1] 
+            textpos = (textpos[0], textpos[1] + 10)
 
         box_text_y2 = box_text_y1 + text_height
 
         image_draw.rectangle(
             boxpos,
             outline=color,
-            width=3
+            width=2
         )
         image_draw.rectangle(
             [(box_text_x1, box_text_y1), (box_text_x2, box_text_y2)],
             fill=color
         )
         image_draw.text(
             textpos,
@@ -98,60 +105,43 @@
             iou_t,
             score_t,
             nimage,
             instance,
             matches,
             extras,
             _,
-            iou_list):
+            iou_list
+        ):
         """
         This method draws detection bounding boxes and texts on the image
         with distinct colors.
 
             - RoyalBlue = ground truth or false negatives.
             - LimeGreen = true positives.
             - OrangeRed = false positives.
 
         Unit-test for this method is defined under:
             file: test/test_detectiondrawer.py
             function: test_draw_with_pillow
 
         Parameters
         ----------
-
             iou_t: float
                 The validation IoU threshold between 0 and 1.
 
             score_t: float
                 The validation score threshold between 0 and 1.
 
             nimage: (H, W, 3) np.ndarray
                 Image represented as an numpy array.
 
             instance: dict
                 This yields one image instance from the ground
                 truth and the model predictions.
-
-                .. code-block:: python
-
-                    {
-                        'gt_instance': {
-                            'image': image numpy array,
-                            'height': height,
-                            'width': width,
-                            'boxes': list bounding boxes,
-                            'labels': list of labels,
-                            'image_path': image_path
-                        },
-                        'dt_instance': {
-                            'boxes': list of prediction bounding boxes,
-                            'labels': list of prediction labels,
-                            'scores': list of confidence scores
-                        }
-                    }
+                See README.md (Method Parameters Format) for more information.
 
             matches: np.ndarray or list
                 This contains the indices of matches
                 [[index_dt, index_gt], ...].
 
             extras: np.ndarray or list
                 This contains indices of extra predictions (local FP).
@@ -165,63 +155,62 @@
                 The image with drawn bounding boxes and text.
         """
 
         image = Image.fromarray(nimage)
         image_draw = ImageDraw.Draw(image)
         width = instance.get('gt_instance').get('width')
         height = instance.get('gt_instance').get('height')
-
+       
         # Draw Ground Truths
         for label, box in zip(
             instance.get('gt_instance').get('labels'),
             instance.get('gt_instance').get('boxes')
-        ):
-
+        ):      
             p1 = (float(box[0]) * width, float(box[1]) * height)
             p2 = (float(box[2]) * width, float(box[3]) * height)
             self.__drawRect(image_draw, "RoyalBlue",
-                            f"{label} ", (p1[0], p2[1] - 12), (p1, p2))
+                            str(label), (p1[0], p2[1] - 12), (p1, p2))
+            
+        # Draw Extra Predictions
+        for extra_index in extras:
+            dt_label = instance.get('dt_instance').get('labels')[extra_index]
+            box = instance.get('dt_instance').get('boxes')[extra_index]
+            score = instance.get('dt_instance').get('scores')[extra_index]*100
+
+            text = self.texts["Loc"] % (dt_label, score)
+            p1 = (float(box[0]) * width, float(box[1]) * height)
+            p2 = (float(box[2]) * width), int(float(box[3]) * height)
+
+            if score >= score_t*100:
+                self.__drawRect(image_draw, "OrangeRed", text,
+                                (p1[0], p1[1] - 10), (p1, p2))
 
         # Draw Predictions
         for match in matches:
             dt_label = instance.get('dt_instance').get('labels')[match[0]]
             gt_label = instance.get('gt_instance').get('labels')[match[1]]
             box = instance.get('dt_instance').get('boxes')[match[0]]
             iou = iou_list[match[0]]
-            score = instance.get('dt_instance').get('scores')[match[0]]
+            score =instance.get('dt_instance').get('scores')[match[0]]*100
 
-            text = f"{dt_label} {round(score * 100, 2)}% {round(float(iou), 2)}"
             p1 = (float(box[0]) * width, float(box[1]) * height)
             p2 = (float(box[2]) * width, float(box[3]) * height)
 
             if dt_label == gt_label:
+                text = self.texts["Match"] % (dt_label, score, iou)
                 if float(iou) >= iou_t:
                     color = "LimeGreen"
                 else:
                     color = "OrangeRed"
             else:
+                text = self.texts["Clf"] % (dt_label, score, iou)
                 color = "OrangeRed"
 
-            if score >= score_t:
-                self.__drawRect(image_draw, color, text,
-                                (p1[0], p1[1] - 10), (p1, p2))
-                
-            if iou < iou_t:
-                text = f'Extra: {dt_label} {round(score * 100, 2)}% {round(float(iou), 2)}'
-                self.__drawRect(image_draw, "OrangeRed", text,
-                                (p1[0], p1[1] - 10), (p1, p2))
-
-        # Draw Extra Predictions
-        for extra_index in extras:
-            dt_label = instance.get('dt_instance').get('labels')[extra_index]
-            box = instance.get('dt_instance').get('boxes')[extra_index]
-            score = instance.get('dt_instance').get('scores')[extra_index]
-
-            text = f'Extra: {dt_label} {round(score * 100, 2)}%'
-            p1 = (float(box[0]) * width, float(box[1]) * height)
-            p2 = (float(box[2]) * width), int(float(box[3]) * height)
-
-            if score >= score_t:
-                self.__drawRect(image_draw, "OrangeRed", text,
+            if score >= score_t*100:
+                if iou < iou_t:
+                    text = self.texts["Match Loc"] % (dt_label, score, iou)
+                    self.__drawRect(image_draw, "OrangeRed", text,
+                                    (p1[0], p1[1] - 10), (p1, p2))
+                else:
+                    self.__drawRect(image_draw, color, text,
                                 (p1[0], p1[1] - 10), (p1, p2))
-
-        return image
+        return image
```

## deepview/validator/visualize/segmentationdrawer.py

```diff
@@ -37,15 +37,14 @@
         Currently only supports two class: person and car.
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_polygon2maskimage
 
         Parameters
         ----------
-
             original_image: PIL Image object
                 The original image opened using pillow.image.
 
             dt_polygon_list: list
                 A list of predictions with polygon vertices
                 [ [cls, x1, y1, x2, y2, x3, y3, ...] ...].
 
@@ -94,15 +93,14 @@
         This method will transform a numpy array of mask into an RGBA image.
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_mask2imagetransform
 
         Parameter
         ---------
-
             mask: (height, width, 3) np.ndarray
                 Array representing the mask.
 
             labels: list
                 The list of prediction labels in the mask.
 
             union: bool
@@ -185,34 +183,17 @@
         with mask prediction on the left and mask ground truth on the right.
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_mask2maskimage
 
         Parameters
         ----------
-
             instances: dict
-                This contains information such as:
-
-                .. code-block:: python
-
-                    instances = {
-                                'gt_instance': {
-                                    'image': image numpy array,
-                                    'height': height,
-                                    'width': width,
-                                    'gt_mask': ground truth mask of the image,
-                                    'labels': list of labels,
-                                    'image_path': image_path
-                                },
-                                'dt_instance': {
-                                    'dt_mask': model mask of the image,
-                                    'labels': list of prediction labels,
-                                }
-                            }
+                This contains information such as ground truht and detection.
+                See README.md (Method Parameters Format) for more information.
 
         Returns
         -------
             image: PIL Image object
                 The image with drawn masks where on the right pane
                 shows the ground truth mask and on the left pane shows
                 the prediction mask.
@@ -284,34 +265,18 @@
 
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_mask2mask4panes
 
         Parameters
         ----------
-
             instances: dict
-                This contains information such as:
-
-                .. code-block:: python
-
-                    instances = {
-                                'gt_instance': {
-                                    'image': image numpy array,
-                                    'height': height,
-                                    'width': width,
-                                    'gt_mask': ground truth mask of the image,
-                                    'labels': list of labels,
-                                    'image_path': image_path
-                                },
-                                'dt_instance': {
-                                    'dt_mask': model mask of the image,
-                                    'labels': list of prediction labels,
-                                }
-                            }
+                This contains information regarding ground truth and 
+                detection.
+                See README.md (Method Parameters Format) for more information.
 
             tp_mask: (height, width) np.ndarray
                 2D numpy array with the same size as the image and
                 each element represents a pixel containing all true positives.
 
             fp_mask: (height, width) np.ndarray
                 2D numpy array with the same size as the image and
@@ -387,12 +352,9 @@
                       font=font, align='left', fill=(0, 0, 0))
         drawtext.text(
             (out_dt.width,
              out_dt.height),
             "TP (Blue/Red), FP/FN(Light Blue/Red)",
             font=font,
             align='left',
-            fill=(
-                0,
-                0,
-                0))
+            fill=(0,0,0))
         return dst
```

## deepview/validator/writers/console.py

```diff
@@ -10,14 +10,15 @@
 
 
 class ConsoleWriter(Writer):
     """
     This class is used to print the metrics on the terminal.
     Unit-test for this class is defined under:
         file: test/test_console.py
+
     Parameters
     ----------
         None
 
     Raises
     ------
         None
@@ -33,119 +34,23 @@
         When this is called, it prints the metrics on the console.
         Unit-test for this method is defined under:
             file: test/test_console.py
             function: test_print_metrics
 
         Parameters
         ----------
-
             message: dict.
-                This message contains information regarding the
-                metrics.
-
-                * For detection:
-
-                    .. code-block:: python
-
-                        {
-                            "model": Name of the model,
-                            "engine": Engine used (npu, cpu, gpu),
-                            "dataset": Name of the dataset,
-                            "numgt": Number of ground truths in the dataset,
-                            "Total TP": Total true positives,
-                            "Total FN": Total false negatives,
-                            "Total Class FP": Total classification false
-                                              positives,
-                            "Total Loc FP": Total localization false positives,
-                            "OP": Overall precision,
-                            "mAP": {'0.5': mAP at 0.5 IoU threshold,
-                                    '0.75': mAP at 0.75 IoU threshold,
-                                    '0.5:0.95': mAP across 0.5-0.95
-                                                IoU thresholds
-                                },
-                            "OR": Overall recall,
-                            "mAR": {'0.5': mAR at 0.5 IoU threshold,
-                                    '0.75': mAR at 0.75 IoU threshold,
-                                    '0.5:0.95': mAR across 0.5-0.95
-                                                IoU thresholds
-                                },
-                            "OA": Overall Accuracy,
-                            "mACC": {'0.5': mACC at 0.5 IoU threshold,
-                                    '0.75': mACC at 0.75 IoU threshold,
-                                    '0.5:0.95': mACC across 0.5-0.95
-                                                IoU thresholds
-                                    },
-                            "LocFPErr": {'0.5': localization false positive
-                                                ratio at 0.5 IoU threshold,
-                                        '0.75': localization false positive
-                                                ratio at 0.75 IoU threshold,
-                                        '0.5:0.95': localization false positive
-                                                    ratio across 0.5-0.95
-                                                    IoU thresholds
-                                        },
-                            "ClassFPErr": { '0.5': classification false
-                                                   positive ratio at 0.5
-                                                   IoU threshold,
-                                            '0.75': classification false
-                                                    positive ratio at 0.75
-                                                    IoU threshold,
-                                            '0.5:0.95': classification false
-                                                        positive ratio across
-                                                        0.5-0.95 IoU thresholds
-                                        },
-                            "timings": timings
-                                (input, inference, decode at min/max/avg)
-                        }
-
-                * For segmentation:
-
-                    .. code-block:: python
-
-                        {
-                            "model": Name of the model,
-                            "engine": Engine used (npu, cpu, gpu),
-                            "dataset": Name of the dataset,
-                            "numgt": Number of ground truths in the dataset,
-                            "Total TP": Total true positives,
-                            "Total FN": Total false negatives,
-                            "Total FP": Total false positives,
-                            "OP": Overall precision,
-                            "OR": Overall recall,
-                            "OA": Overall Accuracy,
-                            "mAP": Mean average precision,
-                            "mAR": Mean average recall,
-                            "mACC": Mean average accuracy,
-                            "timings": timings
-                                (input, inference, decode at min/max/avg)
-                        }
-
+                This message contains information regarding the metrics.
+                See README.md (Method Parameters Format) for more information.
 
             parameters: dict
                 This contains information regarding the model and
                 validation parameters.
-
-                .. code-block:: python
-
-                    {
-                        "validation-iou": Validation IoU used to
-                                          consider true positives,
-                        "detection-iou": Detection IoU for model NMS,
-                        "validation-threshold": Validation score
-                                                threshold to filter
-                                                predictions,
-                        "detection-threshold": Detection score threshold
-                                               for model NMS,
-                        "nms": Type of NMS performed (standard, fast, matrix),
-                        "normalization": Type of image normalization
-                                       performed (raw, signed, unsigned, etc.),
-                        "maximum_detections": Maximum detections set,
-                        "warmup": Number of warmup iterations used,
-                        "label offset": The label offset specified
-                    }
-
+                See README.md (Method Parameters Format) for more information.
+                
             validation_type: str
                 This is the type of validation performed.
                 Either 'detection' or 'segmentation'.
 
         Returns
         -------
 
@@ -158,19 +63,16 @@
             timings: str
                 The formatted timings of the model
 
         """
 
         if validation_type.lower() == 'detection':
             header, summary, timings = self.format_detection_summary(
-                                            message, parameters
-                                        )
+                                            message, parameters)
         elif validation_type.lower() == 'segmentation':
             header, summary, timings = self.format_segmentation_summary(
-                                            message
-                                        )
+                                            message)
         print(header)
         print(summary)
         if timings is not None:
             print(timings)
-
-        return header, summary, timings
+        return header, summary, timings
```

## deepview/validator/writers/core.py

```diff
@@ -39,20 +39,19 @@
         Error, Warning, Info, Success.
         Unit-test for this function is defined under:
             file: test/test_writer.py
             function: test_logger
 
         Parameters
         ----------
+            message: str
+                The message to print to the console.
 
-        message: str
-            The message to print to the console.
-
-        code: str
-            The type of the message (error, warning, info, success).
+            code: str
+                The type of the message (error, warning, info, success).
 
         Returns
         -------
         None
 
         Raises
         ------
@@ -76,85 +75,22 @@
         into a string table.
         Unit-test for this method is defined under:
             file: test/test_writer.py
             function test_format_detection_summary
 
         Parameters
         ----------
-
             metrics: dict
                 This is the summary metrics generated by validation.
-
-                .. code-block:: python
-
-                    {
-                        "model": Name of the model,
-                        "engine": Engine used (npu, cpu, gpu),
-                        "dataset": Name of the dataset,
-                        "numgt": Number of ground truths in the dataset,
-                        "Total TP": Total true positives,
-                        "Total FN": Total false negatives,
-                        "Total Class FP": Total classification false positives,
-                        "Total Loc FP": Total localization false positives,
-                        "OP": Overall precision,
-                        "mAP": {'0.5': mAP at 0.5 IoU threshold,
-                                '0.75': mAP at 0.75 IoU threshold,
-                                '0.5:0.95': mAP across 0.5-0.95 IoU thresholds
-                            },
-                        "OR": Overall recall,
-                        "mAR": {'0.5': mAR at 0.5 IoU threshold,
-                                '0.75': mAR at 0.75 IoU threshold,
-                                '0.5:0.95': mAR across 0.5-0.95 IoU thresholds
-                            },
-                        "OA": Overall Accuracy,
-                        "mACC": {'0.5': mACC at 0.5 IoU threshold,
-                                '0.75': mACC at 0.75 IoU threshold,
-                                '0.5:0.95': mACC across 0.5-0.95 IoU thresholds
-                                },
-                        "LocFPErr": {'0.5': localization false positive ratio
-                                            at 0.5 IoU threshold,
-                                    '0.75': localization false positive ratio
-                                            at 0.75 IoU threshold,
-                                    '0.5:0.95': localization false positive
-                                                ratio across 0.5-0.95
-                                                IoU thresholds
-                                    },
-                        "ClassFPErr": { '0.5': classification false positive
-                                               ratio at 0.5 IoU threshold,
-                                        '0.75': classification false positive
-                                                ratio at 0.75 IoU threshold,
-                                        '0.5:0.95': classification false
-                                                    positive ratio across
-                                                    0.5-0.95 IoU thresholds
-                                    },
-                        "timings": timings
-                            (input, inference, decode at min/max/avg)
-                    }
+                See README.md (Method Parameters Format) for more information.
 
             parameters: dict
                 This contains information regarding the model and
                 validation parameters.
-
-                .. code-block:: python
-
-                    {
-                        "validation-iou": Validation IoU used to
-                                          consider true positives,
-                        "detection-iou": Detection IoU for model NMS,
-                        "validation-threshold": Validation score threshold
-                                                to filter predictions,
-                        "detection-threshold": Detection score threshold
-                                               for model NMS,
-                        "nms": Type of NMS performed (standard, fast, matrix),
-                        "normalization": Type of image normalization performed
-                                        (raw, signed, unsigned, etc.),
-                        "maximum_detections": Maximum detections set,
-                        "warmup": Number of warmup iterations used,
-                        "label offset": The label offset specified
-                    }
+                See README.md (Method Parameters Format) for more information.
 
         Returns
         -------
             header: str
                 The validation header message.
 
             summary: str
@@ -250,38 +186,18 @@
         into a string table.
         Unit-test for this method is defined under:
             file: test/test_writer.py
             function test_format_segmentation_summary
 
         Parameters
         ----------
-
             metrics: dict
                 This is the summary metrics generated by validation.
-
-                    .. code-block:: python
-
-                        {
-                            "model": Name of the model,
-                            "engine": Engine used (npu, cpu, gpu),
-                            "dataset": Name of the dataset,
-                            "numgt": Number of ground truths in the dataset,
-                            "Total TP": Total true positives,
-                            "Total FN": Total false negatives,
-                            "Total FP": Total false positives,
-                            "OP": Overall precision,
-                            "OR": Overall recall,
-                            "OA": Overall Accuracy,
-                            "mAP": Mean average precision,
-                            "mAR": Mean average recall,
-                            "mACC": Mean average accuracy,
-                            "timings": timings
-                                (input, inference, decode at min/max/avg)
-                        }
-
+                See README.md (Method Parameters Format) for more information.
+                
         Returns
         -------
             header: str
                 The validation header message.
 
             summary: str
                 The formatted validation showing the metrics.
@@ -349,36 +265,17 @@
         into a string table.
         Unit-test for this method is defined under:
             file: test/test_writer.py
             function test_format_timings
 
         Parameters
         ----------
-
             timings: dict
-                This contains the timing information
-
-                .. code-block:: python
-
-                    {
-                    'min_inference_time': minimum time to produce
-                                          bounding boxes,
-                    'max_inference_time': maximum time to produce
-                                          bounding boxes,
-                    'min_input_time': minimum time to load an image,
-                    'max_input_time': maximum time to load an image,
-                    'min_decoding_time': minimum time to process
-                                         model predictions,
-                    'max_decoding_time': maximum time to process
-                                         model predictions,
-                    'avg_decoding': average time to process
-                                    model predictions,
-                    'avg_input': average time to load an image,
-                    'avg_inference': average time to produce bounding boxes
-                    }
+                This contains the timing information.
+                See README.md (Method Parameters Format) for more information.            
 
         Returns
         -------
             timings: str
                 The formatted timings in a table.
 
         Raises
@@ -422,31 +319,18 @@
         into a string table.
         Unit-test for this method is defined under:
             file: test/test_writer.py
             function test_format_parameters
 
         Parameters
         ----------
-
             parameters: dict
                 This contains the parameters that was used
                 for running and validating the model.
-
-                .. code-block:: python
-
-                    {
-                        "validation-iou": args.validation_iou,
-                        "detection-iou": args.detection_iou,
-                        "validation-threshold": args.validation_threshold,
-                        "detection-threshold": args.detection_threshold,
-                        "nms": args.nms_type,
-                        "normalization": args.norm,
-                        "maximum_detections": args.max_detection,
-                        "warmup": args.warmup
-                    }
+                See README.md (Method Parameters Format) for more information.
 
         Returns
         -------
             parameters: str
                 The formatted parameters as a string table.
 
         Raises
@@ -463,9 +347,12 @@
             |     detection score threshold: {str(parameters.get("detection-threshold")).ljust(18)}|
             |     nms: {str(parameters.get("nms")).ljust(40)}|
             |     normalization: {str(parameters.get("normalization")).ljust(30)}|
             |     engine: {str(parameters.get('engine')).ljust(37)}|
             |     maximum detections: {str(parameters.get("maximum_detections")).ljust(25)}|
             |     warmup: {str(parameters.get("warmup")).ljust(37)}|
             |     label offset: {str(parameters.get("label offset")).ljust(31)}|
+            |     metric: {str(parameters.get("metric")).ljust(37)}|
+            |     box clamp dimensions: {str(parameters.get("clamp boxes")).ljust(23)}|
+            |     ignore box dimensions: {str(parameters.get("ignore boxes")).ljust(22)}|
             |__________________________________________________|
             """
```

## deepview/validator/writers/tensorboard.py

```diff
@@ -2,62 +2,62 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
+from deepview.validator.exceptions import MissingLibraryException
 from deepview.validator.writers.core import Writer
 import numpy as np
 from os import path
 
-try:
-    import tensorflow as tf
-except ImportError:
-    pass
-
 
 class TensorBoardWriter(Writer):
     """
     This class is used to publish the images and the metrics onto TensorBoard.
     Unit-test for this class is defined under:
         file: test/test_tensorboard.py
 
     Parameters
     ----------
-
         logdir: str
             This is the path to the tfevents file.
 
     Raises
     ------
         None
     """
 
     def __init__(
         self,
         logdir=None
     ):
         super(TensorBoardWriter, self).__init__()
 
+        try:
+            import tensorflow as tf
+        except ImportError:
+            raise MissingLibraryException("TensorFlow library is needed to " +
+                                          "allow tensorboard functionalities")
+
         self.logdir = logdir
         self.writer = None
         if logdir:
             self.writer = tf.summary.create_file_writer(self.logdir)
 
     def __call__(self, image, image_path, step=0):
         """
         When it is called, it publishes the images onto tensorboard.
         Unit-test for this method is defined under:
             file: test/test_tensorboard.py
             function: test_tensorboard_image_display
 
         Parameters
         ----------
-
             image: (height, width, 3) np.ndarray
                 The image array to display to tensorboard
 
             image_path: str
                 The path to the image.
 
             step: int
@@ -69,14 +69,20 @@
             None
 
         Raises
         ------
             None
         """
 
+        try:
+            import tensorflow as tf
+        except ImportError:
+            raise MissingLibraryException("TensorFlow library is needed to " +
+                                          "allow tensorboard functionalities")
+
         with self.writer.as_default():
             nimage = np.expand_dims(image, 0)
             tf.summary.image(
                 path.basename(image_path),
                 nimage,
                 step=step
             )
@@ -92,109 +98,22 @@
         This method publishes the metric summary onto tensorboard.
         Unit-test for this method is defined under:
             file: test/test_tensorboard.py
             function: test_publish_metrics
 
         Parameters
         ----------
-
             message: dict
                 This contains the validation metrics.
-
-                * For detection:
-
-                .. code-block:: python
-
-                    {
-                        "model": Name of the model,
-                        "engine": Engine used (npu, cpu, gpu),
-                        "dataset": Name of the dataset,
-                        "numgt": Number of ground truths in the dataset,
-                        "Total TP": Total true positives,
-                        "Total FN": Total false negatives,
-                        "Total Class FP": Total classification false positives,
-                        "Total Loc FP": Total localization false positives,
-                        "OP": Overall precision,
-                        "mAP": {'0.5': mAP at 0.5 IoU threshold,
-                                '0.75': mAP at 0.75 IoU threshold,
-                                '0.5:0.95': mAP across 0.5-0.95 IoU thresholds
-                            },
-                        "OR": Overall recall,
-                        "mAR": {'0.5': mAR at 0.5 IoU threshold,
-                                '0.75': mAR at 0.75 IoU threshold,
-                                '0.5:0.95': mAR across 0.5-0.95 IoU thresholds
-                            },
-                        "OA": Overall Accuracy,
-                        "mACC": {'0.5': mACC at 0.5 IoU threshold,
-                                '0.75': mACC at 0.75 IoU threshold,
-                                '0.5:0.95': mACC across 0.5-0.95 IoU thresholds
-                                },
-                        "LocFPErr": {'0.5': localization false positive ratio
-                                            at 0.5 IoU threshold,
-                                    '0.75': localization false positive ratio
-                                            at 0.75 IoU threshold,
-                                    '0.5:0.95': localization false positive
-                                                ratio across 0.5-0.95
-                                                IoU thresholds
-                                    },
-                        "ClassFPErr": { '0.5': classification false positive
-                                               ratio at 0.5 IoU threshold,
-                                        '0.75': classification false positive
-                                                ratio at 0.75 IoU threshold,
-                                        '0.5:0.95': classification false
-                                                    positive ratio across
-                                                    0.5-0.95 IoU thresholds
-                                    },
-                        "timings": timings
-                                (input, inference, decode at min/max/avg)
-                    }
-
-                * For segmentation:
-
-                .. code-block:: python
-
-                    {
-                        "model": Name of the model,
-                        "engine": Engine used (npu, cpu, gpu),
-                        "dataset": Name of the dataset,
-                        "numgt": Number of ground truths in the dataset,
-                        "Total TP": Total true positives,
-                        "Total FN": Total false negatives,
-                        "Total FP": Total false positives,
-                        "OP": Overall precision,
-                        "OR": Overall recall,
-                        "OA": Overall Accuracy,
-                        "mAP": Mean average precision,
-                        "mAR": Mean average recall,
-                        "mACC": Mean average accuracy,
-                        "timings": timings
-                            (input, inference, decode at min/max/avg)
-                    }
+                See README.md (Method Parameters Format) for more information.
 
             parameters: dict
                 This contains information regarding the model and
                 validation parameters.
-
-                .. code-block:: python
-
-                    {
-                        "validation-iou": Validation IoU used to
-                                          consider true positives,
-                        "detection-iou": Detection IoU for model NMS,
-                        "validation-threshold": Validation score threshold
-                                                to filter predictions,
-                        "detection-threshold": Detection score threshold
-                                               for model NMS,
-                        "nms": Type of NMS performed (standard, fast, matrix),
-                        "normalization": Type of image normalization
-                                      performed (raw, signed, unsigned, etc.),
-                        "maximum_detections": Maximum detections set,
-                        "warmup": Number of warmup iterations used,
-                        "label offset": The label offset specified
-                    }
+                See README.md (Method Parameters Format) for more information.
 
             step: int
                 This is the iteration number which represents the
                 epoch number when training a model.
 
             validation_type: str
                 This is the type of validation performed
@@ -205,14 +124,20 @@
             None
 
         Raises
         ------
             None
         """
 
+        try:
+            import tensorflow as tf
+        except ImportError:
+            raise MissingLibraryException("TensorFlow library is needed to " +
+                                          "allow tensorboard functionalities")
+
         with self.writer.as_default():
 
             if validation_type.lower() == 'detection':
                 header, summary, timings = self.format_detection_summary(
                     message, parameters
                 )
             elif validation_type.lower() == 'segmentation':
@@ -239,15 +164,14 @@
     """
     This class is used to publish metrics on training a model.
     Unit-test for this class is defined under:
         file: test/test_tensorboard.py
 
     Parameters
     ----------
-
         writer: TF.summary file writer object
             This is the writer object defined in trainer.
 
     Raises
     ------
         None
     """
```

## Comparing `deepview_validator-3.0.8.dist-info/RECORD` & `deepview_validator-3.0.9.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,44 +1,45 @@
 deepview/validator/__init__.py,sha256=OjgWWA-EkgVtRfV0-4yBzPOGUB7AaRD-ipzY5gmq1I4,760
-deepview/validator/__main__.py,sha256=zFqN6mJ4PFhyb6WLrl4FEwzoqGwHWBfP3uZmebll4gk,16195
-deepview/validator/exceptions.py,sha256=wN9LfxtSZ7jijwua-jziTBsrynjPS5Y8-nZerxBe5m4,12847
+deepview/validator/__main__.py,sha256=dmS0yOg7L0uJcW2tEkhTaoXMknIyI6IW9FcnZUP9Res,17902
+deepview/validator/exceptions.py,sha256=4ujfOvKOVJh_QoSHq7mZTs9Wt2Fge8z1gRPwQlgCEdY,12716
 deepview/validator/datasets/__init__.py,sha256=kn3O2am9aQkpCCUapanHpD2Hmqg2IXhMMduOY6LizYY,497
-deepview/validator/datasets/core.py,sha256=FgHug8HZVU1AB_h0sPDmrheKoEbefXXzmWRz7RCIch0,30031
-deepview/validator/datasets/darknet.py,sha256=RDq4im5sR029vqko1078qN05P7Aa9_rPu0MQYOld66s,14688
-deepview/validator/datasets/tfrecord.py,sha256=tuMWxHPSazhzLdKlZv7tPVZVIzevEp7snIWgpT9zuDI,7029
-deepview/validator/datasets/utils.py,sha256=uFo47noinkjsD_88e5Gj1nSFHvyqFY0guLE7I4LWPlU,3786
-deepview/validator/evaluators/__init__.py,sha256=0UF3SF__Siagxw9ERwngKeVVz8DlNCiqUd2I_u8SJvI,530
-deepview/validator/evaluators/core.py,sha256=6B21kTpf9kSZ36huPatmucelPE_v-6LCUSvNuXZ_8yw,6406
-deepview/validator/evaluators/detectionevaluator.py,sha256=lXbU1LTsWA5eB2YcP9bYqoUDKttFLgaJIXMCS8tg-Tg,25209
-deepview/validator/evaluators/segmentationevaluator.py,sha256=t2IXjx9q8tJbs2ersHMVcnmyrfgNIQwySJcY4DexIII,19326
-deepview/validator/metrics/__init__.py,sha256=vp8Dy7OR0tzfFsBPv81Y2OYWetgWrAHRzfBzkkNtazY,830
-deepview/validator/metrics/core.py,sha256=Bz8SHzepcgVZN1kplxRURn8thZ_UA9mdulWJQIpoOWc,11367
-deepview/validator/metrics/detectiondata.py,sha256=UCCT7SXkLbstJOthcaABZ5hImH_YAk2MLwZvPmEEHk4,32340
-deepview/validator/metrics/detectionmetrics.py,sha256=cy38MvXWIAizEtQ00H6QW_7W5uIX4_ybK-LLJivtqzs,26274
-deepview/validator/metrics/detectionutils.py,sha256=4LaHrYYw9TM8C_6pWxnfOTSz2mjHMUCNnBKqfFD6NAk,4548
-deepview/validator/metrics/segmentationdata.py,sha256=28ws5aceUvg8jzNfGWtfKRp4EvFYwYsw6ddpj_SIjv4,17192
-deepview/validator/metrics/segmentationmetrics.py,sha256=Fe68fqugRkYZDTjXw5T_NbsrapKByMkZRXfvVueZXCU,6514
-deepview/validator/metrics/segmentationutils.py,sha256=HdI0xTKKO7GAfil42EjpI3_YAefKHCNDjJFD0wnt9UI,11690
+deepview/validator/datasets/core.py,sha256=4kB-KMIQwKMr7u9Pyt6a_UG0gT5yVi0t_lsR3zhas3s,30711
+deepview/validator/datasets/darknet.py,sha256=jKQCjkI1DftyzGGkzv-WXXxME8shmyDOIBaJhGJgfPU,14726
+deepview/validator/datasets/tfrecord.py,sha256=cs1PU3yj7NRcB_32r2gbY-f9PWelHcNjLAYJ6WbYBR0,7026
+deepview/validator/datasets/utils.py,sha256=ljRiIauE7ZkHTB_yFMrnEs8ZoxHH6AGdkzlBJBs-f4Y,3959
+deepview/validator/evaluators/__init__.py,sha256=pJq8FlHqM-KzcWb_BzRitOxrvG5MJHnAsz0WIlndI1A,536
+deepview/validator/evaluators/core.py,sha256=CJQKn8d1TflRvZ__9zw2D3zBENBf8dZnTFi-d-D_THg,6672
+deepview/validator/evaluators/detectionevaluator.py,sha256=JK0irwnGeZRQitUX1Tp3KwBkvsmJJNvb8-bGFfow8eU,29717
+deepview/validator/evaluators/segmentationevaluator.py,sha256=i2NYXJfpPxtbkAorUK-ALBX1RKeRnlLWtn0WKYY-z3k,16758
+deepview/validator/metrics/__init__.py,sha256=OFXv-RAfJ6kIrg4D5gH0YGNvgx_DYJoTNySPsZS4_vI,836
+deepview/validator/metrics/core.py,sha256=gCkpE2RZFyywYawOUYUsw7JC3EHGux1nhPLBqvjGeaE,11356
+deepview/validator/metrics/detectiondata.py,sha256=ApPeVkuoz3fIoUwFanC2rxNMrit_huxGX4LJNNbMaJY,36080
+deepview/validator/metrics/detectionmetrics.py,sha256=DsQLLGIIv2eRKou_GJuvFtFtOJ-MONa2uKhcb3cW7dI,29321
+deepview/validator/metrics/detectionutils.py,sha256=n-_5qVYVpRDno3PACSBpQBatbCxmVGpO_mKSgdS4WpI,24126
+deepview/validator/metrics/segmentationdata.py,sha256=IfmNQ7JiXDAGych6Cst87pDsluO-6XOaqaoaar9D7bs,16661
+deepview/validator/metrics/segmentationmetrics.py,sha256=2MXx2jIPqYuoG1HlzJ2VxFkog2gA7igN5IZ9q-8ayRk,6512
+deepview/validator/metrics/segmentationutils.py,sha256=wXXEgs7BzyvfTqjiGQaN1TJmrE0qrB2E2T_Oj8J7T_U,11683
 deepview/validator/runners/__init__.py,sha256=gRRd8B9lhTPg6noEF-nRipyIsawSJSp6BapSQzxDXjc,735
-deepview/validator/runners/core.py,sha256=gVikDHobZPPeQxvK1eRzaoKgert1RlJQQBHpz-3zBg4,5144
-deepview/validator/runners/deepviewrt.py,sha256=ipOoc9cjWf7C_-xq5hKXX3T4P2RIMaUHPeKsZmdscc8,13848
-deepview/validator/runners/keras.py,sha256=zhigiA3uBLaJgbV5-VY9zIfkJZ-za_SqFtO8mAjxyZQ,17781
-deepview/validator/runners/offline.py,sha256=DTcgg7LmuUXUuUmFw5qEo0IAYjjgaVaG0lpNCKAzqbM,6147
-deepview/validator/runners/tensorrt.py,sha256=OMBziiYWUeJWmjonHo5QlQZMXSDppHAOx0CDJFEZAfY,16285
-deepview/validator/runners/tflite.py,sha256=-9N6rShjKi0B98MzDeL77RNBF_Pt_10hCkF47XQBNxE,14466
+deepview/validator/runners/core.py,sha256=wIaj9Ts5qJZ7Rq8rxxiW8Hh5jLRgxw86AJ1eClnji10,6173
+deepview/validator/runners/deepviewrt.py,sha256=0v1SVsaWlTs1GD-7oxsdqE7ARzHhvASamY6Y-1SnGIo,17964
+deepview/validator/runners/keras.py,sha256=w641HU434akINVNFDSQo4Iir4DbkhtHJTWySO115miA,17680
+deepview/validator/runners/offline.py,sha256=G6YDXGJmmcBZBobRo7Zno0EJhSbnsY7IoS4XHGxQjOU,6203
+deepview/validator/runners/tensorrt.py,sha256=6bmbPRQXAHj04DpyUGhF38aqGMEQIz0vB0e3tYhg2c0,16289
+deepview/validator/runners/tflite.py,sha256=ucOutusTxFSZiOv3VpAwELm29geyXYxBjRbMjsJvmiM,14459
 deepview/validator/runners/modelclient/__init__.py,sha256=5S2qsbvi8d9y8-FH6Zu7VZS8WlZZKwRg7yhFd58t3uw,612
-deepview/validator/runners/modelclient/boxes.py,sha256=ij5OAoTdQbpGJ6Cj1hXASu0J_A5tpKatX0XLLCuh5Xc,27957
-deepview/validator/runners/modelclient/core.py,sha256=1zK1SxMojNnEk6-BT8tRdyMroad2Bvg9vs5vu9-dNTw,3116
-deepview/validator/runners/modelclient/segmentation.py,sha256=3Q2xG1c7Xlpm1_5pmGGu8V30d2ifwGm3NPXcLhsxdWE,11168
+deepview/validator/runners/modelclient/boxes.py,sha256=ToErcICQ5KN-T2zdTtnjOyKG1YwgDAthMFsvK2jLXAc,28003
+deepview/validator/runners/modelclient/core.py,sha256=OXrWcw0Mp0k5jANobf2BBUJetjCdUeX5lJediwk5cnI,3115
+deepview/validator/runners/modelclient/segmentation.py,sha256=YXe_I9X7CdrPWAGcPyjynltp93bil1la85QCk1IcN5o,11160
 deepview/validator/visualize/__init__.py,sha256=gGM_U0YR1SOagPIhUjQr8PLJAPZ-XzHuY9jLwg3uHds,521
-deepview/validator/visualize/core.py,sha256=gaT4HWQ49FoFRRGSwxn0LkMRB47gh-Q10QW9_lxLxWc,11896
-deepview/validator/visualize/detectiondrawer.py,sha256=imlSA6GWin3zqdkMBIntgsCZBvNRN1gG8TT4BUgGRoI,7277
-deepview/validator/visualize/segmentationdrawer.py,sha256=WYMzIPbqcFy-8_RGrs_TTdqjvI-5Q8A08rQHN5Ymp1w,14370
+deepview/validator/visualize/core.py,sha256=i1Z4SDOfjLUOL7U-duRRWEcnPlU0HPNTK1-n4hKkmzM,649
+deepview/validator/visualize/detectiondrawer.py,sha256=4A_KT5bcseG_3e78dz8Qv1kf6Qfx_y5n022LEcnOBC8,6955
+deepview/validator/visualize/segmentationdrawer.py,sha256=krWkQppx4Ki2c6AsoLLJCTfSAWZmHePYH3ucbPkz8Tc,12968
+deepview/validator/visualize/utils.py,sha256=GzlUbj6hOcPYDDKO9nWHxWdqk_AEeUVl4YyWoqDGNp0,11442
 deepview/validator/writers/__init__.py,sha256=PAF-P5kc2h-0VYIz6KL5Z4jgzpKcu4ulIeDsOldKTjE,574
-deepview/validator/writers/console.py,sha256=oIDnZdDtlc0snO24ODN_-54vrdWrvUB0J3AI08RCtuE,7611
-deepview/validator/writers/core.py,sha256=tOARwmMr7MxKanlBtXqlvFf3HVDnF-pvt8WCWirWG4c,19839
-deepview/validator/writers/tensorboard.py,sha256=NxzZolLWfAlz1G8gyOZxYRlYmzM0F2KijPrHkKvA8nE,9065
-deepview_validator-3.0.8.dist-info/METADATA,sha256=DIkS6YwORKRDNXgY5TcodH8lRJGO8O2TgNcx6eRFgXY,468
-deepview_validator-3.0.8.dist-info/WHEEL,sha256=nvhOrkn7_9sGzJjxuUFjoJ6OkO7SJJqHSjq9VNu0Elc,92
-deepview_validator-3.0.8.dist-info/entry_points.txt,sha256=n4jIdEDC_mPGVLwmS21vEFC8_D7mqNuekZYdtupSSVE,73
-deepview_validator-3.0.8.dist-info/top_level.txt,sha256=FZ_uj5ZExs9dTNq5lw196yb-XR3VHKi6vS0EWgTQtXk,9
-deepview_validator-3.0.8.dist-info/RECORD,,
+deepview/validator/writers/console.py,sha256=OLn1fyH9CZT7kyvnL16YM5mWhf94tccrnWs5t0rdKz4,2402
+deepview/validator/writers/core.py,sha256=GJ8xzMm_FUl9rOtBYKpZwhM2BhhGwIf4_uZa_hnqMGc,14283
+deepview/validator/writers/tensorboard.py,sha256=2d6cxqQDkSA1T3R53OYhY-cuSiOcgTiWJrItXm8211A,5234
+deepview_validator-3.0.9.dist-info/METADATA,sha256=A3M-4Rx8V39WcAnzug5fSBG7c-ry7AuyF_3zr1JDj4M,468
+deepview_validator-3.0.9.dist-info/WHEEL,sha256=nvhOrkn7_9sGzJjxuUFjoJ6OkO7SJJqHSjq9VNu0Elc,92
+deepview_validator-3.0.9.dist-info/entry_points.txt,sha256=n4jIdEDC_mPGVLwmS21vEFC8_D7mqNuekZYdtupSSVE,73
+deepview_validator-3.0.9.dist-info/top_level.txt,sha256=FZ_uj5ZExs9dTNq5lw196yb-XR3VHKi6vS0EWgTQtXk,9
+deepview_validator-3.0.9.dist-info/RECORD,,
```


# Comparing `tmp/cellpy-1.0.0b0.tar.gz` & `tmp/cellpy-1.0.0b1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "cellpy-1.0.0b0.tar", last modified: Mon May 29 19:16:23 2023, max compression
+gzip compressed data, was "cellpy-1.0.0b1.tar", last modified: Sun Jul 16 21:30:30 2023, max compression
```

## Comparing `cellpy-1.0.0b0.tar` & `cellpy-1.0.0b1.tar`

### file list

```diff
@@ -1,215 +1,216 @@
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.529624 cellpy-1.0.0b0/
--rw-rw-rw-   0        0        0      503 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/AUTHORS.rst
--rw-rw-rw-   0        0        0     3086 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/CONTRIBUTING.rst
--rw-rw-rw-   0        0        0     4001 2023-05-25 17:55:44.000000 cellpy-1.0.0b0/HISTORY.rst
--rw-rw-rw-   0        0        0     1089 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/LICENSE
--rw-rw-rw-   0        0        0      645 2022-05-27 12:07:50.000000 cellpy-1.0.0b0/MANIFEST.in
--rw-rw-rw-   0        0        0     6767 2023-05-29 19:16:23.529624 cellpy-1.0.0b0/PKG-INFO
--rw-rw-rw-   0        0        0     1872 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/README.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.237406 cellpy-1.0.0b0/cellpy/
--rw-rw-rw-   0        0        0      805 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/__init__.py
--rw-rw-rw-   0        0        0       24 2023-05-29 19:15:05.000000 cellpy-1.0.0b0/cellpy/_version.py
--rw-rw-rw-   0        0        0    52885 2023-05-28 20:26:35.000000 cellpy-1.0.0b0/cellpy/cli.py
--rw-rw-rw-   0        0        0     1228 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/exceptions.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.249480 cellpy-1.0.0b0/cellpy/internals/
--rw-rw-rw-   0        0        0        0 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/internals/__init__.py
--rw-rw-rw-   0        0        0    27994 2023-05-06 21:51:58.000000 cellpy-1.0.0b0/cellpy/internals/core.py
--rw-rw-rw-   0        0        0     4838 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/log.py
--rw-rw-rw-   0        0        0     1750 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/logging.json
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.258478 cellpy-1.0.0b0/cellpy/parameters/
--rw-rw-rw-   0        0        0     3183 2023-05-02 11:51:31.000000 cellpy-1.0.0b0/cellpy/parameters/.cellpy_prms_default.conf
--rw-rw-rw-   0        0        0        2 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/parameters/__init__.py
--rw-rw-rw-   0        0        0    23452 2023-05-14 17:34:31.000000 cellpy-1.0.0b0/cellpy/parameters/internal_settings.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.261008 cellpy-1.0.0b0/cellpy/parameters/legacy/
--rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/parameters/legacy/__init__.py
--rw-rw-rw-   0        0        0    24146 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/parameters/legacy/update_headers.py
--rw-rw-rw-   0        0        0    12261 2023-05-23 14:50:48.000000 cellpy-1.0.0b0/cellpy/parameters/prmreader.py
--rw-rw-rw-   0        0        0    12241 2023-05-29 17:41:25.000000 cellpy-1.0.0b0/cellpy/parameters/prms.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.271128 cellpy-1.0.0b0/cellpy/readers/
--rw-rw-rw-   0        0        0        2 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/readers/__init__.py
--rw-rw-rw-   0        0        0   242077 2023-05-29 17:41:28.000000 cellpy-1.0.0b0/cellpy/readers/cellreader.py
--rw-rw-rw-   0        0        0    38362 2023-05-29 18:12:16.000000 cellpy-1.0.0b0/cellpy/readers/core.py
--rw-rw-rw-   0        0        0    22998 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/dbreader.py
--rw-rw-rw-   0        0        0    13825 2023-05-06 21:51:58.000000 cellpy-1.0.0b0/cellpy/readers/filefinder.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.296026 cellpy-1.0.0b0/cellpy/readers/instruments/
--rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/readers/instruments/__init__.py
--rw-rw-rw-   0        0        0    49052 2023-05-12 17:21:48.000000 cellpy-1.0.0b0/cellpy/readers/instruments/arbin_res.py
--rw-rw-rw-   0        0        0    19381 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql.py
--rw-rw-rw-   0        0        0    21062 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_7.py
--rw-rw-rw-   0        0        0    11134 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_csv.py
--rw-rw-rw-   0        0        0     7126 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_h5.py
--rw-rw-rw-   0        0        0     9883 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_xlsx.py
--rw-rw-rw-   0        0        0    27324 2023-05-09 19:57:46.000000 cellpy-1.0.0b0/cellpy/readers/instruments/base.py
--rw-rw-rw-   0        0        0    22693 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/biologics_mpr.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.308026 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/
--rw-rw-rw-   0        0        0     6607 2022-06-03 19:58:41.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/__init__.py
--rw-rw-rw-   0        0        0     1700 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_four.py
--rw-rw-rw-   0        0        0     4084 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_one.py
--rw-rw-rw-   0        0        0     1990 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_three.py
--rw-rw-rw-   0        0        0     1788 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_two.py
--rw-rw-rw-   0        0        0     3549 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_zero.py
--rw-rw-rw-   0        0        0     2132 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/configurations/neware_txt_zero.py
--rw-rw-rw-   0        0        0    10327 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/custom.py
--rw-rw-rw-   0        0        0     3760 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/ext_nda_reader.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.310027 cellpy-1.0.0b0/cellpy/readers/instruments/loader_specific_modules/
--rw-rw-rw-   0        0        0        0 2022-06-03 19:58:41.000000 cellpy-1.0.0b0/cellpy/readers/instruments/loader_specific_modules/__init__.py
--rw-rw-rw-   0        0        0    22115 2022-06-03 19:58:41.000000 cellpy-1.0.0b0/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py
--rw-rw-rw-   0        0        0     1067 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/local_instrument.py
--rw-rw-rw-   0        0        0    12886 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/maccor_txt.py
--rw-rw-rw-   0        0        0     3488 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/neware_txt.py
--rw-rw-rw-   0        0        0    16769 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/pec_csv.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.315029 cellpy-1.0.0b0/cellpy/readers/instruments/processors/
--rw-rw-rw-   0        0        0        0 2022-05-27 12:07:50.000000 cellpy-1.0.0b0/cellpy/readers/instruments/processors/__init__.py
--rw-rw-rw-   0        0        0    15265 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/processors/post_processors.py
--rw-rw-rw-   0        0        0     1450 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/instruments/processors/pre_processors.py
--rw-rw-rw-   0        0        0    26852 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/readers/sql_dbreader.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.337026 cellpy-1.0.0b0/cellpy/utils/
--rw-rw-rw-   0        0        0      192 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/utils/__init__.py
--rw-rw-rw-   0        0        0    48182 2023-05-03 09:31:40.000000 cellpy-1.0.0b0/cellpy/utils/batch.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.358026 cellpy-1.0.0b0/cellpy/utils/batch_tools/
--rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/__init__.py
--rw-rw-rw-   0        0        0     7578 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_analyzers.py
--rw-rw-rw-   0        0        0    19566 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_core.py
--rw-rw-rw-   0        0        0    40056 2023-05-16 14:17:12.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_experiments.py
--rw-rw-rw-   0        0        0     2931 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_exporters.py
--rw-rw-rw-   0        0        0    14090 2023-05-06 21:51:58.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_helpers.py
--rw-rw-rw-   0        0        0    27683 2023-05-02 13:39:36.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_journals.py
--rw-rw-rw-   0        0        0    29066 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_plotters.py
--rw-rw-rw-   0        0        0      245 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_reporters.py
--rw-rw-rw-   0        0        0     3339 2022-05-27 12:03:59.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/dumpers.py
--rw-rw-rw-   0        0        0     9872 2023-05-02 10:20:24.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/engines.py
--rw-rw-rw-   0        0        0     5294 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/batch_tools/sqlite_from_excel_db.py
--rw-rw-rw-   0        0        0    61436 2023-05-29 17:36:47.000000 cellpy-1.0.0b0/cellpy/utils/collectors.py
--rw-rw-rw-   0        0        0    44134 2023-05-29 17:41:12.000000 cellpy-1.0.0b0/cellpy/utils/collectors_old.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.359026 cellpy-1.0.0b0/cellpy/utils/data/
--rw-rw-rw-   0        0        0  3700143 2023-05-29 17:38:03.000000 cellpy-1.0.0b0/cellpy/utils/data/20160805_test001_45_cc.h5
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.369553 cellpy-1.0.0b0/cellpy/utils/data/raw/
--rw-rw-rw-   0        0        0  1613824 2021-12-21 09:11:58.000000 cellpy-1.0.0b0/cellpy/utils/data/raw/20160805_test001_45_cc_01.res
--rw-rw-rw-   0        0        0      260 2022-05-27 12:07:50.000000 cellpy-1.0.0b0/cellpy/utils/diagnostics.py
--rw-rw-rw-   0        0        0    79016 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/easyplot.py
--rw-rw-rw-   0        0        0     1576 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/example_data.py
--rw-rw-rw-   0        0        0    38296 2023-05-28 11:22:04.000000 cellpy-1.0.0b0/cellpy/utils/helpers.py
--rw-rw-rw-   0        0        0    37902 2023-05-28 19:25:11.000000 cellpy-1.0.0b0/cellpy/utils/ica.py
--rw-rw-rw-   0        0        0      189 2022-05-27 12:07:50.000000 cellpy-1.0.0b0/cellpy/utils/live.py
--rw-rw-rw-   0        0        0    31789 2023-05-28 20:26:34.000000 cellpy-1.0.0b0/cellpy/utils/ocv_rlx.py
--rw-rw-rw-   0        0        0    45397 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/plotutils.py
--rw-rw-rw-   0        0        0     1787 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/cellpy/utils/processor.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.246933 cellpy-1.0.0b0/cellpy.egg-info/
--rw-rw-rw-   0        0        0     6767 2023-05-29 19:16:22.000000 cellpy-1.0.0b0/cellpy.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     7846 2023-05-29 19:16:23.000000 cellpy-1.0.0b0/cellpy.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-05-29 19:16:22.000000 cellpy-1.0.0b0/cellpy.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       42 2023-05-29 19:16:22.000000 cellpy-1.0.0b0/cellpy.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0        2 2023-05-29 19:16:22.000000 cellpy-1.0.0b0/cellpy.egg-info/not-zip-safe
--rw-rw-rw-   0        0        0      337 2023-05-29 19:16:22.000000 cellpy-1.0.0b0/cellpy.egg-info/requires.txt
--rw-rw-rw-   0        0        0       13 2023-05-29 19:16:22.000000 cellpy-1.0.0b0/cellpy.egg-info/top_level.txt
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.381553 cellpy-1.0.0b0/docs/
--rw-rw-rw-   0        0        0     6939 2022-09-20 08:21:07.000000 cellpy-1.0.0b0/docs/Makefile
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.212675 cellpy-1.0.0b0/docs/_build/
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.211676 cellpy-1.0.0b0/docs/_build/.doctrees/
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.388557 cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/
--rw-rw-rw-   0        0        0    15014 2023-04-30 13:53:57.000000 cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png
--rw-rw-rw-   0        0        0    14599 2023-04-30 13:53:57.000000 cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png
--rw-rw-rw-   0        0        0    13527 2023-04-30 13:53:57.000000 cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png
--rw-rw-rw-   0        0        0    25669 2023-04-30 13:54:02.000000 cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.422083 cellpy-1.0.0b0/docs/_build/_images/
--rw-rw-rw-   0        0        0    15014 2023-04-27 15:02:55.000000 cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png
--rw-rw-rw-   0        0        0    14599 2023-04-27 15:02:55.000000 cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png
--rw-rw-rw-   0        0        0    13527 2023-04-27 15:02:55.000000 cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png
--rw-rw-rw-   0        0        0    25669 2023-04-27 15:03:02.000000 cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png
--rw-rw-rw-   0        0        0    11678 2023-04-29 14:19:31.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-22185705e237fa530df24e4af50cb25833165e25.png
--rw-rw-rw-   0        0        0    22040 2023-04-29 14:33:27.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-246f2486cd940f2ea40a07f847c86c22b2607ca8.png
--rw-rw-rw-   0        0        0    21790 2023-04-29 14:26:00.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-42e9bc826d476a3d361a7f410e989ed34dc1aa85.png
--rw-rw-rw-   0        0        0    32991 2023-04-29 14:26:01.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-619216a42370fd49669c083549129b8470c8fae1.png
--rw-rw-rw-   0        0        0    32991 2023-04-30 20:02:23.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-6412a7c74952b4793798e9032f5bc4e7a1ab70c1.png
--rw-rw-rw-   0        0        0     7231 2023-04-29 14:12:31.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-6deb64a460668e8ef9bf0ca653314119adeeae66.png
--rw-rw-rw-   0        0        0    13360 2023-04-29 14:36:26.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-83b62e03ef369ff0a30f027892dba95b91ea8b6c.png
--rw-rw-rw-   0        0        0    21790 2023-04-30 20:02:22.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-8ec82d564b1a6ea5b95a36a4a213f7a78aaedc63.png
--rw-rw-rw-   0        0        0     5391 2023-04-29 14:10:02.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-ce8a9fe2ba01194aed847e0248d749db4093aca1.png
--rw-rw-rw-   0        0        0    20826 2023-04-29 14:29:06.000000 cellpy-1.0.0b0/docs/_build/_images/graphviz-e94a5352318e02fcc5ef1f813e02a526c39af791.png
--rw-rw-rw-   0        0        0    88743 2023-04-19 13:49:30.000000 cellpy-1.0.0b0/docs/_build/_images/templates_jupyterlab_001.png
--rw-rw-rw-   0        0        0   296908 2022-05-27 12:07:51.000000 cellpy-1.0.0b0/docs/_build/_images/tutorials_utils_plotting_fig1.png
--rw-rw-rw-   0        0        0    54588 2022-05-27 12:07:51.000000 cellpy-1.0.0b0/docs/_build/_images/tutorials_utils_plotting_fig2.png
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.427113 cellpy-1.0.0b0/docs/_build/_static/
--rw-rw-rw-   0        0        0      286 2023-04-25 11:20:36.000000 cellpy-1.0.0b0/docs/_build/_static/file.png
--rw-rw-rw-   0        0        0       90 2023-04-25 11:20:36.000000 cellpy-1.0.0b0/docs/_build/_static/minus.png
--rw-rw-rw-   0        0        0       90 2023-04-25 11:20:36.000000 cellpy-1.0.0b0/docs/_build/_static/plus.png
--rw-rw-rw-   0        0        0     1695 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/adapted_readme.rst
--rw-rw-rw-   0        0        0    10731 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/conf.py
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.441590 cellpy-1.0.0b0/docs/developers_guide/
--rw-rw-rw-   0        0        0     1334 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_data_structure.rst
--rw-rw-rw-   0        0        0     5904 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_folder_structure.rst
--rw-rw-rw-   0        0        0      935 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_packaging_pypi.rst
--rw-rw-rw-   0        0        0     3009 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_setup.rst
--rw-rw-rw-   0        0        0     1821 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_conda_package.rst
--rw-rw-rw-   0        0        0     2136 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_docs.rst
--rw-rw-rw-   0        0        0     5218 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_loaders_and_instruments.rst
--rw-rw-rw-   0        0        0     1768 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/dev_various.rst
--rw-rw-rw-   0        0        0      326 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/developers_guide/index.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.452103 cellpy-1.0.0b0/docs/examples_and_tutorials/
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.465121 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/
--rw-rw-rw-   0        0        0    15786 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/01_getting_started_tutorial.rst
--rw-rw-rw-   0        0        0     3909 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/02_read_cell_data.rst
--rw-rw-rw-   0        0        0     5485 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/03_more_about_get.rst
--rw-rw-rw-   0        0        0     4465 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/04_other_interactions.rst
--rw-rw-rw-   0        0        0     5908 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/05_configuring.rst
--rw-rw-rw-   0        0        0     1052 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/06_pandas.rst
--rw-rw-rw-   0        0        0     1102 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/07_data_mining.rst
--rw-rw-rw-   0        0        0     8224 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/08_the_cellpy_cmd.rst
--rw-rw-rw-   0        0        0      484 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/basics.rst
--rw-rw-rw-   0        0        0      366 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/examples.rst
--rw-rw-rw-   0        0        0      174 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/index.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.474120 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/
--rw-rw-rw-   0        0        0       49 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/01_arbin.rst
--rw-rw-rw-   0        0        0       52 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/02_maccor.rst
--rw-rw-rw-   0        0        0       43 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/03_PEC.rst
--rw-rw-rw-   0        0        0       54 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/04_Neware.rst
--rw-rw-rw-   0        0        0       62 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/05_biologics.rst
--rw-rw-rw-   0        0        0       54 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders/06_custom.rst
--rw-rw-rw-   0        0        0      260 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/loaders.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.214675 cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks/
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.477629 cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks/images/
--rw-rw-rw-   0        0        0    88743 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_001.png
--rw-rw-rw-   0        0        0    95663 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_002.png
--rw-rw-rw-   0        0        0      231 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks.rst
--rw-rw-rw-   0        0        0     1797 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/tips_and_tricks.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.487638 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/
--rw-rw-rw-   0        0        0     4388 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/batch.rst
--rw-rw-rw-   0        0        0       59 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/collectors.rst
--rw-rw-rw-   0        0        0       53 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/easyplot.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.491628 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/figures/
--rw-rw-rw-   0        0        0   296908 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png
--rw-rw-rw-   0        0        0    54588 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png
--rw-rw-rw-   0        0        0     1219 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/ica.rst
--rw-rw-rw-   0        0        0     2063 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/plotting.rst
--rw-rw-rw-   0        0        0      338 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/templates.rst
--rw-rw-rw-   0        0        0     1379 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils/tut_ocv_rlx.rst
--rw-rw-rw-   0        0        0      371 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/examples_and_tutorials/utils.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.498109 cellpy-1.0.0b0/docs/figures/
--rw-rw-rw-   0        0        0     9981 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/figures/cellpy-icon-bw.png
--rw-rw-rw-   0        0        0    10302 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/figures/cellpy-logo-v1.png
--rw-rw-rw-   0        0        0      593 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/index.rst
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.508625 cellpy-1.0.0b0/docs/main_description/
--rw-rw-rw-   0        0        0       32 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/authors.rst
--rw-rw-rw-   0        0        0       37 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/contributing.rst
--rw-rw-rw-   0        0        0    16327 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/formats.rst
--rw-rw-rw-   0        0        0       32 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/history.rst
--rw-rw-rw-   0        0        0      182 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/index.rst
--rw-rw-rw-   0        0        0     4288 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/installation.rst
--rw-rw-rw-   0        0        0     3444 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/main_description/usage.rst
--rwxrwxrwx   0        0        0     6701 2022-09-20 08:21:07.000000 cellpy-1.0.0b0/docs/make.bat
-drwxrwxrwx   0        0        0        0 2023-05-29 19:16:23.527631 cellpy-1.0.0b0/docs/source/
--rw-rw-rw-   0        0        0      367 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.internals.rst
--rw-rw-rw-   0        0        0      447 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.parameters.legacy.rst
--rw-rw-rw-   0        0        0      847 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.parameters.rst
--rw-rw-rw-   0        0        0     1911 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.configurations.rst
--rw-rw-rw-   0        0        0      631 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.loader_specific_modules.rst
--rw-rw-rw-   0        0        0      783 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.processors.rst
--rw-rw-rw-   0        0        0     3413 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.rst
--rw-rw-rw-   0        0        0     1139 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.readers.rst
--rw-rw-rw-   0        0        0      721 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.rst
--rw-rw-rw-   0        0        0     2610 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.utils.batch_tools.rst
--rw-rw-rw-   0        0        0     2222 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/docs/source/cellpy.utils.rst
--rw-rw-rw-   0        0        0       62 2023-05-01 10:14:23.000000 cellpy-1.0.0b0/docs/source/modules.rst
--rw-rw-rw-   0        0        0      281 2023-05-01 18:24:45.000000 cellpy-1.0.0b0/pyproject.toml
--rw-rw-rw-   0        0        0       42 2023-05-29 19:16:23.529624 cellpy-1.0.0b0/setup.cfg
--rw-rw-rw-   0        0        0     2922 2023-05-12 17:21:47.000000 cellpy-1.0.0b0/setup.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.621905 cellpy-1.0.0b1/
+-rw-rw-rw-   0        0        0      503 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/AUTHORS.rst
+-rw-rw-rw-   0        0        0     3377 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/CONTRIBUTING.rst
+-rw-rw-rw-   0        0        0     4157 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/HISTORY.rst
+-rw-rw-rw-   0        0        0     1089 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/LICENSE
+-rw-rw-rw-   0        0        0      645 2022-05-27 12:07:50.000000 cellpy-1.0.0b1/MANIFEST.in
+-rw-rw-rw-   0        0        0     6649 2023-07-16 21:30:30.621905 cellpy-1.0.0b1/PKG-INFO
+-rw-rw-rw-   0        0        0     1754 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/README.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.393515 cellpy-1.0.0b1/cellpy/
+-rw-rw-rw-   0        0        0      805 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/__init__.py
+-rw-rw-rw-   0        0        0       25 2023-07-16 21:29:17.000000 cellpy-1.0.0b1/cellpy/_version.py
+-rw-rw-rw-   0        0        0    54665 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/cellpy/cli.py
+-rw-rw-rw-   0        0        0     1228 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/exceptions.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.408514 cellpy-1.0.0b1/cellpy/internals/
+-rw-rw-rw-   0        0        0        0 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/internals/__init__.py
+-rw-rw-rw-   0        0        0    27994 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/internals/core.py
+-rw-rw-rw-   0        0        0     4838 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/log.py
+-rw-rw-rw-   0        0        0     1750 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/logging.json
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.416522 cellpy-1.0.0b1/cellpy/parameters/
+-rw-rw-rw-   0        0        0     3317 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/parameters/.cellpy_prms_default.conf
+-rw-rw-rw-   0        0        0        2 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/parameters/__init__.py
+-rw-rw-rw-   0        0        0    24176 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/parameters/internal_settings.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.418519 cellpy-1.0.0b1/cellpy/parameters/legacy/
+-rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/parameters/legacy/__init__.py
+-rw-rw-rw-   0        0        0    24146 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/parameters/legacy/update_headers.py
+-rw-rw-rw-   0        0        0    12653 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/parameters/prmreader.py
+-rw-rw-rw-   0        0        0    12646 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/parameters/prms.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.427518 cellpy-1.0.0b1/cellpy/readers/
+-rw-rw-rw-   0        0        0        2 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/readers/__init__.py
+-rw-rw-rw-   0        0        0   249651 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/cellpy/readers/cellreader.py
+-rw-rw-rw-   0        0        0    39632 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/cellpy/readers/core.py
+-rw-rw-rw-   0        0        0    22998 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/dbreader.py
+-rw-rw-rw-   0        0        0    13825 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/readers/filefinder.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.446516 cellpy-1.0.0b1/cellpy/readers/instruments/
+-rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/readers/instruments/__init__.py
+-rw-rw-rw-   0        0        0    49706 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/cellpy/readers/instruments/arbin_res.py
+-rw-rw-rw-   0        0        0    19381 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql.py
+-rw-rw-rw-   0        0        0    21062 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_7.py
+-rw-rw-rw-   0        0        0    11134 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_csv.py
+-rw-rw-rw-   0        0        0     7126 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_h5.py
+-rw-rw-rw-   0        0        0     9883 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_xlsx.py
+-rw-rw-rw-   0        0        0    28071 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/readers/instruments/base.py
+-rw-rw-rw-   0        0        0    22693 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/biologics_mpr.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.455523 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/
+-rw-rw-rw-   0        0        0     6607 2022-06-03 19:58:41.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/__init__.py
+-rw-rw-rw-   0        0        0     1700 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_four.py
+-rw-rw-rw-   0        0        0     4084 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_one.py
+-rw-rw-rw-   0        0        0     1990 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_three.py
+-rw-rw-rw-   0        0        0     1788 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_two.py
+-rw-rw-rw-   0        0        0     3549 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_zero.py
+-rw-rw-rw-   0        0        0     2132 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/configurations/neware_txt_zero.py
+-rw-rw-rw-   0        0        0    10327 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/custom.py
+-rw-rw-rw-   0        0        0     3760 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/ext_nda_reader.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.457344 cellpy-1.0.0b1/cellpy/readers/instruments/loader_specific_modules/
+-rw-rw-rw-   0        0        0        0 2022-06-03 19:58:41.000000 cellpy-1.0.0b1/cellpy/readers/instruments/loader_specific_modules/__init__.py
+-rw-rw-rw-   0        0        0    22115 2022-06-03 19:58:41.000000 cellpy-1.0.0b1/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py
+-rw-rw-rw-   0        0        0     1067 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/local_instrument.py
+-rw-rw-rw-   0        0        0    12886 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/maccor_txt.py
+-rw-rw-rw-   0        0        0     3488 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/neware_txt.py
+-rw-rw-rw-   0        0        0    16769 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/pec_csv.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.461351 cellpy-1.0.0b1/cellpy/readers/instruments/processors/
+-rw-rw-rw-   0        0        0        0 2022-05-27 12:07:50.000000 cellpy-1.0.0b1/cellpy/readers/instruments/processors/__init__.py
+-rw-rw-rw-   0        0        0    15265 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/processors/post_processors.py
+-rw-rw-rw-   0        0        0     1450 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/instruments/processors/pre_processors.py
+-rw-rw-rw-   0        0        0    26852 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/readers/sql_dbreader.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.476978 cellpy-1.0.0b1/cellpy/utils/
+-rw-rw-rw-   0        0        0      192 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/utils/__init__.py
+-rw-rw-rw-   0        0        0    49434 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/utils/batch.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.491976 cellpy-1.0.0b1/cellpy/utils/batch_tools/
+-rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/__init__.py
+-rw-rw-rw-   0        0        0     7578 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_analyzers.py
+-rw-rw-rw-   0        0        0    19566 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_core.py
+-rw-rw-rw-   0        0        0    41058 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_experiments.py
+-rw-rw-rw-   0        0        0     2931 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_exporters.py
+-rw-rw-rw-   0        0        0    14090 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_helpers.py
+-rw-rw-rw-   0        0        0    28472 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_journals.py
+-rw-rw-rw-   0        0        0    29066 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_plotters.py
+-rw-rw-rw-   0        0        0      245 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_reporters.py
+-rw-rw-rw-   0        0        0     3339 2022-05-27 12:03:59.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/dumpers.py
+-rw-rw-rw-   0        0        0    10156 2023-05-29 19:23:43.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/engines.py
+-rw-rw-rw-   0        0        0     5294 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/batch_tools/sqlite_from_excel_db.py
+-rw-rw-rw-   0        0        0    63284 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/utils/collectors.py
+-rw-rw-rw-   0        0        0    45443 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/utils/collectors_old.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.492976 cellpy-1.0.0b1/cellpy/utils/data/
+-rw-rw-rw-   0        0        0  3700143 2023-06-12 14:49:36.000000 cellpy-1.0.0b1/cellpy/utils/data/20160805_test001_45_cc.h5
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.496977 cellpy-1.0.0b1/cellpy/utils/data/raw/
+-rw-rw-rw-   0        0        0  1613824 2021-12-21 09:11:58.000000 cellpy-1.0.0b1/cellpy/utils/data/raw/20160805_test001_45_cc_01.res
+-rw-rw-rw-   0        0        0      260 2022-05-27 12:07:50.000000 cellpy-1.0.0b1/cellpy/utils/diagnostics.py
+-rw-rw-rw-   0        0        0    79016 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/easyplot.py
+-rw-rw-rw-   0        0        0     1576 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/example_data.py
+-rw-rw-rw-   0        0        0    39447 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/utils/helpers.py
+-rw-rw-rw-   0        0        0    38983 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/utils/ica.py
+-rw-rw-rw-   0        0        0      189 2022-05-27 12:07:50.000000 cellpy-1.0.0b1/cellpy/utils/live.py
+-rw-rw-rw-   0        0        0    32747 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/cellpy/utils/ocv_rlx.py
+-rw-rw-rw-   0        0        0    45397 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/plotutils.py
+-rw-rw-rw-   0        0        0     1787 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/cellpy/utils/processor.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.405514 cellpy-1.0.0b1/cellpy.egg-info/
+-rw-rw-rw-   0        0        0     6649 2023-07-16 21:30:30.000000 cellpy-1.0.0b1/cellpy.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0     7904 2023-07-16 21:30:30.000000 cellpy-1.0.0b1/cellpy.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-07-16 21:30:30.000000 cellpy-1.0.0b1/cellpy.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       42 2023-07-16 21:30:30.000000 cellpy-1.0.0b1/cellpy.egg-info/entry_points.txt
+-rw-rw-rw-   0        0        0        2 2023-07-16 21:30:29.000000 cellpy-1.0.0b1/cellpy.egg-info/not-zip-safe
+-rw-rw-rw-   0        0        0      337 2023-07-16 21:30:30.000000 cellpy-1.0.0b1/cellpy.egg-info/requires.txt
+-rw-rw-rw-   0        0        0       13 2023-07-16 21:30:30.000000 cellpy-1.0.0b1/cellpy.egg-info/top_level.txt
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.504984 cellpy-1.0.0b1/docs/
+-rw-rw-rw-   0        0        0     6939 2022-09-20 08:21:07.000000 cellpy-1.0.0b1/docs/Makefile
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.372516 cellpy-1.0.0b1/docs/_build/
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.372516 cellpy-1.0.0b1/docs/_build/.doctrees/
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.509976 cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/
+-rw-rw-rw-   0        0        0    15014 2023-07-16 19:26:52.000000 cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png
+-rw-rw-rw-   0        0        0    14599 2023-07-16 19:26:52.000000 cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png
+-rw-rw-rw-   0        0        0    13527 2023-07-16 19:26:52.000000 cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png
+-rw-rw-rw-   0        0        0    25669 2023-07-16 19:26:57.000000 cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.532986 cellpy-1.0.0b1/docs/_build/_images/
+-rw-rw-rw-   0        0        0    15014 2023-04-27 15:02:55.000000 cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png
+-rw-rw-rw-   0        0        0    14599 2023-04-27 15:02:55.000000 cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png
+-rw-rw-rw-   0        0        0    13527 2023-04-27 15:02:55.000000 cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png
+-rw-rw-rw-   0        0        0    25669 2023-04-27 15:03:02.000000 cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png
+-rw-rw-rw-   0        0        0    11678 2023-04-29 14:19:31.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-22185705e237fa530df24e4af50cb25833165e25.png
+-rw-rw-rw-   0        0        0    22040 2023-04-29 14:33:27.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-246f2486cd940f2ea40a07f847c86c22b2607ca8.png
+-rw-rw-rw-   0        0        0    21790 2023-04-29 14:26:00.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-42e9bc826d476a3d361a7f410e989ed34dc1aa85.png
+-rw-rw-rw-   0        0        0    32991 2023-04-29 14:26:01.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-619216a42370fd49669c083549129b8470c8fae1.png
+-rw-rw-rw-   0        0        0    32991 2023-04-30 20:02:23.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-6412a7c74952b4793798e9032f5bc4e7a1ab70c1.png
+-rw-rw-rw-   0        0        0     7231 2023-04-29 14:12:31.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-6deb64a460668e8ef9bf0ca653314119adeeae66.png
+-rw-rw-rw-   0        0        0    13360 2023-04-29 14:36:26.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-83b62e03ef369ff0a30f027892dba95b91ea8b6c.png
+-rw-rw-rw-   0        0        0    21790 2023-04-30 20:02:22.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-8ec82d564b1a6ea5b95a36a4a213f7a78aaedc63.png
+-rw-rw-rw-   0        0        0     5391 2023-04-29 14:10:02.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-ce8a9fe2ba01194aed847e0248d749db4093aca1.png
+-rw-rw-rw-   0        0        0    20826 2023-04-29 14:29:06.000000 cellpy-1.0.0b1/docs/_build/_images/graphviz-e94a5352318e02fcc5ef1f813e02a526c39af791.png
+-rw-rw-rw-   0        0        0    88743 2023-04-19 13:49:30.000000 cellpy-1.0.0b1/docs/_build/_images/templates_jupyterlab_001.png
+-rw-rw-rw-   0        0        0   296908 2022-05-27 12:07:51.000000 cellpy-1.0.0b1/docs/_build/_images/tutorials_utils_plotting_fig1.png
+-rw-rw-rw-   0        0        0    54588 2022-05-27 12:07:51.000000 cellpy-1.0.0b1/docs/_build/_images/tutorials_utils_plotting_fig2.png
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.536977 cellpy-1.0.0b1/docs/_build/_static/
+-rw-rw-rw-   0        0        0      286 2023-04-25 11:20:36.000000 cellpy-1.0.0b1/docs/_build/_static/file.png
+-rw-rw-rw-   0        0        0       90 2023-04-25 11:20:36.000000 cellpy-1.0.0b1/docs/_build/_static/minus.png
+-rw-rw-rw-   0        0        0       90 2023-04-25 11:20:36.000000 cellpy-1.0.0b1/docs/_build/_static/plus.png
+-rw-rw-rw-   0        0        0     1577 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/adapted_readme.rst
+-rw-rw-rw-   0        0        0    10731 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/conf.py
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.548987 cellpy-1.0.0b1/docs/developers_guide/
+-rw-rw-rw-   0        0        0     1334 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_data_structure.rst
+-rw-rw-rw-   0        0        0     5904 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_folder_structure.rst
+-rw-rw-rw-   0        0        0      935 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_packaging_pypi.rst
+-rw-rw-rw-   0        0        0     3009 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_setup.rst
+-rw-rw-rw-   0        0        0     1821 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_conda_package.rst
+-rw-rw-rw-   0        0        0     2136 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_docs.rst
+-rw-rw-rw-   0        0        0     5218 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_loaders_and_instruments.rst
+-rw-rw-rw-   0        0        0     1768 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/dev_various.rst
+-rw-rw-rw-   0        0        0      326 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/developers_guide/index.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.557985 cellpy-1.0.0b1/docs/examples_and_tutorials/
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.568905 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/
+-rw-rw-rw-   0        0        0    21069 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/01_getting_started_tutorial.rst
+-rw-rw-rw-   0        0        0     4024 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/02_read_cell_data.rst
+-rw-rw-rw-   0        0        0     5485 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/03_more_about_get.rst
+-rw-rw-rw-   0        0        0     4465 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/04_other_interactions.rst
+-rw-rw-rw-   0        0        0     5908 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/05_configuring.rst
+-rw-rw-rw-   0        0        0     1121 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/06_pandas.rst
+-rw-rw-rw-   0        0        0     1102 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/07_data_mining.rst
+-rw-rw-rw-   0        0        0     8218 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/08_the_cellpy_cmd.rst
+-rw-rw-rw-   0        0        0      484 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/basics.rst
+-rw-rw-rw-   0        0        0      366 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/examples.rst
+-rw-rw-rw-   0        0        0      174 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/index.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.576906 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/
+-rw-rw-rw-   0        0        0       49 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/01_arbin.rst
+-rw-rw-rw-   0        0        0       52 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/02_maccor.rst
+-rw-rw-rw-   0        0        0       43 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/03_PEC.rst
+-rw-rw-rw-   0        0        0       54 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/04_Neware.rst
+-rw-rw-rw-   0        0        0       62 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/05_biologics.rst
+-rw-rw-rw-   0        0        0       54 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders/06_custom.rst
+-rw-rw-rw-   0        0        0      260 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/loaders.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.374516 cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks/
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.580906 cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks/images/
+-rw-rw-rw-   0        0        0    88743 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_001.png
+-rw-rw-rw-   0        0        0    95663 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_002.png
+-rw-rw-rw-   0        0        0      231 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks.rst
+-rw-rw-rw-   0        0        0     1797 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/tips_and_tricks.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.591914 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/
+-rw-rw-rw-   0        0        0     4388 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/batch.rst
+-rw-rw-rw-   0        0        0       59 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/collectors.rst
+-rw-rw-rw-   0        0        0       71 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/custom_file_loaders.rst
+-rw-rw-rw-   0        0        0       53 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/easyplot.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.594905 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/figures/
+-rw-rw-rw-   0        0        0   296908 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png
+-rw-rw-rw-   0        0        0    54588 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png
+-rw-rw-rw-   0        0        0     1761 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/ica.rst
+-rw-rw-rw-   0        0        0     2063 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/plotting.rst
+-rw-rw-rw-   0        0        0      338 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/templates.rst
+-rw-rw-rw-   0        0        0     1379 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils/tut_ocv_rlx.rst
+-rw-rw-rw-   0        0        0      407 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/examples_and_tutorials/utils.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.596905 cellpy-1.0.0b1/docs/figures/
+-rw-rw-rw-   0        0        0     9981 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/figures/cellpy-icon-bw.png
+-rw-rw-rw-   0        0        0    10302 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/figures/cellpy-logo-v1.png
+-rw-rw-rw-   0        0        0      593 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/index.rst
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.604906 cellpy-1.0.0b1/docs/main_description/
+-rw-rw-rw-   0        0        0       32 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/main_description/authors.rst
+-rw-rw-rw-   0        0        0       37 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/main_description/contributing.rst
+-rw-rw-rw-   0        0        0    12603 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/main_description/formats.rst
+-rw-rw-rw-   0        0        0       32 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/main_description/history.rst
+-rw-rw-rw-   0        0        0      182 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/main_description/index.rst
+-rw-rw-rw-   0        0        0     4537 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/main_description/installation.rst
+-rw-rw-rw-   0        0        0     3665 2023-07-16 21:26:07.000000 cellpy-1.0.0b1/docs/main_description/usage.rst
+-rwxrwxrwx   0        0        0     6701 2022-09-20 08:21:07.000000 cellpy-1.0.0b1/docs/make.bat
+drwxrwxrwx   0        0        0        0 2023-07-16 21:30:30.619905 cellpy-1.0.0b1/docs/source/
+-rw-rw-rw-   0        0        0      367 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.internals.rst
+-rw-rw-rw-   0        0        0      447 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.parameters.legacy.rst
+-rw-rw-rw-   0        0        0      847 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.parameters.rst
+-rw-rw-rw-   0        0        0     1911 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.configurations.rst
+-rw-rw-rw-   0        0        0      631 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.loader_specific_modules.rst
+-rw-rw-rw-   0        0        0      783 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.processors.rst
+-rw-rw-rw-   0        0        0     3413 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.rst
+-rw-rw-rw-   0        0        0     1139 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.readers.rst
+-rw-rw-rw-   0        0        0      721 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.rst
+-rw-rw-rw-   0        0        0     2610 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.utils.batch_tools.rst
+-rw-rw-rw-   0        0        0     2222 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/docs/source/cellpy.utils.rst
+-rw-rw-rw-   0        0        0       62 2023-05-01 10:14:23.000000 cellpy-1.0.0b1/docs/source/modules.rst
+-rw-rw-rw-   0        0        0      281 2023-05-01 18:24:45.000000 cellpy-1.0.0b1/pyproject.toml
+-rw-rw-rw-   0        0        0       42 2023-07-16 21:30:30.621905 cellpy-1.0.0b1/setup.cfg
+-rw-rw-rw-   0        0        0     3040 2023-06-08 11:36:02.000000 cellpy-1.0.0b1/setup.py
```

### Comparing `cellpy-1.0.0b0/CONTRIBUTING.rst` & `cellpy-1.0.0b1/CONTRIBUTING.rst`

 * *Files 9% similar despite different names*

```diff
@@ -53,50 +53,55 @@
 * Keep the scope as narrow as possible, to make it easier to implement.
 * Remember that this is a volunteer-driven project, and that contributions
   are welcome :)
 
 Get Started!
 ============
 
-Ready to contribute? Here's how to set up `cellpy` for local development.
+Ready to contribute? Here's how to set up ``cellpy`` for local development.
 
-1. Fork the `cellpy` repo on GitHub.
+1. Fork the ``cellpy`` repo on GitHub.
 2. Clone your fork locally::
 
     $ git clone git@github.com:your_name_here/cellpy.git
 
-3. Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development::
+3. Create a local python virtual environment and activate it using python's venv utility::
 
-    $ mkvirtualenv cellpy
-    $ cd cellpy/
-    $ python setup.py develop
+    $ python -m venv .venv
+    $ source .venv/bin/activate  # or .venv\Scripts\activate on Windows
 
-Or use ``conda`` environments. And you could also use `pip install . -e` instead of the last command.
+   Or use ``conda`` environments. See the conda documentation for more information.
+   A suitable environment yaml configuration file
+   can be found in the root of the repository (``dev_environment.yml``; to create the environment,
+   run ``conda env create -f dev_environment.yml``).
 
-4. Create a branch for local development::
+4. Install your local copy into your virtualenv::
+
+    $ python -m pip install . -e
+
+5. Create a branch for local development::
 
     $ git checkout -b name-of-your-bugfix-or-feature
 
    Now you can make your changes locally.
 
-5. When you're done making changes, check that your changes pass flake8 (not that my code usually does that....)
-   and the tests::
+6. When you're done making changes, check that your changes pass the tests::
 
-    $ flake8 cellpy tests
     $ pytest
 
-   To get flake8 and pytest, just pip install them into your virtualenv.
+   If there are any libraries missing (it could happen) just pip install them into your virtual environment (or
+   conda install them into your conda environment).
 
 6. Commit your changes and push your branch to GitHub::
 
     $ git add .
     $ git commit -m "Your detailed description of your changes."
     $ git push origin name-of-your-bugfix-or-feature
 
-7. Submit a pull request through the GitHub website.
+7. Submit a pull request through the GitHub website (or your IDE if that option exists).
 
 
 Pull Request Guidelines
 =======================
 
 Before you submit a pull request, check that it meets these guidelines:
```

### Comparing `cellpy-1.0.0b0/LICENSE` & `cellpy-1.0.0b1/LICENSE`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/MANIFEST.in` & `cellpy-1.0.0b1/MANIFEST.in`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/PKG-INFO` & `cellpy-1.0.0b1/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: cellpy
-Version: 1.0.0b0
+Version: 1.0.0b1
 Summary: Extract and manipulate data from battery data testers.
 Home-page: https://github.com/jepegit/cellpy
 Author: Jan Petter Maehlen
 Author-email: jepe@ife.no
 License: MIT license
 Keywords: cellpy
 Classifier: Development Status :: 5 - Production/Stable
@@ -28,17 +28,14 @@
 cellpy - *a library for assisting in analysing batteries and cells*
 ===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
-.. image:: https://img.shields.io/travis/jepegit/cellpy.svg
-        :target: https://travis-ci.org/jepegit/cellpy
-
 .. image:: https://readthedocs.org/projects/cellpy/badge/?version=latest
         :target: https://cellpy.readthedocs.io/en/latest/?badge=latest
         :alt: Documentation Status
 
 .. image:: https://pepy.tech/badge/cellpy
         :target: https://pepy.tech/project/cellpy
```

### Comparing `cellpy-1.0.0b0/README.rst` & `cellpy-1.0.0b1/README.rst`

 * *Files 9% similar despite different names*

```diff
@@ -6,17 +6,14 @@
 cellpy - *a library for assisting in analysing batteries and cells*
 ===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
-.. image:: https://img.shields.io/travis/jepegit/cellpy.svg
-        :target: https://travis-ci.org/jepegit/cellpy
-
 .. image:: https://readthedocs.org/projects/cellpy/badge/?version=latest
         :target: https://cellpy.readthedocs.io/en/latest/?badge=latest
         :alt: Documentation Status
 
 .. image:: https://pepy.tech/badge/cellpy
         :target: https://pepy.tech/project/cellpy
```

### Comparing `cellpy-1.0.0b0/cellpy/__init__.py` & `cellpy-1.0.0b1/cellpy/__init__.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/cli.py` & `cellpy-1.0.0b1/cellpy/cli.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1735 +1,1735 @@
-import base64
-import getpass
-import logging
-import os
-import pathlib
-import platform
-from pprint import pprint
-import re
-import subprocess
-import sys
-from typing import Union
-import urllib
-from pathlib import Path
-
-import click
-import pkg_resources
-from github import Github
-
-import cellpy._version
-from cellpy.exceptions import ConfigFileNotWritten
-from cellpy.parameters import prmreader
-from cellpy.parameters.internal_settings import OTHERPATHS
-from cellpy.internals.core import OtherPath
-
-VERSION = cellpy._version.__version__
-REPO = "jepegit/cellpy"
-USER = "jepegit"
-GITHUB_PWD_VAR_NAME = "GD_PWD"
-DEFAULT_EDITOR = "vim"
-EDITORS = {"Windows": "notepad"}
-
-
-def save_prm_file(prm_filename):
-    """saves (writes) the prms to file"""
-    prmreader._write_prm_file(prm_filename)
-
-
-def get_package_prm_dir():
-    """gets the folder where the cellpy package lives"""
-    prm_dir = pkg_resources.resource_filename("cellpy", "parameters")
-    return pathlib.Path(prm_dir)
-
-
-def get_default_config_file_path(init_filename=None):
-    """gets the path to the default config-file"""
-    prm_dir = get_package_prm_dir()
-    if not init_filename:
-        init_filename = prmreader.DEFAULT_FILENAME
-    src = prm_dir / init_filename
-    return src
-
-
-def get_dst_file(user_dir, init_filename):
-    user_dir = pathlib.Path(user_dir)
-    dst_file = user_dir / init_filename
-    return dst_file
-
-
-def check_if_needed_modules_exists():
-    pass
-
-
-def modify_config_file():
-    pass
-
-
-def create_cellpy_folders():
-    pass
-
-
-@click.group("cellpy")
-def cli():
-    pass
-
-
-# ----------------------- setup --------------------------------------
-@click.command()
-@click.option(
-    "--interactive",
-    "-i",
-    is_flag=True,
-    default=False,
-    help="Allows you to specify div. folders and setting.",
-)
-@click.option(
-    "--not-relative",
-    "-nr",
-    is_flag=True,
-    default=False,
-    help="If root-dir is given, put it directly in the root (/) folder"
-    " i.e. don't put it in your home directory. Defaults to False. Remark"
-    " that if you specifically write a path name instead of selecting the"
-    " suggested default, the path you write will be used as is.",
-)
-@click.option(
-    "--dry-run",
-    "-dr",
-    is_flag=True,
-    default=False,
-    help="Run setup in dry mode (only print - do not execute). This is"
-    " typically used when developing and testing cellpy. Defaults to"
-    " False.",
-)
-@click.option(
-    "--reset",
-    "-r",
-    is_flag=True,
-    default=False,
-    help="Do not suggest path defaults based on your current configuration-file",
-)
-@click.option(
-    "--root-dir",
-    "-d",
-    default=None,
-    type=click.Path(),
-    help="Use custom root dir. If not given, your home directory"
-    " will be used as the top level where cellpy-folders"
-    " will be put. The folder path must follow"
-    " directly after this option (if used). Example:\n"
-    " $ cellpy setup -d 'MyDir'",
-)
-@click.option(
-    "--folder-name",
-    "-n",
-    default=None,
-    type=click.Path(),
-    help="",
-)
-@click.option(
-    "--test_user", "-t", default=None, help="Fake name for fake user (for testing)"
-)
-def setup(interactive, not_relative, dry_run, reset, root_dir, folder_name, test_user):
-    """This will help you to set up cellpy."""
-
-    click.echo("[cellpy] (setup)")
-    click.echo(f"[cellpy] root-dir: {root_dir}")
-
-    # generate variables
-    init_filename = prmreader.create_custom_init_filename()
-    user_dir, dst_file = prmreader.get_user_dir_and_dst(init_filename)
-
-    if dry_run:
-        click.echo("Create custom init filename and get user_dir and destination")
-        click.echo(f"Got the following parameters:")
-        click.echo(f" - init_filename: {init_filename}")
-        click.echo(f" - user_dir: {user_dir}")
-        click.echo(f" - dst_file: {dst_file}")
-        click.echo(f" - not_relative: {not_relative}")
-
-    if root_dir and not interactive:
-        click.echo("[cellpy] custom root-dir can only be used in interactive mode")
-        click.echo("[cellpy] -> setting interactive mode")
-        interactive = True
-
-    if not root_dir:
-        root_dir = user_dir
-        # root_dir = pathlib.Path(os.getcwd())
-    root_dir = pathlib.Path(root_dir)
-
-    if dry_run:
-        click.echo(f" - root_dir: {root_dir}")
-
-    if test_user:
-        click.echo(f"[cellpy] (setup) DEV-MODE test_user: {test_user}")
-        init_filename = prmreader.create_custom_init_filename(test_user)
-        user_dir = root_dir
-        dst_file = get_dst_file(user_dir, init_filename)
-        click.echo(f"[cellpy] (setup) DEV-MODE user_dir: {user_dir}")
-        click.echo(f"[cellpy] (setup) DEV-MODE dst_file: {dst_file}")
-
-    if not pathlib.Path(dst_file).is_file():
-        click.echo(f"[cellpy] {dst_file} not found -> I will make one for you!")
-        reset = True
-
-    if interactive:
-        click.echo(" interactive mode ".center(80, "-"))
-        _update_paths(
-            custom_dir=root_dir,
-            relative_home=not not_relative,
-            default_dir=folder_name,
-            dry_run=dry_run,
-            reset=reset,
-        )
-        _write_config_file(user_dir, dst_file, init_filename, dry_run)
-        _check(dry_run=dry_run)
-
-    else:
-        if reset:
-            _update_paths(
-                user_dir,
-                False,
-                default_dir=folder_name,
-                dry_run=dry_run,
-                reset=True,
-                silent=True,
-            )
-        _write_config_file(user_dir, dst_file, init_filename, dry_run)
-        _check(dry_run=dry_run)
-
-
-def _update_paths(
-    custom_dir=None,
-    relative_home=True,
-    reset=False,
-    dry_run=False,
-    default_dir=None,
-    silent=False,
-):
-    # please, refactor me :-(
-
-    h = prmreader.get_user_dir()
-
-    if default_dir is None:
-        default_dir = "cellpy_data"
-
-    if dry_run:
-        click.echo(f" - default_dir: {default_dir}")
-        click.echo(f" - custom_dir: {custom_dir}")
-        click.echo(f" - retalive_home: {relative_home}")
-
-    if custom_dir:
-        reset = True
-        if relative_home:
-            h = h / custom_dir
-        if not custom_dir.parts[-1] == default_dir:
-            h = h / default_dir
-
-    if not reset:
-        outdatadir = pathlib.Path(prmreader.prms.Paths.outdatadir)
-        rawdatadir = OtherPath(prmreader.prms.Paths.rawdatadir)
-        cellpydatadir = OtherPath(prmreader.prms.Paths.cellpydatadir)
-        filelogdir = pathlib.Path(prmreader.prms.Paths.filelogdir)
-        examplesdir = pathlib.Path(prmreader.prms.Paths.examplesdir)
-        db_path = pathlib.Path(prmreader.prms.Paths.db_path)
-        db_filename = prmreader.prms.Paths.db_filename
-        notebookdir = pathlib.Path(prmreader.prms.Paths.notebookdir)
-        batchfiledir = pathlib.Path(prmreader.prms.Paths.batchfiledir)
-        templatedir = pathlib.Path(prmreader.prms.Paths.templatedir)
-        instrumentdir = pathlib.Path(prmreader.prms.Paths.instrumentsdir)
-    else:
-        outdatadir = "out"
-        rawdatadir = "raw"
-        cellpydatadir = "cellpyfiles"
-        filelogdir = "logs"
-        examplesdir = "examples"
-        db_path = "db"
-        db_filename = "cellpy_db.xlsx"
-        notebookdir = "notebooks"
-        batchfiledir = "batchfiles"
-        templatedir = "templates"
-        instrumentdir = "instruments"
-
-    outdatadir = h / outdatadir
-    rawdatadir = h / rawdatadir
-    cellpydatadir = h / cellpydatadir
-    filelogdir = h / filelogdir
-    examplesdir = h / examplesdir
-    db_path = h / db_path
-    notebookdir = h / notebookdir
-    batchfiledir = h / batchfiledir
-    templatedir = h / templatedir
-    instrumentdir = h / instrumentdir
-
-    if dry_run:
-        click.echo(f" - base (h): {h}")
-
-    if not silent:
-        outdatadir = _ask_about_path(
-            "where to output processed data and results", outdatadir
-        )
-        rawdatadir = _ask_about_otherpath("where your raw data are located", rawdatadir)
-        cellpydatadir = _ask_about_otherpath("where to put cellpy-files", cellpydatadir)
-        filelogdir = _ask_about_path("where to dump the log-files", filelogdir)
-        examplesdir = _ask_about_path(
-            "where to download cellpy examples and tests", examplesdir
-        )
-        db_path = _ask_about_path("what folder your db file lives in", db_path)
-        db_filename = _ask_about_name("the name of your db-file", db_filename)
-        notebookdir = _ask_about_path(
-            "where to put your jupyter notebooks", notebookdir
-        )
-        batchfiledir = _ask_about_path("where to put your batch files", batchfiledir)
-        templatedir = _ask_about_path("where to put your batch files", templatedir)
-        instrumentdir = _ask_about_path("where to put your batch files", instrumentdir)
-
-    # update folders based on suggestions
-    for d in [
-        outdatadir,
-        rawdatadir,
-        cellpydatadir,
-        filelogdir,
-        examplesdir,
-        notebookdir,
-        db_path,
-        batchfiledir,
-        templatedir,
-        instrumentdir,
-    ]:
-        if not dry_run:
-            _create_dir(d)
-        else:
-            click.echo(f"dry run (so I did not create {d})")
-
-    # update config-file based on suggestions
-    prmreader.prms.Paths.outdatadir = str(outdatadir)
-    prmreader.prms.Paths.rawdatadir = str(rawdatadir)
-    prmreader.prms.Paths.cellpydatadir = str(cellpydatadir)
-    prmreader.prms.Paths.filelogdir = str(filelogdir)
-    prmreader.prms.Paths.examplesdir = str(examplesdir)
-    prmreader.prms.Paths.db_path = str(db_path)
-    prmreader.prms.Paths.db_filename = str(db_filename)
-    prmreader.prms.Paths.notebookdir = str(notebookdir)
-    prmreader.prms.Paths.batchfiledir = str(batchfiledir)
-    prmreader.prms.Paths.templatedir = str(templatedir)
-    prmreader.prms.Paths.instrumentdir = str(instrumentdir)
-
-
-def _ask_about_path(q, p):
-    click.echo(f"\n[cellpy] (setup) input {q}")
-    click.echo(f"[cellpy] (setup) current: {p}")
-    new_path = input("[cellpy] (setup) [KEEP/new value] >>> ").strip()
-    if not new_path:
-        new_path = p
-    return pathlib.Path(new_path)
-
-
-def _ask_about_otherpath(q, p):
-    click.echo(f"\n[cellpy] (setup) input {q}")
-    click.echo(f"[cellpy] (setup) current: {p}")
-    new_path = input("[cellpy] (setup) [KEEP/new value] >>> ").strip()
-    if not new_path:
-        new_path = p
-    return OtherPath(new_path)
-
-
-def _ask_about_name(q, n):
-    click.echo(f"\n[cellpy] (setup) input {q}")
-    click.echo(f"[cellpy] (setup) current: {n}")
-    new_name = input("[cellpy] (setup) [KEEP/new value] >>> ").strip()
-    if not new_name:
-        new_name = n
-    return new_name
-
-
-def _create_dir(path, confirm=True, parents=True, exist_ok=True):
-    if isinstance(path, OtherPath):
-        if path.is_external:
-            return path
-    o = path.resolve()
-    if not o.is_dir():
-        o_parent = o.parent
-        create_dir = True
-        if confirm:
-            if not o_parent.is_dir():
-                create_dir = input(
-                    f"\n[cellpy] (setup) {o_parent} does not exist. Create it [y]/n ?"
-                )
-                if not create_dir:
-                    create_dir = True
-                elif create_dir in ["y", "Y"]:
-                    create_dir = True
-                else:
-                    create_dir = False
-
-        if create_dir:
-            try:
-                o.mkdir(parents=parents, exist_ok=exist_ok)
-                click.echo(f"[cellpy] (setup) Created {o}")
-            except FileExistsError:
-                click.echo(f"[cellpy] (setup) {o} already exists.")
-            except FileNotFoundError:
-                click.echo(f"[cellpy] (setup) {o} not available.")
-            except Exception as e:
-                click.echo(f"[cellpy] (setup) WARNING! Could not create {o}.")
-                logging.debug(e)
-                click.echo(f"[cellpy] (setup) ...continuing anyway.")
-        else:
-            click.echo(f"[cellpy] (setup) Could not create {o}")
-    return o
-
-
-def _check_import_cellpy():
-    try:
-        import cellpy
-        from cellpy import log
-        from cellpy.readers import cellreader
-
-        return True
-    except:
-        return False
-
-
-def _check_import_pyodbc():
-    import platform
-
-    from cellpy.parameters import prms
-
-    ODBC = prms._odbc
-    SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver
-
-    use_subprocess = prms.Instruments.Arbin.use_subprocess
-    detect_subprocess_need = prms.Instruments.Arbin.detect_subprocess_need
-    click.echo(f" reading prms")
-    click.echo(f" - ODBC: {ODBC}")
-    click.echo(f" - SEARCH_FOR_ODBC_DRIVERS: {SEARCH_FOR_ODBC_DRIVERS}")
-    click.echo(f" - use_subprocess: {use_subprocess}")
-    click.echo(f" - detect_subprocess_need: {detect_subprocess_need}")
-    click.echo(f" - stated office version: {prms.Instruments.Arbin.office_version}")
-
-    click.echo(" checking system")
-    is_posix = False
-    is_macos = False
-    if os.name == "posix":
-        is_posix = True
-        click.echo(f" - running on posix")
-    current_platform = platform.system()
-    if current_platform == "Darwin":
-        is_macos = True
-        click.echo(f" - running on a mac")
-
-    python_version, os_version = platform.architecture()
-    click.echo(f" - python version: {python_version}")
-    click.echo(f" - os version: {os_version}")
-
-    if not is_posix:
-        if not prms.Instruments.Arbin.sub_process_path:
-            sub_process_path = str(prms._sub_process_path)
-        else:
-            sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
-        click.echo(f" stated path to sub-process: {sub_process_path}")
-        if not os.path.isfile(sub_process_path):
-            click.echo(f" - OBS! missing")
-
-    if is_posix:
-        click.echo(" checking existence of mdb-export")
-        sub_process_path = "mdb-export"
-        from subprocess import PIPE, run
-
-        command = ["command", "-v", sub_process_path]
-
-        try:
-            result = run(command, stdout=PIPE, stderr=PIPE, universal_newlines=True)
-            if result.returncode == 0:
-                click.echo(f" - found it: {result.stdout}")
-            else:
-                click.echo(f" - failed finding it")
-
-            if is_macos:
-                driver = "/usr/local/lib/libmdbodbc.dylib"
-                click.echo(f" looks like you are on a mac (driver set to\n {driver})")
-                if not os.path.isfile(driver):
-                    click.echo(" - but cannot find it!")
-                    return False
-            return True
-
-        except AssertionError:
-            click.echo(" - not found")
-            return False
-
-    # not posix - checking for odbc drivers
-    # 1) checking if you have defined one
-    try:
-        driver = prms.Instruments.Arbin.odbc_driver
-        if not driver:
-            raise AttributeError
-        click.echo("You have defined an odbc driver in your conifg file")
-        click.echo(f"driver: {driver}")
-    except AttributeError:
-        click.echo("FYI: you have not defined any odbc_driver(s)")
-        click.echo(
-            "(The name of the driver from the configuration file is "
-            "used as a backup when cellpy cannot locate a driver by itself)"
-        )
-
-    use_ado = False
-
-    if ODBC == "ado":
-        use_ado = True
-        click.echo(" you stated that you prefer the ado loader")
-        click.echo(" checking if adodbapi is installed")
-        try:
-            import adodbapi as dbloader
-        except ImportError:
-            use_ado = False
-            click.echo(" Failed! Try setting pyodbc as your loader or install")
-            click.echo(" adodbapi (http://adodbapi.sourceforge.net/)")
-
-    if not use_ado:
-        if ODBC == "pyodbc":
-            click.echo(" you stated that you prefer the pyodbc loader")
-            try:
-                import pyodbc as dbloader
-            except ImportError:
-                click.echo(" Failed! Could not import it.")
-                click.echo(" Try 'pip install pyodbc'")
-                dbloader = None
-
-        elif ODBC == "pypyodbc":
-            click.echo(" you stated that you prefer the pypyodbc loader")
-            try:
-                import pypyodbc as dbloader
-            except ImportError:
-                click.echo(" Failed! Could not import it.")
-                click.echo(" try 'pip install pypyodbc'")
-                click.echo(" or set pyodbc as your loader in your prm file")
-                click.echo(" (and install it)")
-                dbloader = None
-
-    click.echo(" searching for odbc drivers")
-    try:
-        drivers = [
-            driver
-            for driver in dbloader.drivers()
-            if "Microsoft Access Driver" in driver
-        ]
-        click.echo(f"Found these: {drivers}")
-        driver = drivers[0]
-        click.echo(f"odbc driver: {driver}")
-        return True
-
-    except IndexError as e:
-        logging.debug("Unfortunately, it seems the list of drivers is emtpy.")
-        click.echo(
-            "\nCould not find any odbc-drivers suitable for .res-type files. "
-            "Check out the homepage of pydobc for info on installing drivers"
-        )
-        click.echo(
-            "One solution that might work is downloading "
-            "the Microsoft Access database engine "
-            "(in correct bytes (32 or 64)) "
-            "from:\n"
-            "https://www.microsoft.com/en-us/download/details.aspx?id=13255"
-        )
-        click.echo("Or install mdbtools and set it up (check the cellpy docs for help)")
-        click.echo("\n")
-        return False
-
-
-def _check_config_file():
-    prm_file_name = _configloc()
-    prm_dict = prmreader._read_prm_file_without_updating(prm_file_name)
-    try:
-        prm_paths = prm_dict["Paths"]
-        required_dirs = [
-            "cellpydatadir",
-            "examplesdir",
-            "filelogdir",
-            "notebookdir",
-            "outdatadir",
-            "rawdatadir",
-            "batchfiledir",
-            "templatedir",
-            "db_path",
-        ]
-        missing = 0
-        for k in required_dirs:
-            value = prm_paths.get(k, None)
-            click.echo(f"{k}: {value}")
-            # splitting this into two if-statements to make it easier to debug if OtherPath changes
-            if k in OTHERPATHS:
-                print(f"skipping check for external {k} (for now)")
-                # if not OtherPath(
-                #     value
-                # ).is_dir():  # Assuming OtherPath returns True if it is external.
-                #     missing += 1
-                #     click.echo("COULD NOT CONNECT!")
-                #     click.echo(f"({value} is not a directory)")
-            elif value and not pathlib.Path(value).is_dir():
-                missing += 1
-                click.echo("COULD NOT CONNECT!")
-                click.echo(f"({value} is not a directory)")
-            if not value:
-                missing += 1
-                click.echo("MISSING")
-
-        value = prm_paths.get("db_filename", None)
-        click.echo(f"db_filename: {value}")
-        if not value:
-            missing += 1
-            click.echo("MISSING")
-
-        if missing:
-            return False
-        else:
-            return True
-
-    except Exception as e:
-        click.echo("Following error occurred:")
-        click.echo(e)
-        return False
-
-
-def _check(dry_run=False):
-    click.echo(" checking ".center(80, "="))
-    if dry_run:
-        click.echo("*** dry-run: skipping the test")
-        return
-    failed_checks = 0
-    number_of_checks = 0
-
-    def sub_check(check_type, check_func):
-        failed = 0
-        click.echo(f"[cellpy] * - Checking {check_type}")
-        if check_func():
-            click.echo(f"[cellpy] -> succeeded!")
-        else:
-            click.echo("f[cellpy] -> failed!!!!")
-            failed = 1
-        click.echo(80 * "-")
-        return failed
-
-    check_types = ["cellpy imports", "importing pyodbc", "configuration (prm) file"]
-    check_funcs = [_check_import_cellpy, _check_import_pyodbc, _check_config_file]
-
-    for ct, cf in zip(check_types, check_funcs):
-        try:
-            failed_checks += sub_check(ct, cf)
-        except Exception as e:
-            click.echo(f"[cellpy] check raised an exception ({e})")
-        number_of_checks += 1
-    succeeded_checks = number_of_checks - failed_checks
-    if failed_checks > 0:
-        click.echo(f"[cellpy] OH NO!!! You (or I) failed!")
-        click.echo(f"[cellpy] Failed {failed_checks} out of {number_of_checks} checks.")
-    else:
-        click.echo(
-            f"[cellpy] Succeeded {succeeded_checks} out of {number_of_checks} checks."
-        )
-    click.echo(80 * "=")
-
-
-def _write_config_file(user_dir, dst_file, init_filename, dry_run):
-    click.echo(" update configuration ".center(80, "-"))
-    click.echo("[cellpy] (setup) Writing configurations to user directory:")
-    click.echo(f"\n         {user_dir}\n")
-
-    if os.path.isfile(dst_file):
-        click.echo("[cellpy] (setup) File already exists!")
-        click.echo("[cellpy] (setup) Keeping most of the old configuration parameters")
-    try:
-        if dry_run:
-            click.echo(
-                f"*** dry-run: skipping actual saving of {dst_file} ***", color="red"
-            )
-        else:
-            click.echo(f"[cellpy] (setup) Saving file ({dst_file})")
-            save_prm_file(dst_file)
-
-    except ConfigFileNotWritten:
-        click.echo("[cellpy] (setup) Something went wrong! Could not write the file")
-        click.echo(
-            "[cellpy] (setup) Trying to write a file"
-            + f"called {prmreader.DEFAULT_FILENAME} instead"
-        )
-
-        try:
-            user_dir, dst_file = prmreader.get_user_dir_and_dst(init_filename)
-            if dry_run:
-                click.echo(
-                    f"*** dry-run: skipping actual saving of {dst_file} ***",
-                    color="red",
-                )
-            else:
-                save_prm_file(dst_file)
-
-        except ConfigFileNotWritten:
-            _txt = "[cellpy] (setup) No, that did not work either.\n"
-            _txt += "[cellpy] (setup) Well, guess you have to talk to the developers."
-            click.echo(_txt)
-    else:
-        click.echo(f"[cellpy] (setup) Configuration file written!")
-        click.echo(
-            f"[cellpy] (setup) OK! Now you can edit it. For example by "
-            f"issuing \n\n         [your-favourite-editor] {init_filename}\n"
-        )
-
-
-def _get_default_editor():
-    """
-    Return the default text editor.
-
-    This code is based on the `editor` library by @rec.
-    """
-
-    return os.environ.get("VISUAL") or (
-        os.environ.get("EDITOR") or EDITORS.get(platform.system(), DEFAULT_EDITOR)
-    )
-
-
-# ----------------------- edit ---------------------------------------
-@click.command()
-@click.option(
-    "--default-editor",
-    "-e",
-    default=None,
-    type=str,
-    help="try to use this editor instead",
-)
-@click.option("--debug", "-d", is_flag=True, help="Run in debug mode.")
-@click.option("--silent", "-s", is_flag=True, help="Run in silent mode.")
-@click.argument(
-    "name",
-    type=str,
-    default=None,
-    required=False,
-)
-def edit(name, default_editor, debug, silent):
-    """Edit your cellpy config or database files.
-
-    You can use this to edit the configuration file, the database file, or the
-    environment file. If you do not specify which file to edit, the configuration
-    file will be opened.
-
-    Examples:
-
-        edit your cellpy configuration file
-
-            cellpy edit config
-
-        or just
-
-            cellpy edit
-
-        edit your cellpy database file
-
-            cellpy edit db
-
-        edit your cellpy environment file using notepad.exe (on Windows)
-
-            cellpy edit env -e notepad.exe
-
-    """
-
-    if name.lower() == "db":
-        _run_db(debug, silent)
-        return
-
-    elif name.lower() not in ["env", "config"] and name is not None:
-        click.echo("unknown file")
-        return
-
-    if name is None or name.lower() == "config":
-        config_file = _configloc()
-        filename = str(config_file.resolve())
-        if config_file is None:
-            print("could not find the config file")
-            return
-    elif name.lower() == "env":
-        filename = _envloc()
-        if filename is None:
-            print("could not find the env file")
-            return
-    else:
-        filename = name
-
-    if default_editor is None:
-        default_editor = _get_default_editor()
-
-    args = [default_editor, filename]
-    click.echo(f"[cellpy] (edit) Calling '{default_editor}'")
-    try:
-        subprocess.call(args)
-    except:
-        click.echo(f"[cellpy] (edit) Failed!")
-        click.echo(
-            "[cellpy] (edit) Try 'cellpy edit -e notepad.exe' if you are on Windows"
-        )
-
-
-# ----------------------- info ---------------------------------------
-@click.command()
-@click.option("--version", "-v", is_flag=True, help="Print version information.")
-@click.option(
-    "--configloc", "-l", is_flag=True, help="Print full path to the config file."
-)
-@click.option("--params", "-p", is_flag=True, help="Dump all parameters to screen.")
-@click.option(
-    "--check",
-    "-c",
-    is_flag=True,
-    help="Do a sanity check to see if things" " works as they should.",
-)
-def info(version, configloc, params, check):
-    """This will give you some valuable information about your cellpy."""
-    complete_info = True
-
-    if check:
-        complete_info = False
-        _check()
-
-    if version:
-        complete_info = False
-        _version()
-
-    if configloc:
-        complete_info = False
-        _configloc()
-
-    if params:
-        complete_info = False
-        _dump_params()
-
-    if complete_info:
-        _version()
-        _configloc()
-
-
-# ----------------------- run ----------------------------------------
-@click.command()
-@click.option(
-    "--journal",
-    "-j",
-    is_flag=True,
-    help="Run a batch job defined in the given journal-file",
-)
-@click.option("--key", "-k", is_flag=True, help="Run a batch job defined by batch-name")
-@click.option(
-    "--folder",
-    "-f",
-    is_flag=True,
-    help="Run all batch jobs iteratively in a given folder",
-)
-@click.option(
-    "--cellpy-project",
-    "-p",
-    is_flag=True,
-    help="Use PaperMill to run the notebook(s) within the given project folder "
-    "(will only work properly if the notebooks can be sorted in correct run-order by 'sorted'). "
-    "Warning! since we are using `click` - the NAME will be 'converted' when it is loaded "
-    "(same as print(name) does) - "
-    "so you can't use backslash ('\\') as normal in windows (use either '/' or '\\\\' instead).",
-)
-@click.option("--debug", "-d", is_flag=True, help="Run in debug mode.")
-@click.option("--silent", "-s", is_flag=True, help="Run in silent mode.")
-@click.option("--raw", is_flag=True, help="Force loading raw-file(s).")
-@click.option("--cellpyfile", is_flag=True, help="Force cellpy-file(s).")
-@click.option("--minimal", is_flag=True, help="Minimal processing.")
-@click.option(
-    "--nom-cap",
-    default=None,
-    type=float,
-    help="nominal capacity (used in calculating rates etc)",
-)
-@click.option(
-    "--batch_col",
-    default=None,
-    type=str,
-    help="batch column (if selecting running from db)",
-)
-@click.option(
-    "--project",
-    default=None,
-    type=str,
-    help="name of the project (if selecting running from db)",
-)
-@click.option("--list", "-l", "list_", is_flag=True, help="List batch-files.")
-@click.argument("name", default="NONE")
-def run(
-    journal,
-    key,
-    folder,
-    cellpy_project,
-    debug,
-    silent,
-    raw,
-    cellpyfile,
-    minimal,
-    nom_cap,
-    batch_col,
-    project,
-    list_,
-    name,
-):
-    """Run a cellpy process (e.g. a batch-job).
-
-    You can use this to launch specific applications.
-
-    Examples:
-
-        run a batch job described in a journal file
-
-           cellpy run -j my_experiment.json
-
-    """
-    if list_:
-        _run_list(name)
-        return
-
-    if name == "NONE":
-        click.echo(
-            "Usage: cellpy run [OPTIONS] NAME\n"
-            "Try 'cellpy run --help' for help.\n\n"
-            "Error: Missing argument 'NAME'."
-        )
-        sys.exit(-1)
-
-    if debug:
-        click.echo("[cellpy] (run) debug mode on")
-
-    if silent:
-        click.echo("[cellpy] (run) silent mode on")
-
-    click.echo("[cellpy]\n")
-
-    if cellpy_project:
-        _run_project(name)
-
-    elif journal:
-        _run_journal(name, debug, silent, raw, cellpyfile, minimal, nom_cap)
-
-    elif folder:
-        _run_journals(name, debug, silent, raw, cellpyfile, minimal)
-
-    elif key:
-        _run_from_db(
-            name,
-            debug,
-            silent,
-            raw,
-            cellpyfile,
-            minimal,
-            nom_cap,
-            batch_col,
-            project,
-        )
-
-    else:
-        _run(name, debug, silent)
-
-
-def _run_from_db(
-    name,
-    debug,
-    silent,
-    raw,
-    cellpyfile,
-    minimal,
-    nom_cap,
-    batch_col,
-    project,
-):
-    click.echo(
-        f"running from db \nkey={name}, batch_col={batch_col}, project={project}"
-    )
-
-    kwargs = dict()
-    kwargs["name"] = name
-
-    if debug:
-        kwargs["default_log_level"] = "DEBUG"
-    if not minimal:
-        kwargs["export_raw"] = False
-        kwargs["export_cycles"] = False
-        kwargs["export_ica"] = False
-
-    if batch_col is not None:
-        kwargs["batch_col"] = batch_col
-    if project is None:
-        kwargs["project"] = "various"
-    else:
-        kwargs["project"] = project
-
-    click.echo("Warming up ...")
-
-    from cellpy.utils import batch
-
-    click.echo("  - starting batch processing")
-    b = batch.process_batch(
-        force_raw_file=raw,
-        force_cellpy=cellpyfile,
-        nom_cap=nom_cap,
-        backend="matplotlib",
-        **kwargs,
-    )
-
-    if b is not None and not silent:
-        print(b)
-    click.echo("---")
-
-
-def _run_journal(file_name, debug, silent, raw, cellpyfile, minimal, nom_cap):
-    click.echo(f"running journal {file_name}")
-    # click.echo(f" --debug [{debug}]")
-    # click.echo(f" --silent [{silent}]")
-    # click.echo(f" --raw [{raw}]")
-    # click.echo(f" --cellpyfile [{cellpyfile}]")
-    # click.echo(f" --minimal [{minimal}]")
-    # click.echo(f" --nom_cap [{nom_cap}] {type(nom_cap)}")
-
-    kwargs = dict()
-    if debug:
-        kwargs["default_log_level"] = "DEBUG"
-    if not minimal:
-        kwargs["export_raw"] = False
-        kwargs["export_cycles"] = False
-        kwargs["export_ica"] = False
-
-    from cellpy import prms
-    from cellpy.utils import batch
-
-    batchfiledir = pathlib.Path(prms.Paths.batchfiledir)
-    file = pathlib.Path(file_name)
-    if not file.is_file():
-        click.echo(f"file_name={file_name} not found - looking into batchfiledir")
-        if not batchfiledir.is_dir():
-            click.echo("batchfiledir not found - aborting")
-            return
-        file = batchfiledir / file.name
-
-    if not file.is_file():
-        click.echo(f"{file} not found - aborting")
-        return
-
-    b = batch.process_batch(
-        file,
-        force_raw_file=raw,
-        force_cellpy=cellpyfile,
-        nom_cap=nom_cap,
-        backend="matplotlib",
-        **kwargs,
-    )
-    if b is not None and not silent:
-        print(b)
-    click.echo("---")
-
-
-def _run_list(batchfiledir):
-    from cellpy import prms
-
-    if batchfiledir == "NONE" or batchfiledir is None:
-        batchfiledir = pathlib.Path(prms.Paths.batchfiledir)
-    else:
-        batchfiledir = pathlib.Path(batchfiledir).resolve()
-
-    if batchfiledir.is_dir():
-        click.echo(f"Content of '{batchfiledir}':\n")
-        i = 0
-        for i, f in enumerate(batchfiledir.glob("cellpy*.json")):
-            click.echo(f"{f.name}")
-        if i:
-            print(f"\nnumber of batch-files located: {i}")
-        else:
-            print("No batch-files found in this directory.")
-    else:
-        click.echo(f"{batchfiledir} not found.")
-
-
-def _run_journals(folder_name, debug, silent, raw, cellpyfile, minimal):
-    click.echo(f"running journals in {folder_name}")
-    # click.echo(f" --debug [{debug}]")
-    # click.echo(f" --silent [{silent}]")
-    # click.echo(f" --raw [{raw}]")
-    # click.echo(f" --cellpyfile [{cellpyfile}]")
-    # click.echo(f" --minimal [{minimal}]")
-
-    kwargs = dict()
-    if debug:
-        kwargs["default_log_level"] = "DEBUG"
-    if not minimal:
-        kwargs["export_raw"] = False
-        kwargs["export_cycles"] = False
-        kwargs["export_ica"] = False
-
-    from cellpy.utils import batch
-
-    folder_name = pathlib.Path(folder_name).resolve()
-
-    if not folder_name.is_dir():
-        click.echo(f"{folder_name} not found - aborting")
-        return
-
-    batch.iterate_batches(
-        folder_name, force_raw_file=raw, force_cellpy=cellpyfile, silent=True, **kwargs
-    )
-    click.echo("---")
-
-
-def _run_project(our_new_project, **kwargs):
-    try:
-        import papermill as pm
-    except ImportError:
-        click.echo(
-            "[cellpy]: You need to install papermill for automatically execute the notebooks."
-        )
-        click.echo("[cellpy]: You can install it using pip like this:")
-        click.echo(" >> pip install papermill")
-        return
-    our_new_project = pathlib.Path(our_new_project)
-    click.echo(f"[cellpy]: trying to run notebooks in {our_new_project}")
-    notebooks = sorted(list(our_new_project.glob("*.ipynb")))
-    for notebook in notebooks:
-        click.echo(f"[cellpy - papermill] running {notebook.name}")
-        pm.execute_notebook(notebook, notebook, parameters=kwargs)
-
-
-def _run(name, debug, silent):
-    click.echo(f"running {name}")
-    click.echo(f" --debug [{debug}]")
-    click.echo(f" --silent [{silent}]")
-    click.echo("[cellpy]: sorry, I am not allowed to run this on my own")
-
-
-def _run_db(debug, silent):
-    import platform
-
-    from cellpy import prms
-
-    if not silent:
-        click.echo(f"running database editor")
-    if debug:
-        click.echo("running in debug-mode, but nothing to tell")
-
-    db_path = Path(prms.Paths.db_path) / prms.Paths.db_filename
-
-    if platform.system() == "Windows":
-        try:
-            os.system(f'start excel "{str(db_path)}"')
-        except Exception as e:
-            click.echo("Something went wrong trying to open")
-            click.echo(db_path)
-            print()
-            print(e)
-
-    elif platform.system() == "Linux":
-        click.echo("RUNNING LINUX")
-        # not tested
-        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
-
-    elif platform.system() == "Darwin":
-        click.echo(f" - running on a mac")
-        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
-
-    else:
-        print("RUNNING SOMETHING ELSE")
-        print(platform.system())
-        # not tested
-        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
-
-
-# ----------------------- pull ---------------------------------------
-@click.command()
-@click.option("--tests", "-t", is_flag=True, help="Download test-files from repo.")
-@click.option(
-    "--examples", "-e", is_flag=True, help="Download example-files from repo."
-)
-@click.option("--clone", "-c", is_flag=True, help="Clone the full repo.")
-@click.option("--directory", "-d", default=None, help="Save into custom directory DIR")
-@click.option("--password", "-p", default=None, help="Password option for the repo")
-def pull(tests, examples, clone, directory, password):
-    """Download examples or tests from the big internet (needs git)."""
-    if directory is not None:
-        click.echo(f"[cellpy] (pull) custom directory: {directory}")
-    else:
-        directory = pathlib.Path(prmreader.prms.Paths.examplesdir)
-
-    if password is not None:
-        click.echo("DEV MODE: password provided")
-    if clone:
-        _clone_repo(directory, password)
-    else:
-        if tests:
-            _pull_tests(directory, password)
-        if examples:
-            _pull_examples(directory, password)
-        else:
-            click.echo(
-                f"[cellpy] (pull) Nothing selected for pulling. "
-                f"Please select an option (--tests,--examples, -clone, ...) "
-            )
-
-
-def _clone_repo(directory, password):
-    directory = pathlib.Path(directory)
-    txt = "[cellpy] The plan is that this "
-    txt += "[cellpy] cmd will pull (clone) the cellpy repo.\n"
-    txt += "[cellpy] For now it only prints the link to the git-hub\n"
-    txt += "[cellpy] repository:\n"
-    txt += "[cellpy]\n"
-    txt += "[cellpy] https://github.com/jepegit/cellpy.git\n"
-    txt += "[cellpy]\n"
-    click.echo(txt)
-
-
-def _pull_tests(directory, pw=None):
-    txt = (
-        "[cellpy] (pull) Pulling tests from",
-        " https://github.com/jepegit/cellpy.git",
-    )
-    click.echo(txt)
-    _pull(gdirpath="tests", rootpath=directory, pw=pw)
-    _pull(gdirpath="testdata", rootpath=directory, pw=pw)
-
-
-def _pull_examples(directory, pw):
-    txt = (
-        "[cellpy] (pull) Pulling examples from",
-        " https://github.com/jepegit/cellpy.git",
-    )
-    click.echo(txt)
-    _pull(gdirpath="examples", rootpath=directory, pw=pw)
-
-
-def _version():
-    txt = "[cellpy] version: " + str(VERSION)
-    click.echo(txt)
-
-
-def _configloc():
-    _, config_file_name = prmreader.get_user_dir_and_dst()
-    click.echo("[cellpy] ->%s" % config_file_name)
-    if not os.path.isfile(config_file_name):
-        click.echo("[cellpy] File does not exist!")
-    else:
-        return config_file_name
-
-
-def _envloc():
-    click.echo(f"[cellpy] ->{prmreader.get_env_file_name()}")
-    if not os.path.isfile(prmreader.get_env_file_name()):
-        click.echo("[cellpy] File does not exist!")
-    else:
-        return prmreader.get_env_file_name()
-
-
-def _dump_params():
-    click.echo("[cellpy] Dumping parameters to screen:\n")
-    prmreader.info()
-
-
-def _download_g_blob(name, local_path):
-    import urllib.request
-
-    dirs = local_path.parent
-    if not dirs.is_dir():
-        click.echo(f"[cellpy] (pull) creating dir: {dirs}")
-        dirs.mkdir(parents=True)
-
-    filename, headers = urllib.request.urlretrieve(
-        name.download_url, filename=local_path
-    )
-    click.echo(f"[cellpy] (pull) downloaded blob: {filename}")
-
-
-def _parse_g_dir(repo, gdirpath):
-    """parses a repo directory two-levels deep"""
-    for f in repo.get_contents(gdirpath):
-        if f.type == "dir":
-            for sf in repo.get_contents(f.path):
-                yield sf
-        else:
-            yield f
-
-
-def _get_user_name():
-    return "jepegit"
-
-
-def _get_pw(method):
-    if method == "ask":
-        return getpass.getpass()
-    elif method == "env":
-        return os.environ.get(GITHUB_PWD_VAR_NAME, None)
-
-    else:
-        return None
-
-
-def _pull(gdirpath="examples", rootpath=None, u=None, pw=None):
-    if rootpath is None:
-        rootpath = prmreader.prms.Paths.examplesdir
-
-    rootpath = pathlib.Path(rootpath)
-
-    ndirpath = rootpath / gdirpath
-
-    if pw is not None:
-        click.echo(" DEV MODE ".center(80, "-"))
-        u = _get_user_name()
-        if pw == "ask":
-            click.echo("   - ask for password")
-            pw = _get_pw(pw)
-        elif pw == "env":
-            click.echo("   - check environ for password ")
-            pw = _get_pw(pw)
-            click.echo("   - got something")
-            if pw is None:
-                click.echo("   - only None")
-                u = None
-
-    g = Github(u, pw)
-    repo = g.get_repo(REPO)
-
-    click.echo(f"[cellpy] (pull) pulling {gdirpath}")
-    click.echo(f"[cellpy] (pull) -> {ndirpath}")
-
-    if not ndirpath.is_dir():
-        click.echo(f"[cellpy] (pull) creating dir: {ndirpath}")
-        ndirpath.mkdir(parents=True)
-
-    for gfile in _parse_g_dir(repo, gdirpath):
-        gfilename = pathlib.Path(gfile.path)
-        nfilename = rootpath / gfilename
-
-        _download_g_blob(gfile, nfilename)
-
-
-def _get_default_template():
-    template = "standard"
-    try:
-        template = prmreader.prms.Batch.template
-    except:
-        logging.debug("You dont have any default template defined in you .conf file")
-    return template
-
-
-def _read_local_templates(local_templates_path=None):
-    if local_templates_path is None:
-        local_templates_path = pathlib.Path(prmreader.prms.Paths.templatedir)
-    templates = {}
-    for p in list(local_templates_path.rglob("cellpy_cookie*.zip")):
-        label = p.stem.strip()[len("cellpy_cookie_") :]
-        templates[label] = (str(p), None)
-    logging.debug(f"Found the following templates: {templates}")
-    return templates
-
-
-# ----------------------- new ----------------------------------------
-@click.command()
-@click.option("--template", "-t", help="Provide template name.")
-@click.option("--directory", "-d", default=None, help="Create in custom directory.")
-@click.option(
-    "--project",
-    "-p",
-    default=None,
-    help="Provide project name (i.e. sub-directory name).",
-)
-@click.option(
-    "--experiment",
-    "-e",
-    default=None,
-    help="Provide experiment name (i.e. lookup-value).",
-)
-@click.option(
-    "--local-user-template",
-    "-u",
-    is_flag=True,
-    default=False,
-    help="Use local template from the templates directory.",
-)
-@click.option("--serve", "-s", "serve_", is_flag=True, help="Run Jupyter.")
-@click.option(
-    "--run",
-    "-r",
-    "run_",
-    is_flag=True,
-    help="Use PaperMill to run the notebook(s) from the template "
-    "(will only work properly if the notebooks can be sorted in correct run-order by 'sorted'.",
-)
-@click.option(
-    "--lab",
-    "-j",
-    is_flag=True,
-    help="Use Jupyter Lab instead of Notebook when serving.",
-)
-@click.option(
-    "--list", "-l", "list_", is_flag=True, help="List available templates and exit."
-)
-def new(
-    template,
-    directory,
-    project,
-    experiment,
-    local_user_template,
-    serve_,
-    run_,
-    lab,
-    list_,
-):
-    """Set up a batch experiment (might need git installed)."""
-    _new(
-        template,
-        directory=directory,
-        project_dir=project,
-        session_id=experiment,
-        local_user_template=local_user_template,
-        serve_=serve_,
-        run_=run_,
-        lab=lab,
-        list_=list_,
-    )
-
-
-def _new(
-    template: str,
-    directory: Union[Path, str, None] = None,
-    project_dir: Union[str, None] = None,
-    local_user_template: bool = False,
-    serve_: bool = False,
-    run_: bool = False,
-    lab: bool = False,
-    list_: bool = False,
-    session_id: str = "experiment_001",
-    no_input: bool = False,
-    cookie_directory: str = "",
-):
-    """Set up a batch experiment (might need git installed).
-
-    Args:
-        template: short-name of template.
-        directory: the directory for your cellpy projects.
-        local_user_template: use local template if True.
-        serve_: serve the notebook after creation if True.
-        run_: run the notebooks using papermill if True.
-        lab: use jupyter-lab instead of jupyter notebook if True.
-        list_: list all available templates and return if True.
-        project_dir: your project directory.
-        session_id: the lookup value.
-        no_input: accept defaults if True (only valid when providing project_dir and session_id)
-        cookie_directory: name of the directory for your cookie (inside the repository or zip file).
-    Returns:
-        None
-    """
-
-    from cellpy.parameters import prms
-
-    if list_:
-        click.echo(f"\n[cellpy] batch templates")
-
-        default_template = _get_default_template()
-        local_templates = _read_local_templates()
-        local_templates_path = prmreader.prms.Paths.templatedir
-        registered_templates = prms._registered_templates
-        click.echo(f"[cellpy] - default: {default_template}")
-        click.echo("[cellpy] - registered templates (on github):")
-        for label, link in registered_templates.items():
-            click.echo(f"\t\t{label:18s} {link}")
-
-        if local_templates:
-            click.echo(f"[cellpy] - local templates ({local_templates_path}):")
-            for label, link in local_templates.items():
-                click.echo(f"\t\t{label:18s} {link}")
-        else:
-            click.echo(f"[cellpy] - local templates ({local_templates_path}): none")
-
-        return
-
-    if project_dir is None or session_id is None:
-        no_input = False
-
-    if not template:
-        template = _get_default_template()
-
-    if lab:
-        server = "lab"
-    else:
-        server = "notebook"
-
-    try:
-        import cookiecutter.exceptions
-        import cookiecutter.main
-        import cookiecutter.prompt
-
-    except ModuleNotFoundError:
-        click.echo("Could not import cookiecutter.")
-        click.echo("Try installing it, for example by writing:")
-        click.echo("\npip install cookiecutter\n")
-
-    click.echo(f"Template: {template}")
-    if local_user_template:
-        # forcing using local template
-        templates = _read_local_templates()
-
-        if not templates:
-            click.echo(
-                "You asked me to use a local template, but you have none. Aborting."
-            )
-            return
-    else:
-        templates = prms._registered_templates
-        if local_templates := _read_local_templates():
-            templates.update(local_templates)
-
-    if not template.lower() in templates.keys():
-        click.echo("This template does not exist. Aborting.")
-        return
-
-    if directory is None:
-        logging.debug("no dir given")
-        directory = prms.Paths.notebookdir
-
-    if not os.path.isdir(directory):
-        click.echo("Sorry. This did not work as expected!")
-        click.echo(f" - {directory} does not exist")
-        return
-
-    directory = Path(directory)
-    selected_project_dir = None
-
-    if project_dir:
-        selected_project_dir = directory / project_dir
-        if not selected_project_dir.is_dir():
-            if cookiecutter.prompt.read_user_yes_no(
-                f"{project_dir} does not exist. Create?", "yes"
-            ):
-                os.mkdir(selected_project_dir)
-                click.echo(f"Created {selected_project_dir}")
-
-            else:
-                selected_project_dir = None
-                click.echo(f"Select another directory instead")
-
-    if not selected_project_dir:
-        project_dirs = [
-            d.name
-            for d in directory.iterdir()
-            if d.is_dir() and not d.name.startswith(".")
-        ]
-        project_dirs.insert(0, "[create new dir]")
-
-        project_dir = cookiecutter.prompt.read_user_choice(
-            "project folder", project_dirs
-        )
-
-        if project_dir == "[create new dir]":
-            default_name = "cellpy_project"
-            temp_default_name = default_name
-            for j in range(999):
-                if temp_default_name in project_dirs:
-                    temp_default_name = default_name + str(j + 1).zfill(3)
-                else:
-                    default_name = temp_default_name
-                    break
-
-            project_dir = cookiecutter.prompt.read_user_variable(
-                "New name", default_name
-            )
-            try:
-                os.mkdir(directory / project_dir)
-                click.echo(f"created {project_dir}")
-            except FileExistsError:
-                click.echo("OK - but this directory already exists!")
-        selected_project_dir = directory / project_dir
-
-    # get a list of all folders
-    existing_projects = os.listdir(selected_project_dir)
-
-    os.chdir(selected_project_dir)
-    cellpy_version = cellpy.__version__
-
-    try:
-        selected_template, cookie_dir = templates[template.lower()]
-
-        if cookie_directory:
-            cookie_dir = cookie_directory
-        if not cookie_dir:
-            cookie_dir = template.lower()
-
-        cookiecutter.main.cookiecutter(
-            selected_template,
-            extra_context={
-                "author_name": os.getlogin(),
-                "project_name": project_dir,
-                "cellpy_version": cellpy_version,
-                "session_id": session_id,
-            },
-            no_input=no_input,
-            directory=cookie_dir,
-        )
-    except cookiecutter.exceptions.OutputDirExistsException as e:
-        click.echo("Sorry. This did not work as expected!")
-        click.echo(" - cookiecutter refused to create the project")
-        click.echo(e)
-
-    if serve_:
-        os.chdir(directory)
-        _serve(server)
-
-    if run_:
-        try:
-            import papermill as pm
-        except ImportError:
-            click.echo(
-                "[cellpy]: You need to install papermill for automatically execute the notebooks."
-            )
-            click.echo("[cellpy]: You can install it using pip like this:")
-            click.echo(" >> pip install papermill")
-            return
-        new_existing_projects = os.listdir(selected_project_dir)
-        our_new_projects = list(set(new_existing_projects) - set(existing_projects))
-
-        if not len(our_new_projects):
-            click.echo(
-                "[cellpy]: Sorry, could not deiced what is the new project "
-                "- so I don't dare to try to execute automatically."
-            )
-            return
-        our_new_project = selected_project_dir / our_new_projects[0]
-
-        _run_project(our_new_project)
-
-
-def _serve(server):
-    click.echo(f"serving with jupyter {server}")
-    subprocess.run(["jupyter", server], check=True)
-    click.echo("Finished serving.")
-
-
-# ----------------------- serve ---------------------------------------
-@click.command()
-@click.option("--lab", "-l", is_flag=True, help="Use Jupyter Lab instead of Notebook")
-@click.option("--directory", "-d", default=None, help="Start in custom directory DIR")
-def serve(lab, directory):
-    """Start a Jupyter server."""
-
-    from cellpy.parameters import prms
-
-    if directory is None:
-        directory = prms.Paths.notebookdir
-    elif directory == "home":
-        directory = Path().home()
-    elif directory == "here":
-        directory = Path(os.getcwd())
-
-    if not os.path.isdir(directory):
-        click.echo("Sorry. This did not work as expected!")
-        click.echo(f" - {directory} does not exist")
-        return
-
-    if lab:
-        server = "lab"
-    else:
-        server = "notebook"
-
-    os.chdir(directory)
-    _serve(server)
-
-
-cli.add_command(setup)
-cli.add_command(info)
-cli.add_command(edit)
-cli.add_command(pull)
-cli.add_command(run)
-cli.add_command(new)
-cli.add_command(serve)
-
-
-# tests etc
-def _main_pull():
-    if sys.platform == "win32":
-        rootpath = pathlib.Path(r"C:\Temp\cellpy_user")
-    else:
-        rootpath = pathlib.Path("/Users/jepe/scripting/tmp/cellpy_test_user")
-    _pull_examples(rootpath, pw="env")
-    _pull_tests(rootpath, pw="env")
-    # _pull(gdirpath="examples", rootpath=rootpath, u="ask", pw="ask")
-    # _pull(gdirpath="tests", rootpath=rootpath, u="ask", pw="ask")
-    # _pull(gdirpath="testdata", rootpath=rootpath, u="ask", pw="ask")
-
-
-def _main():
-    file_name = prmreader.create_custom_init_filename()
-    click.echo(file_name)
-    user_directory, destination_file_name = prmreader.get_user_dir_and_dst(file_name)
-    click.echo(user_directory)
-    click.echo(destination_file_name)
-    click.echo("trying to save it")
-    save_prm_file(destination_file_name + "_dummy")
-
-    click.echo(" Testing setup ".center(80, "="))
-    setup(["--interactive", "--reset"])
-
-
-def _cli_setup_interactive():
-    from click.testing import CliRunner
-
-    if sys.platform == "win32":
-        root_dir = r"C:\Temp\cellpy_user"
-    else:
-        root_dir = "/Users/jepe/scripting/tmp/cellpy_test_user"
-    testuser = "tester"
-    init_filename = prmreader.create_custom_init_filename(testuser)
-    dst_file = get_dst_file(root_dir, init_filename)
-    init_file = pathlib.Path(dst_file)
-    opts = list()
-    opts.append("setup")
-    opts.append("-i")
-    # opts.append("-nr")
-    opts.append("-r")
-    opts.extend(["-d", root_dir])
-    opts.extend(["-t", testuser])
-
-    input_str = "\n"  # out
-    input_str += "\n"  # rawdatadir
-    input_str += "\n"  # cellpyfiles
-    input_str += "\n"  # log
-    input_str += "\n"  # examples
-    input_str += "\n"  # dbfolder
-    input_str += "\n"  # dbfile
-    runner = CliRunner()
-    result = runner.invoke(cli, opts, input=input_str)
-
-    click.echo(" out ".center(80, "."))
-    click.echo(result.output)
-    from pprint import pprint
-
-    pprint(prmreader.prms.Paths)
-    click.echo(" conf-file ".center(80, "."))
-    click.echo(init_file)
-    click.echo()
-    with init_file.open() as f:
-        for line in f.readlines():
-            click.echo(line.strip())
-
-
-def check_it(var=None):
-    import pathlib
-    import sys
-
-    p_env = pathlib.Path(sys.prefix)
-    print(p_env.name)
-    new(list_=True)
-
-
-if __name__ == "__main__":
-    u1 = os.getlogin()
-    u2 = os.path.expanduser("~")
-    u3 = os.environ.get("USERNAME")
-
-    print(u1)
-    print(u2)
-    print(u3)
-    # check_it()
-    # click.echo("\n\n", " RUNNING MAIN PULL ".center(80, "*"), "\n")
-    # _main_pull()
-    # click.echo("ok")
+import base64
+import getpass
+import logging
+import os
+import pathlib
+import platform
+from pprint import pprint
+import re
+import subprocess
+import sys
+from typing import Union
+import urllib
+from pathlib import Path
+
+import click
+import pkg_resources
+from github import Github
+
+import cellpy._version
+from cellpy.exceptions import ConfigFileNotWritten
+from cellpy.parameters import prmreader
+from cellpy.parameters.internal_settings import OTHERPATHS
+from cellpy.internals.core import OtherPath
+
+VERSION = cellpy._version.__version__
+REPO = "jepegit/cellpy"
+USER = "jepegit"
+GITHUB_PWD_VAR_NAME = "GD_PWD"
+DEFAULT_EDITOR = "vim"
+EDITORS = {"Windows": "notepad"}
+
+
+def save_prm_file(prm_filename):
+    """saves (writes) the prms to file"""
+    prmreader._write_prm_file(prm_filename)
+
+
+def get_package_prm_dir():
+    """gets the folder where the cellpy package lives"""
+    prm_dir = pkg_resources.resource_filename("cellpy", "parameters")
+    return pathlib.Path(prm_dir)
+
+
+def get_default_config_file_path(init_filename=None):
+    """gets the path to the default config-file"""
+    prm_dir = get_package_prm_dir()
+    if not init_filename:
+        init_filename = prmreader.DEFAULT_FILENAME
+    src = prm_dir / init_filename
+    return src
+
+
+def get_dst_file(user_dir, init_filename):
+    user_dir = pathlib.Path(user_dir)
+    dst_file = user_dir / init_filename
+    return dst_file
+
+
+def check_if_needed_modules_exists():
+    pass
+
+
+def modify_config_file():
+    pass
+
+
+def create_cellpy_folders():
+    pass
+
+
+@click.group("cellpy")
+def cli():
+    pass
+
+
+# ----------------------- setup --------------------------------------
+@click.command()
+@click.option(
+    "--interactive",
+    "-i",
+    is_flag=True,
+    default=False,
+    help="Allows you to specify div. folders and setting.",
+)
+@click.option(
+    "--not-relative",
+    "-nr",
+    is_flag=True,
+    default=False,
+    help="If root-dir is given, put it directly in the root (/) folder"
+    " i.e. don't put it in your home directory. Defaults to False. Remark"
+    " that if you specifically write a path name instead of selecting the"
+    " suggested default, the path you write will be used as is.",
+)
+@click.option(
+    "--dry-run",
+    "-dr",
+    is_flag=True,
+    default=False,
+    help="Run setup in dry mode (only print - do not execute). This is"
+    " typically used when developing and testing cellpy. Defaults to"
+    " False.",
+)
+@click.option(
+    "--reset",
+    "-r",
+    is_flag=True,
+    default=False,
+    help="Do not suggest path defaults based on your current configuration-file",
+)
+@click.option(
+    "--root-dir",
+    "-d",
+    default=None,
+    type=click.Path(),
+    help="Use custom root dir. If not given, your home directory"
+    " will be used as the top level where cellpy-folders"
+    " will be put. The folder path must follow"
+    " directly after this option (if used). Example:\n"
+    " $ cellpy setup -d 'MyDir'",
+)
+@click.option(
+    "--folder-name",
+    "-n",
+    default=None,
+    type=click.Path(),
+    help="",
+)
+@click.option(
+    "--test_user", "-t", default=None, help="Fake name for fake user (for testing)"
+)
+def setup(interactive, not_relative, dry_run, reset, root_dir, folder_name, test_user):
+    """This will help you to set up cellpy."""
+
+    click.echo("[cellpy] (setup)")
+    click.echo(f"[cellpy] root-dir: {root_dir}")
+
+    # generate variables
+    init_filename = prmreader.create_custom_init_filename()
+    user_dir, dst_file = prmreader.get_user_dir_and_dst(init_filename)
+
+    if dry_run:
+        click.echo("Create custom init filename and get user_dir and destination")
+        click.echo(f"Got the following parameters:")
+        click.echo(f" - init_filename: {init_filename}")
+        click.echo(f" - user_dir: {user_dir}")
+        click.echo(f" - dst_file: {dst_file}")
+        click.echo(f" - not_relative: {not_relative}")
+
+    if root_dir and not interactive:
+        click.echo("[cellpy] custom root-dir can only be used in interactive mode")
+        click.echo("[cellpy] -> setting interactive mode")
+        interactive = True
+
+    if not root_dir:
+        root_dir = user_dir
+        # root_dir = pathlib.Path(os.getcwd())
+    root_dir = pathlib.Path(root_dir)
+
+    if dry_run:
+        click.echo(f" - root_dir: {root_dir}")
+
+    if test_user:
+        click.echo(f"[cellpy] (setup) DEV-MODE test_user: {test_user}")
+        init_filename = prmreader.create_custom_init_filename(test_user)
+        user_dir = root_dir
+        dst_file = get_dst_file(user_dir, init_filename)
+        click.echo(f"[cellpy] (setup) DEV-MODE user_dir: {user_dir}")
+        click.echo(f"[cellpy] (setup) DEV-MODE dst_file: {dst_file}")
+
+    if not pathlib.Path(dst_file).is_file():
+        click.echo(f"[cellpy] {dst_file} not found -> I will make one for you!")
+        reset = True
+
+    if interactive:
+        click.echo(" interactive mode ".center(80, "-"))
+        _update_paths(
+            custom_dir=root_dir,
+            relative_home=not not_relative,
+            default_dir=folder_name,
+            dry_run=dry_run,
+            reset=reset,
+        )
+        _write_config_file(user_dir, dst_file, init_filename, dry_run)
+        _check(dry_run=dry_run)
+
+    else:
+        if reset:
+            _update_paths(
+                user_dir,
+                False,
+                default_dir=folder_name,
+                dry_run=dry_run,
+                reset=True,
+                silent=True,
+            )
+        _write_config_file(user_dir, dst_file, init_filename, dry_run)
+        _check(dry_run=dry_run)
+
+
+def _update_paths(
+    custom_dir=None,
+    relative_home=True,
+    reset=False,
+    dry_run=False,
+    default_dir=None,
+    silent=False,
+):
+    # please, refactor me :-(
+
+    h = prmreader.get_user_dir()
+
+    if default_dir is None:
+        default_dir = "cellpy_data"
+
+    if dry_run:
+        click.echo(f" - default_dir: {default_dir}")
+        click.echo(f" - custom_dir: {custom_dir}")
+        click.echo(f" - retalive_home: {relative_home}")
+
+    if custom_dir:
+        reset = True
+        if relative_home:
+            h = h / custom_dir
+        if not custom_dir.parts[-1] == default_dir:
+            h = h / default_dir
+
+    if not reset:
+        outdatadir = pathlib.Path(prmreader.prms.Paths.outdatadir)
+        rawdatadir = OtherPath(prmreader.prms.Paths.rawdatadir)
+        cellpydatadir = OtherPath(prmreader.prms.Paths.cellpydatadir)
+        filelogdir = pathlib.Path(prmreader.prms.Paths.filelogdir)
+        examplesdir = pathlib.Path(prmreader.prms.Paths.examplesdir)
+        db_path = pathlib.Path(prmreader.prms.Paths.db_path)
+        db_filename = prmreader.prms.Paths.db_filename
+        notebookdir = pathlib.Path(prmreader.prms.Paths.notebookdir)
+        batchfiledir = pathlib.Path(prmreader.prms.Paths.batchfiledir)
+        templatedir = pathlib.Path(prmreader.prms.Paths.templatedir)
+        instrumentdir = pathlib.Path(prmreader.prms.Paths.instrumentsdir)
+    else:
+        outdatadir = "out"
+        rawdatadir = "raw"
+        cellpydatadir = "cellpyfiles"
+        filelogdir = "logs"
+        examplesdir = "examples"
+        db_path = "db"
+        db_filename = "cellpy_db.xlsx"
+        notebookdir = "notebooks"
+        batchfiledir = "batchfiles"
+        templatedir = "templates"
+        instrumentdir = "instruments"
+
+    outdatadir = h / outdatadir
+    rawdatadir = h / rawdatadir
+    cellpydatadir = h / cellpydatadir
+    filelogdir = h / filelogdir
+    examplesdir = h / examplesdir
+    db_path = h / db_path
+    notebookdir = h / notebookdir
+    batchfiledir = h / batchfiledir
+    templatedir = h / templatedir
+    instrumentdir = h / instrumentdir
+
+    if dry_run:
+        click.echo(f" - base (h): {h}")
+
+    if not silent:
+        outdatadir = _ask_about_path(
+            "where to output processed data and results", outdatadir
+        )
+        rawdatadir = _ask_about_otherpath("where your raw data are located", rawdatadir)
+        cellpydatadir = _ask_about_otherpath("where to put cellpy-files", cellpydatadir)
+        filelogdir = _ask_about_path("where to dump the log-files", filelogdir)
+        examplesdir = _ask_about_path(
+            "where to download cellpy examples and tests", examplesdir
+        )
+        db_path = _ask_about_path("what folder your db file lives in", db_path)
+        db_filename = _ask_about_name("the name of your db-file", db_filename)
+        notebookdir = _ask_about_path(
+            "where to put your jupyter notebooks", notebookdir
+        )
+        batchfiledir = _ask_about_path("where to put your batch files", batchfiledir)
+        templatedir = _ask_about_path("where to put your batch files", templatedir)
+        instrumentdir = _ask_about_path("where to put your batch files", instrumentdir)
+
+    # update folders based on suggestions
+    for d in [
+        outdatadir,
+        rawdatadir,
+        cellpydatadir,
+        filelogdir,
+        examplesdir,
+        notebookdir,
+        db_path,
+        batchfiledir,
+        templatedir,
+        instrumentdir,
+    ]:
+        if not dry_run:
+            _create_dir(d)
+        else:
+            click.echo(f"dry run (so I did not create {d})")
+
+    # update config-file based on suggestions
+    prmreader.prms.Paths.outdatadir = str(outdatadir)
+    prmreader.prms.Paths.rawdatadir = str(rawdatadir)
+    prmreader.prms.Paths.cellpydatadir = str(cellpydatadir)
+    prmreader.prms.Paths.filelogdir = str(filelogdir)
+    prmreader.prms.Paths.examplesdir = str(examplesdir)
+    prmreader.prms.Paths.db_path = str(db_path)
+    prmreader.prms.Paths.db_filename = str(db_filename)
+    prmreader.prms.Paths.notebookdir = str(notebookdir)
+    prmreader.prms.Paths.batchfiledir = str(batchfiledir)
+    prmreader.prms.Paths.templatedir = str(templatedir)
+    prmreader.prms.Paths.instrumentdir = str(instrumentdir)
+
+
+def _ask_about_path(q, p):
+    click.echo(f"\n[cellpy] (setup) input {q}")
+    click.echo(f"[cellpy] (setup) current: {p}")
+    new_path = input("[cellpy] (setup) new value (press enter to keep) >>> ").strip()
+    if not new_path:
+        new_path = p
+    return pathlib.Path(new_path)
+
+
+def _ask_about_otherpath(q, p):
+    click.echo(f"\n[cellpy] (setup) input {q}")
+    click.echo(f"[cellpy] (setup) current: {p}")
+    new_path = input("[cellpy] (setup) new value (press enter to keep) >>> ").strip()
+    if not new_path:
+        new_path = p
+    return OtherPath(new_path)
+
+
+def _ask_about_name(q, n):
+    click.echo(f"\n[cellpy] (setup) input {q}")
+    click.echo(f"[cellpy] (setup) current: {n}")
+    new_name = input("[cellpy] (setup) new value (press enter to keep) >>> ").strip()
+    if not new_name:
+        new_name = n
+    return new_name
+
+
+def _create_dir(path, confirm=True, parents=True, exist_ok=True):
+    if isinstance(path, OtherPath):
+        if path.is_external:
+            return path
+    o = path.resolve()
+    if not o.is_dir():
+        o_parent = o.parent
+        create_dir = True
+        if confirm:
+            if not o_parent.is_dir():
+                create_dir = input(
+                    f"\n[cellpy] (setup) {o_parent} does not exist. Create it [y]/n ?"
+                )
+                if not create_dir:
+                    create_dir = True
+                elif create_dir in ["y", "Y"]:
+                    create_dir = True
+                else:
+                    create_dir = False
+
+        if create_dir:
+            try:
+                o.mkdir(parents=parents, exist_ok=exist_ok)
+                click.echo(f"[cellpy] (setup) Created {o}")
+            except FileExistsError:
+                click.echo(f"[cellpy] (setup) {o} already exists.")
+            except FileNotFoundError:
+                click.echo(f"[cellpy] (setup) {o} not available.")
+            except Exception as e:
+                click.echo(f"[cellpy] (setup) WARNING! Could not create {o}.")
+                logging.debug(e)
+                click.echo(f"[cellpy] (setup) ...continuing anyway.")
+        else:
+            click.echo(f"[cellpy] (setup) Could not create {o}")
+    return o
+
+
+def _check_import_cellpy():
+    try:
+        import cellpy
+        from cellpy import log
+        from cellpy.readers import cellreader
+
+        return True
+    except:
+        return False
+
+
+def _check_import_pyodbc():
+    import platform
+
+    from cellpy.parameters import prms
+
+    ODBC = prms._odbc
+    SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver
+
+    use_subprocess = prms.Instruments.Arbin.use_subprocess
+    detect_subprocess_need = prms.Instruments.Arbin.detect_subprocess_need
+    click.echo(f" reading prms")
+    click.echo(f" - ODBC: {ODBC}")
+    click.echo(f" - SEARCH_FOR_ODBC_DRIVERS: {SEARCH_FOR_ODBC_DRIVERS}")
+    click.echo(f" - use_subprocess: {use_subprocess}")
+    click.echo(f" - detect_subprocess_need: {detect_subprocess_need}")
+    click.echo(f" - stated office version: {prms.Instruments.Arbin.office_version}")
+
+    click.echo(" checking system")
+    is_posix = False
+    is_macos = False
+    if os.name == "posix":
+        is_posix = True
+        click.echo(f" - running on posix")
+    current_platform = platform.system()
+    if current_platform == "Darwin":
+        is_macos = True
+        click.echo(f" - running on a mac")
+
+    python_version, os_version = platform.architecture()
+    click.echo(f" - python version: {python_version}")
+    click.echo(f" - os version: {os_version}")
+
+    if not is_posix:
+        if not prms.Instruments.Arbin.sub_process_path:
+            sub_process_path = str(prms._sub_process_path)
+        else:
+            sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
+        click.echo(f" stated path to sub-process: {sub_process_path}")
+        if not os.path.isfile(sub_process_path):
+            click.echo(f" - OBS! missing")
+
+    if is_posix:
+        click.echo(" checking existence of mdb-export")
+        sub_process_path = "mdb-export"
+        from subprocess import PIPE, run
+
+        command = ["command", "-v", sub_process_path]
+
+        try:
+            result = run(command, stdout=PIPE, stderr=PIPE, universal_newlines=True)
+            if result.returncode == 0:
+                click.echo(f" - found it: {result.stdout}")
+            else:
+                click.echo(f" - failed finding it")
+
+            if is_macos:
+                driver = "/usr/local/lib/libmdbodbc.dylib"
+                click.echo(f" looks like you are on a mac (driver set to\n {driver})")
+                if not os.path.isfile(driver):
+                    click.echo(" - but cannot find it!")
+                    return False
+            return True
+
+        except AssertionError:
+            click.echo(" - not found")
+            return False
+
+    # not posix - checking for odbc drivers
+    # 1) checking if you have defined one
+    try:
+        driver = prms.Instruments.Arbin.odbc_driver
+        if not driver:
+            raise AttributeError
+        click.echo("You have defined an odbc driver in your conifg file")
+        click.echo(f"driver: {driver}")
+    except AttributeError:
+        click.echo("FYI: you have not defined any odbc_driver(s)")
+        click.echo(
+            "(The name of the driver from the configuration file is "
+            "used as a backup when cellpy cannot locate a driver by itself)"
+        )
+
+    use_ado = False
+
+    if ODBC == "ado":
+        use_ado = True
+        click.echo(" you stated that you prefer the ado loader")
+        click.echo(" checking if adodbapi is installed")
+        try:
+            import adodbapi as dbloader
+        except ImportError:
+            use_ado = False
+            click.echo(" Failed! Try setting pyodbc as your loader or install")
+            click.echo(" adodbapi (http://adodbapi.sourceforge.net/)")
+
+    if not use_ado:
+        if ODBC == "pyodbc":
+            click.echo(" you stated that you prefer the pyodbc loader")
+            try:
+                import pyodbc as dbloader
+            except ImportError:
+                click.echo(" Failed! Could not import it.")
+                click.echo(" Try 'pip install pyodbc'")
+                dbloader = None
+
+        elif ODBC == "pypyodbc":
+            click.echo(" you stated that you prefer the pypyodbc loader")
+            try:
+                import pypyodbc as dbloader
+            except ImportError:
+                click.echo(" Failed! Could not import it.")
+                click.echo(" try 'pip install pypyodbc'")
+                click.echo(" or set pyodbc as your loader in your prm file")
+                click.echo(" (and install it)")
+                dbloader = None
+
+    click.echo(" searching for odbc drivers")
+    try:
+        drivers = [
+            driver
+            for driver in dbloader.drivers()
+            if "Microsoft Access Driver" in driver
+        ]
+        click.echo(f"Found these: {drivers}")
+        driver = drivers[0]
+        click.echo(f"odbc driver: {driver}")
+        return True
+
+    except IndexError as e:
+        logging.debug("Unfortunately, it seems the list of drivers is emtpy.")
+        click.echo(
+            "\nCould not find any odbc-drivers suitable for .res-type files. "
+            "Check out the homepage of pydobc for info on installing drivers"
+        )
+        click.echo(
+            "One solution that might work is downloading "
+            "the Microsoft Access database engine "
+            "(in correct bytes (32 or 64)) "
+            "from:\n"
+            "https://www.microsoft.com/en-us/download/details.aspx?id=13255"
+        )
+        click.echo("Or install mdbtools and set it up (check the cellpy docs for help)")
+        click.echo("\n")
+        return False
+
+
+def _check_config_file():
+    prm_file_name = _configloc()
+    prm_dict = prmreader._read_prm_file_without_updating(prm_file_name)
+    try:
+        prm_paths = prm_dict["Paths"]
+        required_dirs = [
+            "cellpydatadir",
+            "examplesdir",
+            "filelogdir",
+            "notebookdir",
+            "outdatadir",
+            "rawdatadir",
+            "batchfiledir",
+            "templatedir",
+            "db_path",
+        ]
+        missing = 0
+        for k in required_dirs:
+            value = prm_paths.get(k, None)
+            click.echo(f"{k}: {value}")
+            # splitting this into two if-statements to make it easier to debug if OtherPath changes
+            if k in OTHERPATHS:
+                print(f"skipping check for external {k} (for now)")
+                # if not OtherPath(
+                #     value
+                # ).is_dir():  # Assuming OtherPath returns True if it is external.
+                #     missing += 1
+                #     click.echo("COULD NOT CONNECT!")
+                #     click.echo(f"({value} is not a directory)")
+            elif value and not pathlib.Path(value).is_dir():
+                missing += 1
+                click.echo("COULD NOT CONNECT!")
+                click.echo(f"({value} is not a directory)")
+            if not value:
+                missing += 1
+                click.echo("MISSING")
+
+        value = prm_paths.get("db_filename", None)
+        click.echo(f"db_filename: {value}")
+        if not value:
+            missing += 1
+            click.echo("MISSING")
+
+        if missing:
+            return False
+        else:
+            return True
+
+    except Exception as e:
+        click.echo("Following error occurred:")
+        click.echo(e)
+        return False
+
+
+def _check(dry_run=False):
+    click.echo(" checking ".center(80, "="))
+    if dry_run:
+        click.echo("*** dry-run: skipping the test")
+        return
+    failed_checks = 0
+    number_of_checks = 0
+
+    def sub_check(check_type, check_func):
+        failed = 0
+        click.echo(f"[cellpy] * - Checking {check_type}")
+        if check_func():
+            click.echo(f"[cellpy] -> succeeded!")
+        else:
+            click.echo("f[cellpy] -> failed!!!!")
+            failed = 1
+        click.echo(80 * "-")
+        return failed
+
+    check_types = ["cellpy imports", "importing pyodbc", "configuration (prm) file"]
+    check_funcs = [_check_import_cellpy, _check_import_pyodbc, _check_config_file]
+
+    for ct, cf in zip(check_types, check_funcs):
+        try:
+            failed_checks += sub_check(ct, cf)
+        except Exception as e:
+            click.echo(f"[cellpy] check raised an exception ({e})")
+        number_of_checks += 1
+    succeeded_checks = number_of_checks - failed_checks
+    if failed_checks > 0:
+        click.echo(f"[cellpy] OH NO!!! You (or I) failed!")
+        click.echo(f"[cellpy] Failed {failed_checks} out of {number_of_checks} checks.")
+    else:
+        click.echo(
+            f"[cellpy] Succeeded {succeeded_checks} out of {number_of_checks} checks."
+        )
+    click.echo(80 * "=")
+
+
+def _write_config_file(user_dir, dst_file, init_filename, dry_run):
+    click.echo(" update configuration ".center(80, "-"))
+    click.echo("[cellpy] (setup) Writing configurations to user directory:")
+    click.echo(f"\n         {user_dir}\n")
+
+    if os.path.isfile(dst_file):
+        click.echo("[cellpy] (setup) File already exists!")
+        click.echo("[cellpy] (setup) Keeping most of the old configuration parameters")
+    try:
+        if dry_run:
+            click.echo(
+                f"*** dry-run: skipping actual saving of {dst_file} ***", color="red"
+            )
+        else:
+            click.echo(f"[cellpy] (setup) Saving file ({dst_file})")
+            save_prm_file(dst_file)
+
+    except ConfigFileNotWritten:
+        click.echo("[cellpy] (setup) Something went wrong! Could not write the file")
+        click.echo(
+            "[cellpy] (setup) Trying to write a file"
+            + f"called {prmreader.DEFAULT_FILENAME} instead"
+        )
+
+        try:
+            user_dir, dst_file = prmreader.get_user_dir_and_dst(init_filename)
+            if dry_run:
+                click.echo(
+                    f"*** dry-run: skipping actual saving of {dst_file} ***",
+                    color="red",
+                )
+            else:
+                save_prm_file(dst_file)
+
+        except ConfigFileNotWritten:
+            _txt = "[cellpy] (setup) No, that did not work either.\n"
+            _txt += "[cellpy] (setup) Well, guess you have to talk to the developers."
+            click.echo(_txt)
+    else:
+        click.echo(f"[cellpy] (setup) Configuration file written!")
+        click.echo(
+            f"[cellpy] (setup) OK! Now you can edit it. For example by "
+            f"issuing \n\n         [your-favourite-editor] {init_filename}\n"
+        )
+
+
+def _get_default_editor():
+    """
+    Return the default text editor.
+
+    This code is based on the `editor` library by @rec.
+    """
+
+    return os.environ.get("VISUAL") or (
+        os.environ.get("EDITOR") or EDITORS.get(platform.system(), DEFAULT_EDITOR)
+    )
+
+
+# ----------------------- edit ---------------------------------------
+@click.command()
+@click.option(
+    "--default-editor",
+    "-e",
+    default=None,
+    type=str,
+    help="try to use this editor instead",
+)
+@click.option("--debug", "-d", is_flag=True, help="Run in debug mode.")
+@click.option("--silent", "-s", is_flag=True, help="Run in silent mode.")
+@click.argument(
+    "name",
+    type=str,
+    default=None,
+    required=False,
+)
+def edit(name, default_editor, debug, silent):
+    """Edit your cellpy config or database files.
+
+    You can use this to edit the configuration file, the database file, or the
+    environment file. If you do not specify which file to edit, the configuration
+    file will be opened.
+
+    Examples:
+
+        edit your cellpy configuration file
+
+            cellpy edit config
+
+        or just
+
+            cellpy edit
+
+        edit your cellpy database file
+
+            cellpy edit db
+
+        edit your cellpy environment file using notepad.exe (on Windows)
+
+            cellpy edit env -e notepad.exe
+
+    """
+
+    if name.lower() == "db":
+        _run_db(debug, silent)
+        return
+
+    elif name.lower() not in ["env", "config"] and name is not None:
+        click.echo("unknown file")
+        return
+
+    if name is None or name.lower() == "config":
+        config_file = _configloc()
+        filename = str(config_file.resolve())
+        if config_file is None:
+            print("could not find the config file")
+            return
+    elif name.lower() == "env":
+        filename = _envloc()
+        if filename is None:
+            print("could not find the env file")
+            return
+    else:
+        filename = name
+
+    if default_editor is None:
+        default_editor = _get_default_editor()
+
+    args = [default_editor, filename]
+    click.echo(f"[cellpy] (edit) Calling '{default_editor}'")
+    try:
+        subprocess.call(args)
+    except:
+        click.echo(f"[cellpy] (edit) Failed!")
+        click.echo(
+            "[cellpy] (edit) Try 'cellpy edit -e notepad.exe' if you are on Windows"
+        )
+
+
+# ----------------------- info ---------------------------------------
+@click.command()
+@click.option("--version", "-v", is_flag=True, help="Print version information.")
+@click.option(
+    "--configloc", "-l", is_flag=True, help="Print full path to the config file."
+)
+@click.option("--params", "-p", is_flag=True, help="Dump all parameters to screen.")
+@click.option(
+    "--check",
+    "-c",
+    is_flag=True,
+    help="Do a sanity check to see if things" " works as they should.",
+)
+def info(version, configloc, params, check):
+    """This will give you some valuable information about your cellpy."""
+    complete_info = True
+
+    if check:
+        complete_info = False
+        _check()
+
+    if version:
+        complete_info = False
+        _version()
+
+    if configloc:
+        complete_info = False
+        _configloc()
+
+    if params:
+        complete_info = False
+        _dump_params()
+
+    if complete_info:
+        _version()
+        _configloc()
+
+
+# ----------------------- run ----------------------------------------
+@click.command()
+@click.option(
+    "--journal",
+    "-j",
+    is_flag=True,
+    help="Run a batch job defined in the given journal-file",
+)
+@click.option("--key", "-k", is_flag=True, help="Run a batch job defined by batch-name")
+@click.option(
+    "--folder",
+    "-f",
+    is_flag=True,
+    help="Run all batch jobs iteratively in a given folder",
+)
+@click.option(
+    "--cellpy-project",
+    "-p",
+    is_flag=True,
+    help="Use PaperMill to run the notebook(s) within the given project folder "
+    "(will only work properly if the notebooks can be sorted in correct run-order by 'sorted'). "
+    "Warning! since we are using `click` - the NAME will be 'converted' when it is loaded "
+    "(same as print(name) does) - "
+    "so you can't use backslash ('\\') as normal in windows (use either '/' or '\\\\' instead).",
+)
+@click.option("--debug", "-d", is_flag=True, help="Run in debug mode.")
+@click.option("--silent", "-s", is_flag=True, help="Run in silent mode.")
+@click.option("--raw", is_flag=True, help="Force loading raw-file(s).")
+@click.option("--cellpyfile", is_flag=True, help="Force cellpy-file(s).")
+@click.option("--minimal", is_flag=True, help="Minimal processing.")
+@click.option(
+    "--nom-cap",
+    default=None,
+    type=float,
+    help="nominal capacity (used in calculating rates etc)",
+)
+@click.option(
+    "--batch_col",
+    default=None,
+    type=str,
+    help="batch column (if selecting running from db)",
+)
+@click.option(
+    "--project",
+    default=None,
+    type=str,
+    help="name of the project (if selecting running from db)",
+)
+@click.option("--list", "-l", "list_", is_flag=True, help="List batch-files.")
+@click.argument("name", default="NONE")
+def run(
+    journal,
+    key,
+    folder,
+    cellpy_project,
+    debug,
+    silent,
+    raw,
+    cellpyfile,
+    minimal,
+    nom_cap,
+    batch_col,
+    project,
+    list_,
+    name,
+):
+    """Run a cellpy process (e.g. a batch-job).
+
+    You can use this to launch specific applications.
+
+    Examples:
+
+        run a batch job described in a journal file
+
+           cellpy run -j my_experiment.json
+
+    """
+    if list_:
+        _run_list(name)
+        return
+
+    if name == "NONE":
+        click.echo(
+            "Usage: cellpy run [OPTIONS] NAME\n"
+            "Try 'cellpy run --help' for help.\n\n"
+            "Error: Missing argument 'NAME'."
+        )
+        sys.exit(-1)
+
+    if debug:
+        click.echo("[cellpy] (run) debug mode on")
+
+    if silent:
+        click.echo("[cellpy] (run) silent mode on")
+
+    click.echo("[cellpy]\n")
+
+    if cellpy_project:
+        _run_project(name)
+
+    elif journal:
+        _run_journal(name, debug, silent, raw, cellpyfile, minimal, nom_cap)
+
+    elif folder:
+        _run_journals(name, debug, silent, raw, cellpyfile, minimal)
+
+    elif key:
+        _run_from_db(
+            name,
+            debug,
+            silent,
+            raw,
+            cellpyfile,
+            minimal,
+            nom_cap,
+            batch_col,
+            project,
+        )
+
+    else:
+        _run(name, debug, silent)
+
+
+def _run_from_db(
+    name,
+    debug,
+    silent,
+    raw,
+    cellpyfile,
+    minimal,
+    nom_cap,
+    batch_col,
+    project,
+):
+    click.echo(
+        f"running from db \nkey={name}, batch_col={batch_col}, project={project}"
+    )
+
+    kwargs = dict()
+    kwargs["name"] = name
+
+    if debug:
+        kwargs["default_log_level"] = "DEBUG"
+    if not minimal:
+        kwargs["export_raw"] = False
+        kwargs["export_cycles"] = False
+        kwargs["export_ica"] = False
+
+    if batch_col is not None:
+        kwargs["batch_col"] = batch_col
+    if project is None:
+        kwargs["project"] = "various"
+    else:
+        kwargs["project"] = project
+
+    click.echo("Warming up ...")
+
+    from cellpy.utils import batch
+
+    click.echo("  - starting batch processing")
+    b = batch.process_batch(
+        force_raw_file=raw,
+        force_cellpy=cellpyfile,
+        nom_cap=nom_cap,
+        backend="matplotlib",
+        **kwargs,
+    )
+
+    if b is not None and not silent:
+        print(b)
+    click.echo("---")
+
+
+def _run_journal(file_name, debug, silent, raw, cellpyfile, minimal, nom_cap):
+    click.echo(f"running journal {file_name}")
+    # click.echo(f" --debug [{debug}]")
+    # click.echo(f" --silent [{silent}]")
+    # click.echo(f" --raw [{raw}]")
+    # click.echo(f" --cellpyfile [{cellpyfile}]")
+    # click.echo(f" --minimal [{minimal}]")
+    # click.echo(f" --nom_cap [{nom_cap}] {type(nom_cap)}")
+
+    kwargs = dict()
+    if debug:
+        kwargs["default_log_level"] = "DEBUG"
+    if not minimal:
+        kwargs["export_raw"] = False
+        kwargs["export_cycles"] = False
+        kwargs["export_ica"] = False
+
+    from cellpy import prms
+    from cellpy.utils import batch
+
+    batchfiledir = pathlib.Path(prms.Paths.batchfiledir)
+    file = pathlib.Path(file_name)
+    if not file.is_file():
+        click.echo(f"file_name={file_name} not found - looking into batchfiledir")
+        if not batchfiledir.is_dir():
+            click.echo("batchfiledir not found - aborting")
+            return
+        file = batchfiledir / file.name
+
+    if not file.is_file():
+        click.echo(f"{file} not found - aborting")
+        return
+
+    b = batch.process_batch(
+        file,
+        force_raw_file=raw,
+        force_cellpy=cellpyfile,
+        nom_cap=nom_cap,
+        backend="matplotlib",
+        **kwargs,
+    )
+    if b is not None and not silent:
+        print(b)
+    click.echo("---")
+
+
+def _run_list(batchfiledir):
+    from cellpy import prms
+
+    if batchfiledir == "NONE" or batchfiledir is None:
+        batchfiledir = pathlib.Path(prms.Paths.batchfiledir)
+    else:
+        batchfiledir = pathlib.Path(batchfiledir).resolve()
+
+    if batchfiledir.is_dir():
+        click.echo(f"Content of '{batchfiledir}':\n")
+        i = 0
+        for i, f in enumerate(batchfiledir.glob("cellpy*.json")):
+            click.echo(f"{f.name}")
+        if i:
+            print(f"\nnumber of batch-files located: {i}")
+        else:
+            print("No batch-files found in this directory.")
+    else:
+        click.echo(f"{batchfiledir} not found.")
+
+
+def _run_journals(folder_name, debug, silent, raw, cellpyfile, minimal):
+    click.echo(f"running journals in {folder_name}")
+    # click.echo(f" --debug [{debug}]")
+    # click.echo(f" --silent [{silent}]")
+    # click.echo(f" --raw [{raw}]")
+    # click.echo(f" --cellpyfile [{cellpyfile}]")
+    # click.echo(f" --minimal [{minimal}]")
+
+    kwargs = dict()
+    if debug:
+        kwargs["default_log_level"] = "DEBUG"
+    if not minimal:
+        kwargs["export_raw"] = False
+        kwargs["export_cycles"] = False
+        kwargs["export_ica"] = False
+
+    from cellpy.utils import batch
+
+    folder_name = pathlib.Path(folder_name).resolve()
+
+    if not folder_name.is_dir():
+        click.echo(f"{folder_name} not found - aborting")
+        return
+
+    batch.iterate_batches(
+        folder_name, force_raw_file=raw, force_cellpy=cellpyfile, silent=True, **kwargs
+    )
+    click.echo("---")
+
+
+def _run_project(our_new_project, **kwargs):
+    try:
+        import papermill as pm
+    except ImportError:
+        click.echo(
+            "[cellpy]: You need to install papermill for automatically execute the notebooks."
+        )
+        click.echo("[cellpy]: You can install it using pip like this:")
+        click.echo(" >> pip install papermill")
+        return
+    our_new_project = pathlib.Path(our_new_project)
+    click.echo(f"[cellpy]: trying to run notebooks in {our_new_project}")
+    notebooks = sorted(list(our_new_project.glob("*.ipynb")))
+    for notebook in notebooks:
+        click.echo(f"[cellpy - papermill] running {notebook.name}")
+        pm.execute_notebook(notebook, notebook, parameters=kwargs)
+
+
+def _run(name, debug, silent):
+    click.echo(f"running {name}")
+    click.echo(f" --debug [{debug}]")
+    click.echo(f" --silent [{silent}]")
+    click.echo("[cellpy]: sorry, I am not allowed to run this on my own")
+
+
+def _run_db(debug, silent):
+    import platform
+
+    from cellpy import prms
+
+    if not silent:
+        click.echo(f"running database editor")
+    if debug:
+        click.echo("running in debug-mode, but nothing to tell")
+
+    db_path = Path(prms.Paths.db_path) / prms.Paths.db_filename
+
+    if platform.system() == "Windows":
+        try:
+            os.system(f'start excel "{str(db_path)}"')
+        except Exception as e:
+            click.echo("Something went wrong trying to open")
+            click.echo(db_path)
+            print()
+            print(e)
+
+    elif platform.system() == "Linux":
+        click.echo("RUNNING LINUX")
+        # not tested
+        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
+
+    elif platform.system() == "Darwin":
+        click.echo(f" - running on a mac")
+        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
+
+    else:
+        print("RUNNING SOMETHING ELSE")
+        print(platform.system())
+        # not tested
+        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
+
+
+# ----------------------- pull ---------------------------------------
+@click.command()
+@click.option("--tests", "-t", is_flag=True, help="Download test-files from repo.")
+@click.option(
+    "--examples", "-e", is_flag=True, help="Download example-files from repo."
+)
+@click.option("--clone", "-c", is_flag=True, help="Clone the full repo.")
+@click.option("--directory", "-d", default=None, help="Save into custom directory DIR")
+@click.option("--password", "-p", default=None, help="Password option for the repo")
+def pull(tests, examples, clone, directory, password):
+    """Download examples or tests from the big internet (needs git)."""
+    if directory is not None:
+        click.echo(f"[cellpy] (pull) custom directory: {directory}")
+    else:
+        directory = pathlib.Path(prmreader.prms.Paths.examplesdir)
+
+    if password is not None:
+        click.echo("DEV MODE: password provided")
+    if clone:
+        _clone_repo(directory, password)
+    else:
+        if tests:
+            _pull_tests(directory, password)
+        if examples:
+            _pull_examples(directory, password)
+        else:
+            click.echo(
+                f"[cellpy] (pull) Nothing selected for pulling. "
+                f"Please select an option (--tests,--examples, -clone, ...) "
+            )
+
+
+def _clone_repo(directory, password):
+    directory = pathlib.Path(directory)
+    txt = "[cellpy] The plan is that this "
+    txt += "[cellpy] cmd will pull (clone) the cellpy repo.\n"
+    txt += "[cellpy] For now it only prints the link to the git-hub\n"
+    txt += "[cellpy] repository:\n"
+    txt += "[cellpy]\n"
+    txt += "[cellpy] https://github.com/jepegit/cellpy.git\n"
+    txt += "[cellpy]\n"
+    click.echo(txt)
+
+
+def _pull_tests(directory, pw=None):
+    txt = (
+        "[cellpy] (pull) Pulling tests from",
+        " https://github.com/jepegit/cellpy.git",
+    )
+    click.echo(txt)
+    _pull(gdirpath="tests", rootpath=directory, pw=pw)
+    _pull(gdirpath="testdata", rootpath=directory, pw=pw)
+
+
+def _pull_examples(directory, pw):
+    txt = (
+        "[cellpy] (pull) Pulling examples from",
+        " https://github.com/jepegit/cellpy.git",
+    )
+    click.echo(txt)
+    _pull(gdirpath="examples", rootpath=directory, pw=pw)
+
+
+def _version():
+    txt = "[cellpy] version: " + str(VERSION)
+    click.echo(txt)
+
+
+def _configloc():
+    _, config_file_name = prmreader.get_user_dir_and_dst()
+    click.echo("[cellpy] ->%s" % config_file_name)
+    if not os.path.isfile(config_file_name):
+        click.echo("[cellpy] File does not exist!")
+    else:
+        return config_file_name
+
+
+def _envloc():
+    click.echo(f"[cellpy] ->{prmreader.get_env_file_name()}")
+    if not os.path.isfile(prmreader.get_env_file_name()):
+        click.echo("[cellpy] File does not exist!")
+    else:
+        return prmreader.get_env_file_name()
+
+
+def _dump_params():
+    click.echo("[cellpy] Dumping parameters to screen:\n")
+    prmreader.info()
+
+
+def _download_g_blob(name, local_path):
+    import urllib.request
+
+    dirs = local_path.parent
+    if not dirs.is_dir():
+        click.echo(f"[cellpy] (pull) creating dir: {dirs}")
+        dirs.mkdir(parents=True)
+
+    filename, headers = urllib.request.urlretrieve(
+        name.download_url, filename=local_path
+    )
+    click.echo(f"[cellpy] (pull) downloaded blob: {filename}")
+
+
+def _parse_g_dir(repo, gdirpath):
+    """parses a repo directory two-levels deep"""
+    for f in repo.get_contents(gdirpath):
+        if f.type == "dir":
+            for sf in repo.get_contents(f.path):
+                yield sf
+        else:
+            yield f
+
+
+def _get_user_name():
+    return "jepegit"
+
+
+def _get_pw(method):
+    if method == "ask":
+        return getpass.getpass()
+    elif method == "env":
+        return os.environ.get(GITHUB_PWD_VAR_NAME, None)
+
+    else:
+        return None
+
+
+def _pull(gdirpath="examples", rootpath=None, u=None, pw=None):
+    if rootpath is None:
+        rootpath = prmreader.prms.Paths.examplesdir
+
+    rootpath = pathlib.Path(rootpath)
+
+    ndirpath = rootpath / gdirpath
+
+    if pw is not None:
+        click.echo(" DEV MODE ".center(80, "-"))
+        u = _get_user_name()
+        if pw == "ask":
+            click.echo("   - ask for password")
+            pw = _get_pw(pw)
+        elif pw == "env":
+            click.echo("   - check environ for password ")
+            pw = _get_pw(pw)
+            click.echo("   - got something")
+            if pw is None:
+                click.echo("   - only None")
+                u = None
+
+    g = Github(u, pw)
+    repo = g.get_repo(REPO)
+
+    click.echo(f"[cellpy] (pull) pulling {gdirpath}")
+    click.echo(f"[cellpy] (pull) -> {ndirpath}")
+
+    if not ndirpath.is_dir():
+        click.echo(f"[cellpy] (pull) creating dir: {ndirpath}")
+        ndirpath.mkdir(parents=True)
+
+    for gfile in _parse_g_dir(repo, gdirpath):
+        gfilename = pathlib.Path(gfile.path)
+        nfilename = rootpath / gfilename
+
+        _download_g_blob(gfile, nfilename)
+
+
+def _get_default_template():
+    template = "standard"
+    try:
+        template = prmreader.prms.Batch.template
+    except:
+        logging.debug("You dont have any default template defined in you .conf file")
+    return template
+
+
+def _read_local_templates(local_templates_path=None):
+    if local_templates_path is None:
+        local_templates_path = pathlib.Path(prmreader.prms.Paths.templatedir)
+    templates = {}
+    for p in list(local_templates_path.rglob("cellpy_cookie*.zip")):
+        label = p.stem.strip()[len("cellpy_cookie_") :]
+        templates[label] = (str(p), None)
+    logging.debug(f"Found the following templates: {templates}")
+    return templates
+
+
+# ----------------------- new ----------------------------------------
+@click.command()
+@click.option("--template", "-t", help="Provide template name.")
+@click.option("--directory", "-d", default=None, help="Create in custom directory.")
+@click.option(
+    "--project",
+    "-p",
+    default=None,
+    help="Provide project name (i.e. sub-directory name).",
+)
+@click.option(
+    "--experiment",
+    "-e",
+    default=None,
+    help="Provide experiment name (i.e. lookup-value).",
+)
+@click.option(
+    "--local-user-template",
+    "-u",
+    is_flag=True,
+    default=False,
+    help="Use local template from the templates directory.",
+)
+@click.option("--serve", "-s", "serve_", is_flag=True, help="Run Jupyter.")
+@click.option(
+    "--run",
+    "-r",
+    "run_",
+    is_flag=True,
+    help="Use PaperMill to run the notebook(s) from the template "
+    "(will only work properly if the notebooks can be sorted in correct run-order by 'sorted'.",
+)
+@click.option(
+    "--lab",
+    "-j",
+    is_flag=True,
+    help="Use Jupyter Lab instead of Notebook when serving.",
+)
+@click.option(
+    "--list", "-l", "list_", is_flag=True, help="List available templates and exit."
+)
+def new(
+    template,
+    directory,
+    project,
+    experiment,
+    local_user_template,
+    serve_,
+    run_,
+    lab,
+    list_,
+):
+    """Set up a batch experiment (might need git installed)."""
+    _new(
+        template,
+        directory=directory,
+        project_dir=project,
+        session_id=experiment,
+        local_user_template=local_user_template,
+        serve_=serve_,
+        run_=run_,
+        lab=lab,
+        list_=list_,
+    )
+
+
+def _new(
+    template: str,
+    directory: Union[Path, str, None] = None,
+    project_dir: Union[str, None] = None,
+    local_user_template: bool = False,
+    serve_: bool = False,
+    run_: bool = False,
+    lab: bool = False,
+    list_: bool = False,
+    session_id: str = "experiment_001",
+    no_input: bool = False,
+    cookie_directory: str = "",
+):
+    """Set up a batch experiment (might need git installed).
+
+    Args:
+        template: short-name of template.
+        directory: the directory for your cellpy projects.
+        local_user_template: use local template if True.
+        serve_: serve the notebook after creation if True.
+        run_: run the notebooks using papermill if True.
+        lab: use jupyter-lab instead of jupyter notebook if True.
+        list_: list all available templates and return if True.
+        project_dir: your project directory.
+        session_id: the lookup value.
+        no_input: accept defaults if True (only valid when providing project_dir and session_id)
+        cookie_directory: name of the directory for your cookie (inside the repository or zip file).
+    Returns:
+        None
+    """
+
+    from cellpy.parameters import prms
+
+    if list_:
+        click.echo(f"\n[cellpy] batch templates")
+
+        default_template = _get_default_template()
+        local_templates = _read_local_templates()
+        local_templates_path = prmreader.prms.Paths.templatedir
+        registered_templates = prms._registered_templates
+        click.echo(f"[cellpy] - default: {default_template}")
+        click.echo("[cellpy] - registered templates (on github):")
+        for label, link in registered_templates.items():
+            click.echo(f"\t\t{label:18s} {link}")
+
+        if local_templates:
+            click.echo(f"[cellpy] - local templates ({local_templates_path}):")
+            for label, link in local_templates.items():
+                click.echo(f"\t\t{label:18s} {link}")
+        else:
+            click.echo(f"[cellpy] - local templates ({local_templates_path}): none")
+
+        return
+
+    if project_dir is None or session_id is None:
+        no_input = False
+
+    if not template:
+        template = _get_default_template()
+
+    if lab:
+        server = "lab"
+    else:
+        server = "notebook"
+
+    try:
+        import cookiecutter.exceptions
+        import cookiecutter.main
+        import cookiecutter.prompt
+
+    except ModuleNotFoundError:
+        click.echo("Could not import cookiecutter.")
+        click.echo("Try installing it, for example by writing:")
+        click.echo("\npip install cookiecutter\n")
+
+    click.echo(f"Template: {template}")
+    if local_user_template:
+        # forcing using local template
+        templates = _read_local_templates()
+
+        if not templates:
+            click.echo(
+                "You asked me to use a local template, but you have none. Aborting."
+            )
+            return
+    else:
+        templates = prms._registered_templates
+        if local_templates := _read_local_templates():
+            templates.update(local_templates)
+
+    if not template.lower() in templates.keys():
+        click.echo("This template does not exist. Aborting.")
+        return
+
+    if directory is None:
+        logging.debug("no dir given")
+        directory = prms.Paths.notebookdir
+
+    if not os.path.isdir(directory):
+        click.echo("Sorry. This did not work as expected!")
+        click.echo(f" - {directory} does not exist")
+        return
+
+    directory = Path(directory)
+    selected_project_dir = None
+
+    if project_dir:
+        selected_project_dir = directory / project_dir
+        if not selected_project_dir.is_dir():
+            if cookiecutter.prompt.read_user_yes_no(
+                f"{project_dir} does not exist. Create?", "yes"
+            ):
+                os.mkdir(selected_project_dir)
+                click.echo(f"Created {selected_project_dir}")
+
+            else:
+                selected_project_dir = None
+                click.echo(f"Select another directory instead")
+
+    if not selected_project_dir:
+        project_dirs = [
+            d.name
+            for d in directory.iterdir()
+            if d.is_dir() and not d.name.startswith(".")
+        ]
+        project_dirs.insert(0, "[create new dir]")
+
+        project_dir = cookiecutter.prompt.read_user_choice(
+            "project folder", project_dirs
+        )
+
+        if project_dir == "[create new dir]":
+            default_name = "cellpy_project"
+            temp_default_name = default_name
+            for j in range(999):
+                if temp_default_name in project_dirs:
+                    temp_default_name = default_name + str(j + 1).zfill(3)
+                else:
+                    default_name = temp_default_name
+                    break
+
+            project_dir = cookiecutter.prompt.read_user_variable(
+                "New name", default_name
+            )
+            try:
+                os.mkdir(directory / project_dir)
+                click.echo(f"created {project_dir}")
+            except FileExistsError:
+                click.echo("OK - but this directory already exists!")
+        selected_project_dir = directory / project_dir
+
+    # get a list of all folders
+    existing_projects = os.listdir(selected_project_dir)
+
+    os.chdir(selected_project_dir)
+    cellpy_version = cellpy.__version__
+
+    try:
+        selected_template, cookie_dir = templates[template.lower()]
+
+        if cookie_directory:
+            cookie_dir = cookie_directory
+        if not cookie_dir:
+            cookie_dir = template.lower()
+
+        cookiecutter.main.cookiecutter(
+            selected_template,
+            extra_context={
+                "author_name": os.getlogin(),
+                "project_name": project_dir,
+                "cellpy_version": cellpy_version,
+                "session_id": session_id,
+            },
+            no_input=no_input,
+            directory=cookie_dir,
+        )
+    except cookiecutter.exceptions.OutputDirExistsException as e:
+        click.echo("Sorry. This did not work as expected!")
+        click.echo(" - cookiecutter refused to create the project")
+        click.echo(e)
+
+    if serve_:
+        os.chdir(directory)
+        _serve(server)
+
+    if run_:
+        try:
+            import papermill as pm
+        except ImportError:
+            click.echo(
+                "[cellpy]: You need to install papermill for automatically execute the notebooks."
+            )
+            click.echo("[cellpy]: You can install it using pip like this:")
+            click.echo(" >> pip install papermill")
+            return
+        new_existing_projects = os.listdir(selected_project_dir)
+        our_new_projects = list(set(new_existing_projects) - set(existing_projects))
+
+        if not len(our_new_projects):
+            click.echo(
+                "[cellpy]: Sorry, could not deiced what is the new project "
+                "- so I don't dare to try to execute automatically."
+            )
+            return
+        our_new_project = selected_project_dir / our_new_projects[0]
+
+        _run_project(our_new_project)
+
+
+def _serve(server):
+    click.echo(f"serving with jupyter {server}")
+    subprocess.run(["jupyter", server], check=True)
+    click.echo("Finished serving.")
+
+
+# ----------------------- serve ---------------------------------------
+@click.command()
+@click.option("--lab", "-l", is_flag=True, help="Use Jupyter Lab instead of Notebook")
+@click.option("--directory", "-d", default=None, help="Start in custom directory DIR")
+def serve(lab, directory):
+    """Start a Jupyter server."""
+
+    from cellpy.parameters import prms
+
+    if directory is None:
+        directory = prms.Paths.notebookdir
+    elif directory == "home":
+        directory = Path().home()
+    elif directory == "here":
+        directory = Path(os.getcwd())
+
+    if not os.path.isdir(directory):
+        click.echo("Sorry. This did not work as expected!")
+        click.echo(f" - {directory} does not exist")
+        return
+
+    if lab:
+        server = "lab"
+    else:
+        server = "notebook"
+
+    os.chdir(directory)
+    _serve(server)
+
+
+cli.add_command(setup)
+cli.add_command(info)
+cli.add_command(edit)
+cli.add_command(pull)
+cli.add_command(run)
+cli.add_command(new)
+cli.add_command(serve)
+
+
+# tests etc
+def _main_pull():
+    if sys.platform == "win32":
+        rootpath = pathlib.Path(r"C:\Temp\cellpy_user")
+    else:
+        rootpath = pathlib.Path("/Users/jepe/scripting/tmp/cellpy_test_user")
+    _pull_examples(rootpath, pw="env")
+    _pull_tests(rootpath, pw="env")
+    # _pull(gdirpath="examples", rootpath=rootpath, u="ask", pw="ask")
+    # _pull(gdirpath="tests", rootpath=rootpath, u="ask", pw="ask")
+    # _pull(gdirpath="testdata", rootpath=rootpath, u="ask", pw="ask")
+
+
+def _main():
+    file_name = prmreader.create_custom_init_filename()
+    click.echo(file_name)
+    user_directory, destination_file_name = prmreader.get_user_dir_and_dst(file_name)
+    click.echo(user_directory)
+    click.echo(destination_file_name)
+    click.echo("trying to save it")
+    save_prm_file(destination_file_name + "_dummy")
+
+    click.echo(" Testing setup ".center(80, "="))
+    setup(["--interactive", "--reset"])
+
+
+def _cli_setup_interactive():
+    from click.testing import CliRunner
+
+    if sys.platform == "win32":
+        root_dir = r"C:\Temp\cellpy_user"
+    else:
+        root_dir = "/Users/jepe/scripting/tmp/cellpy_test_user"
+    testuser = "tester"
+    init_filename = prmreader.create_custom_init_filename(testuser)
+    dst_file = get_dst_file(root_dir, init_filename)
+    init_file = pathlib.Path(dst_file)
+    opts = list()
+    opts.append("setup")
+    opts.append("-i")
+    # opts.append("-nr")
+    opts.append("-r")
+    opts.extend(["-d", root_dir])
+    opts.extend(["-t", testuser])
+
+    input_str = "\n"  # out
+    input_str += "\n"  # rawdatadir
+    input_str += "\n"  # cellpyfiles
+    input_str += "\n"  # log
+    input_str += "\n"  # examples
+    input_str += "\n"  # dbfolder
+    input_str += "\n"  # dbfile
+    runner = CliRunner()
+    result = runner.invoke(cli, opts, input=input_str)
+
+    click.echo(" out ".center(80, "."))
+    click.echo(result.output)
+    from pprint import pprint
+
+    pprint(prmreader.prms.Paths)
+    click.echo(" conf-file ".center(80, "."))
+    click.echo(init_file)
+    click.echo()
+    with init_file.open() as f:
+        for line in f.readlines():
+            click.echo(line.strip())
+
+
+def check_it(var=None):
+    import pathlib
+    import sys
+
+    p_env = pathlib.Path(sys.prefix)
+    print(p_env.name)
+    new(list_=True)
+
+
+if __name__ == "__main__":
+    u1 = os.getlogin()
+    u2 = os.path.expanduser("~")
+    u3 = os.environ.get("USERNAME")
+
+    print(u1)
+    print(u2)
+    print(u3)
+    # check_it()
+    # click.echo("\n\n", " RUNNING MAIN PULL ".center(80, "*"), "\n")
+    # _main_pull()
+    # click.echo("ok")
```

### Comparing `cellpy-1.0.0b0/cellpy/exceptions.py` & `cellpy-1.0.0b1/cellpy/exceptions.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/internals/core.py` & `cellpy-1.0.0b1/cellpy/internals/core.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/log.py` & `cellpy-1.0.0b1/cellpy/log.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/logging.json` & `cellpy-1.0.0b1/cellpy/logging.json`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/parameters/internal_settings.py` & `cellpy-1.0.0b1/cellpy/parameters/internal_settings.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,724 +1,724 @@
-"""Internal settings and definitions and functions for getting them."""
-import logging
-import warnings
-from collections import UserDict
-from dataclasses import dataclass, fields, asdict
-from typing import List, Optional
-
-import pandas as pd
-
-from cellpy import prms
-
-CELLPY_FILE_VERSION = 8
-MINIMUM_CELLPY_FILE_VERSION = 4
-STEP_TABLE_VERSION = 5
-RAW_TABLE_VERSION = 5
-SUMMARY_TABLE_VERSION = 7
-# if you change this, remember that both loading and saving uses this
-# constant at the moment, and check that loading old files still works
-# - and possibly refactor so that the old-file loaders contain the
-# appropriate pickle protocol:
-PICKLE_PROTOCOL = 4
-
-# For creating the sqlite database from Excel:
-TABLE_NAME_SQLITE = "cells"
-COLUMNS_EXCEL_PK = "id"
-COLUMNS_RENAMER = {
-    COLUMNS_EXCEL_PK: "pk",
-    "batch": "comment_history",
-    "cell_name": "name",
-    "exists": "cell_exists",
-    "group": "cell_group",
-    "raw_file_names": "raw_data",
-    "argument": "cell_spec",
-    "nom_cap": "nominal_capacity",
-    "freeze": "frozen",
-}
-ATTRS_TO_IMPORT_FROM_EXCEL_SQLITE = [
-    "name",
-    "label",
-    "project",
-    "cell_group",
-    "cellpy_file_name",
-    "instrument",
-    "cell_type",
-    "cell_design",
-    "channel",
-    "experiment_type",
-    "mass_active",
-    "area",
-    "mass_total",
-    "loading_active",
-    "nominal_capacity",
-    "comment_slurry",
-    "comment_cell",
-    "comment_general",
-    "comment_history",
-    "selected",
-    "freeze",
-    "cell_exists",
-]
-BATCH_ATTRS_TO_IMPORT_FROM_EXCEL_SQLITE = [
-    "comment_history",
-    "sub_batch_01",
-    "sub_batch_02",
-    "sub_batch_03",
-    "sub_batch_04",
-    "sub_batch_05",
-    "sub_batch_06",
-    "sub_batch_07",
-]
-
-
-OTHERPATHS = ["rawdatadir", "cellpydatadir"]
-
-
-@dataclass
-class CellpyMeta:
-    def update(self, as_list: bool = False, **kwargs):
-        """Updates from dictionary of form {key: [values]}
-
-        Args:
-            as_list (bool): pick only first scalar if True.
-            **kwargs (dict): key word attributes to update.
-
-        Returns:
-            None
-        """
-
-        for k, v in kwargs.items():
-            if not as_list:
-                v = v[0]
-            if hasattr(self, k):
-                logging.debug(f"{k} -> {v}")
-                setattr(self, k, v)
-            else:
-                logging.debug(f"[NOT-VALID]{k}:{v}")
-
-    def digest(self, as_list: bool = False, **kwargs):
-        """Pops from dictionary of form {key: [values]}
-
-        Args:
-            as_list (bool): pick only first scalar if True.
-            **kwargs (dict): key word attributes to pick.
-
-        Returns:
-            Dictionary containing the non-digested part.
-        """
-        not_digested = {}
-        for k, v in kwargs.items():
-            if not as_list:
-                v = v[0]
-            if hasattr(self, k):
-                logging.debug(f"{k} -> {v}")
-                setattr(self, k, v)
-            else:
-                logging.debug(f"{k}:{v} ->")
-                not_digested[k] = v
-        return not_digested
-
-    def to_frame(self):
-        """Converts to pandas dataframe"""
-        df = pd.DataFrame.from_dict(asdict(self), orient="index")
-        df.index.name = "key"
-        n_rows, n_cols = df.shape
-        if n_cols == 1:
-            columns = ["value"]
-        else:
-            columns = [f"value_{i:02}" for i in range(n_cols)]
-        df.columns = columns
-
-        return df
-
-
-@dataclass
-class CellpyMetaCommon(CellpyMeta):
-    # about test
-    cell_name: Optional[str] = None  # used as property
-    start_datetime: Optional[str] = None
-    time_zone: Optional[str] = None
-    comment: Optional[prms.CellPyDataConfig] = prms.CellInfo.comment
-    file_errors: Optional[str] = None  # not in use at the moment
-    raw_id: Optional[str] = None  # used as property
-    cellpy_file_version: int = CELLPY_FILE_VERSION
-
-    # about tester
-    tester_ID: Optional[prms.CellPyDataConfig] = None
-    tester_server_software_version: Optional[prms.CellPyDataConfig] = None
-    tester_client_software_version: Optional[prms.CellPyDataConfig] = None
-    tester_calibration_date: Optional[prms.CellPyDataConfig] = None
-
-    # about cell
-    material: Optional[prms.CellPyDataConfig] = prms.Materials.default_material
-    # TODO @jepe: Maybe we should use values with units here instead (pint)?
-    mass: Optional[
-        prms.CellPyDataConfig
-    ] = prms.Materials.default_mass  # active material
-    tot_mass: Optional[
-        prms.CellPyDataConfig
-    ] = prms.Materials.default_mass  # total material
-    nom_cap: Optional[
-        prms.CellPyDataConfig
-    ] = prms.Materials.default_nom_cap  # nominal capacity   # used as property
-    nom_cap_specifics: Optional[
-        prms.CellPyDataConfig
-    ] = (
-        prms.Materials.default_nom_cap_specifics
-    )  # nominal capacity type  # used as property
-
-    active_electrode_area: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.active_electrode_area
-    active_electrode_thickness: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.active_electrode_thickness
-    electrolyte_volume: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.electrolyte_volume
-
-    electrolyte_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.electrolyte_type
-    active_electrode_type: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.active_electrode_type
-    counter_electrode_type: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.counter_electrode_type
-    reference_electrode_type: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.reference_electrode_type
-    experiment_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.experiment_type
-    cell_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.cell_type
-    separator_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.separator_type
-    active_electrode_current_collector: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.active_electrode_current_collector
-    reference_electrode_current_collector: Optional[
-        prms.CellPyDataConfig
-    ] = prms.CellInfo.reference_electrode_current_collector
-
-
-@dataclass
-class CellpyMetaIndividualTest(CellpyMeta):
-    # ---------------- test dependent -------------------------------
-    channel_index: Optional[prms.CellPyDataConfig] = None
-    creator: Optional[str] = None
-    schedule_file_name = None
-    test_type: Optional[
-        prms.CellPyDataConfig
-    ] = None  # Not used (and might be put inside test_ID)
-    voltage_lim_low: Optional[prms.CellPyDataConfig] = prms.CellInfo.voltage_lim_low
-    voltage_lim_high: Optional[prms.CellPyDataConfig] = prms.CellInfo.voltage_lim_high
-    cycle_mode: Optional[prms.CellPyDataConfig] = prms.Reader.cycle_mode
-    test_ID: Optional[
-        prms.CellPyDataConfig
-    ] = None  # id for the test - currently just a number; could become a list or more in the future
-
-
-# TODO: remove import of this
-class HeaderDict(UserDict):
-    """Sub-classing dict to allow for tab-completion."""
-
-    def __setitem__(self, key: str, value: str) -> None:
-        if key == "data":
-            raise KeyError("protected key")
-        super().__setitem__(key, value)
-        self.__dict__[key] = value
-
-
-@dataclass
-class DictLikeClass:
-    """Add some dunder-methods so that it does not break old code that used
-    dictionaries for storing settings
-
-    Remarks: it is not a complete dictionary experience - for example,
-    setting new attributes (new keys) is not supported (raises ``KeyError``
-    if using the typical dict setting method) since it uses the
-    ``dataclasses.fields`` method to find its members.
-
-    """
-
-    def __getitem__(self, key):
-        if key not in self._field_names:
-            logging.debug(f"{key} not in fields")
-        try:
-            return getattr(self, key)
-        except AttributeError:
-            raise KeyError(f"missing key: {key}")
-
-    def __setitem__(self, key, value):
-        if key not in self._field_names:
-            raise KeyError(f"creating new key not allowed: {key}")
-        setattr(self, key, value)
-
-    def __missing__(self, key):
-        raise KeyError
-
-    @property
-    def _field_names(self):
-        return [field.name for field in fields(self)]
-
-    def __iter__(self):
-        for field in self._field_names:
-            yield field
-
-    def _value_iter(self):
-        for field in self._field_names:
-            yield getattr(self, field)
-
-    def keys(self):
-        return [key for key in self.__iter__()]
-
-    def values(self):
-        return [v for v in self._value_iter()]
-
-    def items(self):
-        return zip(self.keys(), self.values())
-
-
-@dataclass
-class BaseSettings(DictLikeClass):
-    """Base class for internal cellpy settings.
-
-    Usage::
-
-         @dataclass
-         class MyCoolCellpySetting(BaseSetting):
-             var1: str = "first var"
-             var2: int = 12
-
-    """
-
-    def get(self, key):
-        """Get the value (postfixes not supported)."""
-        if key not in self.keys():
-            logging.critical(f"the column header '{key}' not found")
-            return
-        else:
-            return self[key]
-
-    def to_frame(self):
-        """Converts to pandas dataframe"""
-        df = pd.DataFrame.from_dict(asdict(self), orient="index")
-        df.index.name = "key"
-        n_rows, n_cols = df.shape
-        if n_cols == 1:
-            columns = ["value"]
-        else:
-            columns = [f"value_{i:02}" for i in range(n_cols)]
-        df.columns = columns
-
-        return df
-
-
-@dataclass
-class BaseHeaders(BaseSettings):
-    """Extending BaseSetting so that it's allowed to add postfixes.
-
-    Example:
-         >>> header["key_postfix"]  # returns "value_postfix"
-    """
-
-    postfixes = []
-
-    def __getitem__(self, key):
-        postfix = ""
-        if key not in self._field_names:
-            # check postfix:
-            subs = key.split("_")
-            _key = "_".join(subs[:-1])
-            _postfix = subs[-1]
-            if _postfix in self.postfixes:
-                postfix = f"_{_postfix}"
-                key = _key
-        try:
-            v = getattr(self, key)
-            return f"{v}{postfix}"
-        except AttributeError:
-            raise KeyError(f"missing key: {key}")
-
-
-@dataclass
-class InstrumentSettings(DictLikeClass):
-    """Base class for instrument settings.
-
-    Usage::
-
-        @dataclass
-        class MyCoolInstrumentSetting(InstrumentSettings):
-            var1: str = "first var"
-            var2: int = 12
-
-    Remark! Try to use it as you would use a normal dataclass.
-
-    """
-
-    ...
-
-
-@dataclass
-class CellpyUnits(BaseSettings):
-    """These are the units used inside Cellpy.
-
-    At least two sets of units needs to be defined; `cellpy_units` and `raw_units`.
-    The `data.raw` dataframe is given in `raw_units` where the units are defined
-    inside the instrument loader used. Since the `data.steps` dataframe is a summary of
-    the step statistics from the `data.raw` dataframe, this also uses the `raw_units`.
-    The `data.summary` dataframe contains columns with values directly from the `data.raw` dataframe
-    given in `raw_units` as well as calculated columns given in `cellpy_units`.
-
-    Remark that all input to cellpy through user interaction (or utils) should be in `cellpy_units`.
-    This is also true for meta-data collected from the raw files. The instrument loader needs to
-    take care of the translation from its raw units to `cellpy_units` during loading the raw data
-    file for the meta-data (remark that this is not necessary and not recommended for the actual
-    "raw" data that is going to be stored in the `data.raw` dataframe).
-
-    As of 2022.09.29, cellpy does not automatically ensure unit conversion for input of meta-data,
-    but has an internal method (`CellPyData.to_cellpy_units`) that can be used.
-
-    These are the different attributes currently supported for data in the dataframes::
-
-        current: str = "A"
-        charge: str = "mAh"
-        voltage: str = "V"
-        time: str = "sec"
-        resistance: str = "Ohms"
-        power: str = "W"
-        energy: str = "Wh"
-        frequency: str = "hz"
-
-    And here are the different attributes currently supported for meta-data::
-
-        # output-units for specific capacity etc.
-        specific_gravimetric: str = "g"
-        specific_areal: str = "cm**2"  # used for calculating specific capacity etc.
-        specific_volumetric: str = "cm**3"  # used for calculating specific capacity etc.
-
-        # other meta-data
-        nominal_capacity: str = "mAh/g"  # used for calculating rates etc.
-        mass: str = "mg"
-        length: str = "cm"
-        area: str = "cm**2"
-        volume: str = "cm**3"
-        temperature: str = "C"
-
-    """
-
-    current: str = "A"
-    charge: str = "mAh"
-    voltage: str = "V"
-    time: str = "sec"
-    resistance: str = "ohm"
-    power: str = "W"
-    energy: str = "Wh"
-    frequency: str = "hz"
-    mass: str = "mg"  # for mass
-    nominal_capacity: str = "mAh/g"
-    specific_gravimetric: str = "g"  # g in specific capacity etc
-    specific_areal: str = "cm**2"  # m2 in specific capacity etc
-    specific_volumetric: str = "cm**3"  # m3 in specific capacity etc
-
-    length: str = "cm"
-    area: str = "cm**2"
-    volume: str = "cm**3"
-    temperature: str = "C"
-    pressure: str = "bar"
-
-    def update(self, new_units: dict):
-        """Update the units."""
-
-        logging.debug(f"{new_units=}")
-        for k in new_units:
-            if k in self.keys():
-                self[k] = new_units[k]
-
-
-@dataclass
-class CellpyLimits(BaseSettings):
-    """These are the limits used inside ``cellpy`` for finding step types.
-
-    Since all instruments have an inherent inaccuracy, it is naive to assume that
-    for example the voltage within a constant voltage step does not change at all.
-    Therefore, we need to define some limits for what we consider to be a constant and
-    what we assume to be zero.
-
-    """
-
-    current_hard: float = 1e-13
-    current_soft: float = 1e-05
-    stable_current_hard: float = 2.0
-    stable_current_soft: float = 4.0
-    stable_voltage_hard: float = 2.0
-    stable_voltage_soft: float = 4.0
-    stable_charge_hard: float = 0.9
-    stable_charge_soft: float = 5.0
-    ir_change: float = 1e-05
-
-
-@dataclass
-class HeadersNormal(BaseHeaders):
-    aci_phase_angle_txt: str = "aci_phase_angle"
-    ref_aci_phase_angle_txt: str = "ref_aci_phase_angle"
-    ac_impedance_txt: str = "ac_impedance"
-    ref_ac_impedance_txt: str = "ref_ac_impedance"
-    charge_capacity_txt: str = "charge_capacity"
-    charge_energy_txt: str = "charge_energy"
-    current_txt: str = "current"
-    cycle_index_txt: str = "cycle_index"
-    data_point_txt: str = "data_point"
-    datetime_txt: str = "date_time"
-    discharge_capacity_txt: str = "discharge_capacity"
-    discharge_energy_txt: str = "discharge_energy"
-    internal_resistance_txt: str = "internal_resistance"
-    power_txt: str = "power"
-    is_fc_data_txt: str = "is_fc_data"
-    step_index_txt: str = "step_index"
-    sub_step_index_txt: str = "sub_step_index"
-    step_time_txt: str = "step_time"
-    sub_step_time_txt: str = "sub_step_time"
-    test_id_txt: str = "test_id"
-    test_time_txt: str = "test_time"
-    voltage_txt: str = "voltage"
-    ref_voltage_txt: str = "reference_voltage"
-    dv_dt_txt: str = "dv_dt"
-    frequency_txt: str = "frequency"
-    amplitude_txt: str = "amplitude"
-    channel_id_txt: str = "channel_id"
-    data_flag_txt: str = "data_flag"
-    test_name_txt: str = "test_name"
-
-
-@dataclass
-class HeadersSummary(BaseHeaders):
-    """In addition to the headers defined here, the summary might also contain
-    specific headers (ending in _gravimetric or _areal).
-    """
-
-    postfixes = ["gravimetric", "areal"]
-
-    cycle_index: str = "cycle_index"
-    data_point: str = "data_point"
-    test_time: str = "test_time"
-    datetime: str = "date_time"
-    discharge_capacity_raw: str = "discharge_capacity"
-    charge_capacity_raw: str = "charge_capacity"
-    test_name: str = "test_name"
-    data_flag: str = "data_flag"
-    channel_id: str = "channel_id"
-
-    coulombic_efficiency: str = "coulombic_efficiency"
-    cumulated_coulombic_efficiency: str = "cumulated_coulombic_efficiency"
-
-    discharge_capacity: str = "discharge_capacity"
-    charge_capacity: str = "charge_capacity"
-    cumulated_charge_capacity: str = "cumulated_charge_capacity"
-    cumulated_discharge_capacity: str = "cumulated_discharge_capacity"
-
-    coulombic_difference: str = "coulombic_difference"
-    cumulated_coulombic_difference: str = "cumulated_coulombic_difference"
-    discharge_capacity_loss: str = "discharge_capacity_loss"
-    charge_capacity_loss: str = "charge_capacity_loss"
-    cumulated_discharge_capacity_loss: str = "cumulated_discharge_capacity_loss"
-    cumulated_charge_capacity_loss: str = "cumulated_charge_capacity_loss"
-
-    normalized_charge_capacity: str = "normalized_charge_capacity"
-    normalized_discharge_capacity: str = "normalized_discharge_capacity"
-
-    shifted_charge_capacity: str = "shifted_charge_capacity"
-    shifted_discharge_capacity: str = "shifted_discharge_capacity"
-
-    ir_discharge: str = "ir_discharge"
-    ir_charge: str = "ir_charge"
-    ocv_first_min: str = "ocv_first_min"
-    ocv_second_min: str = "ocv_second_min"
-    ocv_first_max: str = "ocv_first_max"
-    ocv_second_max: str = "ocv_second_max"
-    end_voltage_discharge: str = "end_voltage_discharge"
-    end_voltage_charge: str = "end_voltage_charge"
-    cumulated_ric_disconnect: str = "cumulated_ric_disconnect"
-    cumulated_ric_sei: str = "cumulated_ric_sei"
-    cumulated_ric: str = "cumulated_ric"
-    normalized_cycle_index: str = "normalized_cycle_index"
-    low_level: str = "low_level"
-    high_level: str = "high_level"
-
-    temperature_last: str = "temperature_last"
-    temperature_mean: str = "temperature_mean"
-
-    charge_c_rate: str = "charge_c_rate"
-    discharge_c_rate: str = "discharge_c_rate"
-    pre_aux: str = "aux_"
-
-    @property
-    def areal_charge_capacity(self) -> str:
-        warnings.warn(
-            "using old-type look-up (areal_charge_capacity) -> will be deprecated soon",
-            DeprecationWarning,
-            stacklevel=2,
-        )
-        return f"{self.charge_capacity}_areal"
-
-    @property
-    def areal_discharge_capacity(self) -> str:
-        warnings.warn(
-            "using old-type look-up (areal_discharge_capacity) -> will be deprecated soon",
-            DeprecationWarning,
-            stacklevel=2,
-        )
-        return f"{self.discharge_capacity}_areal"
-
-    @property
-    def specific_columns(self) -> List[str]:
-        return [
-            self.discharge_capacity,
-            self.charge_capacity,
-            self.cumulated_charge_capacity,
-            self.cumulated_discharge_capacity,
-            self.coulombic_difference,
-            self.cumulated_coulombic_difference,
-            self.discharge_capacity_loss,
-            self.charge_capacity_loss,
-            self.cumulated_discharge_capacity_loss,
-            self.cumulated_charge_capacity_loss,
-            self.shifted_charge_capacity,
-            self.shifted_discharge_capacity,
-            # self.cumulated_ric_disconnect,
-            # self.cumulated_ric_sei,
-            # self.cumulated_ric,
-            # self.normalized_cycle_index,
-        ]
-
-
-@dataclass
-class HeadersStepTable(BaseHeaders):
-    test: str = "test"
-    ustep: str = "ustep"
-    cycle: str = "cycle"
-    step: str = "step"
-    test_time: str = "test_time"
-    step_time: str = "step_time"
-    sub_step: str = "sub_step"
-    type: str = "type"
-    sub_type: str = "sub_type"
-    info: str = "info"
-    voltage: str = "voltage"
-    current: str = "current"
-    charge: str = "charge"
-    discharge: str = "discharge"
-    point: str = "point"
-    internal_resistance: str = "ir"
-    internal_resistance_change: str = "ir_pct_change"
-    rate_avr: str = "rate_avr"
-
-
-@dataclass
-class HeadersJournal(BaseHeaders):
-    filename: str = "filename"
-    mass: str = "mass"
-    total_mass: str = "total_mass"
-    loading: str = "loading"
-    area: str = "area"
-    nom_cap: str = "nom_cap"
-    experiment: str = "experiment"
-    fixed: str = "fixed"
-    label: str = "label"
-    cell_type: str = "cell_type"
-    instrument: str = "instrument"
-    raw_file_names: str = "raw_file_names"
-    cellpy_file_name: str = "cellpy_file_name"
-    group: str = "group"
-    sub_group: str = "sub_group"
-    comment: str = "comment"
-    argument: str = "argument"
-
-
-keys_journal_session = ["starred", "bad_cells", "bad_cycles", "notes"]
-
-headers_step_table = HeadersStepTable()
-headers_journal = HeadersJournal()
-headers_summary = HeadersSummary()
-headers_normal = HeadersNormal()
-cellpy_units = CellpyUnits()
-
-base_columns_float = [
-    headers_normal.test_time_txt,
-    headers_normal.step_time_txt,
-    headers_normal.current_txt,
-    headers_normal.voltage_txt,
-    headers_normal.ref_voltage_txt,
-    headers_normal.charge_capacity_txt,
-    headers_normal.discharge_capacity_txt,
-    headers_normal.internal_resistance_txt,
-]
-
-base_columns_int = [
-    headers_normal.data_point_txt,
-    headers_normal.step_index_txt,
-    headers_normal.cycle_index_txt,
-]
-
-
-def get_cellpy_units(*args, **kwargs) -> CellpyUnits:
-    """Returns an augmented global dictionary with units"""
-    return cellpy_units
-
-
-def get_default_output_units(*args, **kwargs) -> CellpyUnits:
-    """Returns an augmented dictionary with units to use as default."""
-    return CellpyUnits()
-
-
-def get_default_cellpy_file_raw_units(*args, **kwargs) -> CellpyUnits:
-    """Returns a dictionary with units to use as default for old versions of cellpy files"""
-    return CellpyUnits(
-        charge="Ah",
-        mass="mg",
-    )
-
-
-def get_default_raw_units(*args, **kwargs) -> CellpyUnits:
-    """Returns a dictionary with units as default for raw data"""
-    return CellpyUnits(
-        charge="Ah",
-        mass="mg",
-    )
-
-
-def get_default_raw_limits() -> CellpyLimits:
-    """Returns an augmented dictionary with units as default for raw data"""
-    return CellpyLimits()
-
-
-def get_headers_normal() -> HeadersNormal:
-    """Returns an augmented global dictionary containing the header-strings for the normal data
-    (used as column headers for the main data pandas DataFrames)"""
-    return headers_normal
-
-
-def get_headers_step_table() -> HeadersStepTable:
-    """Returns an augmented global dictionary containing the header-strings for the steps table
-    (used as column headers for the steps pandas DataFrames)"""
-    return headers_step_table
-
-
-def get_headers_journal() -> HeadersJournal:
-    """Returns an augmented global dictionary containing the header-strings for the journal (batch)
-    (used as column headers for the journal pandas DataFrames)"""
-    return headers_journal
-
-
-def get_headers_summary() -> HeadersSummary:
-    """Returns an augmented global dictionary containing the header-strings for the summary
-    (used as column headers for the summary pandas DataFrames)"""
-    return headers_summary
-
-
-def get_default_custom_headers_summary() -> HeadersSummary:
-    """Returns an augmented dictionary that can be used to create custom header-strings for the summary
-    (used as column headers for the summary pandas DataFrames)
-
-    This function is mainly implemented to provide an example.
-
-    """
-    # maybe I can do some tricks in here so that tab completion works in pycharm?
-    # solution: ctrl + space works
-    return HeadersSummary()
+"""Internal settings and definitions and functions for getting them."""
+import logging
+import warnings
+from collections import UserDict
+from dataclasses import dataclass, fields, asdict
+from typing import List, Optional
+
+import pandas as pd
+
+from cellpy import prms
+
+CELLPY_FILE_VERSION = 8
+MINIMUM_CELLPY_FILE_VERSION = 4
+STEP_TABLE_VERSION = 5
+RAW_TABLE_VERSION = 5
+SUMMARY_TABLE_VERSION = 7
+# if you change this, remember that both loading and saving uses this
+# constant at the moment, and check that loading old files still works
+# - and possibly refactor so that the old-file loaders contain the
+# appropriate pickle protocol:
+PICKLE_PROTOCOL = 4
+
+# For creating the sqlite database from Excel:
+TABLE_NAME_SQLITE = "cells"
+COLUMNS_EXCEL_PK = "id"
+COLUMNS_RENAMER = {
+    COLUMNS_EXCEL_PK: "pk",
+    "batch": "comment_history",
+    "cell_name": "name",
+    "exists": "cell_exists",
+    "group": "cell_group",
+    "raw_file_names": "raw_data",
+    "argument": "cell_spec",
+    "nom_cap": "nominal_capacity",
+    "freeze": "frozen",
+}
+ATTRS_TO_IMPORT_FROM_EXCEL_SQLITE = [
+    "name",
+    "label",
+    "project",
+    "cell_group",
+    "cellpy_file_name",
+    "instrument",
+    "cell_type",
+    "cell_design",
+    "channel",
+    "experiment_type",
+    "mass_active",
+    "area",
+    "mass_total",
+    "loading_active",
+    "nominal_capacity",
+    "comment_slurry",
+    "comment_cell",
+    "comment_general",
+    "comment_history",
+    "selected",
+    "freeze",
+    "cell_exists",
+]
+BATCH_ATTRS_TO_IMPORT_FROM_EXCEL_SQLITE = [
+    "comment_history",
+    "sub_batch_01",
+    "sub_batch_02",
+    "sub_batch_03",
+    "sub_batch_04",
+    "sub_batch_05",
+    "sub_batch_06",
+    "sub_batch_07",
+]
+
+
+OTHERPATHS = ["rawdatadir", "cellpydatadir"]
+
+
+@dataclass
+class CellpyMeta:
+    def update(self, as_list: bool = False, **kwargs):
+        """Updates from dictionary of form {key: [values]}
+
+        Args:
+            as_list (bool): pick only first scalar if True.
+            **kwargs (dict): key word attributes to update.
+
+        Returns:
+            None
+        """
+
+        for k, v in kwargs.items():
+            if not as_list:
+                v = v[0]
+            if hasattr(self, k):
+                logging.debug(f"{k} -> {v}")
+                setattr(self, k, v)
+            else:
+                logging.debug(f"[NOT-VALID]{k}:{v}")
+
+    def digest(self, as_list: bool = False, **kwargs):
+        """Pops from dictionary of form {key: [values]}
+
+        Args:
+            as_list (bool): pick only first scalar if True.
+            **kwargs (dict): key word attributes to pick.
+
+        Returns:
+            Dictionary containing the non-digested part.
+        """
+        not_digested = {}
+        for k, v in kwargs.items():
+            if not as_list:
+                v = v[0]
+            if hasattr(self, k):
+                logging.debug(f"{k} -> {v}")
+                setattr(self, k, v)
+            else:
+                logging.debug(f"{k}:{v} ->")
+                not_digested[k] = v
+        return not_digested
+
+    def to_frame(self):
+        """Converts to pandas dataframe"""
+        df = pd.DataFrame.from_dict(asdict(self), orient="index")
+        df.index.name = "key"
+        n_rows, n_cols = df.shape
+        if n_cols == 1:
+            columns = ["value"]
+        else:
+            columns = [f"value_{i:02}" for i in range(n_cols)]
+        df.columns = columns
+
+        return df
+
+
+@dataclass
+class CellpyMetaCommon(CellpyMeta):
+    # about test
+    cell_name: Optional[str] = None  # used as property
+    start_datetime: Optional[str] = None
+    time_zone: Optional[str] = None
+    comment: Optional[prms.CellPyDataConfig] = prms.CellInfo.comment
+    file_errors: Optional[str] = None  # not in use at the moment
+    raw_id: Optional[str] = None  # used as property
+    cellpy_file_version: int = CELLPY_FILE_VERSION
+
+    # about tester
+    tester_ID: Optional[prms.CellPyDataConfig] = None
+    tester_server_software_version: Optional[prms.CellPyDataConfig] = None
+    tester_client_software_version: Optional[prms.CellPyDataConfig] = None
+    tester_calibration_date: Optional[prms.CellPyDataConfig] = None
+
+    # about cell
+    material: Optional[prms.CellPyDataConfig] = prms.Materials.default_material
+    # TODO @jepe: Maybe we should use values with units here instead (pint)?
+    mass: Optional[
+        prms.CellPyDataConfig
+    ] = prms.Materials.default_mass  # active material
+    tot_mass: Optional[
+        prms.CellPyDataConfig
+    ] = prms.Materials.default_mass  # total material
+    nom_cap: Optional[
+        prms.CellPyDataConfig
+    ] = prms.Materials.default_nom_cap  # nominal capacity   # used as property
+    nom_cap_specifics: Optional[
+        prms.CellPyDataConfig
+    ] = (
+        prms.Materials.default_nom_cap_specifics
+    )  # nominal capacity type  # used as property
+
+    active_electrode_area: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.active_electrode_area
+    active_electrode_thickness: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.active_electrode_thickness
+    electrolyte_volume: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.electrolyte_volume
+
+    electrolyte_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.electrolyte_type
+    active_electrode_type: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.active_electrode_type
+    counter_electrode_type: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.counter_electrode_type
+    reference_electrode_type: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.reference_electrode_type
+    experiment_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.experiment_type
+    cell_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.cell_type
+    separator_type: Optional[prms.CellPyDataConfig] = prms.CellInfo.separator_type
+    active_electrode_current_collector: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.active_electrode_current_collector
+    reference_electrode_current_collector: Optional[
+        prms.CellPyDataConfig
+    ] = prms.CellInfo.reference_electrode_current_collector
+
+
+@dataclass
+class CellpyMetaIndividualTest(CellpyMeta):
+    # ---------------- test dependent -------------------------------
+    channel_index: Optional[prms.CellPyDataConfig] = None
+    creator: Optional[str] = None
+    schedule_file_name = None
+    test_type: Optional[
+        prms.CellPyDataConfig
+    ] = None  # Not used (and might be put inside test_ID)
+    voltage_lim_low: Optional[prms.CellPyDataConfig] = prms.CellInfo.voltage_lim_low
+    voltage_lim_high: Optional[prms.CellPyDataConfig] = prms.CellInfo.voltage_lim_high
+    cycle_mode: Optional[prms.CellPyDataConfig] = prms.Reader.cycle_mode
+    test_ID: Optional[
+        prms.CellPyDataConfig
+    ] = None  # id for the test - currently just a number; could become a list or more in the future
+
+
+# TODO: remove import of this
+class HeaderDict(UserDict):
+    """Sub-classing dict to allow for tab-completion."""
+
+    def __setitem__(self, key: str, value: str) -> None:
+        if key == "data":
+            raise KeyError("protected key")
+        super().__setitem__(key, value)
+        self.__dict__[key] = value
+
+
+@dataclass
+class DictLikeClass:
+    """Add some dunder-methods so that it does not break old code that used
+    dictionaries for storing settings
+
+    Remarks: it is not a complete dictionary experience - for example,
+    setting new attributes (new keys) is not supported (raises ``KeyError``
+    if using the typical dict setting method) since it uses the
+    ``dataclasses.fields`` method to find its members.
+
+    """
+
+    def __getitem__(self, key):
+        if key not in self._field_names:
+            logging.debug(f"{key} not in fields")
+        try:
+            return getattr(self, key)
+        except AttributeError:
+            raise KeyError(f"missing key: {key}")
+
+    def __setitem__(self, key, value):
+        if key not in self._field_names:
+            raise KeyError(f"creating new key not allowed: {key}")
+        setattr(self, key, value)
+
+    def __missing__(self, key):
+        raise KeyError
+
+    @property
+    def _field_names(self):
+        return [field.name for field in fields(self)]
+
+    def __iter__(self):
+        for field in self._field_names:
+            yield field
+
+    def _value_iter(self):
+        for field in self._field_names:
+            yield getattr(self, field)
+
+    def keys(self):
+        return [key for key in self.__iter__()]
+
+    def values(self):
+        return [v for v in self._value_iter()]
+
+    def items(self):
+        return zip(self.keys(), self.values())
+
+
+@dataclass
+class BaseSettings(DictLikeClass):
+    """Base class for internal cellpy settings.
+
+    Usage::
+
+         @dataclass
+         class MyCoolCellpySetting(BaseSetting):
+             var1: str = "first var"
+             var2: int = 12
+
+    """
+
+    def get(self, key):
+        """Get the value (postfixes not supported)."""
+        if key not in self.keys():
+            logging.critical(f"the column header '{key}' not found")
+            return
+        else:
+            return self[key]
+
+    def to_frame(self):
+        """Converts to pandas dataframe"""
+        df = pd.DataFrame.from_dict(asdict(self), orient="index")
+        df.index.name = "key"
+        n_rows, n_cols = df.shape
+        if n_cols == 1:
+            columns = ["value"]
+        else:
+            columns = [f"value_{i:02}" for i in range(n_cols)]
+        df.columns = columns
+
+        return df
+
+
+@dataclass
+class BaseHeaders(BaseSettings):
+    """Extending BaseSetting so that it's allowed to add postfixes.
+
+    Example:
+         >>> header["key_postfix"]  # returns "value_postfix"
+    """
+
+    postfixes = []
+
+    def __getitem__(self, key):
+        postfix = ""
+        if key not in self._field_names:
+            # check postfix:
+            subs = key.split("_")
+            _key = "_".join(subs[:-1])
+            _postfix = subs[-1]
+            if _postfix in self.postfixes:
+                postfix = f"_{_postfix}"
+                key = _key
+        try:
+            v = getattr(self, key)
+            return f"{v}{postfix}"
+        except AttributeError:
+            raise KeyError(f"missing key: {key}")
+
+
+@dataclass
+class InstrumentSettings(DictLikeClass):
+    """Base class for instrument settings.
+
+    Usage::
+
+        @dataclass
+        class MyCoolInstrumentSetting(InstrumentSettings):
+            var1: str = "first var"
+            var2: int = 12
+
+    Remark! Try to use it as you would use a normal dataclass.
+
+    """
+
+    ...
+
+
+@dataclass
+class CellpyUnits(BaseSettings):
+    """These are the units used inside Cellpy.
+
+    At least two sets of units needs to be defined; `cellpy_units` and `raw_units`.
+    The `data.raw` dataframe is given in `raw_units` where the units are defined
+    inside the instrument loader used. Since the `data.steps` dataframe is a summary of
+    the step statistics from the `data.raw` dataframe, this also uses the `raw_units`.
+    The `data.summary` dataframe contains columns with values directly from the `data.raw` dataframe
+    given in `raw_units` as well as calculated columns given in `cellpy_units`.
+
+    Remark that all input to cellpy through user interaction (or utils) should be in `cellpy_units`.
+    This is also true for meta-data collected from the raw files. The instrument loader needs to
+    take care of the translation from its raw units to `cellpy_units` during loading the raw data
+    file for the meta-data (remark that this is not necessary and not recommended for the actual
+    "raw" data that is going to be stored in the `data.raw` dataframe).
+
+    As of 2022.09.29, cellpy does not automatically ensure unit conversion for input of meta-data,
+    but has an internal method (`CellPyData.to_cellpy_units`) that can be used.
+
+    These are the different attributes currently supported for data in the dataframes::
+
+        current: str = "A"
+        charge: str = "mAh"
+        voltage: str = "V"
+        time: str = "sec"
+        resistance: str = "Ohms"
+        power: str = "W"
+        energy: str = "Wh"
+        frequency: str = "hz"
+
+    And here are the different attributes currently supported for meta-data::
+
+        # output-units for specific capacity etc.
+        specific_gravimetric: str = "g"
+        specific_areal: str = "cm**2"  # used for calculating specific capacity etc.
+        specific_volumetric: str = "cm**3"  # used for calculating specific capacity etc.
+
+        # other meta-data
+        nominal_capacity: str = "mAh/g"  # used for calculating rates etc.
+        mass: str = "mg"
+        length: str = "cm"
+        area: str = "cm**2"
+        volume: str = "cm**3"
+        temperature: str = "C"
+
+    """
+
+    current: str = "A"
+    charge: str = "mAh"
+    voltage: str = "V"
+    time: str = "sec"
+    resistance: str = "ohm"
+    power: str = "W"
+    energy: str = "Wh"
+    frequency: str = "hz"
+    mass: str = "mg"  # for mass
+    nominal_capacity: str = "mAh/g"
+    specific_gravimetric: str = "g"  # g in specific capacity etc
+    specific_areal: str = "cm**2"  # m2 in specific capacity etc
+    specific_volumetric: str = "cm**3"  # m3 in specific capacity etc
+
+    length: str = "cm"
+    area: str = "cm**2"
+    volume: str = "cm**3"
+    temperature: str = "C"
+    pressure: str = "bar"
+
+    def update(self, new_units: dict):
+        """Update the units."""
+
+        logging.debug(f"{new_units=}")
+        for k in new_units:
+            if k in self.keys():
+                self[k] = new_units[k]
+
+
+@dataclass
+class CellpyLimits(BaseSettings):
+    """These are the limits used inside ``cellpy`` for finding step types.
+
+    Since all instruments have an inherent inaccuracy, it is naive to assume that
+    for example the voltage within a constant voltage step does not change at all.
+    Therefore, we need to define some limits for what we consider to be a constant and
+    what we assume to be zero.
+
+    """
+
+    current_hard: float = 1e-13
+    current_soft: float = 1e-05
+    stable_current_hard: float = 2.0
+    stable_current_soft: float = 4.0
+    stable_voltage_hard: float = 2.0
+    stable_voltage_soft: float = 4.0
+    stable_charge_hard: float = 0.9
+    stable_charge_soft: float = 5.0
+    ir_change: float = 1e-05
+
+
+@dataclass
+class HeadersNormal(BaseHeaders):
+    aci_phase_angle_txt: str = "aci_phase_angle"
+    ref_aci_phase_angle_txt: str = "ref_aci_phase_angle"
+    ac_impedance_txt: str = "ac_impedance"
+    ref_ac_impedance_txt: str = "ref_ac_impedance"
+    charge_capacity_txt: str = "charge_capacity"
+    charge_energy_txt: str = "charge_energy"
+    current_txt: str = "current"
+    cycle_index_txt: str = "cycle_index"
+    data_point_txt: str = "data_point"
+    datetime_txt: str = "date_time"
+    discharge_capacity_txt: str = "discharge_capacity"
+    discharge_energy_txt: str = "discharge_energy"
+    internal_resistance_txt: str = "internal_resistance"
+    power_txt: str = "power"
+    is_fc_data_txt: str = "is_fc_data"
+    step_index_txt: str = "step_index"
+    sub_step_index_txt: str = "sub_step_index"
+    step_time_txt: str = "step_time"
+    sub_step_time_txt: str = "sub_step_time"
+    test_id_txt: str = "test_id"
+    test_time_txt: str = "test_time"
+    voltage_txt: str = "voltage"
+    ref_voltage_txt: str = "reference_voltage"
+    dv_dt_txt: str = "dv_dt"
+    frequency_txt: str = "frequency"
+    amplitude_txt: str = "amplitude"
+    channel_id_txt: str = "channel_id"
+    data_flag_txt: str = "data_flag"
+    test_name_txt: str = "test_name"
+
+
+@dataclass
+class HeadersSummary(BaseHeaders):
+    """In addition to the headers defined here, the summary might also contain
+    specific headers (ending in _gravimetric or _areal).
+    """
+
+    postfixes = ["gravimetric", "areal"]
+
+    cycle_index: str = "cycle_index"
+    data_point: str = "data_point"
+    test_time: str = "test_time"
+    datetime: str = "date_time"
+    discharge_capacity_raw: str = "discharge_capacity"
+    charge_capacity_raw: str = "charge_capacity"
+    test_name: str = "test_name"
+    data_flag: str = "data_flag"
+    channel_id: str = "channel_id"
+
+    coulombic_efficiency: str = "coulombic_efficiency"
+    cumulated_coulombic_efficiency: str = "cumulated_coulombic_efficiency"
+
+    discharge_capacity: str = "discharge_capacity"
+    charge_capacity: str = "charge_capacity"
+    cumulated_charge_capacity: str = "cumulated_charge_capacity"
+    cumulated_discharge_capacity: str = "cumulated_discharge_capacity"
+
+    coulombic_difference: str = "coulombic_difference"
+    cumulated_coulombic_difference: str = "cumulated_coulombic_difference"
+    discharge_capacity_loss: str = "discharge_capacity_loss"
+    charge_capacity_loss: str = "charge_capacity_loss"
+    cumulated_discharge_capacity_loss: str = "cumulated_discharge_capacity_loss"
+    cumulated_charge_capacity_loss: str = "cumulated_charge_capacity_loss"
+
+    normalized_charge_capacity: str = "normalized_charge_capacity"
+    normalized_discharge_capacity: str = "normalized_discharge_capacity"
+
+    shifted_charge_capacity: str = "shifted_charge_capacity"
+    shifted_discharge_capacity: str = "shifted_discharge_capacity"
+
+    ir_discharge: str = "ir_discharge"
+    ir_charge: str = "ir_charge"
+    ocv_first_min: str = "ocv_first_min"
+    ocv_second_min: str = "ocv_second_min"
+    ocv_first_max: str = "ocv_first_max"
+    ocv_second_max: str = "ocv_second_max"
+    end_voltage_discharge: str = "end_voltage_discharge"
+    end_voltage_charge: str = "end_voltage_charge"
+    cumulated_ric_disconnect: str = "cumulated_ric_disconnect"
+    cumulated_ric_sei: str = "cumulated_ric_sei"
+    cumulated_ric: str = "cumulated_ric"
+    normalized_cycle_index: str = "normalized_cycle_index"
+    low_level: str = "low_level"
+    high_level: str = "high_level"
+
+    temperature_last: str = "temperature_last"
+    temperature_mean: str = "temperature_mean"
+
+    charge_c_rate: str = "charge_c_rate"
+    discharge_c_rate: str = "discharge_c_rate"
+    pre_aux: str = "aux_"
+
+    @property
+    def areal_charge_capacity(self) -> str:
+        warnings.warn(
+            "using old-type look-up (areal_charge_capacity) -> will be deprecated soon",
+            DeprecationWarning,
+            stacklevel=2,
+        )
+        return f"{self.charge_capacity}_areal"
+
+    @property
+    def areal_discharge_capacity(self) -> str:
+        warnings.warn(
+            "using old-type look-up (areal_discharge_capacity) -> will be deprecated soon",
+            DeprecationWarning,
+            stacklevel=2,
+        )
+        return f"{self.discharge_capacity}_areal"
+
+    @property
+    def specific_columns(self) -> List[str]:
+        return [
+            self.discharge_capacity,
+            self.charge_capacity,
+            self.cumulated_charge_capacity,
+            self.cumulated_discharge_capacity,
+            self.coulombic_difference,
+            self.cumulated_coulombic_difference,
+            self.discharge_capacity_loss,
+            self.charge_capacity_loss,
+            self.cumulated_discharge_capacity_loss,
+            self.cumulated_charge_capacity_loss,
+            self.shifted_charge_capacity,
+            self.shifted_discharge_capacity,
+            # self.cumulated_ric_disconnect,
+            # self.cumulated_ric_sei,
+            # self.cumulated_ric,
+            # self.normalized_cycle_index,
+        ]
+
+
+@dataclass
+class HeadersStepTable(BaseHeaders):
+    test: str = "test"
+    ustep: str = "ustep"
+    cycle: str = "cycle"
+    step: str = "step"
+    test_time: str = "test_time"
+    step_time: str = "step_time"
+    sub_step: str = "sub_step"
+    type: str = "type"
+    sub_type: str = "sub_type"
+    info: str = "info"
+    voltage: str = "voltage"
+    current: str = "current"
+    charge: str = "charge"
+    discharge: str = "discharge"
+    point: str = "point"
+    internal_resistance: str = "ir"
+    internal_resistance_change: str = "ir_pct_change"
+    rate_avr: str = "rate_avr"
+
+
+@dataclass
+class HeadersJournal(BaseHeaders):
+    filename: str = "filename"
+    mass: str = "mass"
+    total_mass: str = "total_mass"
+    loading: str = "loading"
+    area: str = "area"
+    nom_cap: str = "nom_cap"
+    experiment: str = "experiment"
+    fixed: str = "fixed"
+    label: str = "label"
+    cell_type: str = "cell_type"
+    instrument: str = "instrument"
+    raw_file_names: str = "raw_file_names"
+    cellpy_file_name: str = "cellpy_file_name"
+    group: str = "group"
+    sub_group: str = "sub_group"
+    comment: str = "comment"
+    argument: str = "argument"
+
+
+keys_journal_session = ["starred", "bad_cells", "bad_cycles", "notes"]
+
+headers_step_table = HeadersStepTable()
+headers_journal = HeadersJournal()
+headers_summary = HeadersSummary()
+headers_normal = HeadersNormal()
+cellpy_units = CellpyUnits()
+
+base_columns_float = [
+    headers_normal.test_time_txt,
+    headers_normal.step_time_txt,
+    headers_normal.current_txt,
+    headers_normal.voltage_txt,
+    headers_normal.ref_voltage_txt,
+    headers_normal.charge_capacity_txt,
+    headers_normal.discharge_capacity_txt,
+    headers_normal.internal_resistance_txt,
+]
+
+base_columns_int = [
+    headers_normal.data_point_txt,
+    headers_normal.step_index_txt,
+    headers_normal.cycle_index_txt,
+]
+
+
+def get_cellpy_units(*args, **kwargs) -> CellpyUnits:
+    """Returns an augmented global dictionary with units"""
+    return cellpy_units
+
+
+def get_default_output_units(*args, **kwargs) -> CellpyUnits:
+    """Returns an augmented dictionary with units to use as default."""
+    return CellpyUnits()
+
+
+def get_default_cellpy_file_raw_units(*args, **kwargs) -> CellpyUnits:
+    """Returns a dictionary with units to use as default for old versions of cellpy files"""
+    return CellpyUnits(
+        charge="Ah",
+        mass="mg",
+    )
+
+
+def get_default_raw_units(*args, **kwargs) -> CellpyUnits:
+    """Returns a dictionary with units as default for raw data"""
+    return CellpyUnits(
+        charge="Ah",
+        mass="mg",
+    )
+
+
+def get_default_raw_limits() -> CellpyLimits:
+    """Returns an augmented dictionary with units as default for raw data"""
+    return CellpyLimits()
+
+
+def get_headers_normal() -> HeadersNormal:
+    """Returns an augmented global dictionary containing the header-strings for the normal data
+    (used as column headers for the main data pandas DataFrames)"""
+    return headers_normal
+
+
+def get_headers_step_table() -> HeadersStepTable:
+    """Returns an augmented global dictionary containing the header-strings for the steps table
+    (used as column headers for the steps pandas DataFrames)"""
+    return headers_step_table
+
+
+def get_headers_journal() -> HeadersJournal:
+    """Returns an augmented global dictionary containing the header-strings for the journal (batch)
+    (used as column headers for the journal pandas DataFrames)"""
+    return headers_journal
+
+
+def get_headers_summary() -> HeadersSummary:
+    """Returns an augmented global dictionary containing the header-strings for the summary
+    (used as column headers for the summary pandas DataFrames)"""
+    return headers_summary
+
+
+def get_default_custom_headers_summary() -> HeadersSummary:
+    """Returns an augmented dictionary that can be used to create custom header-strings for the summary
+    (used as column headers for the summary pandas DataFrames)
+
+    This function is mainly implemented to provide an example.
+
+    """
+    # maybe I can do some tricks in here so that tab completion works in pycharm?
+    # solution: ctrl + space works
+    return HeadersSummary()
```

### Comparing `cellpy-1.0.0b0/cellpy/parameters/legacy/update_headers.py` & `cellpy-1.0.0b1/cellpy/parameters/legacy/update_headers.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/parameters/prmreader.py` & `cellpy-1.0.0b1/cellpy/parameters/prmreader.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,392 +1,392 @@
-# -*- coding: utf-8 -*-
-import getpass
-import glob
-import logging
-import os
-import pathlib
-import sys
-import warnings
-from collections import OrderedDict
-from dataclasses import asdict, dataclass
-from pprint import pprint
-
-import box
-import dotenv
-import ruamel
-from ruamel.yaml import YAML
-from ruamel.yaml.error import YAMLError
-
-from cellpy.exceptions import ConfigFileNotRead, ConfigFileNotWritten
-from cellpy.parameters import prms
-from cellpy.parameters.internal_settings import OTHERPATHS
-from cellpy.internals.core import OtherPath
-
-DEFAULT_FILENAME_START = ".cellpy_prms_"
-DEFAULT_FILENAME_END = ".conf"
-USE_MY_DOCUMENTS = False
-
-DEFAULT_FILENAME = DEFAULT_FILENAME_START + "default" + DEFAULT_FILENAME_END
-
-# logger = logging.getLogger(__name__)
-
-yaml = YAML()
-
-
-def initialize():
-    """initializes cellpy by reading the config file and the environment file"""
-    try:
-        _read_prm_file(_get_prm_file())
-        _load_env_file()
-    except FileNotFoundError:
-        warnings.warn("Could not find the config-file")
-    except UserWarning:
-        warnings.warn("Could not read the config-file")
-
-
-def _load_env_file():
-    """loads the environment file"""
-    env_file = pathlib.Path(prms.Paths.env_file)
-    env_file_in_user_dir = pathlib.Path.home() / prms.Paths.env_file
-    if env_file.is_file():
-        dotenv.load_dotenv(env_file)
-    elif env_file_in_user_dir.is_file():
-        dotenv.load_dotenv(env_file_in_user_dir)
-    else:
-        logging.debug("No .env file found")
-
-
-def get_user_name():
-    """get the username of the current user (cross-platform)"""
-    return getpass.getuser()
-
-
-def create_custom_init_filename(user_name=None):
-    """creates a custom prms filename"""
-    if user_name is None:
-        return DEFAULT_FILENAME_START + get_user_name() + DEFAULT_FILENAME_END
-    else:
-        return DEFAULT_FILENAME_START + user_name + DEFAULT_FILENAME_END
-
-
-def get_user_dir_and_dst(init_filename=None):
-    """gets the name of the user directory and full prm filepath"""
-    if init_filename is None:
-        init_filename = create_custom_init_filename()
-    user_dir = get_user_dir()
-    dst_file = user_dir / init_filename
-    return user_dir, dst_file
-
-
-def get_user_dir():
-    """gets the name of the user directory"""
-    # user_dir = pathlib.Path(os.path.abspath(os.path.expanduser("~")))
-    user_dir = pathlib.Path().home().resolve()
-    if os.name == "nt" and USE_MY_DOCUMENTS:
-        _user_dir = user_dir / "documents"
-        if _user_dir.is_dir():
-            user_dir = _user_dir
-    return user_dir
-
-
-def _write_prm_file(file_name=None):
-    logging.debug("saving configuration to %s" % file_name)
-    config_dict = _pack_prms()
-
-    try:
-        with open(file_name, "w") as config_file:
-            yaml.allow_unicode = True
-            yaml.default_flow_style = False
-            yaml.explicit_start = True
-            yaml.explicit_end = True
-            yaml.dump(config_dict, config_file)
-    except YAMLError:
-        raise ConfigFileNotWritten
-
-
-def _update_prms(config_dict):
-    """updates the prms with the values in the config_dict"""
-    # config_dict is your current config
-    # _config_attr is the attribute in the prms module (i.e. the defaults)
-
-    logging.debug("updating parameters")
-    logging.debug(f"new prms: {config_dict}")
-    for key in config_dict:
-        if config_dict[key] is None:
-            logging.debug(f"{config_dict[key]} is None")
-            continue
-        if key == "Paths":
-            _config_attr = getattr(prms, key)
-            for k in config_dict[key]:
-                z = config_dict[key][k]
-
-                _txt = f"{k}: {z}"
-                if k.lower() == "db_filename":
-                    # special hack because it is a filename and not a path
-                    pass
-                elif k.lower() in OTHERPATHS:
-                    logging.debug("converting to OtherPath")
-                    # special hack because it is possibly an external location
-                    z = OtherPath(
-                        str(z)
-                    ).resolve()  # v1.0.0: this is only resolving local paths
-                else:
-                    logging.debug("converting to pathlib.Path")
-                    z = pathlib.Path(z).resolve()
-                _txt += f" -> {z}"
-
-                logging.debug(_txt)
-                setattr(_config_attr, k, z)
-
-        elif hasattr(prms, key):
-            _config_attr = getattr(prms, key)
-            if _config_attr is None:
-                logging.debug(f"{_config_attr} is None")
-                continue
-            for k in config_dict[key]:
-                z = config_dict[key][k]
-                if isinstance(z, dict):
-                    y = getattr(_config_attr, k)
-                    z = box.Box({**y, **z})
-                if isinstance(z, ruamel.yaml.comments.CommentedMap):
-                    z = box.Box(z)
-                setattr(_config_attr, k, z)
-        else:
-            logging.info("\n  not-supported prm: %s" % key)
-
-
-def _convert_instruments_to_dict(x):
-    # Converting instruments to dictionary (since it contains box.Box objects)
-    d = asdict(x)
-    for k, v in d.items():
-        try:
-            d[k] = v.to_dict()
-        except AttributeError:
-            pass
-
-    return d
-
-
-def _convert_to_dict(x):
-    try:
-        dictionary = x.to_dict()
-    except AttributeError:
-        dictionary = asdict(x)
-    return dictionary
-
-
-def _convert_paths_to_dict(x):
-    dictionary = {}
-    for k in x.keys():
-        # hack to get around the leading underscore (since they are properties):
-        if len(k) > 1 and k[0] == "_" and k.lower()[1:] in OTHERPATHS:
-            t = getattr(x, k).full_path
-            k = k[1:]
-        else:
-            t = str(getattr(x, k))
-        dictionary[k] = t
-    return dictionary
-
-
-def _update_and_convert_to_dict(parameter_name):
-    """check that all the parameters are correct in the prm-file"""
-    # update from old prm-file (before v1.0.0):
-    if parameter_name == "DbCols":
-        if hasattr(prms, "DbCols"):
-            db_cols = _convert_to_dict(prms.DbCols)
-            if db_cols is None:
-                return prms.DbColsClass()
-
-            for k in db_cols:
-                if isinstance(db_cols[k], (list, tuple)):
-                    db_cols[k] = db_cols[k][0]
-            return db_cols
-        else:
-            return prms.DbColsClass()
-
-
-def _pack_prms():
-    """if you introduce new 'save-able' parameter dictionaries, then you have
-    to include them here"""
-
-    config_dict = {
-        "Paths": _convert_paths_to_dict(prms.Paths),
-        "FileNames": _convert_to_dict(prms.FileNames),
-        "Db": _convert_to_dict(prms.Db),
-        "DbCols": _update_and_convert_to_dict("DbCols"),
-        "CellInfo": _convert_to_dict(prms.CellInfo),
-        "Reader": _convert_to_dict(prms.Reader),
-        "Materials": _convert_to_dict(prms.Materials),
-        "Instruments": _convert_instruments_to_dict(prms.Instruments),
-        "Batch": _convert_to_dict(prms.Batch),
-    }
-    return config_dict
-
-
-def _read_prm_file(prm_filename):
-    """read the prm file"""
-    logging.debug("Reading config-file: %s" % prm_filename)
-    try:
-        with open(prm_filename, "r") as config_file:
-            prm_dict = yaml.load(config_file)
-
-    except YAMLError as e:
-        raise ConfigFileNotRead from e
-    else:
-        if isinstance(prm_dict, dict):
-            _update_prms(prm_dict)
-        else:
-            print(type(prm_dict))
-
-
-def _read_prm_file_without_updating(prm_filename):
-    """read the prm file but do not update the params"""
-    logging.debug("Reading config-file: %s" % prm_filename)
-    try:
-        with open(prm_filename, "r") as config_file:
-            prm_dict = yaml.load(config_file)
-
-    except YAMLError as e:
-        raise ConfigFileNotRead from e
-    return prm_dict
-
-
-def __look_at(file_name):
-    with open(file_name, "r") as config_file:
-        t = yaml.load(config_file)
-    print(t)
-
-
-def _get_prm_file(file_name=None, search_order=None):
-    """returns name of the prm file"""
-    if file_name is not None:
-        if os.path.isfile(file_name):
-            return file_name
-        else:
-            logging.info("Could not find the prm-file")
-
-    default_name = prms._prm_default_name  # NOQA
-    prm_globtxt = prms._prm_globtxt  # NOQA
-
-    script_dir = os.path.abspath(os.path.dirname(__file__))
-
-    search_path = dict()
-    search_path["curdir"] = os.path.abspath(os.path.dirname(sys.argv[0]))
-    search_path["filedir"] = script_dir
-    search_path["user_dir"] = get_user_dir()
-
-    if search_order is None:
-        search_order = ["user_dir"]  # ["curdir","filedir", "user_dir",]
-    else:
-        search_order = search_order
-
-    # The default name for the prm file is at the moment in the script-dir,@
-    # while default searching is in the user_dir (yes, I know):
-    prm_default = os.path.join(script_dir, default_name)
-
-    # -searching-----------------------
-    search_dict: OrderedDict[Any] = OrderedDict()
-
-    for key in search_order:
-        search_dict[key] = [None, None]
-        prm_directory = search_path[key]
-        default_file = os.path.join(prm_directory, default_name)
-
-        if os.path.isfile(default_file):
-            # noinspection PyTypeChecker
-            search_dict[key][0] = default_file
-
-        prm_globtxt_full = os.path.join(prm_directory, prm_globtxt)
-
-        user_files = glob.glob(prm_globtxt_full)
-
-        for f in user_files:
-            if os.path.basename(f) != os.path.basename(default_file):
-                search_dict[key][1] = f
-                break
-
-    # -selecting----------------------
-    prm_file = None
-    for key, file_list in search_dict.items():
-        if file_list[-1]:
-            prm_file = file_list[-1]
-            break
-        else:
-            if not prm_file:
-                prm_file = file_list[0]
-
-    if prm_file:
-        prm_filename = prm_file
-    else:
-        prm_filename = prm_default
-    return prm_filename
-
-
-def _save_current_prms_to_user_dir():
-    # This should be put into the cellpy setup script
-    file_name = os.path.join(prms.user_dir, prms._prm_default_name)  # NOQA
-    _write_prm_file(file_name)
-
-
-def get_env_file_name():
-    """returns the location of the env-file"""
-    return pathlib.Path(prms.Paths.env_file)
-
-
-def info():
-    """this function will show only the 'box'-type
-    attributes and their content in the cellpy.prms module"""
-    print("Convenience function for listing prms")
-    print(prms.__name__)
-    print(f"prm file (for current user): {_get_prm_file()}")
-    print()
-
-    for key, current_object in prms.__dict__.items():
-        if key.startswith("_") and not key.startswith("__") and prms._debug:  # NOQA
-            print(f"Internal: {key} (type={type(current_object)}): {current_object}")
-
-        elif isinstance(current_object, box.Box):
-            print()
-            print(" OLD-TYPE PRM ".center(80, "="))
-            print(f"prms.{key}:")
-            print(80 * "-")
-            for subkey in current_object:
-                print(f"prms.{key}.{subkey} = ", f"{current_object[subkey]}")
-            print()
-
-        elif key == "Paths":
-            print(" NEW-TYPE PRM WITH OTHERPATHS ".center(80, "*"))
-            attributes = {
-                k: v for k, v in vars(current_object).items() if not k.startswith("_")
-            }
-            for attr in OTHERPATHS:
-                attributes[attr] = getattr(current_object, attr)
-            pprint(attributes, width=1)
-
-        elif isinstance(current_object, (prms.CellPyConfig, prms.CellPyDataConfig)):
-            print(" NEW-TYPE PRM ".center(80, "="))
-            attributes = {
-                k: v for k, v in vars(current_object).items() if not k.startswith("_")
-            }
-            print(f" {key} ".center(80, "="))
-            pprint(attributes, width=1)
-            print()
-
-
-def main():
-    print(" STARTING THE ACTUAL SCRIPT ".center(80, "-"))
-    print("PRM FILE:")
-    f = _get_prm_file()
-    print(f)
-    print("READING:")
-    _read_prm_file(f)
-    print("PACKING:")
-    pprint(_pack_prms())
-    print("INFO:")
-    info()
-    print(prms)
-    pprint(str(prms.Batch), width=1)
-    print(prms.Batch.summary_plot_height_fractions)
-
-
-if __name__ == "__main__":
-    main()
+# -*- coding: utf-8 -*-
+import getpass
+import glob
+import logging
+import os
+import pathlib
+import sys
+import warnings
+from collections import OrderedDict
+from dataclasses import asdict, dataclass
+from pprint import pprint
+
+import box
+import dotenv
+import ruamel
+from ruamel.yaml import YAML
+from ruamel.yaml.error import YAMLError
+
+from cellpy.exceptions import ConfigFileNotRead, ConfigFileNotWritten
+from cellpy.parameters import prms
+from cellpy.parameters.internal_settings import OTHERPATHS
+from cellpy.internals.core import OtherPath
+
+DEFAULT_FILENAME_START = ".cellpy_prms_"
+DEFAULT_FILENAME_END = ".conf"
+USE_MY_DOCUMENTS = False
+
+DEFAULT_FILENAME = DEFAULT_FILENAME_START + "default" + DEFAULT_FILENAME_END
+
+# logger = logging.getLogger(__name__)
+
+yaml = YAML()
+
+
+def initialize():
+    """initializes cellpy by reading the config file and the environment file"""
+    try:
+        _read_prm_file(_get_prm_file())
+        _load_env_file()
+    except FileNotFoundError:
+        warnings.warn("Could not find the config-file")
+    except UserWarning:
+        warnings.warn("Could not read the config-file")
+
+
+def _load_env_file():
+    """loads the environment file"""
+    env_file = pathlib.Path(prms.Paths.env_file)
+    env_file_in_user_dir = pathlib.Path.home() / prms.Paths.env_file
+    if env_file.is_file():
+        dotenv.load_dotenv(env_file)
+    elif env_file_in_user_dir.is_file():
+        dotenv.load_dotenv(env_file_in_user_dir)
+    else:
+        logging.debug("No .env file found")
+
+
+def get_user_name():
+    """get the username of the current user (cross-platform)"""
+    return getpass.getuser()
+
+
+def create_custom_init_filename(user_name=None):
+    """creates a custom prms filename"""
+    if user_name is None:
+        return DEFAULT_FILENAME_START + get_user_name() + DEFAULT_FILENAME_END
+    else:
+        return DEFAULT_FILENAME_START + user_name + DEFAULT_FILENAME_END
+
+
+def get_user_dir_and_dst(init_filename=None):
+    """gets the name of the user directory and full prm filepath"""
+    if init_filename is None:
+        init_filename = create_custom_init_filename()
+    user_dir = get_user_dir()
+    dst_file = user_dir / init_filename
+    return user_dir, dst_file
+
+
+def get_user_dir():
+    """gets the name of the user directory"""
+    # user_dir = pathlib.Path(os.path.abspath(os.path.expanduser("~")))
+    user_dir = pathlib.Path().home().resolve()
+    if os.name == "nt" and USE_MY_DOCUMENTS:
+        _user_dir = user_dir / "documents"
+        if _user_dir.is_dir():
+            user_dir = _user_dir
+    return user_dir
+
+
+def _write_prm_file(file_name=None):
+    logging.debug("saving configuration to %s" % file_name)
+    config_dict = _pack_prms()
+
+    try:
+        with open(file_name, "w") as config_file:
+            yaml.allow_unicode = True
+            yaml.default_flow_style = False
+            yaml.explicit_start = True
+            yaml.explicit_end = True
+            yaml.dump(config_dict, config_file)
+    except YAMLError:
+        raise ConfigFileNotWritten
+
+
+def _update_prms(config_dict):
+    """updates the prms with the values in the config_dict"""
+    # config_dict is your current config
+    # _config_attr is the attribute in the prms module (i.e. the defaults)
+
+    logging.debug("updating parameters")
+    logging.debug(f"new prms: {config_dict}")
+    for key in config_dict:
+        if config_dict[key] is None:
+            logging.debug(f"{config_dict[key]} is None")
+            continue
+        if key == "Paths":
+            _config_attr = getattr(prms, key)
+            for k in config_dict[key]:
+                z = config_dict[key][k]
+
+                _txt = f"{k}: {z}"
+                if k.lower() == "db_filename":
+                    # special hack because it is a filename and not a path
+                    pass
+                elif k.lower() in OTHERPATHS:
+                    logging.debug("converting to OtherPath")
+                    # special hack because it is possibly an external location
+                    z = OtherPath(
+                        str(z)
+                    ).resolve()  # v1.0.0: this is only resolving local paths
+                else:
+                    logging.debug("converting to pathlib.Path")
+                    z = pathlib.Path(z).resolve()
+                _txt += f" -> {z}"
+
+                logging.debug(_txt)
+                setattr(_config_attr, k, z)
+
+        elif hasattr(prms, key):
+            _config_attr = getattr(prms, key)
+            if _config_attr is None:
+                logging.debug(f"{_config_attr} is None")
+                continue
+            for k in config_dict[key]:
+                z = config_dict[key][k]
+                if isinstance(z, dict):
+                    y = getattr(_config_attr, k)
+                    z = box.Box({**y, **z})
+                if isinstance(z, ruamel.yaml.comments.CommentedMap):
+                    z = box.Box(z)
+                setattr(_config_attr, k, z)
+        else:
+            logging.info("\n  not-supported prm: %s" % key)
+
+
+def _convert_instruments_to_dict(x):
+    # Converting instruments to dictionary (since it contains box.Box objects)
+    d = asdict(x)
+    for k, v in d.items():
+        try:
+            d[k] = v.to_dict()
+        except AttributeError:
+            pass
+
+    return d
+
+
+def _convert_to_dict(x):
+    try:
+        dictionary = x.to_dict()
+    except AttributeError:
+        dictionary = asdict(x)
+    return dictionary
+
+
+def _convert_paths_to_dict(x):
+    dictionary = {}
+    for k in x.keys():
+        # hack to get around the leading underscore (since they are properties):
+        if len(k) > 1 and k[0] == "_" and k.lower()[1:] in OTHERPATHS:
+            t = getattr(x, k).full_path
+            k = k[1:]
+        else:
+            t = str(getattr(x, k))
+        dictionary[k] = t
+    return dictionary
+
+
+def _update_and_convert_to_dict(parameter_name):
+    """check that all the parameters are correct in the prm-file"""
+    # update from old prm-file (before v1.0.0):
+    if parameter_name == "DbCols":
+        if hasattr(prms, "DbCols"):
+            db_cols = _convert_to_dict(prms.DbCols)
+            if db_cols is None:
+                return prms.DbColsClass()
+
+            for k in db_cols:
+                if isinstance(db_cols[k], (list, tuple)):
+                    db_cols[k] = db_cols[k][0]
+            return db_cols
+        else:
+            return prms.DbColsClass()
+
+
+def _pack_prms():
+    """if you introduce new 'save-able' parameter dictionaries, then you have
+    to include them here"""
+
+    config_dict = {
+        "Paths": _convert_paths_to_dict(prms.Paths),
+        "FileNames": _convert_to_dict(prms.FileNames),
+        "Db": _convert_to_dict(prms.Db),
+        "DbCols": _update_and_convert_to_dict("DbCols"),
+        "CellInfo": _convert_to_dict(prms.CellInfo),
+        "Reader": _convert_to_dict(prms.Reader),
+        "Materials": _convert_to_dict(prms.Materials),
+        "Instruments": _convert_instruments_to_dict(prms.Instruments),
+        "Batch": _convert_to_dict(prms.Batch),
+    }
+    return config_dict
+
+
+def _read_prm_file(prm_filename):
+    """read the prm file"""
+    logging.debug("Reading config-file: %s" % prm_filename)
+    try:
+        with open(prm_filename, "r") as config_file:
+            prm_dict = yaml.load(config_file)
+
+    except YAMLError as e:
+        raise ConfigFileNotRead from e
+    else:
+        if isinstance(prm_dict, dict):
+            _update_prms(prm_dict)
+        else:
+            print(type(prm_dict))
+
+
+def _read_prm_file_without_updating(prm_filename):
+    """read the prm file but do not update the params"""
+    logging.debug("Reading config-file: %s" % prm_filename)
+    try:
+        with open(prm_filename, "r") as config_file:
+            prm_dict = yaml.load(config_file)
+
+    except YAMLError as e:
+        raise ConfigFileNotRead from e
+    return prm_dict
+
+
+def __look_at(file_name):
+    with open(file_name, "r") as config_file:
+        t = yaml.load(config_file)
+    print(t)
+
+
+def _get_prm_file(file_name=None, search_order=None):
+    """returns name of the prm file"""
+    if file_name is not None:
+        if os.path.isfile(file_name):
+            return file_name
+        else:
+            logging.info("Could not find the prm-file")
+
+    default_name = prms._prm_default_name  # NOQA
+    prm_globtxt = prms._prm_globtxt  # NOQA
+
+    script_dir = os.path.abspath(os.path.dirname(__file__))
+
+    search_path = dict()
+    search_path["curdir"] = os.path.abspath(os.path.dirname(sys.argv[0]))
+    search_path["filedir"] = script_dir
+    search_path["user_dir"] = get_user_dir()
+
+    if search_order is None:
+        search_order = ["user_dir"]  # ["curdir","filedir", "user_dir",]
+    else:
+        search_order = search_order
+
+    # The default name for the prm file is at the moment in the script-dir,@
+    # while default searching is in the user_dir (yes, I know):
+    prm_default = os.path.join(script_dir, default_name)
+
+    # -searching-----------------------
+    search_dict: OrderedDict[Any] = OrderedDict()
+
+    for key in search_order:
+        search_dict[key] = [None, None]
+        prm_directory = search_path[key]
+        default_file = os.path.join(prm_directory, default_name)
+
+        if os.path.isfile(default_file):
+            # noinspection PyTypeChecker
+            search_dict[key][0] = default_file
+
+        prm_globtxt_full = os.path.join(prm_directory, prm_globtxt)
+
+        user_files = glob.glob(prm_globtxt_full)
+
+        for f in user_files:
+            if os.path.basename(f) != os.path.basename(default_file):
+                search_dict[key][1] = f
+                break
+
+    # -selecting----------------------
+    prm_file = None
+    for key, file_list in search_dict.items():
+        if file_list[-1]:
+            prm_file = file_list[-1]
+            break
+        else:
+            if not prm_file:
+                prm_file = file_list[0]
+
+    if prm_file:
+        prm_filename = prm_file
+    else:
+        prm_filename = prm_default
+    return prm_filename
+
+
+def _save_current_prms_to_user_dir():
+    # This should be put into the cellpy setup script
+    file_name = os.path.join(prms.user_dir, prms._prm_default_name)  # NOQA
+    _write_prm_file(file_name)
+
+
+def get_env_file_name():
+    """returns the location of the env-file"""
+    return pathlib.Path(prms.Paths.env_file)
+
+
+def info():
+    """this function will show only the 'box'-type
+    attributes and their content in the cellpy.prms module"""
+    print("Convenience function for listing prms")
+    print(prms.__name__)
+    print(f"prm file (for current user): {_get_prm_file()}")
+    print()
+
+    for key, current_object in prms.__dict__.items():
+        if key.startswith("_") and not key.startswith("__") and prms._debug:  # NOQA
+            print(f"Internal: {key} (type={type(current_object)}): {current_object}")
+
+        elif isinstance(current_object, box.Box):
+            print()
+            print(" OLD-TYPE PRM ".center(80, "="))
+            print(f"prms.{key}:")
+            print(80 * "-")
+            for subkey in current_object:
+                print(f"prms.{key}.{subkey} = ", f"{current_object[subkey]}")
+            print()
+
+        elif key == "Paths":
+            print(" NEW-TYPE PRM WITH OTHERPATHS ".center(80, "*"))
+            attributes = {
+                k: v for k, v in vars(current_object).items() if not k.startswith("_")
+            }
+            for attr in OTHERPATHS:
+                attributes[attr] = getattr(current_object, attr)
+            pprint(attributes, width=1)
+
+        elif isinstance(current_object, (prms.CellPyConfig, prms.CellPyDataConfig)):
+            print(" NEW-TYPE PRM ".center(80, "="))
+            attributes = {
+                k: v for k, v in vars(current_object).items() if not k.startswith("_")
+            }
+            print(f" {key} ".center(80, "="))
+            pprint(attributes, width=1)
+            print()
+
+
+def main():
+    print(" STARTING THE ACTUAL SCRIPT ".center(80, "-"))
+    print("PRM FILE:")
+    f = _get_prm_file()
+    print(f)
+    print("READING:")
+    _read_prm_file(f)
+    print("PACKING:")
+    pprint(_pack_prms())
+    print("INFO:")
+    info()
+    print(prms)
+    pprint(str(prms.Batch), width=1)
+    print(prms.Batch.summary_plot_height_fractions)
+
+
+if __name__ == "__main__":
+    main()
```

### Comparing `cellpy-1.0.0b0/cellpy/parameters/prms.py` & `cellpy-1.0.0b1/cellpy/parameters/prms.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,405 +1,405 @@
-"""cellpy parameters"""
-from __future__ import annotations
-import os
-import sys
-from dataclasses import dataclass, field
-from pathlib import Path
-from typing import List, Tuple, Union, Optional, TYPE_CHECKING
-
-# Using TYPE_CHECKING to avoid circular imports
-# (this will only work without from __future__ import annotations for python 3.11 and above)
-from cellpy.internals.core import OtherPath
-
-import box
-
-# When adding prms, please
-#   1) check / update the internal_settings.py file as well to
-#      ensure that copying / splitting cellpy objects
-#      behaves properly.
-#   2) check / update the .cellpy_prms_default.conf file
-
-# locations etc. for reading custom parameters
-script_dir = os.path.abspath(os.path.dirname(__file__))
-cur_dir = os.path.abspath(os.path.dirname(sys.argv[0]))
-user_dir = Path.home()
-wdir = Path(cur_dir)
-op_wdir = str(wdir)
-
-
-@dataclass
-class CellPyDataConfig:
-    """Settings that can be unique for each CellpyCell instance."""
-
-    ...
-
-
-@dataclass
-class CellPyConfig:
-    """Session settings (global)."""
-
-    def keys(self):
-        return self.__dataclass_fields__.keys()
-
-
-# If updating this, you will have to do a lot of tweaks.
-#   .cellpy_prms_default.conf
-#   cli.py (_update_paths)
-#   test_cli_setup_interactive (NUMBER_OF_DIRS)
-#   test_prms.py (config_file_txt)
-#   _convert_paths_to_dict
-
-
-# This can stay global:
-@dataclass
-class PathsClass(CellPyConfig):
-    outdatadir: Union[Path, str] = wdir
-    _rawdatadir: Union[OtherPath, str] = op_wdir
-    _cellpydatadir: Union[OtherPath, str] = op_wdir
-    db_path: Union[Path, str] = wdir  # used for simple excel db reader
-    filelogdir: Union[Path, str] = wdir
-    examplesdir: Union[Path, str] = wdir
-    notebookdir: Union[Path, str] = wdir
-    templatedir: Union[Path, str] = wdir
-    batchfiledir: Union[Path, str] = wdir
-    instrumentdir: Union[Path, str] = wdir
-    db_filename: str = "cellpy_db.xlsx"  # used for simple excel db reader
-    env_file: Union[Path, str] = user_dir / ".env_cellpy"
-
-    @property
-    def rawdatadir(self) -> OtherPath:
-        return OtherPath(self._rawdatadir)
-
-    @rawdatadir.setter
-    def rawdatadir(self, value: Union[OtherPath, Path, str]):
-        self._rawdatadir = OtherPath(value)
-
-    @property
-    def cellpydatadir(self) -> OtherPath:
-        return OtherPath(self._cellpydatadir)
-
-    @cellpydatadir.setter
-    def cellpydatadir(self, value: Union[OtherPath, Path, str]):
-        self._cellpydatadir = OtherPath(value)
-
-
-@dataclass
-class BatchClass(CellPyConfig):
-    template: str = "standard"
-    fig_extension: str = "png"
-    backend: str = "bokeh"
-    notebook: bool = True
-    dpi: int = 300
-    markersize: int = 4
-    symbol_label: str = "simple"
-    color_style_label: str = "seaborn-deep"
-    figure_type: str = "unlimited"
-    summary_plot_width: int = 900
-    summary_plot_height: int = 800
-    summary_plot_height_fractions: List[float] = field(
-        default_factory=lambda: [0.2, 0.5, 0.3]
-    )
-
-
-@dataclass
-class FileNamesClass(CellPyConfig):
-    file_name_format: str = "YYYYMMDD_[NAME]EEE_CC_TT_RR"
-    raw_extension: str = "res"
-    reg_exp: str = None
-    sub_folders: bool = True
-    file_list_location: str = None
-    file_list_type: str = None
-    file_list_name: str = None
-    cellpy_file_extension: str = "h5"
-
-
-@dataclass
-class ReaderClass(CellPyConfig):
-    diagnostics: bool = False
-    filestatuschecker: str = "size"
-    force_step_table_creation: bool = True
-    force_all: bool = False  # not used yet - should be used when saving
-    sep: str = ";"
-    cycle_mode: str = "anode"
-    sorted_data: bool = True  # finding step-types assumes sorted data
-    select_minimal: bool = False
-    limit_loaded_cycles: Optional[
-        int
-    ] = None  # limit loading cycles to given cycle number
-    ensure_step_table: bool = False
-    ensure_summary_table: bool = False
-    voltage_interpolation_step: float = 0.01
-    time_interpolation_step: float = 10.0
-    capacity_interpolation_step: float = 2.0
-    use_cellpy_stat_file: bool = False
-    auto_dirs: bool = (
-        True  # v2.0 search in prm-file for res and hdf5 dirs in cellpy.get()
-    )
-
-
-@dataclass
-class DbClass(CellPyConfig):
-    db_type: str = "simple_excel_reader"
-    db_table_name: str = "db_table"  # used for simple excel db reader
-    db_header_row: int = 0  # used for simple excel db reader
-    db_unit_row: int = 1  # used for simple excel db reader
-    db_data_start_row: int = 2  # used for simple excel db reader
-    db_search_start_row: int = 2  # used for simple excel db reader
-    db_search_end_row: int = -1  # used for simple excel db reader
-    db_file_sqlite: str = "excel.db"  # used when converting from Excel to sqlite
-    # database connection string - used for more advanced db readers:
-    db_connection: Optional[str] = None
-
-
-@dataclass
-class DbColsClass(CellPyConfig):  # used for simple excel db reader
-    # Note to developers:
-    #  1) This is ONLY for the excel-reader (dbreader.py)! More advanced
-    #     readers should get their own way of handling the db-columns.
-    #  2) If you would like to change the names of the attributes,
-    #     you will have to change the names in the
-    #        a .cellpy_prms_default.conf
-    #        b. dbreader.py
-    #        c. test_dbreader.py
-    #        d. internal_settings.py (renaming when making sqlite from Excel)
-    #     As well as the DbColsTypeClass below.
-
-    id: str = "id"
-    exists: str = "exists"
-    project: str = "project"
-    label: str = "label"
-    group: str = "group"
-    selected: str = "selected"
-    cell_name: str = "cell"
-    cell_type: str = "cell_type"
-    experiment_type: str = "experiment_type"
-    mass_active: str = "mass_active_material"
-    area: str = "area"
-    mass_total: str = "mass_total"
-    loading: str = "loading_active_material"
-    nom_cap: str = "nominal_capacity"
-    file_name_indicator: str = "file_name_indicator"
-    instrument: str = "instrument"
-    raw_file_names: str = "raw_file_names"
-    cellpy_file_name: str = "cellpy_file_name"
-    comment_slurry: str = "comment_slurry"
-    comment_cell: str = "comment_cell"
-    comment_general: str = "comment_general"
-    freeze: str = "freeze"
-    argument: str = "argument"
-
-    batch: str = "batch"
-    sub_batch_01: str = "b01"
-    sub_batch_02: str = "b02"
-    sub_batch_03: str = "b03"
-    sub_batch_04: str = "b04"
-    sub_batch_05: str = "b05"
-    sub_batch_06: str = "b06"
-    sub_batch_07: str = "b07"
-
-
-@dataclass
-class DbColsUnitClass(CellPyConfig):
-    # Note to developers:
-    #  1) This is ONLY for the excel-reader (dbreader.py)! More advanced
-    #     readers should get their own way of handling the db-columns.
-
-    id: str = "str"
-    exists: str = "int"
-    project: str = "str"
-    label: str = "str"
-    group: str = "str"
-    selected: str = "int"
-    cell_name: str = "str"
-    cell_type: str = "str"
-    experiment_type: str = "str"
-    mass_active: str = "float"
-    area: str = "float"
-    mass_total: str = "float"
-    loading: str = "float"
-    nom_cap: str = "float"
-    file_name_indicator: str = "str"
-    instrument: str = "str"
-    raw_file_names: str = "str"
-    cellpy_file_name: str = "str"
-    comment_slurry: str = "str"
-    comment_cell: str = "str"
-    comment_general: str = "str"
-    freeze: str = "int"
-    argument: str = "str"
-
-    batch: str = "str"
-    sub_batch_01: str = "str"
-    sub_batch_02: str = "str"
-    sub_batch_03: str = "str"
-    sub_batch_04: str = "str"
-    sub_batch_05: str = "str"
-    sub_batch_06: str = "str"
-    sub_batch_07: str = "str"
-
-
-@dataclass
-class CellInfoClass(CellPyDataConfig):
-    """Values used for setting the parameters related to the cell and the cycling"""
-
-    voltage_lim_low: float = 0.0
-    voltage_lim_high: float = 1.0
-    active_electrode_area: float = 1.0
-    active_electrode_thickness: float = 1.0
-    electrolyte_volume: float = 1.0
-
-    electrolyte_type: str = "standard"
-    active_electrode_type: str = "standard"
-    counter_electrode_type: str = "standard"
-    reference_electrode_type: str = "standard"
-    experiment_type: str = "cycling"
-    cell_type: str = "standard"
-    separator_type: str = "standard"
-    active_electrode_current_collector: str = "standard"
-    reference_electrode_current_collector: str = "standard"
-    comment: str = ""
-
-
-@dataclass
-class MaterialsClass(CellPyDataConfig):
-    """Default material-specific values used in processing the data."""
-
-    cell_class: str = "Li-Ion"
-    default_material: str = "silicon"
-    default_mass: float = 1.0
-    default_nom_cap: float = 1.0
-    default_nom_cap_specifics: str = "gravimetric"
-
-
-Paths = PathsClass()
-FileNames = FileNamesClass()
-Reader = ReaderClass()
-Db = DbClass()
-DbCols = DbColsClass()
-CellInfo = CellInfoClass()
-Materials = MaterialsClass()
-Batch = BatchClass(summary_plot_height_fractions=[0.2, 0.5, 0.3])
-
-
-# ------------------------------------------------------------------------------
-# Instruments
-#
-#  This should be updated - currently using dicts instead of subclasses of
-#  dataclasses. I guess I could update this but is a bit challenging
-#  so maybe replace later  using e.g. pydantic
-# ------------------------------------------------------------------------------
-
-
-# This can stay global:
-# remark! using box.Box for each instrument
-@dataclass
-class InstrumentsClass(CellPyConfig):
-    tester: Union[str, None]
-    custom_instrument_definitions_file: Union[str, None]
-    Arbin: box.Box
-    Maccor: box.Box
-    Neware: box.Box
-
-
-# Pre-defined instruments:
-# These can stay global:
-Arbin = {
-    "max_res_filesize": 150_000_000,
-    "chunk_size": None,
-    "max_chunks": None,
-    "use_subprocess": False,
-    "detect_subprocess_need": False,
-    "sub_process_path": None,
-    "office_version": "64bit",
-    "SQL_server": r"localhost\SQLEXPRESS",
-    "SQL_UID": "sa",
-    "SQL_PWD": "ChangeMe123",
-    "SQL_Driver": "SQL Server",
-}
-
-Arbin = box.Box(Arbin)
-
-Maccor = {"default_model": "one"}
-Maccor = box.Box(Maccor)
-
-Neware = {"default_model": "one"}
-Neware = box.Box(Neware)
-
-Instruments = InstrumentsClass(
-    tester=None,  # TODO: moving this to DataSetClass (deprecate)
-    custom_instrument_definitions_file=None,
-    Arbin=Arbin,
-    Maccor=Maccor,
-    Neware=Neware,
-)
-
-
-# ------------------------------------------------------------------------------
-# Other secret- or non-config (only for developers)
-# ------------------------------------------------------------------------------
-
-_db_cols_unit = DbColsUnitClass()
-_debug = False
-_variable_that_is_not_saved_to_config = "Hei"
-_prm_default_name = ".cellpy_prms_default.conf"
-_prm_globtxt = ".cellpy_prms*.conf"
-_odbcs = ["pyodbc", "ado", "pypyodbc"]
-_odbc = "pyodbc"
-_search_for_odbc_driver = True
-_allow_multi_test_file = False
-_use_filename_cache = True
-_sub_process_path = Path(__file__) / "../../../bin/mdbtools-win/mdb-export"
-_sub_process_path = _sub_process_path.resolve()
-_sort_if_subprocess = True
-
-_cellpyfile_root = "CellpyData"
-_cellpyfile_raw = "/raw"
-_cellpyfile_step = "/steps"
-_cellpyfile_summary = "/summary"
-_cellpyfile_fid = "/fid"
-_cellpyfile_common_meta = "/info"
-_cellpyfile_test_dependent_meta = "/info_test_dependent"
-
-_cellpyfile_raw_unit_pre_id = "raw_unit_"
-_cellpyfile_raw_limit_pre_id = ""
-
-_cellpyfile_complevel = 1
-_cellpyfile_complib = None  # currently, defaults to "zlib"
-_cellpyfile_raw_format = "table"
-_cellpyfile_summary_format = "table"
-_cellpyfile_stepdata_format = "table"
-_cellpyfile_infotable_format = "fixed"
-_cellpyfile_fidtable_format = "fixed"
-
-# templates
-_standard_template_uri = "https://github.com/jepegit/cellpy_cookies.git"
-
-_registered_templates = {
-    "standard": (_standard_template_uri, "standard"),  # (repository, name-of-folder)
-    "ife": (_standard_template_uri, "ife"),
-}
-
-# used as global variables
-_globals_status = ""
-_globals_errors = []
-_globals_message = []
-
-
-# general settings for loaders
-_minimum_columns_to_keep_for_raw_if_exists = [
-    "data_point_txt",
-    "datetime_txt",
-    "test_time_txt",
-    "step_time_txt",
-    "cycle_index_txt",
-    "step_time_txt",
-    "step_index_txt",
-    "current_txt",
-    "voltage_txt",
-    "charge_capacity_txt",
-    "discharge_capacity_txt",
-    "power_txt",
-]
-
-# used during development for testing new features
-
-_res_chunk = 0
+"""cellpy parameters"""
+from __future__ import annotations
+import os
+import sys
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import List, Tuple, Union, Optional, TYPE_CHECKING
+
+# Using TYPE_CHECKING to avoid circular imports
+# (this will only work without from __future__ import annotations for python 3.11 and above)
+from cellpy.internals.core import OtherPath
+
+import box
+
+# When adding prms, please
+#   1) check / update the internal_settings.py file as well to
+#      ensure that copying / splitting cellpy objects
+#      behaves properly.
+#   2) check / update the .cellpy_prms_default.conf file
+
+# locations etc. for reading custom parameters
+script_dir = os.path.abspath(os.path.dirname(__file__))
+cur_dir = os.path.abspath(os.path.dirname(sys.argv[0]))
+user_dir = Path.home()
+wdir = Path(cur_dir)
+op_wdir = str(wdir)
+
+
+@dataclass
+class CellPyDataConfig:
+    """Settings that can be unique for each CellpyCell instance."""
+
+    ...
+
+
+@dataclass
+class CellPyConfig:
+    """Session settings (global)."""
+
+    def keys(self):
+        return self.__dataclass_fields__.keys()
+
+
+# If updating this, you will have to do a lot of tweaks.
+#   .cellpy_prms_default.conf
+#   cli.py (_update_paths)
+#   test_cli_setup_interactive (NUMBER_OF_DIRS)
+#   test_prms.py (config_file_txt)
+#   _convert_paths_to_dict
+
+
+# This can stay global:
+@dataclass
+class PathsClass(CellPyConfig):
+    outdatadir: Union[Path, str] = wdir
+    _rawdatadir: Union[OtherPath, str] = op_wdir
+    _cellpydatadir: Union[OtherPath, str] = op_wdir
+    db_path: Union[Path, str] = wdir  # used for simple excel db reader
+    filelogdir: Union[Path, str] = wdir
+    examplesdir: Union[Path, str] = wdir
+    notebookdir: Union[Path, str] = wdir
+    templatedir: Union[Path, str] = wdir
+    batchfiledir: Union[Path, str] = wdir
+    instrumentdir: Union[Path, str] = wdir
+    db_filename: str = "cellpy_db.xlsx"  # used for simple excel db reader
+    env_file: Union[Path, str] = user_dir / ".env_cellpy"
+
+    @property
+    def rawdatadir(self) -> OtherPath:
+        return OtherPath(self._rawdatadir)
+
+    @rawdatadir.setter
+    def rawdatadir(self, value: Union[OtherPath, Path, str]):
+        self._rawdatadir = OtherPath(value)
+
+    @property
+    def cellpydatadir(self) -> OtherPath:
+        return OtherPath(self._cellpydatadir)
+
+    @cellpydatadir.setter
+    def cellpydatadir(self, value: Union[OtherPath, Path, str]):
+        self._cellpydatadir = OtherPath(value)
+
+
+@dataclass
+class BatchClass(CellPyConfig):
+    template: str = "standard"
+    fig_extension: str = "png"
+    backend: str = "bokeh"
+    notebook: bool = True
+    dpi: int = 300
+    markersize: int = 4
+    symbol_label: str = "simple"
+    color_style_label: str = "seaborn-deep"
+    figure_type: str = "unlimited"
+    summary_plot_width: int = 900
+    summary_plot_height: int = 800
+    summary_plot_height_fractions: List[float] = field(
+        default_factory=lambda: [0.2, 0.5, 0.3]
+    )
+
+
+@dataclass
+class FileNamesClass(CellPyConfig):
+    file_name_format: str = "YYYYMMDD_[NAME]EEE_CC_TT_RR"
+    raw_extension: str = "res"
+    reg_exp: str = None
+    sub_folders: bool = True
+    file_list_location: str = None
+    file_list_type: str = None
+    file_list_name: str = None
+    cellpy_file_extension: str = "h5"
+
+
+@dataclass
+class ReaderClass(CellPyConfig):
+    diagnostics: bool = False
+    filestatuschecker: str = "size"
+    force_step_table_creation: bool = True
+    force_all: bool = False  # not used yet - should be used when saving
+    sep: str = ";"
+    cycle_mode: str = "anode"
+    sorted_data: bool = True  # finding step-types assumes sorted data
+    select_minimal: bool = False
+    limit_loaded_cycles: Optional[
+        int
+    ] = None  # limit loading cycles to given cycle number
+    ensure_step_table: bool = False
+    ensure_summary_table: bool = False
+    voltage_interpolation_step: float = 0.01
+    time_interpolation_step: float = 10.0
+    capacity_interpolation_step: float = 2.0
+    use_cellpy_stat_file: bool = False
+    auto_dirs: bool = (
+        True  # v2.0 search in prm-file for res and hdf5 dirs in cellpy.get()
+    )
+
+
+@dataclass
+class DbClass(CellPyConfig):
+    db_type: str = "simple_excel_reader"
+    db_table_name: str = "db_table"  # used for simple excel db reader
+    db_header_row: int = 0  # used for simple excel db reader
+    db_unit_row: int = 1  # used for simple excel db reader
+    db_data_start_row: int = 2  # used for simple excel db reader
+    db_search_start_row: int = 2  # used for simple excel db reader
+    db_search_end_row: int = -1  # used for simple excel db reader
+    db_file_sqlite: str = "excel.db"  # used when converting from Excel to sqlite
+    # database connection string - used for more advanced db readers:
+    db_connection: Optional[str] = None
+
+
+@dataclass
+class DbColsClass(CellPyConfig):  # used for simple excel db reader
+    # Note to developers:
+    #  1) This is ONLY for the excel-reader (dbreader.py)! More advanced
+    #     readers should get their own way of handling the db-columns.
+    #  2) If you would like to change the names of the attributes,
+    #     you will have to change the names in the
+    #        a .cellpy_prms_default.conf
+    #        b. dbreader.py
+    #        c. test_dbreader.py
+    #        d. internal_settings.py (renaming when making sqlite from Excel)
+    #     As well as the DbColsTypeClass below.
+
+    id: str = "id"
+    exists: str = "exists"
+    project: str = "project"
+    label: str = "label"
+    group: str = "group"
+    selected: str = "selected"
+    cell_name: str = "cell"
+    cell_type: str = "cell_type"
+    experiment_type: str = "experiment_type"
+    mass_active: str = "mass_active_material"
+    area: str = "area"
+    mass_total: str = "mass_total"
+    loading: str = "loading_active_material"
+    nom_cap: str = "nominal_capacity"
+    file_name_indicator: str = "file_name_indicator"
+    instrument: str = "instrument"
+    raw_file_names: str = "raw_file_names"
+    cellpy_file_name: str = "cellpy_file_name"
+    comment_slurry: str = "comment_slurry"
+    comment_cell: str = "comment_cell"
+    comment_general: str = "comment_general"
+    freeze: str = "freeze"
+    argument: str = "argument"
+
+    batch: str = "batch"
+    sub_batch_01: str = "b01"
+    sub_batch_02: str = "b02"
+    sub_batch_03: str = "b03"
+    sub_batch_04: str = "b04"
+    sub_batch_05: str = "b05"
+    sub_batch_06: str = "b06"
+    sub_batch_07: str = "b07"
+
+
+@dataclass
+class DbColsUnitClass(CellPyConfig):
+    # Note to developers:
+    #  1) This is ONLY for the excel-reader (dbreader.py)! More advanced
+    #     readers should get their own way of handling the db-columns.
+
+    id: str = "str"
+    exists: str = "int"
+    project: str = "str"
+    label: str = "str"
+    group: str = "str"
+    selected: str = "int"
+    cell_name: str = "str"
+    cell_type: str = "str"
+    experiment_type: str = "str"
+    mass_active: str = "float"
+    area: str = "float"
+    mass_total: str = "float"
+    loading: str = "float"
+    nom_cap: str = "float"
+    file_name_indicator: str = "str"
+    instrument: str = "str"
+    raw_file_names: str = "str"
+    cellpy_file_name: str = "str"
+    comment_slurry: str = "str"
+    comment_cell: str = "str"
+    comment_general: str = "str"
+    freeze: str = "int"
+    argument: str = "str"
+
+    batch: str = "str"
+    sub_batch_01: str = "str"
+    sub_batch_02: str = "str"
+    sub_batch_03: str = "str"
+    sub_batch_04: str = "str"
+    sub_batch_05: str = "str"
+    sub_batch_06: str = "str"
+    sub_batch_07: str = "str"
+
+
+@dataclass
+class CellInfoClass(CellPyDataConfig):
+    """Values used for setting the parameters related to the cell and the cycling"""
+
+    voltage_lim_low: float = 0.0
+    voltage_lim_high: float = 1.0
+    active_electrode_area: float = 1.0
+    active_electrode_thickness: float = 1.0
+    electrolyte_volume: float = 1.0
+
+    electrolyte_type: str = "standard"
+    active_electrode_type: str = "standard"
+    counter_electrode_type: str = "standard"
+    reference_electrode_type: str = "standard"
+    experiment_type: str = "cycling"
+    cell_type: str = "standard"
+    separator_type: str = "standard"
+    active_electrode_current_collector: str = "standard"
+    reference_electrode_current_collector: str = "standard"
+    comment: str = ""
+
+
+@dataclass
+class MaterialsClass(CellPyDataConfig):
+    """Default material-specific values used in processing the data."""
+
+    cell_class: str = "Li-Ion"
+    default_material: str = "silicon"
+    default_mass: float = 1.0
+    default_nom_cap: float = 1.0
+    default_nom_cap_specifics: str = "gravimetric"
+
+
+Paths = PathsClass()
+FileNames = FileNamesClass()
+Reader = ReaderClass()
+Db = DbClass()
+DbCols = DbColsClass()
+CellInfo = CellInfoClass()
+Materials = MaterialsClass()
+Batch = BatchClass(summary_plot_height_fractions=[0.2, 0.5, 0.3])
+
+
+# ------------------------------------------------------------------------------
+# Instruments
+#
+#  This should be updated - currently using dicts instead of subclasses of
+#  dataclasses. I guess I could update this but is a bit challenging
+#  so maybe replace later  using e.g. pydantic
+# ------------------------------------------------------------------------------
+
+
+# This can stay global:
+# remark! using box.Box for each instrument
+@dataclass
+class InstrumentsClass(CellPyConfig):
+    tester: Union[str, None]
+    custom_instrument_definitions_file: Union[str, None]
+    Arbin: box.Box
+    Maccor: box.Box
+    Neware: box.Box
+
+
+# Pre-defined instruments:
+# These can stay global:
+Arbin = {
+    "max_res_filesize": 150_000_000,
+    "chunk_size": None,
+    "max_chunks": None,
+    "use_subprocess": False,
+    "detect_subprocess_need": False,
+    "sub_process_path": None,
+    "office_version": "64bit",
+    "SQL_server": r"localhost\SQLEXPRESS",
+    "SQL_UID": "sa",
+    "SQL_PWD": "ChangeMe123",
+    "SQL_Driver": "SQL Server",
+}
+
+Arbin = box.Box(Arbin)
+
+Maccor = {"default_model": "one"}
+Maccor = box.Box(Maccor)
+
+Neware = {"default_model": "one"}
+Neware = box.Box(Neware)
+
+Instruments = InstrumentsClass(
+    tester=None,  # TODO: moving this to DataSetClass (deprecate)
+    custom_instrument_definitions_file=None,
+    Arbin=Arbin,
+    Maccor=Maccor,
+    Neware=Neware,
+)
+
+
+# ------------------------------------------------------------------------------
+# Other secret- or non-config (only for developers)
+# ------------------------------------------------------------------------------
+
+_db_cols_unit = DbColsUnitClass()
+_debug = False
+_variable_that_is_not_saved_to_config = "Hei"
+_prm_default_name = ".cellpy_prms_default.conf"
+_prm_globtxt = ".cellpy_prms*.conf"
+_odbcs = ["pyodbc", "ado", "pypyodbc"]
+_odbc = "pyodbc"
+_search_for_odbc_driver = True
+_allow_multi_test_file = False
+_use_filename_cache = True
+_sub_process_path = Path(__file__) / "../../../bin/mdbtools-win/mdb-export"
+_sub_process_path = _sub_process_path.resolve()
+_sort_if_subprocess = True
+
+_cellpyfile_root = "CellpyData"
+_cellpyfile_raw = "/raw"
+_cellpyfile_step = "/steps"
+_cellpyfile_summary = "/summary"
+_cellpyfile_fid = "/fid"
+_cellpyfile_common_meta = "/info"
+_cellpyfile_test_dependent_meta = "/info_test_dependent"
+
+_cellpyfile_raw_unit_pre_id = "raw_unit_"
+_cellpyfile_raw_limit_pre_id = ""
+
+_cellpyfile_complevel = 1
+_cellpyfile_complib = None  # currently, defaults to "zlib"
+_cellpyfile_raw_format = "table"
+_cellpyfile_summary_format = "table"
+_cellpyfile_stepdata_format = "table"
+_cellpyfile_infotable_format = "fixed"
+_cellpyfile_fidtable_format = "fixed"
+
+# templates
+_standard_template_uri = "https://github.com/jepegit/cellpy_cookies.git"
+
+_registered_templates = {
+    "standard": (_standard_template_uri, "standard"),  # (repository, name-of-folder)
+    "ife": (_standard_template_uri, "ife"),
+}
+
+# used as global variables
+_globals_status = ""
+_globals_errors = []
+_globals_message = []
+
+
+# general settings for loaders
+_minimum_columns_to_keep_for_raw_if_exists = [
+    "data_point_txt",
+    "datetime_txt",
+    "test_time_txt",
+    "step_time_txt",
+    "cycle_index_txt",
+    "step_time_txt",
+    "step_index_txt",
+    "current_txt",
+    "voltage_txt",
+    "charge_capacity_txt",
+    "discharge_capacity_txt",
+    "power_txt",
+]
+
+# used during development for testing new features
+
+_res_chunk = 0
```

### Comparing `cellpy-1.0.0b0/cellpy/readers/core.py` & `cellpy-1.0.0b1/cellpy/readers/core.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1227 +1,1228 @@
-""" This module contains several of the most important classes used in cellpy.
-
-It also contains functions that are used by readers and utils.
-And it has the file version definitions.
-"""
-import abc
-import datetime
-import importlib
-import logging
-import os
-import pathlib
-import pickle
-import sys
-import time
-import warnings
-from typing import Any, Tuple, Dict, List, Union, TypeVar
-
-import numpy as np
-import pandas as pd
-import pint
-from scipy import interpolate
-
-from cellpy.exceptions import NullData
-from cellpy.internals.core import OtherPath
-from cellpy.parameters.internal_settings import (
-    get_headers_normal,
-    get_headers_step_table,
-    get_headers_summary,
-    get_default_raw_units,
-    get_default_raw_limits,
-    CellpyMetaCommon,
-    CellpyMetaIndividualTest,
-)
-
-HEADERS_NORMAL = get_headers_normal()  # TODO @jepe refactor this (not needed)
-HEADERS_SUMMARY = get_headers_summary()  # TODO @jepe refactor this (not needed)
-HEADERS_STEP_TABLE = get_headers_step_table()  # TODO @jepe refactor this (not needed)
-
-
-# pint (https://pint.readthedocs.io/en/stable/)
-ureg = pint.UnitRegistry()
-ureg.default_format = "~P"
-Q = ureg.Quantity
-
-
-# TODO: in future versions (maybe 1.1.0) we should "copy-paste" the whole pathlib module
-#  from CPython and add the functionality we need to it. This will make
-#  it easier to keep up with changes in the pathlib module.
-
-
-# https://stackoverflow.com/questions/60067953/
-# 'is-it-possible-to-specify-the-pickle-protocol-when-writing-pandas-to-hdf5
-class PickleProtocol:
-    """Context for using a specific pickle protocol."""
-
-    def __init__(self, level):
-        self.previous = pickle.HIGHEST_PROTOCOL
-        self.level = level
-
-    def __enter__(self):
-        importlib.reload(pickle)
-        pickle.HIGHEST_PROTOCOL = self.level
-
-    def __exit__(self, *exc):
-        importlib.reload(pickle)
-        pickle.HIGHEST_PROTOCOL = self.previous
-
-
-def pickle_protocol(level):
-    return PickleProtocol(level)
-
-
-class BaseDbReader(metaclass=abc.ABCMeta):
-    """Base class for database readers."""
-
-    @abc.abstractmethod
-    def select_batch(self, batch: str) -> List[int]:
-        pass
-
-    @abc.abstractmethod
-    def get_mass(self, pk: int) -> float:
-        pass
-
-    @abc.abstractmethod
-    def get_area(self, pk: int) -> float:
-        pass
-
-    @abc.abstractmethod
-    def get_loading(self, pk: int) -> float:
-        pass
-
-    @abc.abstractmethod
-    def get_nom_cap(self, pk: int) -> float:
-        pass
-
-    @abc.abstractmethod
-    def get_total_mass(self, pk: int) -> float:
-        pass
-
-    @abc.abstractmethod
-    def get_cell_name(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def get_cell_type(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def get_label(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def get_comment(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def get_group(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def get_args(self, pk: int) -> dict:
-        pass
-
-    @abc.abstractmethod
-    def get_experiment_type(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def get_instrument(self, pk: int) -> str:
-        pass
-
-    @abc.abstractmethod
-    def inspect_hd5f_fixed(self, pk: int) -> int:
-        pass
-
-    @abc.abstractmethod
-    def get_by_column_label(self, pk: int, name: str) -> Any:
-        pass
-
-    @abc.abstractmethod
-    def from_batch(
-        self,
-        batch_name: str,
-        include_key: bool = False,
-        include_individual_arguments: bool = False,
-    ) -> dict:
-        pass
-
-
-class FileID:
-    """class for storing information about the raw-data files.
-
-    This class is used for storing and handling raw-data file information.
-    It is important to keep track of when the data was extracted from the
-    raw-data files so that it is easy to know if the hdf5-files used for
-    @storing "treated" data is up-to-date.
-
-    Attributes:
-        name (str): Filename of the raw-data file.
-        full_name (str): Filename including path of the raw-data file.
-        size (float): Size of the raw-data file.
-        last_modified (datetime): Last time of modification of the raw-data
-            file.
-        last_accessed (datetime): last time of access of the raw-data file.
-        last_info_changed (datetime): st_ctime of the raw-data file.
-        location (str): Location of the raw-data file.
-
-    """
-
-    def __init__(self, filename: Union[str, OtherPath] = None, is_db: bool = False):
-        """Initialize the FileID class."""
-
-        self.is_db: bool = is_db
-        self._last_data_point: Optional[int] = None
-        self.name: Optional[str] = None
-        self.full_name: Optional[str] = None
-        self.size: Optional[int] = None
-        self.last_modified: Optional[int] = None
-        self.last_accessed: Optional[int] = None
-        self.last_info_changed: Optional[int] = None
-        self.location: Optional[int] = None
-
-        if self.is_db:
-            self._from_db(filename)
-            return
-
-        make_defaults = True
-        if filename is not None:
-            if not isinstance(filename, OtherPath):
-                logging.debug("filename is not an OtherPath object")
-                filename = OtherPath(filename)
-
-            if filename.is_file():
-                self.populate(filename)
-                make_defaults = False
-
-        if make_defaults:
-            self.name = None
-            self.full_name = None
-            self.size = 0
-            self.last_modified = None
-            self.last_accessed = None
-            self.last_info_changed = None
-            self.location = None
-            self._last_data_point = 0  # to be used later when updating is implemented
-
-    def __str__(self):
-        """Return a string representation of the FileID object."""
-        try:
-            if self.is_db:
-                txt = "\n<fileID><is_db>\n"
-            else:
-                txt = "\n<fileID><is_file>\n"
-        except AttributeError:
-            txt = "\n<fileID><is_file>\n"
-
-        txt += f"full name: {self.full_name}\n"
-        txt += f"name: {self.name}\n"
-        txt += f"location: {self.location}\n"
-
-        if self.last_modified is not None:
-            txt += f"modified: {self.last_modified}\n"
-        else:
-            txt += "modified: NAN\n"
-
-        if self.size is not None:
-            txt += f"size: {self.size}\n"
-        else:
-            txt += "size: NAN\n"
-
-        txt += f"last data point: {self.last_data_point}\n"
-
-        return txt
-
-    def _from_db(self, filename):
-        self.name = filename
-        self.full_name = filename
-        self.size = 0
-        self.last_modified = None
-        self.last_accessed = None
-        self.last_info_changed = None
-        self.location = None
-        self._last_data_point = 0
-
-    @property
-    def last_data_point(self):
-        # TODO: consider including a method here to find the last data point (raw data)
-        # ideally, this value should be set when loading the raw data before
-        # merging files (if it consists of several files)
-        return self._last_data_point
-
-    @last_data_point.setter
-    def last_data_point(self, value):
-        self._last_data_point = value
-
-    def populate(self, filename: Union[str, OtherPath]):
-        """Finds the file-stats and populates the class with stat values.
-
-        Args:
-            filename (str, OtherPath): name of the file.
-        """
-        if not isinstance(filename, OtherPath):
-            logging.debug("filename is not an OtherPath object")
-            filename = OtherPath(filename)
-
-        if filename.is_file():
-            fid_st = filename.stat()
-            self.name = filename.name
-            self.full_name = filename.full_path
-            self.size = fid_st.st_size
-            self.last_modified = fid_st.st_mtime
-            self.last_accessed = fid_st.st_atime
-            self.last_info_changed = fid_st.st_ctime
-            self.location = str(filename.parent)
-
-    def get_raw(self):
-        """Get a list with information about the file.
-
-        The returned list contains name, size, last_modified and location.
-        """
-        return [self.name, self.size, self.last_modified, self.location]
-
-    def get_name(self):
-        """Get the filename."""
-        return self.name
-
-    def get_size(self):
-        """Get the size of the file."""
-        return self.size
-
-    def get_last(self):
-        """Get last modification time of the file."""
-        return self.last_modified
-
-
-class Data:
-    """Object to store data for a cell-test.
-
-    This class is used for storing all the relevant data for a cell-test, i.e. all
-    the data collected by the tester as stored in the raw-files, and user-provided
-    metadata about the cell-test.
-    """
-
-    def _repr_html_(self):
-        txt = f"<h2>Data-object</h2> id={hex(id(self))}"
-        txt += "<p>"
-        for p in dir(self):
-            if not p.startswith("_"):
-                if p not in ["raw", "summary", "steps", "logger"]:
-                    value = self.__getattribute__(p)
-                    txt += f"<b>{p}</b>: {value}<br>"
-        txt += "</p>"
-        try:
-            raw_txt = f"<p><b>raw data-frame (summary)</b><br>{self.raw.describe()._repr_html_()}</p>"  # noqa
-            raw_txt += f"<p><b>raw data-frame (head)</b><br>{self.raw.head()._repr_html_()}</p>"  # noqa
-        except AttributeError:
-            raw_txt = "<p><b>raw data-frame </b><br> not found!</p>"
-        except ValueError:
-            raw_txt = "<p><b>raw data-frame </b><br> does not contain any columns!</p>"
-
-        try:
-            summary_txt = f"<p><b>summary data-frame (summary)</b><br>{self.summary.describe()._repr_html_()}</p>"  # noqa
-            summary_txt += f"<p><b>summary data-frame (head)</b><br>{self.summary.head()._repr_html_()}</p>"  # noqa
-        except AttributeError:
-            summary_txt = "<p><b>summary data-frame </b><br> not found!</p>"
-        except ValueError:
-            summary_txt = (
-                "<p><b>summary data-frame </b><br> does not contain any columns!</p>"
-            )
-
-        try:
-            steps_txt = f"<p><b>steps data-frame (summary)</b><br>{self.steps.describe()._repr_html_()}</p>"  # noqa
-            steps_txt += f"<p><b>steps data-frame (head)</b><br>{self.steps.head()._repr_html_()}</p>"  # noqa
-        except AttributeError:
-            steps_txt = "<p><b>steps data-frame </b><br> not found!</p>"
-        except ValueError:
-            steps_txt = (
-                "<p><b>steps data-frame </b><br> does not contain any columns!</p>"
-            )
-
-        return txt + summary_txt + steps_txt + raw_txt
-
-    def __init__(self, **kwargs):
-        self.logger = logging.getLogger(__name__)
-        self.logger.debug("created DataSet instance")
-
-        self.raw_data_files = []
-        self.raw_data_files_length = []
-        self.loaded_from = None
-        self._raw_id = None
-        self.raw_units = get_default_raw_units()
-        self.raw_limits = get_default_raw_limits()
-
-        self.raw = pd.DataFrame()
-        self.summary = pd.DataFrame()
-        self.steps = pd.DataFrame()
-
-        self.meta_common = CellpyMetaCommon()
-        # TODO: v2.0 consider making this a list of several CellpyMetaIndividualTest
-        self.meta_test_dependent = CellpyMetaIndividualTest()
-
-        # custom meta-data
-        for k in kwargs:
-            if hasattr(self, k):
-                setattr(self, k, kwargs[k])
-
-    # ---------------- left-over-properties v7 -> v8 -----------------
-    # these now belong to the CellpyMeta attributes
-    #   however, since they are extensively used in the instrument
-    #   loaders and cellreader, they are also accessible here as properties
-
-    @property
-    def raw_id(self):
-        return self.meta_common.raw_id
-
-    @property
-    def start_datetime(self):
-        return self.meta_common.start_datetime
-
-    @start_datetime.setter
-    def start_datetime(self, n):
-        self.meta_common.start_datetime = n
-
-    @property
-    def material(self):
-        return self.meta_common.material
-
-    @material.setter
-    def material(self, n):
-        self.meta_common.material = n
-
-    @property
-    def mass(self):
-        return self.meta_common.mass
-
-    @mass.setter
-    def mass(self, n):
-        self.meta_common.mass = n
-
-    @property
-    def tot_mass(self):
-        return self.meta_common.tot_mass
-
-    @tot_mass.setter
-    def tot_mass(self, n):
-        self.meta_common.tot_mass = n
-
-    @property
-    def active_electrode_area(self):
-        return self.meta_common.active_electrode_area
-
-    @active_electrode_area.setter
-    def active_electrode_area(self, area):
-        self.meta_common.active_electrode_area = area
-
-    @property
-    def cell_name(self):
-        return self.meta_common.cell_name
-
-    @cell_name.setter
-    def cell_name(self, cell_name):
-        self.meta_common.cell_name = cell_name
-
-    @property
-    def nom_cap(self):
-        return self.meta_common.nom_cap
-
-    @nom_cap.setter
-    def nom_cap(self, value):
-        if value < 0.1:
-            warnings.warn(
-                f"POSSIBLE BUG: NOMINAL CAPACITY LESS THAN 0.1 ({value}).",
-                DeprecationWarning,
-                stacklevel=2,
-            )
-        self.meta_common.nom_cap = value  # nominal capacity
-
-    @staticmethod
-    def _header_str(hdr):
-        txt = "\n"
-        txt += 80 * "-" + "\n"
-        txt += f" {hdr} ".center(80) + "\n"
-        txt += 80 * "-" + "\n"
-        return txt
-
-    def __str__(self):
-        txt = "<Data>\n"
-        txt += "loaded from file(s)\n"
-        if isinstance(self.loaded_from, (list, tuple)):
-            for f in self.loaded_from:
-                txt += str(f)
-                txt += "\n"
-
-        else:
-            txt += str(self.loaded_from)
-            txt += "\n"
-        txt += "\n* GLOBAL\n"
-        txt += f"material:            {self.meta_common.material}\n"
-        txt += f"mass (active):       {self.meta_common.mass}\n"
-        txt += f"mass (total):        {self.meta_common.tot_mass}\n"
-        txt += f"nominal capacity:    {self.meta_common.nom_cap}\n"
-        txt += f"test ID:             {self.meta_test_dependent.test_ID}\n"
-        txt += f"channel index:       {self.meta_test_dependent.channel_index}\n"
-        txt += f"creator:             {self.meta_test_dependent.creator}\n"
-        txt += f"schedule file name:  {self.meta_test_dependent.schedule_file_name}\n"
-
-        try:
-            if self.start_datetime:
-                start_datetime_str = xldate_as_datetime(self.start_datetime)
-            else:
-                start_datetime_str = "Not given"
-        except AttributeError:
-            start_datetime_str = "NOT READABLE YET"
-
-        txt += f"start-date:         {start_datetime_str}\n"
-
-        txt += self._header_str("DATA")
-        try:
-            txt += str(self.raw.describe())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-
-        txt += self._header_str("SUMMARY")
-        try:
-            txt += str(self.summary.describe())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-
-        txt += self._header_str("STEP TABLE")
-        try:
-            txt += str(self.steps.describe())
-            txt += str(self.steps.head())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-
-        txt += self._header_str("RAW UNITS")
-        try:
-            txt += str(self.raw_units)
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-        return txt
-
-    def populate_defaults(self):
-        # modify this method upon need
-        logging.debug("checking and populating defaults for the cell")
-
-        if not self.active_electrode_area:
-            self.active_electrode_area = 1.0
-            logging.debug(
-                f"active_electrode_area not set -> setting to: {self.active_electrode_area}"
-            )
-
-        if not self.mass:
-            self.mass = 1.0
-            logging.debug(f"mass not set -> setting to: {self.mass}")
-
-        if not self.tot_mass:
-            self.tot_mass = self.mass
-            logging.debug(
-                f"total mass not set -> setting to same as mass: {self.tot_mass}"
-            )
-
-        return True
-
-    @property
-    def empty(self):
-        if self.has_data:
-            return False
-        return True
-
-    @property
-    def has_summary(self):
-        """check if the summary table exists"""
-        try:
-            empty = self.summary.empty
-            # TODO: check if the summary has the expected columns
-            #  (since it can be unprocessed directly from the raw data)
-        except AttributeError:
-            empty = True
-        return not empty
-
-    @property
-    def has_steps(self):
-        """check if the step table exists"""
-        try:
-            empty = self.steps.empty
-        except AttributeError:
-            empty = True
-        return not empty
-
-    @property
-    def has_data(self):
-        try:
-            empty = self.raw.empty
-        except AttributeError:
-            empty = True
-        return not empty
-
-
-class InstrumentFactory:
-    def __init__(self):
-        self._builders = {}
-        self._kwargs = {}
-
-    def register_builder(self, key: str, builder: Tuple[str, Any], **kwargs) -> None:
-        """register an instrument loader module.
-
-        Args:
-            key: instrument id
-            builder: (module_name, module_path)
-            **kwargs: stored in the factory (will be used in the future for allowing to set
-               defaults to the builders to allow for using .query).
-        """
-
-        logging.debug(f"Registering instrument {key}")
-        self._builders[key] = builder
-        self._kwargs[key] = kwargs
-
-    def create(self, key: Union[str, None], **kwargs):
-        """Create the instrument loader module and initialize the loader class.
-
-        Args:
-            key: instrument id
-            **kwargs: sent to the initializer of the loader class.
-
-        Returns:
-            instance of loader class.
-        """
-
-        module_name, module_path = self._builders.get(key, (None, None))
-
-        # constant:
-        instrument_class = "DataLoader"
-
-        if not module_name:
-            raise ValueError(key)
-
-        spec = importlib.util.spec_from_file_location(module_name, module_path)
-        loader_module = importlib.util.module_from_spec(spec)
-        sys.modules[module_name] = loader_module  # noqa
-        spec.loader.exec_module(loader_module)
-        cls = getattr(loader_module, instrument_class)
-
-        # TODO: get stored kwargs from self.__kwargs and merge them with the supplied kwargs
-        #  (supplied should have preference)
-
-        return cls(**kwargs)
-
-    def query(self, key: str, variable: str) -> Any:
-        """performs a get_params lookup for the instrument loader.
-
-        Args:
-            key: instrument id.
-            variable: the variable you want to lookup.
-
-        Returns:
-            The value of the variable if the loaders get_params method supports it.
-        """
-        loader = self.create(key)
-        try:
-            value = loader.get_params(variable)
-            logging.debug(f"GOT {variable}={value} for {key}")
-            return value
-
-        except (AttributeError, NotImplementedError, KeyError):
-            logging.debug(f"COULD NOT RETRIEVE {variable} for {key}")
-        return
-
-
-def generate_default_factory():
-    """This function searches for all available instrument readers
-    and registers them in an InstrumentFactory instance.
-
-    Returns:
-        InstrumentFactory
-    """
-    instrument_factory = InstrumentFactory()
-    instruments = find_all_instruments()
-    for instrument_id, instrument in instruments.items():
-        instrument_factory.register_builder(instrument_id, instrument)
-    return instrument_factory
-
-
-def find_all_instruments() -> Dict[str, Tuple[str, pathlib.Path]]:
-    """finds all the supported instruments"""
-
-    import cellpy.readers.instruments as hard_coded_instruments_site
-
-    instruments_found = {}
-    logging.debug("Searching for modules in base instrument folder:")
-
-    hard_coded_instruments_site = pathlib.Path(
-        hard_coded_instruments_site.__file__
-    ).parent
-    modules_in_hard_coded_instruments_site = [
-        s
-        for s in hard_coded_instruments_site.glob("*.py")
-        if not (
-            str(s.name).startswith("_")
-            or str(s.name).startswith("dev_")
-            or str(s.name).startswith("base")
-            or str(s.name).startswith("backup")
-            or str(s.name).startswith("registered_loaders")
-        )
-    ]
-
-    for module_path in modules_in_hard_coded_instruments_site:
-        module_name = module_path.name.rstrip(".py")
-        logging.debug(module_name)
-        instruments_found[module_name] = (
-            module_name,
-            module_path,
-        )
-        logging.debug(" -> added")
-
-    logging.debug("Searching for module configurations in user instrument folder:")
-    # These are only yaml-files and should ideally import the appropriate
-    #    custom loader class
-    # Might not be needed.
-    logging.debug("- Not implemented yet")
-
-    logging.debug("Searching for modules through plug-ins:")
-    # Not sure how to do this yet. Probably also some importlib trick.
-    logging.debug("- Not implemented yet")
-    return instruments_found
-
-
-def identify_last_data_point(data):
-    """Find the last data point and store it in the fid instance"""
-
-    logging.debug("searching for last data point")
-    hdr_data_point = HEADERS_NORMAL.data_point_txt
-    try:
-        if hdr_data_point in data.raw.columns:
-            last_data_point = data.raw[hdr_data_point].max()
-        else:
-            last_data_point = data.raw.index.max()
-    except AttributeError:
-        logging.debug("AttributeError - setting last data point to 0")
-        last_data_point = 0
-    if not last_data_point > 0:
-        last_data_point = 0
-    data.raw_data_files[0].last_data_point = last_data_point
-    logging.debug(f"last data point: {last_data_point}")
-    return data
-
-
-# TODO: move this to internals/core
-def check64bit(current_system="python"):
-    """checks if you are on a 64-bit platform"""
-    if current_system == "python":
-        return sys.maxsize > 2147483647
-    elif current_system == "os":
-        import platform
-
-        pm = platform.machine()
-        if pm != ".." and pm.endswith("64"):  # recent Python (not Iron)
-            return True
-        else:
-            if "PROCESSOR_ARCHITEW6432" in os.environ:
-                return True  # 32 bit program running on 64-bit Windows
-            try:
-                # 64-bit Windows 64 bit program
-                return os.environ["PROCESSOR_ARCHITECTURE"].endswith("64")
-            except IndexError:
-                pass  # not Windows
-            try:
-                # this often works in Linux
-                return "64" in platform.architecture()[0]
-            except Exception:  # noqa
-                # is an older version of Python, assume also an older os@
-                # (best we can guess)
-                return False
-
-
-# TODO: move this to internals/core
-def humanize_bytes(b, precision=1):
-    """Return a humanized string representation of a number of b."""
-
-    abbrevs = (
-        (1 << 50, "PB"),
-        (1 << 40, "TB"),
-        (1 << 30, "GB"),
-        (1 << 20, "MB"),
-        (1 << 10, "kB"),
-        (1, "b"),
-    )
-    if b == 1:
-        return "1 byte"
-    for factor, suffix in abbrevs:
-        if b >= factor:
-            break
-    # return '%.*f %s' % (precision, old_div(b, factor), suffix)
-    return "%.*f %s" % (precision, b // factor, suffix)  # noqa
-
-
-# TODO: move this to internals/core
-def xldate_as_datetime(xldate, datemode=0, option="to_datetime"):
-    """Converts a xls date stamp to a more sensible format.
-
-    Args:
-        xldate (str, int): date stamp in Excel format.
-        datemode (int): 0 for 1900-based, 1 for 1904-based.
-        option (str): option in ("to_datetime", "to_float", "to_string"),
-            return value
-
-    Returns:
-        datetime (datetime object, float, or string).
-
-    """
-
-    # This does not work for numpy-arrays
-
-    if option == "to_float":
-        d = (xldate - 25589) * 86400.0
-    else:
-        try:
-            d = datetime.datetime(1899, 12, 30) + datetime.timedelta(
-                days=xldate + 1462 * datemode
-            )
-            # date_format = "%Y-%m-%d %H:%M:%S:%f" # with microseconds,
-            # Excel cannot cope with this!
-            if option == "to_string":
-                date_format = "%Y-%m-%d %H:%M:%S"  # without microseconds
-                d = d.strftime(date_format)
-        except TypeError:
-            logging.info(f"The date is not of correct type [{xldate}]")
-            d = xldate
-    return d
-
-
-def collect_capacity_curves(
-    cell,
-    direction="charge",
-    trim_taper_steps=None,
-    steps_to_skip=None,
-    steptable=None,
-    max_cycle_number=None,
-    **kwargs,
-):
-    """Create a list of pandas.DataFrames, one for each charge step.
-
-    The DataFrames are named by its cycle number.
-
-    Args:
-        cell (``CellpyCell``):  object
-        direction (str):
-        trim_taper_steps (integer): number of taper steps to skip (counted
-            from the end, i.e. 1 means skip last step in each cycle).
-        steps_to_skip (list): step numbers that should not be included.
-        steptable (``pandas.DataFrame``): optional steptable.
-        max_cycle_number (int): only select cycles up to this value.
-
-    Returns:
-        list of pandas.DataFrames,
-        list of cycle numbers,
-        minimum voltage value,
-        maximum voltage value
-
-    """
-
-    # TODO: should allow for giving cycle numbers as input (e.g. cycle=[1, 2, 10]
-    #  or cycle=2), not only max_cycle_number. Intermediate solution:
-    #  The cycle keyword will not break the method but raise a warning:
-    for arg in kwargs:
-        if arg in ["cycle", "cycles"]:
-            logging.warning(
-                f"{arg} is not implemented yet, but might exist in newer versions of cellpy."
-            )
-        else:
-            logging.warning(
-                f"collect_capacity_curve received unknown key-word argument: {arg=}"
-            )
-
-    minimum_v_value = np.Inf
-    maximum_v_value = -np.Inf
-    charge_list = []
-    cycles = kwargs.pop("cycle", None)
-
-    if cycles is None:
-        cycles = cell.get_cycle_numbers()
-
-    if max_cycle_number is None:
-        max_cycle_number = max(cycles)
-
-    for cycle in cycles:
-        if cycle > max_cycle_number:
-            break
-        try:
-            if direction == "charge":
-                q, v = cell.get_ccap(
-                    cycle,
-                    trim_taper_steps=trim_taper_steps,
-                    steps_to_skip=steps_to_skip,
-                    steptable=steptable,
-                    as_frame=False,
-                )
-            else:
-                q, v = cell.get_dcap(
-                    cycle,
-                    trim_taper_steps=trim_taper_steps,
-                    steps_to_skip=steps_to_skip,
-                    steptable=steptable,
-                    as_frame=False,
-                )
-
-        except NullData as e:
-            logging.warning(e)
-            d = pd.DataFrame()
-            d.name = cycle
-            charge_list.append(d)
-        else:
-            d = pd.DataFrame({"q": q, "v": v})
-            # d.name = f"{cycle}"
-            d.name = cycle
-            charge_list.append(d)
-            v_min = v.min()
-            v_max = v.max()
-            if v_min < minimum_v_value:
-                minimum_v_value = v_min
-            if v_max > maximum_v_value:
-                maximum_v_value = v_max
-    return charge_list, cycles, minimum_v_value, maximum_v_value
-
-
-def interpolate_y_on_x(
-    df,
-    x=None,
-    y=None,
-    new_x=None,
-    dx=10.0,
-    number_of_points=None,
-    direction=1,
-    **kwargs,
-):
-    """Interpolate a column based on another column.
-
-    Args:
-        df: DataFrame with the (cycle) data.
-        x: Column name for the x-value (defaults to the step-time column).
-        y: Column name for the y-value (defaults to the voltage column).
-        new_x (numpy array or None): Interpolate using these new x-values
-            instead of generating x-values based on dx or number_of_points.
-        dx: step-value (defaults to 10.0)
-        number_of_points: number of points for interpolated values (use
-            instead of dx and overrides dx if given).
-        direction (-1,1): if direction is negative, then invert the
-            x-values before interpolating.
-        **kwargs: arguments passed to ``scipy.interpolate.interp1d``
-
-    Returns: DataFrame with interpolated y-values based on given or
-        generated x-values.
-
-    """
-
-    # TODO: allow for giving a fixed interpolation range (x-values).
-    #  Remember to treat extrapolation properly (e.g. replace with NaN?).
-
-    if x is None:
-        x = df.columns[0]
-    if y is None:
-        y = df.columns[1]
-
-    xs = df[x].values
-    ys = df[y].values
-
-    if direction > 0:
-        x_min = xs.min()
-        x_max = xs.max()
-    else:
-        x_max = xs.min()
-        x_min = xs.max()
-        dx = -dx
-
-    bounds_error = kwargs.pop("bounds_error", False)
-    f = interpolate.interp1d(xs, ys, bounds_error=bounds_error, **kwargs)
-    if new_x is None:
-        if number_of_points:
-            new_x = np.linspace(x_min, x_max, number_of_points)
-        else:
-            new_x = np.arange(x_min, x_max, dx)
-
-    new_y = f(new_x)
-
-    new_df = pd.DataFrame({x: new_x, y: new_y})
-
-    return new_df
-
-
-def group_by_interpolate(
-    df,
-    x=None,
-    y=None,
-    group_by=None,
-    number_of_points=100,
-    tidy=False,
-    individual_x_cols=False,
-    header_name="Unit",
-    dx=10.0,
-    generate_new_x=True,
-):
-    """Do a pandas.DataFrame.group_by and perform interpolation for all groups.
-
-    This function is a wrapper around an internal interpolation function in
-    cellpy (that uses scipy.interpolate.interp1d) that combines doing a group-by
-    operation and interpolation.
-
-    Args:
-        df (pandas.DataFrame): the dataframe to morph.
-        x (str): the header for the x-value
-            (defaults to normal header step_time_txt) (remark that the default
-            group_by column is the cycle column, and each cycle normally
-            consist of several steps (so you risk interpolating / merging
-            several curves on top of each other (not good)).
-        y (str): the header for the y-value
-            (defaults to normal header voltage_txt).
-        group_by (str): the header to group by
-            (defaults to normal header cycle_index_txt)
-        number_of_points (int): if generating new x-column, how many values it
-            should contain.
-        tidy (bool): return the result in tidy (i.e. long) format.
-        individual_x_cols (bool): return as xy xy xy ... data.
-        header_name (str): name for the second level of the columns (only
-            applies for xy xy xy ... data) (defaults to "Unit").
-        dx (float): if generating new x-column and number_of_points is None or
-            zero, distance between the generated values.
-        generate_new_x (bool): create a new x-column by
-            using the x-min and x-max values from the original dataframe where
-            the method is set by the number_of_points key-word:
-
-            1)  if number_of_points is not None (default is 100):
-
-                ```
-                new_x = np.linspace(x_max, x_min, number_of_points)
-                ```
-            2)  else:
-                ```
-                new_x = np.arange(x_max, x_min, dx)
-                ```
-
-
-    Returns: pandas.DataFrame with interpolated x- and y-values. The returned
-        dataframe is in tidy (long) format for tidy=True.
-
-    """
-    # TODO: @jepe - create more tests
-    time_00 = time.time()
-    if x is None:
-        x = HEADERS_NORMAL.step_time_txt
-    if y is None:
-        y = HEADERS_NORMAL.voltage_txt
-    if group_by is None:
-        group_by = [HEADERS_NORMAL.cycle_index_txt]
-
-    if not isinstance(group_by, (list, tuple)):
-        group_by = [group_by]
-
-    if not generate_new_x:
-        # check if it makes sense
-        if (not tidy) and (not individual_x_cols):
-            logging.warning("Unlogical condition")
-            generate_new_x = True
-
-    new_x = None
-
-    if generate_new_x:
-        x_max = df[x].max()
-        x_min = df[x].min()
-        if number_of_points:
-            new_x = np.linspace(x_max, x_min, number_of_points)
-        else:
-            new_x = np.arange(x_max, x_min, dx)
-
-    new_dfs = []
-    keys = []
-
-    for name, group in df.groupby(group_by):
-        keys.append(name)
-        if not isinstance(name, (list, tuple)):
-            name = [name]
-
-        new_group = interpolate_y_on_x(
-            group, x=x, y=y, new_x=new_x, number_of_points=number_of_points, dx=dx
-        )
-
-        if tidy or (not tidy and not individual_x_cols):
-            for i, j in zip(group_by, name):
-                new_group[i] = j
-        new_dfs.append(new_group)
-
-    if tidy:
-        new_df = pd.concat(new_dfs)
-    else:
-        if individual_x_cols:
-            new_df = pd.concat(new_dfs, axis=1, keys=keys)
-            group_by.append(header_name)
-            new_df.columns.names = group_by
-        else:
-            new_df = pd.concat(new_dfs)
-            new_df = new_df.pivot(index=x, columns=group_by[0], values=y)
-
-    time_01 = time.time() - time_00
-    logging.debug(f"duration: {time_01} seconds")
-    return new_df
-
-
-def convert_from_simple_unit_label_to_string_unit_label(k, v):
-    old_raw_units = {
-        "current": 1.0,
-        "charge": 1.0,
-        "voltage": 1.0,
-        "time": 1.0,
-        "resistance": 1.0,
-        "power": 1.0,
-        "energy": 1.0,
-        "frequency": 1.0,
-        "mass": 0.001,
-        "nominal_capacity": 1.0,
-        "specific_gravimetric": 1.0,
-        "specific_areal": 1.0,
-        "specific_volumetric": 1.0,
-        "length": 1.0,
-        "area": 1.0,
-        "volume": 1.0,
-        "temperature": 1.0,
-        "pressure": 1.0,
-    }
-    old_unit = old_raw_units[k]
-    value = v / old_unit
-    default_units = get_default_raw_units()
-
-    new_unit = default_units[k]
-    value = Q(value, new_unit)
-    str_value = str(value)
-    return str_value
-
-
-# ---------------- LOCAL DEV TESTS ----------------
-
-
-def check_convert_from_simple_unit_label_to_string_unit_label():
-    k = "resistance"
-    v = 1.0
-    n = convert_from_simple_unit_label_to_string_unit_label(k, v)
-    print(n)
-
-
-def check_path_things():
-    p = "//jepe@mymachine.my.no/./path/file.txt"
-    p2 = pathlib.Path(p)
-    print(f"{p2=}")
-    print(f"{p2.resolve()=}")
-    print(f"{p2.drive=}")
-    print(f"{p2.as_uri()=}")
-    print(f"{p2.root=}")
-    print(f"{p2.anchor=}")
-    print(f"{p2.parent=}")
-    print(f"{p2.name=}")
-    print(f"{p2.stem=}")
-    print(f"{p2.suffix=}")
-    print(f"{p2.suffixes=}")
-    print(f"{p2.parts=}")
-    print(f"{p2.is_absolute()=}")
-    print(f"{p2.is_reserved()=}")
-    print(f"{p2.is_dir()=}")
-    print(f"{p2.is_file()=}")
-
-    try:
-        print(f"{p2.is_socket()=}")
-    except NotImplementedError as e:
-        print(f"{e}")
-    try:
-        print(f"{p2.is_mount()=}")
-    except NotImplementedError as e:
-        print(f"{e}")
-    try:
-        print(f"{p2.is_symlink()=}")
-    except NotImplementedError as e:
-        print(f"{e}")
-
-    try:
-        print(f"{p2.owner()=}")
-    except NotImplementedError as e:
-        print(f"{e}")
-
-    try:
-        print(f"{p2.group()=}")
-    except NotImplementedError as e:
-        print(f"{e}")
-
-    print(f"{p2.exists()=}")
-
-
-def check_another_path_things():
-    p01 = r"C:\scripting\cellpy\testdata\data\20160805_test001_45_cc_01.res"
-    p02 = r"ssh://jepe@server.no/home/jepe/cellpy/testdata/data/20160805_test001_45_cc_01.res"
-    p03 = r"scripting\cellpy\testdata\data\20160805_test001_45_cc_01.res"
-    p04 = r"..\data\20160805_test001_45_cc_01.res"
-    p05 = pathlib.Path(p01)
-    p06 = pathlib.Path(p02)
-    for p in [p01, p02, p03, p04, p05, p06]:
-        print(f"{p}".center(110, "-"))
-        p2 = OtherPath(p)
-        print(f"{p2=}")
-        print(p2)
-        print(f"{p2.resolve()=}")
-        print(f"{p2.drive=}")
-        print(f"{p2.exists()=}")
-        print(f"{p2._is_external=}")  # noqa
-        print(f"{p2._location=}")  # noqa
-        print(f"{p2._uri_prefix=}")  # noqa
-        print(f"{p2.resolve()=}")
-        if p2.is_absolute():
-            print(f"{p2.as_uri()=}")
-        print(f"{p2.is_external=}")
-        print(f"{p2.location=}")
-        print(f"{p2.uri_prefix=}")
-        print(f"{p2.root=}")
-        print(f"{p2.anchor=}")
-        print(f"{p2.parent=}")
-        print(f"{p2.name=}")
-        print(f"{p2.stem=}")
-        print(f"{p2.suffix=}")
-        print(f"{p2.suffixes=}")
-        print(f"{p2.parts=}")
-        print(f"{type(p2)}")
-        print(f"{isinstance(p2, pathlib.Path)=}")
-        print(f"{isinstance(p2, OtherPath)=}")
-        print()
-
-
-def check_how_other_path_works():
-    p01 = r"C:\scripting\cellpy\testdata\data\20160805_test001_45_cc_01.res"
-    p02 = r"ssh://jepe@somewhere.else.no/home/jepe/cellpy/testdata/data/20160805_test001_45_cc_01.res"
-    p03 = None
-    p03b = OtherPath(p03)
-    p05 = pathlib.Path(p01)
-    p06 = pathlib.Path(p02)
-    p07 = OtherPath(p01)
-    p08 = OtherPath(p02)
-    print(80 * "=")
-    for p in [p01, p02, p03, p03b, p05, p06, p07, p08]:
-        print(f"{p}".center(110, "-"))
-        print(f"{type(p)}".center(110, "*"))
-        p2 = OtherPath(p)
-        print(f"{p2=}")
-        print(p2)
-        print(f"{p2.raw_path=}")
-        print(f"{p2.is_external=}")
-        print(f"{p2.location=}")
-        print(f"{p2.uri_prefix=}")
-        print(f"{p2._original=}")  # noqa
-        print(f"{p2.full_path=}")
-        print(f"{p2.parts=}")
-
-
-def check_copy_external_file():
-    from cellpy import prms
-
-    prms.Paths.env_file = r"C:\scripting\cellpy\local\.env_cellpy"
-    dst = r"C:\scripting\cellpy\tmp\20210629_moz_cat_02_cc_01.res"
-    src = "ssh://jepe@not.in.no/home/jepe@ad.ife.no/Temp/20210629_moz_cat_02_cc_01.res"
-    copy_external_file(src, dst)
-
-
-if __name__ == "__main__":
-    check_how_other_path_works()
+""" This module contains several of the most important classes used in cellpy.
+
+It also contains functions that are used by readers and utils.
+And it has the file version definitions.
+"""
+import abc
+import datetime
+import importlib
+import logging
+import os
+import pathlib
+import pickle
+import sys
+import time
+import warnings
+from typing import Any, Tuple, Dict, List, Union, TypeVar
+
+import numpy as np
+import pandas as pd
+import pint
+from scipy import interpolate
+
+from cellpy.exceptions import NullData
+from cellpy.internals.core import OtherPath
+from cellpy.parameters.internal_settings import (
+    get_headers_normal,
+    get_headers_step_table,
+    get_headers_summary,
+    get_default_raw_units,
+    get_default_raw_limits,
+    CellpyMetaCommon,
+    CellpyMetaIndividualTest,
+)
+
+HEADERS_NORMAL = get_headers_normal()  # TODO @jepe refactor this (not needed)
+HEADERS_SUMMARY = get_headers_summary()  # TODO @jepe refactor this (not needed)
+HEADERS_STEP_TABLE = get_headers_step_table()  # TODO @jepe refactor this (not needed)
+
+
+# pint (https://pint.readthedocs.io/en/stable/)
+ureg = pint.UnitRegistry()
+ureg.default_format = "~P"
+Q = ureg.Quantity
+
+
+# TODO: in future versions (maybe 1.1.0) we should "copy-paste" the whole pathlib module
+#  from CPython and add the functionality we need to it. This will make
+#  it easier to keep up with changes in the pathlib module.
+
+
+# https://stackoverflow.com/questions/60067953/
+# 'is-it-possible-to-specify-the-pickle-protocol-when-writing-pandas-to-hdf5
+class PickleProtocol:
+    """Context for using a specific pickle protocol."""
+
+    def __init__(self, level):
+        self.previous = pickle.HIGHEST_PROTOCOL
+        self.level = level
+
+    def __enter__(self):
+        importlib.reload(pickle)
+        pickle.HIGHEST_PROTOCOL = self.level
+
+    def __exit__(self, *exc):
+        importlib.reload(pickle)
+        pickle.HIGHEST_PROTOCOL = self.previous
+
+
+def pickle_protocol(level):
+    return PickleProtocol(level)
+
+
+class BaseDbReader(metaclass=abc.ABCMeta):
+    """Base class for database readers."""
+
+    @abc.abstractmethod
+    def select_batch(self, batch: str) -> List[int]:
+        pass
+
+    @abc.abstractmethod
+    def get_mass(self, pk: int) -> float:
+        pass
+
+    @abc.abstractmethod
+    def get_area(self, pk: int) -> float:
+        pass
+
+    @abc.abstractmethod
+    def get_loading(self, pk: int) -> float:
+        pass
+
+    @abc.abstractmethod
+    def get_nom_cap(self, pk: int) -> float:
+        pass
+
+    @abc.abstractmethod
+    def get_total_mass(self, pk: int) -> float:
+        pass
+
+    @abc.abstractmethod
+    def get_cell_name(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def get_cell_type(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def get_label(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def get_comment(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def get_group(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def get_args(self, pk: int) -> dict:
+        pass
+
+    @abc.abstractmethod
+    def get_experiment_type(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def get_instrument(self, pk: int) -> str:
+        pass
+
+    @abc.abstractmethod
+    def inspect_hd5f_fixed(self, pk: int) -> int:
+        pass
+
+    @abc.abstractmethod
+    def get_by_column_label(self, pk: int, name: str) -> Any:
+        pass
+
+    @abc.abstractmethod
+    def from_batch(
+        self,
+        batch_name: str,
+        include_key: bool = False,
+        include_individual_arguments: bool = False,
+    ) -> dict:
+        pass
+
+
+class FileID:
+    """class for storing information about the raw-data files.
+
+    This class is used for storing and handling raw-data file information.
+    It is important to keep track of when the data was extracted from the
+    raw-data files so that it is easy to know if the hdf5-files used for
+    @storing "treated" data is up-to-date.
+
+    Attributes:
+        name (str): Filename of the raw-data file.
+        full_name (str): Filename including path of the raw-data file.
+        size (float): Size of the raw-data file.
+        last_modified (datetime): Last time of modification of the raw-data
+            file.
+        last_accessed (datetime): last time of access of the raw-data file.
+        last_info_changed (datetime): st_ctime of the raw-data file.
+        location (str): Location of the raw-data file.
+
+    """
+
+    def __init__(self, filename: Union[str, OtherPath] = None, is_db: bool = False):
+        """Initialize the FileID class."""
+
+        self.is_db: bool = is_db
+        self._last_data_point: Optional[int] = None
+        self.name: Optional[str] = None
+        self.full_name: Optional[str] = None
+        self.size: Optional[int] = None
+        self.last_modified: Optional[int] = None
+        self.last_accessed: Optional[int] = None
+        self.last_info_changed: Optional[int] = None
+        self.location: Optional[int] = None
+
+        if self.is_db:
+            self._from_db(filename)
+            return
+
+        make_defaults = True
+        if filename is not None:
+            if not isinstance(filename, OtherPath):
+                logging.debug("filename is not an OtherPath object")
+                filename = OtherPath(filename)
+
+            if filename.is_file():
+                self.populate(filename)
+                make_defaults = False
+
+        if make_defaults:
+            self.name = None
+            self.full_name = None
+            self.size = 0
+            self.last_modified = None
+            self.last_accessed = None
+            self.last_info_changed = None
+            self.location = None
+            self._last_data_point = 0  # to be used later when updating is implemented
+
+    def __str__(self):
+        """Return a string representation of the FileID object."""
+        try:
+            if self.is_db:
+                txt = "\n<fileID><is_db>\n"
+            else:
+                txt = "\n<fileID><is_file>\n"
+        except AttributeError:
+            txt = "\n<fileID><is_file>\n"
+
+        txt += f"full name: {self.full_name}\n"
+        txt += f"name: {self.name}\n"
+        txt += f"location: {self.location}\n"
+
+        if self.last_modified is not None:
+            txt += f"modified: {self.last_modified}\n"
+        else:
+            txt += "modified: NAN\n"
+
+        if self.size is not None:
+            txt += f"size: {self.size}\n"
+        else:
+            txt += "size: NAN\n"
+
+        txt += f"last data point: {self.last_data_point}\n"
+
+        return txt
+
+    def _from_db(self, filename):
+        self.name = filename
+        self.full_name = filename
+        self.size = 0
+        self.last_modified = None
+        self.last_accessed = None
+        self.last_info_changed = None
+        self.location = None
+        self._last_data_point = 0
+
+    @property
+    def last_data_point(self):
+        # TODO: consider including a method here to find the last data point (raw data)
+        # ideally, this value should be set when loading the raw data before
+        # merging files (if it consists of several files)
+        return self._last_data_point
+
+    @last_data_point.setter
+    def last_data_point(self, value):
+        self._last_data_point = value
+
+    def populate(self, filename: Union[str, OtherPath]):
+        """Finds the file-stats and populates the class with stat values.
+
+        Args:
+            filename (str, OtherPath): name of the file.
+        """
+        if not isinstance(filename, OtherPath):
+            logging.debug("filename is not an OtherPath object")
+            filename = OtherPath(filename)
+
+        if filename.is_file():
+            fid_st = filename.stat()
+            self.name = filename.name
+            self.full_name = filename.full_path
+            self.size = fid_st.st_size
+            self.last_modified = fid_st.st_mtime
+            self.last_accessed = fid_st.st_atime
+            self.last_info_changed = fid_st.st_ctime
+            self.location = str(filename.parent)
+
+    def get_raw(self):
+        """Get a list with information about the file.
+
+        The returned list contains name, size, last_modified and location.
+        """
+        return [self.name, self.size, self.last_modified, self.location]
+
+    def get_name(self):
+        """Get the filename."""
+        return self.name
+
+    def get_size(self):
+        """Get the size of the file."""
+        return self.size
+
+    def get_last(self):
+        """Get last modification time of the file."""
+        return self.last_modified
+
+
+class Data:
+    """Object to store data for a cell-test.
+
+    This class is used for storing all the relevant data for a cell-test, i.e. all
+    the data collected by the tester as stored in the raw-files, and user-provided
+    metadata about the cell-test.
+    """
+
+    def _repr_html_(self):
+        txt = f"<h2>Data-object</h2> id={hex(id(self))}"
+        txt += "<p>"
+        for p in dir(self):
+            if not p.startswith("_"):
+                if p not in ["raw", "summary", "steps", "logger"]:
+                    value = self.__getattribute__(p)
+                    txt += f"<b>{p}</b>: {value}<br>"
+        txt += "</p>"
+        try:
+            raw_txt = f"<p><b>raw data-frame (summary)</b><br>{self.raw.describe()._repr_html_()}</p>"  # noqa
+            raw_txt += f"<p><b>raw data-frame (head)</b><br>{self.raw.head()._repr_html_()}</p>"  # noqa
+        except AttributeError:
+            raw_txt = "<p><b>raw data-frame </b><br> not found!</p>"
+        except ValueError:
+            raw_txt = "<p><b>raw data-frame </b><br> does not contain any columns!</p>"
+
+        try:
+            summary_txt = f"<p><b>summary data-frame (summary)</b><br>{self.summary.describe()._repr_html_()}</p>"  # noqa
+            summary_txt += f"<p><b>summary data-frame (head)</b><br>{self.summary.head()._repr_html_()}</p>"  # noqa
+        except AttributeError:
+            summary_txt = "<p><b>summary data-frame </b><br> not found!</p>"
+        except ValueError:
+            summary_txt = (
+                "<p><b>summary data-frame </b><br> does not contain any columns!</p>"
+            )
+
+        try:
+            steps_txt = f"<p><b>steps data-frame (summary)</b><br>{self.steps.describe()._repr_html_()}</p>"  # noqa
+            steps_txt += f"<p><b>steps data-frame (head)</b><br>{self.steps.head()._repr_html_()}</p>"  # noqa
+        except AttributeError:
+            steps_txt = "<p><b>steps data-frame </b><br> not found!</p>"
+        except ValueError:
+            steps_txt = (
+                "<p><b>steps data-frame </b><br> does not contain any columns!</p>"
+            )
+
+        return txt + summary_txt + steps_txt + raw_txt
+
+    def __init__(self, **kwargs):
+        self.logger = logging.getLogger(__name__)
+        self.logger.debug("created DataSet instance")
+
+        self.raw_data_files = []
+        self.raw_data_files_length = []
+        self.loaded_from = None
+        self._raw_id = None
+        self._internal_test_number = None
+        self.raw_units = get_default_raw_units()
+        self.raw_limits = get_default_raw_limits()
+
+        self.raw = pd.DataFrame()
+        self.summary = pd.DataFrame()
+        self.steps = pd.DataFrame()
+
+        self.meta_common = CellpyMetaCommon()
+        # TODO: v2.0 consider making this a list of several CellpyMetaIndividualTest
+        self.meta_test_dependent = CellpyMetaIndividualTest()
+
+        # custom meta-data
+        for k in kwargs:
+            if hasattr(self, k):
+                setattr(self, k, kwargs[k])
+
+    # ---------------- left-over-properties v7 -> v8 -----------------
+    # these now belong to the CellpyMeta attributes
+    #   however, since they are extensively used in the instrument
+    #   loaders and cellreader, they are also accessible here as properties
+
+    @property
+    def raw_id(self):
+        return self.meta_common.raw_id
+
+    @property
+    def start_datetime(self):
+        return self.meta_common.start_datetime
+
+    @start_datetime.setter
+    def start_datetime(self, n):
+        self.meta_common.start_datetime = n
+
+    @property
+    def material(self):
+        return self.meta_common.material
+
+    @material.setter
+    def material(self, n):
+        self.meta_common.material = n
+
+    @property
+    def mass(self):
+        return self.meta_common.mass
+
+    @mass.setter
+    def mass(self, n):
+        self.meta_common.mass = n
+
+    @property
+    def tot_mass(self):
+        return self.meta_common.tot_mass
+
+    @tot_mass.setter
+    def tot_mass(self, n):
+        self.meta_common.tot_mass = n
+
+    @property
+    def active_electrode_area(self):
+        return self.meta_common.active_electrode_area
+
+    @active_electrode_area.setter
+    def active_electrode_area(self, area):
+        self.meta_common.active_electrode_area = area
+
+    @property
+    def cell_name(self):
+        return self.meta_common.cell_name
+
+    @cell_name.setter
+    def cell_name(self, cell_name):
+        self.meta_common.cell_name = cell_name
+
+    @property
+    def nom_cap(self):
+        return self.meta_common.nom_cap
+
+    @nom_cap.setter
+    def nom_cap(self, value):
+        if value < 0.1:
+            warnings.warn(
+                f"POSSIBLE BUG: NOMINAL CAPACITY LESS THAN 0.1 ({value}).",
+                DeprecationWarning,
+                stacklevel=2,
+            )
+        self.meta_common.nom_cap = value  # nominal capacity
+
+    @staticmethod
+    def _header_str(hdr):
+        txt = "\n"
+        txt += 80 * "-" + "\n"
+        txt += f" {hdr} ".center(80) + "\n"
+        txt += 80 * "-" + "\n"
+        return txt
+
+    def __str__(self):
+        txt = "<Data>\n"
+        txt += "loaded from file(s)\n"
+        if isinstance(self.loaded_from, (list, tuple)):
+            for f in self.loaded_from:
+                txt += str(f)
+                txt += "\n"
+
+        else:
+            txt += str(self.loaded_from)
+            txt += "\n"
+        txt += "\n* GLOBAL\n"
+        txt += f"material:            {self.meta_common.material}\n"
+        txt += f"mass (active):       {self.meta_common.mass}\n"
+        txt += f"mass (total):        {self.meta_common.tot_mass}\n"
+        txt += f"nominal capacity:    {self.meta_common.nom_cap}\n"
+        txt += f"test ID:             {self.meta_test_dependent.test_ID}\n"
+        txt += f"channel index:       {self.meta_test_dependent.channel_index}\n"
+        txt += f"creator:             {self.meta_test_dependent.creator}\n"
+        txt += f"schedule file name:  {self.meta_test_dependent.schedule_file_name}\n"
+
+        try:
+            if self.start_datetime:
+                start_datetime_str = xldate_as_datetime(self.start_datetime)
+            else:
+                start_datetime_str = "Not given"
+        except AttributeError:
+            start_datetime_str = "NOT READABLE YET"
+
+        txt += f"start-date:         {start_datetime_str}\n"
+
+        txt += self._header_str("DATA")
+        try:
+            txt += str(self.raw.describe())
+        except (AttributeError, ValueError):
+            txt += "EMPTY (Not processed yet)\n"
+
+        txt += self._header_str("SUMMARY")
+        try:
+            txt += str(self.summary.describe())
+        except (AttributeError, ValueError):
+            txt += "EMPTY (Not processed yet)\n"
+
+        txt += self._header_str("STEP TABLE")
+        try:
+            txt += str(self.steps.describe())
+            txt += str(self.steps.head())
+        except (AttributeError, ValueError):
+            txt += "EMPTY (Not processed yet)\n"
+
+        txt += self._header_str("RAW UNITS")
+        try:
+            txt += str(self.raw_units)
+        except (AttributeError, ValueError):
+            txt += "EMPTY (Not processed yet)\n"
+        return txt
+
+    def populate_defaults(self):
+        # modify this method upon need
+        logging.debug("checking and populating defaults for the cell")
+
+        if not self.active_electrode_area:
+            self.active_electrode_area = 1.0
+            logging.debug(
+                f"active_electrode_area not set -> setting to: {self.active_electrode_area}"
+            )
+
+        if not self.mass:
+            self.mass = 1.0
+            logging.debug(f"mass not set -> setting to: {self.mass}")
+
+        if not self.tot_mass:
+            self.tot_mass = self.mass
+            logging.debug(
+                f"total mass not set -> setting to same as mass: {self.tot_mass}"
+            )
+
+        return True
+
+    @property
+    def empty(self):
+        if self.has_data:
+            return False
+        return True
+
+    @property
+    def has_summary(self):
+        """check if the summary table exists"""
+        try:
+            empty = self.summary.empty
+            # TODO: check if the summary has the expected columns
+            #  (since it can be unprocessed directly from the raw data)
+        except AttributeError:
+            empty = True
+        return not empty
+
+    @property
+    def has_steps(self):
+        """check if the step table exists"""
+        try:
+            empty = self.steps.empty
+        except AttributeError:
+            empty = True
+        return not empty
+
+    @property
+    def has_data(self):
+        try:
+            empty = self.raw.empty
+        except AttributeError:
+            empty = True
+        return not empty
+
+
+class InstrumentFactory:
+    def __init__(self):
+        self._builders = {}
+        self._kwargs = {}
+
+    def register_builder(self, key: str, builder: Tuple[str, Any], **kwargs) -> None:
+        """register an instrument loader module.
+
+        Args:
+            key: instrument id
+            builder: (module_name, module_path)
+            **kwargs: stored in the factory (will be used in the future for allowing to set
+               defaults to the builders to allow for using .query).
+        """
+
+        logging.debug(f"Registering instrument {key}")
+        self._builders[key] = builder
+        self._kwargs[key] = kwargs
+
+    def create(self, key: Union[str, None], **kwargs):
+        """Create the instrument loader module and initialize the loader class.
+
+        Args:
+            key: instrument id
+            **kwargs: sent to the initializer of the loader class.
+
+        Returns:
+            instance of loader class.
+        """
+
+        module_name, module_path = self._builders.get(key, (None, None))
+
+        # constant:
+        instrument_class = "DataLoader"
+
+        if not module_name:
+            raise ValueError(key)
+
+        spec = importlib.util.spec_from_file_location(module_name, module_path)
+        loader_module = importlib.util.module_from_spec(spec)
+        sys.modules[module_name] = loader_module  # noqa
+        spec.loader.exec_module(loader_module)
+        cls = getattr(loader_module, instrument_class)
+
+        # TODO: get stored kwargs from self.__kwargs and merge them with the supplied kwargs
+        #  (supplied should have preference)
+
+        return cls(**kwargs)
+
+    def query(self, key: str, variable: str) -> Any:
+        """performs a get_params lookup for the instrument loader.
+
+        Args:
+            key: instrument id.
+            variable: the variable you want to lookup.
+
+        Returns:
+            The value of the variable if the loaders get_params method supports it.
+        """
+        loader = self.create(key)
+        try:
+            value = loader.get_params(variable)
+            logging.debug(f"GOT {variable}={value} for {key}")
+            return value
+
+        except (AttributeError, NotImplementedError, KeyError):
+            logging.debug(f"COULD NOT RETRIEVE {variable} for {key}")
+        return
+
+
+def generate_default_factory():
+    """This function searches for all available instrument readers
+    and registers them in an InstrumentFactory instance.
+
+    Returns:
+        InstrumentFactory
+    """
+    instrument_factory = InstrumentFactory()
+    instruments = find_all_instruments()
+    for instrument_id, instrument in instruments.items():
+        instrument_factory.register_builder(instrument_id, instrument)
+    return instrument_factory
+
+
+def find_all_instruments() -> Dict[str, Tuple[str, pathlib.Path]]:
+    """finds all the supported instruments"""
+
+    import cellpy.readers.instruments as hard_coded_instruments_site
+
+    instruments_found = {}
+    logging.debug("Searching for modules in base instrument folder:")
+
+    hard_coded_instruments_site = pathlib.Path(
+        hard_coded_instruments_site.__file__
+    ).parent
+    modules_in_hard_coded_instruments_site = [
+        s
+        for s in hard_coded_instruments_site.glob("*.py")
+        if not (
+            str(s.name).startswith("_")
+            or str(s.name).startswith("dev_")
+            or str(s.name).startswith("base")
+            or str(s.name).startswith("backup")
+            or str(s.name).startswith("registered_loaders")
+        )
+    ]
+
+    for module_path in modules_in_hard_coded_instruments_site:
+        module_name = module_path.name.rstrip(".py")
+        logging.debug(module_name)
+        instruments_found[module_name] = (
+            module_name,
+            module_path,
+        )
+        logging.debug(" -> added")
+
+    logging.debug("Searching for module configurations in user instrument folder:")
+    # These are only yaml-files and should ideally import the appropriate
+    #    custom loader class
+    # Might not be needed.
+    logging.debug("- Not implemented yet")
+
+    logging.debug("Searching for modules through plug-ins:")
+    # Not sure how to do this yet. Probably also some importlib trick.
+    logging.debug("- Not implemented yet")
+    return instruments_found
+
+
+def identify_last_data_point(data):
+    """Find the last data point and store it in the fid instance"""
+
+    logging.debug("searching for last data point")
+    hdr_data_point = HEADERS_NORMAL.data_point_txt
+    try:
+        if hdr_data_point in data.raw.columns:
+            last_data_point = data.raw[hdr_data_point].max()
+        else:
+            last_data_point = data.raw.index.max()
+    except AttributeError:
+        logging.debug("AttributeError - setting last data point to 0")
+        last_data_point = 0
+    if not last_data_point > 0:
+        last_data_point = 0
+    data.raw_data_files[0].last_data_point = last_data_point
+    logging.debug(f"last data point: {last_data_point}")
+    return data
+
+
+# TODO: move this to internals/core
+def check64bit(current_system="python"):
+    """checks if you are on a 64-bit platform"""
+    if current_system == "python":
+        return sys.maxsize > 2147483647
+    elif current_system == "os":
+        import platform
+
+        pm = platform.machine()
+        if pm != ".." and pm.endswith("64"):  # recent Python (not Iron)
+            return True
+        else:
+            if "PROCESSOR_ARCHITEW6432" in os.environ:
+                return True  # 32 bit program running on 64-bit Windows
+            try:
+                # 64-bit Windows 64 bit program
+                return os.environ["PROCESSOR_ARCHITECTURE"].endswith("64")
+            except IndexError:
+                pass  # not Windows
+            try:
+                # this often works in Linux
+                return "64" in platform.architecture()[0]
+            except Exception:  # noqa
+                # is an older version of Python, assume also an older os@
+                # (best we can guess)
+                return False
+
+
+# TODO: move this to internals/core
+def humanize_bytes(b, precision=1):
+    """Return a humanized string representation of a number of b."""
+
+    abbrevs = (
+        (1 << 50, "PB"),
+        (1 << 40, "TB"),
+        (1 << 30, "GB"),
+        (1 << 20, "MB"),
+        (1 << 10, "kB"),
+        (1, "b"),
+    )
+    if b == 1:
+        return "1 byte"
+    for factor, suffix in abbrevs:
+        if b >= factor:
+            break
+    # return '%.*f %s' % (precision, old_div(b, factor), suffix)
+    return "%.*f %s" % (precision, b // factor, suffix)  # noqa
+
+
+# TODO: move this to internals/core
+def xldate_as_datetime(xldate, datemode=0, option="to_datetime"):
+    """Converts a xls date stamp to a more sensible format.
+
+    Args:
+        xldate (str, int): date stamp in Excel format.
+        datemode (int): 0 for 1900-based, 1 for 1904-based.
+        option (str): option in ("to_datetime", "to_float", "to_string"),
+            return value
+
+    Returns:
+        datetime (datetime object, float, or string).
+
+    """
+
+    # This does not work for numpy-arrays
+
+    if option == "to_float":
+        d = (xldate - 25589) * 86400.0
+    else:
+        try:
+            d = datetime.datetime(1899, 12, 30) + datetime.timedelta(
+                days=xldate + 1462 * datemode
+            )
+            # date_format = "%Y-%m-%d %H:%M:%S:%f" # with microseconds,
+            # Excel cannot cope with this!
+            if option == "to_string":
+                date_format = "%Y-%m-%d %H:%M:%S"  # without microseconds
+                d = d.strftime(date_format)
+        except TypeError:
+            logging.info(f"The date is not of correct type [{xldate}]")
+            d = xldate
+    return d
+
+
+def collect_capacity_curves(
+    cell,
+    direction="charge",
+    trim_taper_steps=None,
+    steps_to_skip=None,
+    steptable=None,
+    max_cycle_number=None,
+    **kwargs,
+):
+    """Create a list of pandas.DataFrames, one for each charge step.
+
+    The DataFrames are named by its cycle number.
+
+    Args:
+        cell (``CellpyCell``):  object
+        direction (str):
+        trim_taper_steps (integer): number of taper steps to skip (counted
+            from the end, i.e. 1 means skip last step in each cycle).
+        steps_to_skip (list): step numbers that should not be included.
+        steptable (``pandas.DataFrame``): optional steptable.
+        max_cycle_number (int): only select cycles up to this value.
+
+    Returns:
+        list of pandas.DataFrames,
+        list of cycle numbers,
+        minimum voltage value,
+        maximum voltage value
+
+    """
+
+    # TODO: should allow for giving cycle numbers as input (e.g. cycle=[1, 2, 10]
+    #  or cycle=2), not only max_cycle_number. Intermediate solution:
+    #  The cycle keyword will not break the method but raise a warning:
+    for arg in kwargs:
+        if arg in ["cycle", "cycles"]:
+            logging.warning(
+                f"{arg} is not implemented yet, but might exist in newer versions of cellpy."
+            )
+        else:
+            logging.warning(
+                f"collect_capacity_curve received unknown key-word argument: {arg=}"
+            )
+
+    minimum_v_value = np.Inf
+    maximum_v_value = -np.Inf
+    charge_list = []
+    cycles = kwargs.pop("cycle", None)
+
+    if cycles is None:
+        cycles = cell.get_cycle_numbers()
+
+    if max_cycle_number is None:
+        max_cycle_number = max(cycles)
+
+    for cycle in cycles:
+        if cycle > max_cycle_number:
+            break
+        try:
+            if direction == "charge":
+                q, v = cell.get_ccap(
+                    cycle,
+                    trim_taper_steps=trim_taper_steps,
+                    steps_to_skip=steps_to_skip,
+                    steptable=steptable,
+                    as_frame=False,
+                )
+            else:
+                q, v = cell.get_dcap(
+                    cycle,
+                    trim_taper_steps=trim_taper_steps,
+                    steps_to_skip=steps_to_skip,
+                    steptable=steptable,
+                    as_frame=False,
+                )
+
+        except NullData as e:
+            logging.warning(e)
+            d = pd.DataFrame()
+            d.name = cycle
+            charge_list.append(d)
+        else:
+            d = pd.DataFrame({"q": q, "v": v})
+            # d.name = f"{cycle}"
+            d.name = cycle
+            charge_list.append(d)
+            v_min = v.min()
+            v_max = v.max()
+            if v_min < minimum_v_value:
+                minimum_v_value = v_min
+            if v_max > maximum_v_value:
+                maximum_v_value = v_max
+    return charge_list, cycles, minimum_v_value, maximum_v_value
+
+
+def interpolate_y_on_x(
+    df,
+    x=None,
+    y=None,
+    new_x=None,
+    dx=10.0,
+    number_of_points=None,
+    direction=1,
+    **kwargs,
+):
+    """Interpolate a column based on another column.
+
+    Args:
+        df: DataFrame with the (cycle) data.
+        x: Column name for the x-value (defaults to the step-time column).
+        y: Column name for the y-value (defaults to the voltage column).
+        new_x (numpy array or None): Interpolate using these new x-values
+            instead of generating x-values based on dx or number_of_points.
+        dx: step-value (defaults to 10.0)
+        number_of_points: number of points for interpolated values (use
+            instead of dx and overrides dx if given).
+        direction (-1,1): if direction is negative, then invert the
+            x-values before interpolating.
+        **kwargs: arguments passed to ``scipy.interpolate.interp1d``
+
+    Returns: DataFrame with interpolated y-values based on given or
+        generated x-values.
+
+    """
+
+    # TODO: allow for giving a fixed interpolation range (x-values).
+    #  Remember to treat extrapolation properly (e.g. replace with NaN?).
+
+    if x is None:
+        x = df.columns[0]
+    if y is None:
+        y = df.columns[1]
+
+    xs = df[x].values
+    ys = df[y].values
+
+    if direction > 0:
+        x_min = xs.min()
+        x_max = xs.max()
+    else:
+        x_max = xs.min()
+        x_min = xs.max()
+        dx = -dx
+
+    bounds_error = kwargs.pop("bounds_error", False)
+    f = interpolate.interp1d(xs, ys, bounds_error=bounds_error, **kwargs)
+    if new_x is None:
+        if number_of_points:
+            new_x = np.linspace(x_min, x_max, number_of_points)
+        else:
+            new_x = np.arange(x_min, x_max, dx)
+
+    new_y = f(new_x)
+
+    new_df = pd.DataFrame({x: new_x, y: new_y})
+
+    return new_df
+
+
+def group_by_interpolate(
+    df,
+    x=None,
+    y=None,
+    group_by=None,
+    number_of_points=100,
+    tidy=False,
+    individual_x_cols=False,
+    header_name="Unit",
+    dx=10.0,
+    generate_new_x=True,
+):
+    """Do a pandas.DataFrame.group_by and perform interpolation for all groups.
+
+    This function is a wrapper around an internal interpolation function in
+    cellpy (that uses scipy.interpolate.interp1d) that combines doing a group-by
+    operation and interpolation.
+
+    Args:
+        df (pandas.DataFrame): the dataframe to morph.
+        x (str): the header for the x-value
+            (defaults to normal header step_time_txt) (remark that the default
+            group_by column is the cycle column, and each cycle normally
+            consist of several steps (so you risk interpolating / merging
+            several curves on top of each other (not good)).
+        y (str): the header for the y-value
+            (defaults to normal header voltage_txt).
+        group_by (str): the header to group by
+            (defaults to normal header cycle_index_txt)
+        number_of_points (int): if generating new x-column, how many values it
+            should contain.
+        tidy (bool): return the result in tidy (i.e. long) format.
+        individual_x_cols (bool): return as xy xy xy ... data.
+        header_name (str): name for the second level of the columns (only
+            applies for xy xy xy ... data) (defaults to "Unit").
+        dx (float): if generating new x-column and number_of_points is None or
+            zero, distance between the generated values.
+        generate_new_x (bool): create a new x-column by
+            using the x-min and x-max values from the original dataframe where
+            the method is set by the number_of_points key-word:
+
+            1)  if number_of_points is not None (default is 100):
+
+                ```
+                new_x = np.linspace(x_max, x_min, number_of_points)
+                ```
+            2)  else:
+                ```
+                new_x = np.arange(x_max, x_min, dx)
+                ```
+
+
+    Returns: pandas.DataFrame with interpolated x- and y-values. The returned
+        dataframe is in tidy (long) format for tidy=True.
+
+    """
+    # TODO: @jepe - create more tests
+    time_00 = time.time()
+    if x is None:
+        x = HEADERS_NORMAL.step_time_txt
+    if y is None:
+        y = HEADERS_NORMAL.voltage_txt
+    if group_by is None:
+        group_by = [HEADERS_NORMAL.cycle_index_txt]
+
+    if not isinstance(group_by, (list, tuple)):
+        group_by = [group_by]
+
+    if not generate_new_x:
+        # check if it makes sense
+        if (not tidy) and (not individual_x_cols):
+            logging.warning("Unlogical condition")
+            generate_new_x = True
+
+    new_x = None
+
+    if generate_new_x:
+        x_max = df[x].max()
+        x_min = df[x].min()
+        if number_of_points:
+            new_x = np.linspace(x_max, x_min, number_of_points)
+        else:
+            new_x = np.arange(x_max, x_min, dx)
+
+    new_dfs = []
+    keys = []
+
+    for name, group in df.groupby(group_by):
+        keys.append(name)
+        if not isinstance(name, (list, tuple)):
+            name = [name]
+
+        new_group = interpolate_y_on_x(
+            group, x=x, y=y, new_x=new_x, number_of_points=number_of_points, dx=dx
+        )
+
+        if tidy or (not tidy and not individual_x_cols):
+            for i, j in zip(group_by, name):
+                new_group[i] = j
+        new_dfs.append(new_group)
+
+    if tidy:
+        new_df = pd.concat(new_dfs)
+    else:
+        if individual_x_cols:
+            new_df = pd.concat(new_dfs, axis=1, keys=keys)
+            group_by.append(header_name)
+            new_df.columns.names = group_by
+        else:
+            new_df = pd.concat(new_dfs)
+            new_df = new_df.pivot(index=x, columns=group_by[0], values=y)
+
+    time_01 = time.time() - time_00
+    logging.debug(f"duration: {time_01} seconds")
+    return new_df
+
+
+def convert_from_simple_unit_label_to_string_unit_label(k, v):
+    old_raw_units = {
+        "current": 1.0,
+        "charge": 1.0,
+        "voltage": 1.0,
+        "time": 1.0,
+        "resistance": 1.0,
+        "power": 1.0,
+        "energy": 1.0,
+        "frequency": 1.0,
+        "mass": 0.001,
+        "nominal_capacity": 1.0,
+        "specific_gravimetric": 1.0,
+        "specific_areal": 1.0,
+        "specific_volumetric": 1.0,
+        "length": 1.0,
+        "area": 1.0,
+        "volume": 1.0,
+        "temperature": 1.0,
+        "pressure": 1.0,
+    }
+    old_unit = old_raw_units[k]
+    value = v / old_unit
+    default_units = get_default_raw_units()
+
+    new_unit = default_units[k]
+    value = Q(value, new_unit)
+    str_value = str(value)
+    return str_value
+
+
+# ---------------- LOCAL DEV TESTS ----------------
+
+
+def check_convert_from_simple_unit_label_to_string_unit_label():
+    k = "resistance"
+    v = 1.0
+    n = convert_from_simple_unit_label_to_string_unit_label(k, v)
+    print(n)
+
+
+def check_path_things():
+    p = "//jepe@mymachine.my.no/./path/file.txt"
+    p2 = pathlib.Path(p)
+    print(f"{p2=}")
+    print(f"{p2.resolve()=}")
+    print(f"{p2.drive=}")
+    print(f"{p2.as_uri()=}")
+    print(f"{p2.root=}")
+    print(f"{p2.anchor=}")
+    print(f"{p2.parent=}")
+    print(f"{p2.name=}")
+    print(f"{p2.stem=}")
+    print(f"{p2.suffix=}")
+    print(f"{p2.suffixes=}")
+    print(f"{p2.parts=}")
+    print(f"{p2.is_absolute()=}")
+    print(f"{p2.is_reserved()=}")
+    print(f"{p2.is_dir()=}")
+    print(f"{p2.is_file()=}")
+
+    try:
+        print(f"{p2.is_socket()=}")
+    except NotImplementedError as e:
+        print(f"{e}")
+    try:
+        print(f"{p2.is_mount()=}")
+    except NotImplementedError as e:
+        print(f"{e}")
+    try:
+        print(f"{p2.is_symlink()=}")
+    except NotImplementedError as e:
+        print(f"{e}")
+
+    try:
+        print(f"{p2.owner()=}")
+    except NotImplementedError as e:
+        print(f"{e}")
+
+    try:
+        print(f"{p2.group()=}")
+    except NotImplementedError as e:
+        print(f"{e}")
+
+    print(f"{p2.exists()=}")
+
+
+def check_another_path_things():
+    p01 = r"C:\scripting\cellpy\testdata\data\20160805_test001_45_cc_01.res"
+    p02 = r"ssh://jepe@server.no/home/jepe/cellpy/testdata/data/20160805_test001_45_cc_01.res"
+    p03 = r"scripting\cellpy\testdata\data\20160805_test001_45_cc_01.res"
+    p04 = r"..\data\20160805_test001_45_cc_01.res"
+    p05 = pathlib.Path(p01)
+    p06 = pathlib.Path(p02)
+    for p in [p01, p02, p03, p04, p05, p06]:
+        print(f"{p}".center(110, "-"))
+        p2 = OtherPath(p)
+        print(f"{p2=}")
+        print(p2)
+        print(f"{p2.resolve()=}")
+        print(f"{p2.drive=}")
+        print(f"{p2.exists()=}")
+        print(f"{p2._is_external=}")  # noqa
+        print(f"{p2._location=}")  # noqa
+        print(f"{p2._uri_prefix=}")  # noqa
+        print(f"{p2.resolve()=}")
+        if p2.is_absolute():
+            print(f"{p2.as_uri()=}")
+        print(f"{p2.is_external=}")
+        print(f"{p2.location=}")
+        print(f"{p2.uri_prefix=}")
+        print(f"{p2.root=}")
+        print(f"{p2.anchor=}")
+        print(f"{p2.parent=}")
+        print(f"{p2.name=}")
+        print(f"{p2.stem=}")
+        print(f"{p2.suffix=}")
+        print(f"{p2.suffixes=}")
+        print(f"{p2.parts=}")
+        print(f"{type(p2)}")
+        print(f"{isinstance(p2, pathlib.Path)=}")
+        print(f"{isinstance(p2, OtherPath)=}")
+        print()
+
+
+def check_how_other_path_works():
+    p01 = r"C:\scripting\cellpy\testdata\data\20160805_test001_45_cc_01.res"
+    p02 = r"ssh://jepe@somewhere.else.no/home/jepe/cellpy/testdata/data/20160805_test001_45_cc_01.res"
+    p03 = None
+    p03b = OtherPath(p03)
+    p05 = pathlib.Path(p01)
+    p06 = pathlib.Path(p02)
+    p07 = OtherPath(p01)
+    p08 = OtherPath(p02)
+    print(80 * "=")
+    for p in [p01, p02, p03, p03b, p05, p06, p07, p08]:
+        print(f"{p}".center(110, "-"))
+        print(f"{type(p)}".center(110, "*"))
+        p2 = OtherPath(p)
+        print(f"{p2=}")
+        print(p2)
+        print(f"{p2.raw_path=}")
+        print(f"{p2.is_external=}")
+        print(f"{p2.location=}")
+        print(f"{p2.uri_prefix=}")
+        print(f"{p2._original=}")  # noqa
+        print(f"{p2.full_path=}")
+        print(f"{p2.parts=}")
+
+
+def check_copy_external_file():
+    from cellpy import prms
+
+    prms.Paths.env_file = r"C:\scripting\cellpy\local\.env_cellpy"
+    dst = r"C:\scripting\cellpy\tmp\20210629_moz_cat_02_cc_01.res"
+    src = "ssh://jepe@not.in.no/home/jepe@ad.ife.no/Temp/20210629_moz_cat_02_cc_01.res"
+    copy_external_file(src, dst)
+
+
+if __name__ == "__main__":
+    check_how_other_path_works()
```

### Comparing `cellpy-1.0.0b0/cellpy/readers/dbreader.py` & `cellpy-1.0.0b1/cellpy/readers/dbreader.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/filefinder.py` & `cellpy-1.0.0b1/cellpy/readers/filefinder.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/arbin_res.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/arbin_res.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1305 +1,1281 @@
-"""arbin res-type data files"""
-import logging
-import os
-import pathlib
-import platform
-import shutil
-import sys
-import tempfile
-import time
-import warnings
-
-import numpy as np
-import pandas as pd
-import sqlalchemy as sa
-
-from cellpy import prms
-from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
-from cellpy.readers.core import (
-    Data,
-    FileID,
-    check64bit,
-    humanize_bytes,
-    xldate_as_datetime,
-)
-from cellpy.readers.instruments.base import MINIMUM_SELECTION, BaseLoader
-
-# TODO: use InstrumentSettings (dataclass) from internal_settings instead of HeaderDict.
-
-DEBUG_MODE = prms.Reader.diagnostics
-ALLOW_MULTI_TEST_FILE = False
-USE_SQLALCHEMY_ACCESS_ENGINE = True
-
-# Select odbc module
-ODBC = prms._odbc
-SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver
-
-use_subprocess = prms.Instruments.Arbin.use_subprocess
-detect_subprocess_need = prms.Instruments.Arbin.detect_subprocess_need
-
-is_posix = False
-is_macos = False
-if os.name == "posix":
-    is_posix = True
-current_platform = platform.system()
-if current_platform == "Darwin":
-    is_macos = True
-
-if DEBUG_MODE:
-    logging.debug("DEBUG_MODE")
-    logging.debug(f"ODBC: {ODBC}")
-    logging.debug(f"SEARCH_FOR_ODBC_DRIVERS: {SEARCH_FOR_ODBC_DRIVERS}")
-    logging.debug(f"use_subprocess: {use_subprocess}")
-    logging.debug(f"detect_subprocess_need: {detect_subprocess_need}")
-    logging.debug(f"current_platform: {current_platform}")
-
-if detect_subprocess_need:
-    logging.debug("detect_subprocess_need is True: checking versions")
-    python_version, os_version = platform.architecture()
-    if python_version == "64bit" and prms.Instruments.Arbin.office_version == "32bit":
-        logging.debug(
-            "python 64bit and office 32bit -> " "setting use_subprocess to True"
-        )
-        use_subprocess = True
-
-if use_subprocess and not is_posix:
-    # The Windows users most likely have a strange custom path to mdbtools etc.
-    logging.debug(
-        "using subprocess (most likely mdbtools) on non-posix (most likely windows)"
-    )
-    if not prms.Instruments.Arbin.sub_process_path:
-        sub_process_path = str(prms._sub_process_path)
-    else:
-        sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
-
-if is_posix:
-    sub_process_path = "mdb-export"
-
-try:
-    driver_dll = prms.Instruments.Arbin.odbc_driver
-except AttributeError:
-    driver_dll = None
-
-if ODBC == "pyodbc":
-    try:
-        import pyodbc as dbloader
-    except ImportError:
-        warnings.warn("COULD NOT LOAD DBLOADER!", ImportWarning)
-        dbloader = None
-
-elif ODBC == "pypyodbc":
-    try:
-        import pypyodbc as dbloader
-    except ImportError:
-        warnings.warn("COULD NOT LOAD DBLOADER!", ImportWarning)
-        dbloader = None
-
-if DEBUG_MODE:
-    logging.debug(f"dbloader: {dbloader}")
-
-
-# Names of the tables in the .res db that is used by cellpy
-TABLE_NAMES = {
-    "normal": "Channel_Normal_Table",
-    "global": "Global_Table",
-    "statistic": "Channel_Statistic_Table",
-    "aux_global": "Aux_Global_Data_Table",
-    "aux": "Auxiliary_Table",
-}
-
-summary_headers_renaming_dict = {
-    "test_id_txt": "Test_ID",
-    "data_point_txt": "Data_Point",
-    "vmax_on_cycle_txt": "Vmax_On_Cycle",
-    "charge_time_txt": "Charge_Time",
-    "discharge_time_txt": "Discharge_Time",
-}
-
-normal_headers_renaming_dict = {
-    "aci_phase_angle_txt": "ACI_Phase_Angle",
-    "ref_aci_phase_angle_txt": "Reference_ACI_Phase_Angle",
-    "ac_impedance_txt": "AC_Impedance",
-    "ref_ac_impedance_txt": "Reference_AC_Impedance",
-    "charge_capacity_txt": "Charge_Capacity",
-    "charge_energy_txt": "Charge_Energy",
-    "current_txt": "Current",
-    "cycle_index_txt": "Cycle_Index",
-    "data_point_txt": "Data_Point",
-    "datetime_txt": "DateTime",
-    "discharge_capacity_txt": "Discharge_Capacity",
-    "discharge_energy_txt": "Discharge_Energy",
-    "internal_resistance_txt": "Internal_Resistance",
-    "is_fc_data_txt": "Is_FC_Data",
-    "step_index_txt": "Step_Index",
-    "sub_step_index_txt": "Sub_Step_Index",  # new
-    "step_time_txt": "Step_Time",
-    "sub_step_time_txt": "Sub_Step_Time",  # new
-    "test_id_txt": "Test_ID",
-    "test_time_txt": "Test_Time",
-    "voltage_txt": "Voltage",
-    "ref_voltage_txt": "Reference_Voltage",  # new
-    "dv_dt_txt": "dV/dt",
-    "frequency_txt": "Frequency",  # new
-    "amplitude_txt": "Amplitude",  # new
-}
-
-
-class DataLoader(BaseLoader):
-    """Class for loading arbin-data from res-files.
-
-    Implemented Cellpy params (prms.Instruments.Arbin):
-        max_res_filesize
-        chunk_size
-        max_chunks
-        use_subprocess
-        detect_subprocess_need
-        sub_process_path
-        office_version
-        SQL_server
-
-    """
-
-    instrument_name = "arbin_res"
-    raw_ext = "res"
-
-    def __init__(self, *args, **kwargs):
-        """initiates the ArbinLoader class"""
-        # could use __init__(self, cellpydata_object) and
-        # set self.logger = cellpydata_object.logger etc.
-        # then remember to include that as prm in "out of class" functions
-        # self.prms = prms
-        self.raw_ext = "res"
-        self.logger = logging.getLogger(__name__)
-        # use the following prm to limit to loading only
-        # one cycle or from cycle>x to cycle<x+n
-        # prms.Reader.limit_loaded_cycles = [cycle from, cycle to]
-
-        self.arbin_headers_normal = (
-            self.get_headers_normal()
-        )  # the column headers defined by Arbin
-        self.cellpy_headers_normal = (
-            get_headers_normal()
-        )  # the column headers defined by cellpy
-        self.arbin_headers_global = self.get_headers_global()
-        self.arbin_headers_aux_global = self.get_headers_aux_global()
-        self.arbin_headers_aux = self.get_headers_aux()
-        self.current_chunk = 0  # use this to set chunks to load
-
-    @staticmethod
-    def get_raw_units():
-        raw_units = dict()
-        raw_units["current"] = "A"
-        raw_units["charge"] = "Ah"
-        raw_units["mass"] = "g"
-        raw_units["voltage"] = "V"
-        return raw_units
-
-    @staticmethod
-    def get_headers_normal():
-        """Defines the so-called normal column headings for Arbin .res-files"""
-        headers = HeaderDict()
-        # - normal (raw-data) column headings (specific for Arbin)
-
-        headers["aci_phase_angle_txt"] = "ACI_Phase_Angle"
-        headers["ref_aci_phase_angle_txt"] = "Reference_ACI_Phase_Angle"
-
-        headers["ac_impedance_txt"] = "AC_Impedance"
-        headers["ref_ac_impedance_txt"] = "Reference_AC_Impedance"  # new
-
-        headers["charge_capacity_txt"] = "Charge_Capacity"
-        headers["charge_energy_txt"] = "Charge_Energy"
-        headers["current_txt"] = "Current"
-        headers["cycle_index_txt"] = "Cycle_Index"
-        headers["data_point_txt"] = "Data_Point"
-        headers["datetime_txt"] = "DateTime"
-        headers["discharge_capacity_txt"] = "Discharge_Capacity"
-        headers["discharge_energy_txt"] = "Discharge_Energy"
-        headers["internal_resistance_txt"] = "Internal_Resistance"
-
-        headers["is_fc_data_txt"] = "Is_FC_Data"
-        headers["step_index_txt"] = "Step_Index"
-        headers["sub_step_index_txt"] = "Sub_Step_Index"  # new
-
-        headers["step_time_txt"] = "Step_Time"
-        headers["sub_step_time_txt"] = "Sub_Step_Time"  # new
-
-        headers["test_id_txt"] = "Test_ID"
-        headers["test_time_txt"] = "Test_Time"
-
-        headers["voltage_txt"] = "Voltage"
-        headers["ref_voltage_txt"] = "Reference_Voltage"  # new
-
-        headers["dv_dt_txt"] = "dV/dt"
-        headers["frequency_txt"] = "Frequency"  # new
-        headers["amplitude_txt"] = "Amplitude"  # new
-
-        return headers
-
-    @staticmethod
-    def get_headers_aux():
-        """Defines the so-called auxiliary table column headings for Arbin .res-files"""
-        headers = HeaderDict()
-        # - aux column headings (specific for Arbin)
-        headers["test_id_txt"] = "Test_ID"
-        headers["data_point_txt"] = "Data_Point"
-        headers["aux_index_txt"] = "Auxiliary_Index"
-        headers["data_type_txt"] = "Data_Type"
-        headers["x_value_txt"] = "X"
-        headers["x_dt_value"] = "dX_dt"
-        return headers
-
-    @staticmethod
-    def get_headers_aux_global():
-        """Defines the so-called auxiliary global column headings for Arbin .res-files"""
-        headers = HeaderDict()
-        # - aux global column headings (specific for Arbin)
-        headers["channel_index_txt"] = "Channel_Index"
-        headers["aux_index_txt"] = "Auxiliary_Index"
-        headers["data_type_txt"] = "Data_Type"
-        headers["aux_name_txt"] = "Nickname"
-        headers["aux_unit_txt"] = "Unit"
-        return headers
-
-    @staticmethod
-    def get_headers_global():
-        """Defines the so-called global column headings for Arbin .res-files"""
-        headers = HeaderDict()
-        # - global column headings (specific for Arbin)
-        headers["applications_path_txt"] = "Applications_Path"
-        headers["channel_index_txt"] = "Channel_Index"
-        headers["channel_number_txt"] = "Channel_Number"
-        headers["channel_type_txt"] = "Channel_Type"
-        headers["comments_txt"] = "Comments"
-        headers["creator_txt"] = "Creator"
-        headers["daq_index_txt"] = "DAQ_Index"
-        headers["item_id_txt"] = "Item_ID"
-        headers["log_aux_data_flag_txt"] = "Log_Aux_Data_Flag"
-        headers["log_chanstat_data_flag_txt"] = "Log_ChanStat_Data_Flag"
-        headers["log_event_data_flag_txt"] = "Log_Event_Data_Flag"
-        headers["log_smart_battery_data_flag_txt"] = "Log_Smart_Battery_Data_Flag"
-        headers["mapped_aux_conc_cnumber_txt"] = "Mapped_Aux_Conc_CNumber"
-        headers["mapped_aux_di_cnumber_txt"] = "Mapped_Aux_DI_CNumber"
-        headers["mapped_aux_do_cnumber_txt"] = "Mapped_Aux_DO_CNumber"
-        headers["mapped_aux_flow_rate_cnumber_txt"] = "Mapped_Aux_Flow_Rate_CNumber"
-        headers["mapped_aux_ph_number_txt"] = "Mapped_Aux_PH_Number"
-        headers["mapped_aux_pressure_number_txt"] = "Mapped_Aux_Pressure_Number"
-        headers["mapped_aux_temperature_number_txt"] = "Mapped_Aux_Temperature_Number"
-        headers["mapped_aux_voltage_number_txt"] = "Mapped_Aux_Voltage_Number"
-        headers[
-            "schedule_file_name_txt"
-        ] = "Schedule_File_Name"  # KEEP FOR CELLPY FILE FORMAT
-        headers["start_datetime_txt"] = "Start_DateTime"
-        headers["test_id_txt"] = "Test_ID"  # KEEP FOR CELLPY FILE FORMAT
-        headers["test_name_txt"] = "Test_Name"  # KEEP FOR CELLPY FILE FORMAT
-        return headers
-
-    @staticmethod
-    def get_raw_limits():
-        raw_limits = dict()
-        raw_limits["current_hard"] = 0.000_000_000_000_1
-        raw_limits["current_soft"] = 0.000_01
-        raw_limits["stable_current_hard"] = 2.0
-        raw_limits["stable_current_soft"] = 4.0
-        raw_limits["stable_voltage_hard"] = 2.0
-        raw_limits["stable_voltage_soft"] = 4.0
-        raw_limits["stable_charge_hard"] = 0.001
-        raw_limits["stable_charge_soft"] = 5.0
-        raw_limits["ir_change"] = 0.00001
-        return raw_limits
-
-    def _get_res_connector(self, temp_filename):
-        """Returns a connection to the .res-file"""
-
-        if dbloader is None:
-            txt = f"{ODBC=}\n"
-            txt += f"{SEARCH_FOR_ODBC_DRIVERS=}\n"
-            txt += f"{use_subprocess=}\n"
-            txt += f"{detect_subprocess_need=}\n"
-            txt += f"{current_platform=}\n"
-            raise ValueError(
-                f"Something went seriously wrong." f"dbloader is None.\n{txt}"
-            )
-
-        if SEARCH_FOR_ODBC_DRIVERS:
-            logging.debug("Searching for odbc drivers")
-            try:
-                drivers = [
-                    driver
-                    for driver in dbloader.drivers()
-                    if "Microsoft Access Driver" in driver
-                ]
-                logging.debug(f"Found these: {drivers}")
-                driver = drivers[0]
-
-            except AttributeError as e:
-                print("ODBC drivers not found.")
-
-            except IndexError as e:
-                logging.debug("Unfortunately, it seems the list of drivers is emtpy.")
-                logging.debug("Use driver-name from config (if existing).")
-                driver = driver_dll
-                if is_macos:
-                    driver = "/usr/local/lib/libmdbodbc.dylib"
-                else:
-                    if not driver:
-                        print(
-                            "\nCould not find any odbc-drivers suitable "
-                            "for .res-type files. "
-                            "Check out the homepage of pydobc for info on "
-                            "installing drivers"
-                        )
-                        print(
-                            "One solution that might work is downloading "
-                            "the Microsoft Access database engine (in correct"
-                            " bytes (32 or 64)) "
-                            "from:\n"
-                            "https://www.microsoft.com/en-us/download/"
-                            "details.aspx?id=13255"
-                        )
-                        print(
-                            "Or install mdbtools and set it up "
-                            "(check the cellpy docs for help)"
-                        )
-                        print("\n")
-                    else:
-                        logging.debug("Using driver dll from config file")
-                        logging.debug(f"driver dll: {driver}")
-
-            self.logger.debug(f"odbc constr: {driver}")
-
-        else:
-            is64bit_python = check64bit(current_system="python")
-            if is64bit_python:
-                driver = "{Microsoft Access Driver (*.mdb, *.accdb)}"
-            else:
-                driver = "Microsoft Access Driver (*.mdb)"
-            self.logger.debug(f"odbc constr: {driver}")
-
-        constr = f"Driver={driver};Dbq={temp_filename};ExtendedAnsiSQL=1;"
-        logging.debug(f"constr: {constr}")
-
-        return constr
-
-    def _get_connection_or_engine(self, temp_filename):
-        # updated to use sqlalchemy - needs sqlalchemy-access
-        constr = self._get_res_connector(temp_filename)
-        self.logger.debug(f"constr str: {constr}")
-        connection_url = sa.engine.URL.create(
-            "access+pyodbc", query={"odbc_connect": constr}
-        )
-        engine = sa.create_engine(connection_url)
-        return engine
-
-    def _clean_up_loadres(self, cur, conn, filename):
-        if cur is not None:
-            cur.close()  # adodbapi
-        if conn is not None:
-            conn.close()  # adodbapi
-        if os.path.isfile(filename):
-            try:
-                os.remove(filename)
-            except WindowsError as e:
-                self.logger.warning("could not remove tmp-file\n%s %s" % (filename, e))
-
-    def _post_process(self, data):
-        fix_datetime = True
-        set_index = True
-        rename_headers = True
-
-        # TODO:  insert post-processing and div tests here
-        #    - check dtypes
-
-        # Remark that we also set index during saving the file to hdf5 if
-        #   it is not set.
-
-        if rename_headers:
-            columns = {}
-            for key in self.arbin_headers_normal:
-                old_header = normal_headers_renaming_dict[key]
-                new_header = self.cellpy_headers_normal[key]
-                columns[old_header] = new_header
-
-            data.raw.rename(index=str, columns=columns, inplace=True)
-            try:
-                # TODO: check if summary df is existing (to only check if it is
-                #  empty will give an error later!)
-                columns = {}
-                for key, old_header in summary_headers_renaming_dict.items():
-                    try:
-                        columns[old_header] = self.cellpy_headers_normal[key]
-                    except KeyError:
-                        columns[old_header] = old_header.lower()
-                data.summary.rename(index=str, columns=columns, inplace=True)
-            except Exception as e:
-                txt = (
-                    f"Exception raised ({e})\n"
-                    f"key: {key} old_header: {old_header}"
-                    f"cellpy headers normal type {type(self.cellpy_headers_normal)}"
-                )
-                raise Exception(txt)
-
-        if fix_datetime:
-            h_datetime = self.cellpy_headers_normal.datetime_txt
-            logging.debug("converting to datetime format")
-            # print(data.raw.columns)
-            data.raw[h_datetime] = data.raw[h_datetime].apply(
-                xldate_as_datetime, option="to_datetime"
-            )
-
-            h_datetime = h_datetime
-            if h_datetime in data.summary:
-                data.summary[h_datetime] = data.summary[h_datetime].apply(
-                    xldate_as_datetime, option="to_datetime"
-                )
-
-        if set_index:
-            hdr_data_point = self.cellpy_headers_normal.data_point_txt
-            if data.raw.index.name != hdr_data_point:
-                data.raw = data.raw.set_index(hdr_data_point, drop=False)
-
-        return data
-
-    def _inspect(self, run_data):
-        """Inspect the file -> reports to log (debug)"""
-
-        if not any([DEBUG_MODE]):
-            return run_data
-
-        if DEBUG_MODE:
-            new_cols = run_data.raw.columns
-            for col in self.arbin_headers_normal:
-                if col not in new_cols:
-                    logging.debug(f"Missing col: {col}")
-                    # data.raw[col] = np.nan
-            return run_data
-
-    def _iterdump(self, file_name, headers=None):  # Deprecated - use on own risk
-        """
-        Function for dumping values from a file.
-
-        Should only be used by developers.
-
-        Args:
-            file_name: name of the file
-            headers: list of headers to pick
-                default:
-                ["Discharge_Capacity", "Charge_Capacity"]
-
-        Returns: pandas.DataFrame
-
-        """
-        if headers is None:
-            headers = ["Discharge_Capacity", "Charge_Capacity"]
-
-        step_txt = self.arbin_headers_normal.step_index_txt
-        point_txt = self.arbin_headers_normal.data_point_txt
-        cycle_txt = self.arbin_headers_normal.cycle_index_txt
-
-        self.logger.debug("iterating through file: %s" % file_name)
-        if not os.path.isfile(file_name):
-            print("Missing file_\n   %s" % file_name)
-
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        self.logger.info(txt)
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-
-        # creating temporary file and connection
-        self.copy_to_temporary()
-
-        constr = self._get_res_connector(self._temp_file_path)
-        conn = dbloader.connect(constr, autocommit=True)
-
-        self.logger.debug("tmp file: %s" % self._temp_file_path)
-        self.logger.debug("constr str: %s" % constr)
-
-        # --------- read global-data ------------------------------------
-        self.logger.debug("reading global data table")
-        sql = "select * from %s" % table_name_global
-
-        global_data_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-        # col_names = list(global_data_df.columns.values)
-        self.logger.debug("sql statement: %s" % sql)
-
-        tests = global_data_df[self.arbin_headers_normal.test_id_txt]
-        number_of_sets = len(tests)
-        self.logger.debug("number of datasets: %i" % number_of_sets)
-        self.logger.debug("only selecting first test")
-        test_no = 0
-        self.logger.debug("setting data for test number %i" % test_no)
-        loaded_from = file_name
-        # fid = FileID(file_name)
-        start_datetime = global_data_df[
-            self.arbin_headers_global["start_datetime_txt"]
-        ][test_no]
-        _internal_test_number = int(
-            global_data_df[self.arbin_headers_normal.test_id_txt][test_no]
-        )  # OBS
-        test_name = global_data_df[self.arbin_headers_global["test_name_txt"]][test_no]
-
-        # --------- read raw-data (normal-data) -------------------------
-        self.logger.debug("reading raw-data")
-
-        columns = ["Data_Point", "Step_Index", "Cycle_Index"]
-        columns.extend(headers)
-        columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
-
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (
-            self.arbin_headers_normal.test_id_txt,
-            _internal_test_number,
-        )
-        sql_5 = "order by %s" % self.arbin_headers_normal.data_point_txt
-        import time
-
-        info_list = []
-        info_header = ["cycle", "row_count", "start_point", "end_point"]
-        info_header.extend(headers)
-        self.logger.info(" ".join(info_header))
-        self.logger.info("-------------------------------------------------")
-
-        for cycle_number in range(1, 2000):
-            t1 = time.time()
-            self.logger.debug("picking cycle %i" % cycle_number)
-            sql_4 = "AND %s=%i " % (cycle_txt, cycle_number)
-            sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
-            self.logger.debug("sql statement: %s" % sql)
-            normal_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-            t2 = time.time()
-            dt = t2 - t1
-            self.logger.debug("time: %f" % dt)
-            if normal_df.empty:
-                self.logger.debug("reached the end")
-                break
-            row_count, _ = normal_df.shape
-            start_point = normal_df[point_txt].min()
-            end_point = normal_df[point_txt].max()
-            last = normal_df.iloc[-1, :]
-
-            step_list = [cycle_number, row_count, start_point, end_point]
-            step_list.extend([last[x] for x in headers])
-            info_list.append(step_list)
-
-        self._clean_up_loadres(None, conn, self._temp_file_path)
-        info_dict = pd.DataFrame(info_list, columns=info_header)
-        return info_dict
-
-    def repair(self, file_name):
-        """try to repair a broken/corrupted file"""
-        raise NotImplemented
-
-    def _query_table(self, table_name, conn, sql=None):
-        from sqlalchemy import create_engine, text
-
-        self.logger.debug(f"reading {table_name}")
-        if sql is None:
-            sql = f"select * from {table_name}"
-        self.logger.debug(f"sql statement: {sql}")
-        df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-        return df
-
-    def _make_name_from_frame(self, df, aux_index, data_type, dx_dt=False):
-        df_names = df.loc[
-            (df[self.arbin_headers_aux_global.aux_index_txt] == aux_index)
-            & (df[self.arbin_headers_aux_global.data_type_txt] == data_type),
-            :,
-        ]
-        unit = df_names[self.arbin_headers_aux_global.aux_unit_txt].values[0]
-        nick = (
-            df_names[self.arbin_headers_aux_global.aux_name_txt].values[0] or aux_index
-        )
-        if dx_dt:
-            name = f"aux_d_{nick}_dt_u_d{unit}_dt"
-        else:
-            name = f"aux_{nick}_u_{unit}"
-        return name
-
-    def _loader_win(
-        self,
-        file_name,
-        temp_filename,
-        *args,
-        bad_steps=None,
-        dataset_number=None,
-        data_points=None,
-        **kwargs,
-    ):
-        conn = None
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_aux_global = TABLE_NAMES["aux_global"]
-        table_name_aux = TABLE_NAMES["aux"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-
-        if DEBUG_MODE:
-            time_0 = time.time()
-
-        conn = self._get_connection_or_engine(temp_filename)
-
-        self.logger.debug("reading global data table")
-
-        global_data_df = self._query_table(table_name=table_name_global, conn=conn)
-        tests = global_data_df[self.arbin_headers_normal.test_id_txt]
-        number_of_sets = len(tests)
-        self.logger.debug(f"number of datasets: {number_of_sets}")
-
-        if dataset_number is not None:
-            self.logger.info(f"Dataset number given: {dataset_number}")
-            self.logger.info(f"Available dataset numbers: {tests}")
-
-        else:
-            dataset_number = 0
-
-        data = self._init_data(file_name, global_data_df, dataset_number)
-        test_id = data._internal_test_number
-        self.logger.debug("reading raw-data")
-
-        # --------- read raw-data (normal-data) ------------------------
-        length_of_test, normal_df = self._load_res_normal_table(
-            conn, test_id, bad_steps, data_points
-        )
-        # --------- read auxiliary data (aux-data) ---------------------
-        normal_df = self._load_win_res_auxiliary_table(
-            conn, normal_df, table_name_aux, table_name_aux_global, test_id
-        )
-
-        # --------- read stats-data (summary-data) ---------------------
-        sql = "select * from %s where %s=%s order by %s" % (
-            table_name_stats,
-            self.arbin_headers_normal.test_id_txt,
-            data._internal_test_number,
-            self.arbin_headers_normal.data_point_txt,
-        )
-        summary_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-
-        if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-            txt = "\nCould not find any summary (stats-file)!"
-            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-            logging.debug(txt)
-            # TODO: Enforce creating a summary df or modify renaming summary df (post process part)
-        # normal_df = normal_df.set_index("Data_Point")
-
-        data.summary = summary_df
-        if DEBUG_MODE:
-            mem_usage = normal_df.memory_usage()
-            logging.debug(
-                f"memory usage for "
-                f"loaded data: \n{mem_usage}"
-                f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-            )
-            logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
-
-        data.raw = normal_df
-        data.raw_data_files_length.append(length_of_test)
-        data = self._post_process(data)
-        data = self.identify_last_data_point(data)
-        return data
-
-    def _load_win_res_auxiliary_table(
-        self, conn, normal_df, table_name_aux, table_name_aux_global, test_id
-    ):
-        aux_global_data_df = self._query_table(table_name_aux_global, conn)
-        if not aux_global_data_df.empty:
-            aux_df = self._get_aux_df(conn, test_id, table_name_aux)
-            aux_df, aux_global_data_df = self._aux_to_wide(aux_df, aux_global_data_df)
-            aux_df = self._rename_aux_cols(aux_df, aux_global_data_df)
-
-            if not aux_df.empty:
-                normal_df = self._join_aux_to_normal(aux_df, normal_df)
-        return normal_df
-
-    def _load_posix_res_auxiliary_table(self, aux_global_data_df, aux_df, normal_df):
-        if not aux_global_data_df.empty:
-            aux_df, aux_global_data_df = self._aux_to_wide(aux_df, aux_global_data_df)
-            aux_df = self._rename_aux_cols(aux_df, aux_global_data_df)
-
-            if not aux_df.empty:
-                normal_df = self._join_aux_to_normal(aux_df, normal_df)
-        return normal_df
-
-    def _join_aux_to_normal(self, aux_df, normal_df):
-        # TODO: clean up setting index (Data_Point). This is currently done in _post_process after
-        #    the column names are changed to cellpy-column names ("data_point").
-        #    It also keeps a copy of the "data_point"
-        #    column. And is that really necessary.
-        normal_df.set_index(self.arbin_headers_normal.data_point_txt, inplace=True)
-        normal_df = normal_df.join(aux_df, how="left")
-        normal_df.reset_index(inplace=True)
-        return normal_df
-
-    def _rename_aux_cols(self, aux_df, aux_global_data_df):
-        aux_dfs = []
-        if self.arbin_headers_aux.x_value_txt in aux_df.columns:
-            aux_df_x = aux_df[self.arbin_headers_aux.x_value_txt].copy()
-            aux_df_x.columns = [
-                self._make_name_from_frame(aux_global_data_df, z[1], z[0])
-                for z in aux_df_x.columns
-            ]
-            aux_dfs.append(aux_df_x)
-        if self.arbin_headers_aux.x_dt_value in aux_df.columns:
-            aux_df_dx_dt = aux_df[self.arbin_headers_aux.x_dt_value].copy()
-            aux_df_dx_dt.columns = [
-                self._make_name_from_frame(aux_global_data_df, z[1], z[0], True)
-                for z in aux_df_dx_dt.columns
-            ]
-            aux_dfs.append(aux_df_dx_dt)
-        aux_df = pd.concat(aux_dfs, axis=1)
-        return aux_df
-
-    def _aux_to_wide(self, aux_df, aux_global_data_df):
-        aux_df = aux_df.drop(self.arbin_headers_aux.test_id_txt, axis=1)
-        keys = [
-            self.arbin_headers_aux.data_point_txt,
-            self.arbin_headers_aux.aux_index_txt,
-            self.arbin_headers_aux.data_type_txt,
-        ]
-        aux_df = aux_df.set_index(keys=keys)
-        aux_df = aux_df.unstack(2).unstack(1).dropna(axis=1)
-        aux_global_data_df = aux_global_data_df.fillna(0)
-        return aux_df, aux_global_data_df
-
-    def _get_aux_df(self, conn, test_id, table_name_aux):
-        columns_txt = "*"
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_aux
-        sql_3 = "where %s=%s " % (self.arbin_headers_aux.test_id_txt, test_id)
-        sql_4 = ""
-        sql_aux = sql_1 + sql_2 + sql_3 + sql_4
-        aux_df = self._query_table(table_name_aux, conn, sql=sql_aux)
-        return aux_df
-
-    def _loader_posix(
-        self,
-        file_name,
-        temp_filename,
-        temp_dir,
-        *args,
-        bad_steps=None,
-        dataset_number=None,
-        data_points=None,
-        **kwargs,
-    ):
-        # TODO: auxiliary channels (table)
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-        table_name_aux_global = TABLE_NAMES["aux_global"]
-        table_name_aux = TABLE_NAMES["aux"]
-
-        if is_posix:
-            if is_macos:
-                self.logger.debug("\nMAC OSX USING MDBTOOLS")
-            else:
-                self.logger.debug("\nPOSIX USING MDBTOOLS")
-        else:
-            self.logger.debug("\nWINDOWS USING MDBTOOLS-WIN")
-
-        if DEBUG_MODE:
-            time_0 = time.time()
-
-        (
-            tmp_name_global,
-            tmp_name_raw,
-            tmp_name_stats,
-            tmp_name_aux_global,
-            tmp_name_aux,
-        ) = self._create_tmp_files(
-            table_name_global,
-            table_name_normal,
-            table_name_stats,
-            table_name_aux_global,
-            table_name_aux,
-            temp_dir,
-            temp_filename,
-        )
-
-        # use pandas to load in the data
-        global_data_df = pd.read_csv(tmp_name_global)
-        tests = global_data_df[self.arbin_headers_normal.test_id_txt]
-        number_of_sets = len(tests)
-        self.logger.debug("number of datasets: %i" % number_of_sets)
-
-        if dataset_number is not None:
-            self.logger.info(f"Dataset number given: {dataset_number}")
-            self.logger.info(f"Available dataset numbers: {tests}")
-        else:
-            dataset_number = 0
-
-        data = self._init_data(file_name, global_data_df, dataset_number)
-
-        self.logger.debug("reading raw-data")
-
-        (
-            length_of_test,
-            normal_df,
-            summary_df,
-            aux_global_data_df,
-            aux_df,
-        ) = self._load_from_tmp_files(
-            data,
-            tmp_name_global,
-            tmp_name_raw,
-            tmp_name_stats,
-            tmp_name_aux_global,
-            tmp_name_aux,
-            temp_filename,
-            bad_steps,
-            data_points,
-        )
-
-        # --------- read auxiliary data (aux-data) ---------------------
-        normal_df = self._load_posix_res_auxiliary_table(
-            aux_global_data_df, aux_df, normal_df
-        )
-
-        if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-            txt = "\nCould not find any summary (stats-file)!"
-            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-            logging.debug(txt)
-        # normal_df = normal_df.set_index("Data_Point")
-
-        data.summary = summary_df
-        if DEBUG_MODE:
-            mem_usage = normal_df.memory_usage()
-            logging.debug(
-                f"memory usage for "
-                f"loaded data: \n{mem_usage}"
-                f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-            )
-            logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
-
-        data.raw = normal_df
-        data.raw_data_files_length.append(length_of_test)
-        data = self._post_process(data)
-        data = self.identify_last_data_point(data)
-        return data
-
-    def _check_size(self):
-        file_size = os.path.getsize(self.temp_file_path)
-        hfilesize = humanize_bytes(file_size)
-        txt = f"File size: {file_size} ({hfilesize})"
-        self.logger.debug(txt)
-        if file_size > prms.Instruments.Arbin.max_res_filesize:
-            error_message = "\nERROR (loader):\n"
-            error_message += (
-                f"{hfilesize} > {humanize_bytes(prms.Instruments.Arbin.max_res_filesize)} "
-                f"- File is too big!\n"
-            )
-            error_message += "(edit prms.Instruments.Arbin ['max_res_filesize'])\n"
-            logging.critical(error_message)
-            return False
-        return True
-
-    def loader(
-        self,
-        name,
-        *args,
-        bad_steps=None,
-        dataset_number=None,
-        data_points=None,
-        **kwargs,
-    ):
-        """Loads data from arbin .res files.
-
-        Args:
-            name (str): path to .res file.
-            bad_steps (list of tuples): (c, s) tuples of steps s (in cycle c)
-                to skip loading.
-            dataset_number (int): the data set number to select if you are dealing
-                with arbin files with more than one data-set.
-            data_points (tuple of ints): load only data from data_point[0] to
-                    data_point[1] (use None for infinite).
-
-        Returns:
-            new data (Data)
-        """
-        # TODO: @jepe - insert kwargs - current chunk, only normal data, etc
-        try:
-            not_too_big = self._check_size()
-            if not not_too_big:
-                return None
-        except Exception as e:
-            self.logger.debug(f"could not get file size: {e}")
-
-        use_mdbtools = False
-        if use_subprocess:
-            use_mdbtools = True
-        if is_posix:
-            use_mdbtools = True
-
-        if use_mdbtools:
-            new_data = self._loader_posix(
-                self.name,
-                self.temp_file_path,
-                self.temp_file_path.parent,
-                *args,
-                bad_steps=bad_steps,
-                dataset_number=dataset_number,
-                data_points=data_points,
-                **kwargs,
-            )
-        else:
-            new_data = self._loader_win(
-                self.name,
-                self.temp_file_path,
-                *args,
-                bad_steps=bad_steps,
-                dataset_number=dataset_number,
-                data_points=data_points,
-                **kwargs,
-            )
-
-        new_data = self._inspect(new_data)
-
-        return new_data
-
-    @staticmethod
-    def _create_tmp_files(
-        table_name_global,
-        table_name_normal,
-        table_name_stats,
-        table_name_aux_global,
-        table_name_aux,
-        temp_dir,
-        temp_filename,
-    ):
-        import subprocess
-
-        # creating tmp-filenames
-        temp_csv_filename_global = os.path.join(temp_dir, "global_tmp.csv")
-        temp_csv_filename_normal = os.path.join(temp_dir, "normal_tmp.csv")
-        temp_csv_filename_stats = os.path.join(temp_dir, "stats_tmp.csv")
-        temp_csv_filename_aux_global = os.path.join(temp_dir, "aux_global_tmp.csv")
-        temp_csv_filename_aux = os.path.join(temp_dir, "aux_tmp.csv")
-        # making the cmds
-        mdb_prms = [
-            (table_name_global, temp_csv_filename_global),
-            (table_name_normal, temp_csv_filename_normal),
-            (table_name_stats, temp_csv_filename_stats),
-            (table_name_aux_global, temp_csv_filename_aux_global),
-            (table_name_aux, temp_csv_filename_aux),
-        ]
-        # executing cmds
-        for table_name, tmp_file in mdb_prms:
-            with open(tmp_file, "w") as f:
-                try:
-                    subprocess.call(
-                        [sub_process_path, temp_filename, table_name], stdout=f
-                    )
-                    logging.debug(f"ran mdb-export {str(f)} {table_name}")
-                except FileNotFoundError as e:
-                    logging.critical(
-                        f"Could not run {sub_process_path} on {temp_filename}"
-                    )
-                    logging.critical(f"Possible work-around: install mdbtools")
-                    raise e
-        return (
-            temp_csv_filename_global,
-            temp_csv_filename_normal,
-            temp_csv_filename_stats,
-            temp_csv_filename_aux_global,
-            temp_csv_filename_aux,
-        )
-
-    def _load_from_tmp_files(
-        self,
-        data,
-        temp_csv_filename_global,
-        temp_csv_filename_normal,
-        temp_csv_filename_stats,
-        temp_csv_filename_aux_global,
-        temp_csv_filename_aux,
-        temp_filename,
-        bad_steps,
-        data_points,
-    ):
-        """
-        if bad_steps is not None:
-            if not isinstance(bad_steps, (list, tuple)):
-                bad_steps = [bad_steps]
-            for bad_cycle, bad_step in bad_steps:
-                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
-                sql_4 += "AND NOT (%s=%i " % (
-                    self.headers_normal.cycle_index_txt,
-                    bad_cycle,
-                )
-                sql_4 += "AND %s=%i) " % (self.headers_normal.step_index_txt, bad_step)
-
-        """
-        # should include a more efficient to load the csv (maybe a loop where
-        #   we load only chuncks and only keep the parts that fullfill the
-        #   filters (e.g. bad_steps, data_points,...)
-        normal_df = pd.read_csv(temp_csv_filename_normal)
-        # filter on test ID
-        normal_df = normal_df[
-            normal_df[self.arbin_headers_normal.test_id_txt]
-            == data._internal_test_number
-        ]
-        # sort on data point
-        if prms._sort_if_subprocess:
-            normal_df = normal_df.sort_values(self.arbin_headers_normal.data_point_txt)
-
-        if bad_steps is not None:
-            logging.debug("removing bad steps")
-            if not isinstance(bad_steps, (list, tuple)):
-                bad_steps = [bad_steps]
-            if not isinstance(bad_steps[0], (list, tuple)):
-                bad_steps = [bad_steps]
-            for bad_cycle, bad_step in bad_steps:
-                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
-
-                selector = (
-                    normal_df[self.arbin_headers_normal.cycle_index_txt] == bad_cycle
-                ) & (normal_df[self.arbin_headers_normal.step_index_txt] == bad_step)
-
-                normal_df = normal_df.loc[~selector, :]
-
-        if prms.Reader.limit_loaded_cycles:
-            logging.debug("Not yet tested for aux data")
-            if len(prms.Reader.limit_loaded_cycles) > 1:
-                c1, c2 = prms.Reader.limit_loaded_cycles
-                selector = (
-                    normal_df[self.arbin_headers_normal.cycle_index_txt] > c1
-                ) & (normal_df[self.arbin_headers_normal.cycle_index_txt] < c2)
-
-            else:
-                c1 = prms.Reader.limit_loaded_cycles[0]
-                selector = normal_df[self.arbin_headers_normal.cycle_index_txt] == c1
-
-            normal_df = normal_df.loc[selector, :]
-
-        if data_points is not None:
-            logging.debug("selecting data-point range")
-            logging.debug("Not yet tested for aux data")
-            d1, d2 = data_points
-
-            if d1 is not None:
-                selector = normal_df[self.arbin_headers_normal.data_point_txt] >= d1
-                normal_df = normal_df.loc[selector, :]
-
-            if d2 is not None:
-                selector = normal_df[self.arbin_headers_normal.data_point_txt] <= d2
-                normal_df = normal_df.loc[selector, :]
-
-        length_of_test = normal_df.shape[0]
-        summary_df = pd.read_csv(temp_csv_filename_stats)
-        aux_global_df = pd.read_csv(temp_csv_filename_aux_global)
-        aux_df = pd.read_csv(temp_csv_filename_aux)
-
-        # clean up
-        for f in [
-            temp_filename,
-            temp_csv_filename_stats,
-            temp_csv_filename_normal,
-            temp_csv_filename_global,
-            temp_csv_filename_aux_global,
-            temp_csv_filename_aux,
-        ]:
-            if os.path.isfile(f):
-                try:
-                    os.remove(f)
-                except WindowsError as e:
-                    logging.warning(f"could not remove tmp-file\n{f} {e}")
-        return length_of_test, normal_df, summary_df, aux_global_df, aux_df
-
-    def _init_data(self, file_name, global_data_df, test_no):
-        data = Data()
-        data.loaded_from = file_name
-        self.generate_fid()
-        # name of the .res file it is loaded from:
-        # data.parent_filename = os.path.basename(file_name)
-        data.channel_index = int(
-            global_data_df[self.arbin_headers_global["channel_index_txt"]][test_no]
-        )
-
-        data.creator = global_data_df[self.arbin_headers_global["creator_txt"]][test_no]
-        data.test_ID = global_data_df[self.arbin_headers_global["item_id_txt"]][test_no]
-        data.schedule_file_name = global_data_df[
-            self.arbin_headers_global["schedule_file_name_txt"]
-        ][test_no]
-        data.start_datetime = global_data_df[
-            self.arbin_headers_global["start_datetime_txt"]
-        ][test_no]
-        data._internal_test_number = int(
-            global_data_df[self.arbin_headers_normal.test_id_txt][test_no]
-        )
-        data.test_name = global_data_df[self.arbin_headers_global["test_name_txt"]][
-            test_no
-        ]
-        data.raw_data_files.append(self.fid)
-        return data
-
-    def _normal_table_generator(self, **kwargs):
-        pass
-
-    def _load_res_normal_table(
-        self, conn, _internal_test_number, bad_steps, data_points
-    ):
-        self.logger.debug("starting loading raw-data")
-        self.logger.debug(
-            f"connection: {conn} internal test-ID: {_internal_test_number}"
-        )
-        self.logger.debug(f"bad steps:  {bad_steps}")
-
-        table_name_normal = TABLE_NAMES["normal"]
-
-        if prms.Reader.select_minimal:  # SETTING
-            columns = MINIMUM_SELECTION
-            columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
-        else:
-            columns_txt = "*"
-
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (
-            self.arbin_headers_normal.test_id_txt,
-            _internal_test_number,
-        )
-        sql_4 = ""
-
-        if bad_steps is not None:
-            if not isinstance(bad_steps, (list, tuple)):
-                bad_steps = [bad_steps]
-            if not isinstance(bad_steps[0], (list, tuple)):
-                bad_steps = [bad_steps]
-            for bad_cycle, bad_step in bad_steps:
-                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
-                sql_4 += "AND NOT (%s=%i " % (
-                    self.arbin_headers_normal.cycle_index_txt,
-                    bad_cycle,
-                )
-                sql_4 += "AND %s=%i) " % (
-                    self.arbin_headers_normal.step_index_txt,
-                    bad_step,
-                )
-
-        if prms.Reader.limit_loaded_cycles:
-            if len(prms.Reader.limit_loaded_cycles) > 1:
-                sql_4 += "AND %s>%i " % (
-                    self.arbin_headers_normal.cycle_index_txt,
-                    prms.Reader.limit_loaded_cycles[0],
-                )
-                sql_4 += "AND %s<%i " % (
-                    self.arbin_headers_normal.cycle_index_txt,
-                    prms.Reader.limit_loaded_cycles[-1],
-                )
-            else:
-                sql_4 = "AND %s=%i " % (
-                    self.arbin_headers_normal.cycle_index_txt,
-                    prms.Reader.limit_loaded_cycles[0],
-                )
-
-        if data_points is not None:
-            d1, d2 = data_points
-            if d1 is not None:
-                sql_4 += "AND %s>=%i " % (self.arbin_headers_normal.data_point_txt, d1)
-            if d2 is not None:
-                sql_4 += "AND %s<=%i " % (self.arbin_headers_normal.data_point_txt, d2)
-
-        sql_5 = "order by %s" % self.arbin_headers_normal.data_point_txt
-        sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
-
-        self.logger.debug("INFO ABOUT LOAD RES NORMAL")
-        self.logger.debug("sql statement: %s" % sql)
-
-        if DEBUG_MODE:
-            current_memory_usage = sys.getsizeof(self)
-
-        if not prms.Instruments.Arbin.chunk_size:
-            self.logger.debug("no chunk-size given")
-            # memory here
-            normal_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-            # memory here
-            length_of_test = normal_df.shape[0]
-        else:
-            self.logger.debug(f"chunk-size: {prms.Instruments.Arbin.chunk_size}")
-            self.logger.debug("creating a pd.read_sql_query generator")
-
-            normal_df_reader = pd.read_sql_query(
-                sql=sa.text(sql),
-                con=conn.connect(),
-                chunksize=prms.Instruments.Arbin.chunk_size,
-            )
-            normal_df = None
-            chunk_number = 0
-            self.logger.debug("created pandas sql reader")
-            self.logger.debug("iterating chunk-wise")
-            for i, chunk in enumerate(normal_df_reader):
-                self.logger.debug(f"iteration number {i}")
-                if prms.Instruments.Arbin.max_chunks:
-                    self.logger.debug(
-                        f"max number of chunks mode "
-                        f"({prms.Instruments.Arbin.max_chunks})"
-                    )
-                    if chunk_number < prms.Instruments.Arbin.max_chunks:
-                        normal_df = pd.concat([normal_df, chunk], ignore_index=True)
-                        self.logger.debug(
-                            f"chunk {i} of {prms.Instruments.Arbin.max_chunks}"
-                        )
-                    else:
-                        break
-                else:
-                    try:
-                        normal_df = pd.concat([normal_df, chunk], ignore_index=True)
-                        self.logger.debug("concatenated new chunk")
-                    except MemoryError:
-                        self.logger.error(
-                            " - Could not read complete file (MemoryError)."
-                        )
-                        self.logger.error(
-                            f"Last successfully loaded chunk " f"number: {chunk_number}"
-                        )
-                        self.logger.error(
-                            f"Chunk size: {prms.Instruments.Arbin.chunk_size}"
-                        )
-                        break
-                chunk_number += 1
-            length_of_test = normal_df.shape[0]
-            self.logger.debug(f"finished iterating (#rows: {length_of_test})")
-
-        self.logger.debug(f"loaded to normal_df (length =  {length_of_test})")
-        self.logger.debug(f"Headers:\n{normal_df.columns}")
-
-        if normal_df is None:
-            normal_df = pd.DataFrame(columns=self.arbin_headers_normal.values())
-        return length_of_test, normal_df
-
-
-def check_loader_aux():
-    from pathlib import Path
-
-    from cellpy import log
-
-    log.setup_logging(default_level="CRITICAL")
-    p = Path(r"C:\scripts\cellpy_dev_resources\2020_jinpeng_aux_temperature")
-    f1 = p / "BIT_LFP5p12s_Pack02_CAP_Cyc200_T25_Nov23.res"
-    f2 = p / "BIT_LFP50_12S1P_SOP_0_97_T5_cyc200_3500W_20191231.res"
-    f3 = p / "TJP_LR1865SZ_OCV_19_Cyc150_T25_201105.res"
-
-    n = DataLoader().loader(f1)
-    print(n[0].raw.tail())
-
-
-def check_loader_empty_normal():
-    from cellpy import log
-
-    log.setup_logging(default_level="CRITICAL")
-
-    a = DataLoader()
-    cols = a.arbin_headers_normal
-    df = pd.DataFrame(columns=cols.values())
-    print(df)
-    print(df.empty)
-
-
-if __name__ == "__main__":
-    print(" arbin-res-py ".center(80, "="))
-    check_loader_empty_normal()
-    print(" finished ".center(80, "="))
+"""arbin res-type data files"""
+import logging
+import os
+import pathlib
+import platform
+import shutil
+import sys
+import tempfile
+import time
+import warnings
+
+import numpy as np
+import pandas as pd
+import sqlalchemy as sa
+
+from cellpy import prms
+from cellpy.exceptions import NullData
+from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
+from cellpy.readers.core import (
+    Data,
+    FileID,
+    check64bit,
+    humanize_bytes,
+    xldate_as_datetime,
+)
+from cellpy.readers.instruments.base import MINIMUM_SELECTION, BaseLoader
+
+# TODO: use InstrumentSettings (dataclass) from internal_settings instead of HeaderDict.
+
+DEBUG_MODE = prms.Reader.diagnostics
+ALLOW_MULTI_TEST_FILE = False
+USE_SQLALCHEMY_ACCESS_ENGINE = True
+
+# Select odbc module
+ODBC = prms._odbc
+SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver
+
+use_subprocess = prms.Instruments.Arbin.use_subprocess
+detect_subprocess_need = prms.Instruments.Arbin.detect_subprocess_need
+
+is_posix = False
+is_macos = False
+if os.name == "posix":
+    is_posix = True
+current_platform = platform.system()
+if current_platform == "Darwin":
+    is_macos = True
+
+if DEBUG_MODE:
+    logging.debug("DEBUG_MODE")
+    logging.debug(f"ODBC: {ODBC}")
+    logging.debug(f"SEARCH_FOR_ODBC_DRIVERS: {SEARCH_FOR_ODBC_DRIVERS}")
+    logging.debug(f"use_subprocess: {use_subprocess}")
+    logging.debug(f"detect_subprocess_need: {detect_subprocess_need}")
+    logging.debug(f"current_platform: {current_platform}")
+
+if detect_subprocess_need:
+    logging.debug("detect_subprocess_need is True: checking versions")
+    python_version, os_version = platform.architecture()
+    if python_version == "64bit" and prms.Instruments.Arbin.office_version == "32bit":
+        logging.debug(
+            "python 64bit and office 32bit -> " "setting use_subprocess to True"
+        )
+        use_subprocess = True
+
+if use_subprocess and not is_posix:
+    # The Windows users most likely have a strange custom path to mdbtools etc.
+    logging.debug(
+        "using subprocess (most likely mdbtools) on non-posix (most likely windows)"
+    )
+    if not prms.Instruments.Arbin.sub_process_path:
+        sub_process_path = str(prms._sub_process_path)
+    else:
+        sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
+
+if is_posix:
+    sub_process_path = "mdb-export"
+
+try:
+    driver_dll = prms.Instruments.Arbin.odbc_driver
+except AttributeError:
+    driver_dll = None
+
+if ODBC == "pyodbc":
+    try:
+        import pyodbc as dbloader
+    except ImportError:
+        warnings.warn("COULD NOT LOAD DBLOADER!", ImportWarning)
+        dbloader = None
+
+elif ODBC == "pypyodbc":
+    try:
+        import pypyodbc as dbloader
+    except ImportError:
+        warnings.warn("COULD NOT LOAD DBLOADER!", ImportWarning)
+        dbloader = None
+
+if DEBUG_MODE:
+    logging.debug(f"dbloader: {dbloader}")
+
+
+# Names of the tables in the .res db that is used by cellpy
+TABLE_NAMES = {
+    "normal": "Channel_Normal_Table",
+    "global": "Global_Table",
+    "statistic": "Channel_Statistic_Table",
+    "aux_global": "Aux_Global_Data_Table",
+    "aux": "Auxiliary_Table",
+}
+
+summary_headers_renaming_dict = {
+    "test_id_txt": "Test_ID",
+    "data_point_txt": "Data_Point",
+    "vmax_on_cycle_txt": "Vmax_On_Cycle",
+    "charge_time_txt": "Charge_Time",
+    "discharge_time_txt": "Discharge_Time",
+}
+
+normal_headers_renaming_dict = {
+    "aci_phase_angle_txt": "ACI_Phase_Angle",
+    "ref_aci_phase_angle_txt": "Reference_ACI_Phase_Angle",
+    "ac_impedance_txt": "AC_Impedance",
+    "ref_ac_impedance_txt": "Reference_AC_Impedance",
+    "charge_capacity_txt": "Charge_Capacity",
+    "charge_energy_txt": "Charge_Energy",
+    "current_txt": "Current",
+    "cycle_index_txt": "Cycle_Index",
+    "data_point_txt": "Data_Point",
+    "datetime_txt": "DateTime",
+    "discharge_capacity_txt": "Discharge_Capacity",
+    "discharge_energy_txt": "Discharge_Energy",
+    "internal_resistance_txt": "Internal_Resistance",
+    "is_fc_data_txt": "Is_FC_Data",
+    "step_index_txt": "Step_Index",
+    "sub_step_index_txt": "Sub_Step_Index",  # new
+    "step_time_txt": "Step_Time",
+    "sub_step_time_txt": "Sub_Step_Time",  # new
+    "test_id_txt": "Test_ID",
+    "test_time_txt": "Test_Time",
+    "voltage_txt": "Voltage",
+    "ref_voltage_txt": "Reference_Voltage",  # new
+    "dv_dt_txt": "dV/dt",
+    "frequency_txt": "Frequency",  # new
+    "amplitude_txt": "Amplitude",  # new
+}
+
+
+class DataLoader(BaseLoader):
+    """Class for loading arbin-data from res-files.
+
+    Implemented Cellpy params (prms.Instruments.Arbin):
+        max_res_filesize
+        chunk_size
+        max_chunks
+        use_subprocess
+        detect_subprocess_need
+        sub_process_path
+        office_version
+        SQL_server
+
+    """
+
+    instrument_name = "arbin_res"
+    raw_ext = "res"
+
+    def __init__(self, *args, **kwargs):
+        """initiates the ArbinLoader class"""
+        # could use __init__(self, cellpydata_object) and
+        # set self.logger = cellpydata_object.logger etc.
+        # then remember to include that as prm in "out of class" functions
+        # self.prms = prms
+        self.raw_ext = "res"
+        self.logger = logging.getLogger(__name__)
+        # use the following prm to limit to loading only
+        # one cycle or from cycle>x to cycle<x+n
+        # prms.Reader.limit_loaded_cycles = [cycle from, cycle to]
+
+        self.arbin_headers_normal = (
+            self.get_headers_normal()
+        )  # the column headers defined by Arbin
+        self.cellpy_headers_normal = (
+            get_headers_normal()
+        )  # the column headers defined by cellpy
+        self.arbin_headers_global = self.get_headers_global()
+        self.arbin_headers_aux_global = self.get_headers_aux_global()
+        self.arbin_headers_aux = self.get_headers_aux()
+        self.current_chunk = 0  # use this to set chunks to load
+
+    @staticmethod
+    def get_raw_units():
+        raw_units = dict()
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
+        return raw_units
+
+    @staticmethod
+    def get_headers_normal():
+        """Defines the so-called normal column headings for Arbin .res-files"""
+        headers = HeaderDict()
+        # - normal (raw-data) column headings (specific for Arbin)
+
+        headers["aci_phase_angle_txt"] = "ACI_Phase_Angle"
+        headers["ref_aci_phase_angle_txt"] = "Reference_ACI_Phase_Angle"
+
+        headers["ac_impedance_txt"] = "AC_Impedance"
+        headers["ref_ac_impedance_txt"] = "Reference_AC_Impedance"  # new
+
+        headers["charge_capacity_txt"] = "Charge_Capacity"
+        headers["charge_energy_txt"] = "Charge_Energy"
+        headers["current_txt"] = "Current"
+        headers["cycle_index_txt"] = "Cycle_Index"
+        headers["data_point_txt"] = "Data_Point"
+        headers["datetime_txt"] = "DateTime"
+        headers["discharge_capacity_txt"] = "Discharge_Capacity"
+        headers["discharge_energy_txt"] = "Discharge_Energy"
+        headers["internal_resistance_txt"] = "Internal_Resistance"
+
+        headers["is_fc_data_txt"] = "Is_FC_Data"
+        headers["step_index_txt"] = "Step_Index"
+        headers["sub_step_index_txt"] = "Sub_Step_Index"  # new
+
+        headers["step_time_txt"] = "Step_Time"
+        headers["sub_step_time_txt"] = "Sub_Step_Time"  # new
+
+        headers["test_id_txt"] = "Test_ID"
+        headers["test_time_txt"] = "Test_Time"
+
+        headers["voltage_txt"] = "Voltage"
+        headers["ref_voltage_txt"] = "Reference_Voltage"  # new
+
+        headers["dv_dt_txt"] = "dV/dt"
+        headers["frequency_txt"] = "Frequency"  # new
+        headers["amplitude_txt"] = "Amplitude"  # new
+
+        return headers
+
+    @staticmethod
+    def get_headers_aux():
+        """Defines the so-called auxiliary table column headings for Arbin .res-files"""
+        headers = HeaderDict()
+        # - aux column headings (specific for Arbin)
+        headers["test_id_txt"] = "Test_ID"
+        headers["data_point_txt"] = "Data_Point"
+        headers["aux_index_txt"] = "Auxiliary_Index"
+        headers["data_type_txt"] = "Data_Type"
+        headers["x_value_txt"] = "X"
+        headers["x_dt_value"] = "dX_dt"
+        return headers
+
+    @staticmethod
+    def get_headers_aux_global():
+        """Defines the so-called auxiliary global column headings for Arbin .res-files"""
+        headers = HeaderDict()
+        # - aux global column headings (specific for Arbin)
+        headers["channel_index_txt"] = "Channel_Index"
+        headers["aux_index_txt"] = "Auxiliary_Index"
+        headers["data_type_txt"] = "Data_Type"
+        headers["aux_name_txt"] = "Nickname"
+        headers["aux_unit_txt"] = "Unit"
+        return headers
+
+    @staticmethod
+    def get_headers_global():
+        """Defines the so-called global column headings for Arbin .res-files"""
+        headers = HeaderDict()
+        # - global column headings (specific for Arbin)
+        headers["applications_path_txt"] = "Applications_Path"
+        headers["channel_index_txt"] = "Channel_Index"
+        headers["channel_number_txt"] = "Channel_Number"
+        headers["channel_type_txt"] = "Channel_Type"
+        headers["comments_txt"] = "Comments"
+        headers["creator_txt"] = "Creator"
+        headers["daq_index_txt"] = "DAQ_Index"
+        headers["item_id_txt"] = "Item_ID"
+        headers["log_aux_data_flag_txt"] = "Log_Aux_Data_Flag"
+        headers["log_chanstat_data_flag_txt"] = "Log_ChanStat_Data_Flag"
+        headers["log_event_data_flag_txt"] = "Log_Event_Data_Flag"
+        headers["log_smart_battery_data_flag_txt"] = "Log_Smart_Battery_Data_Flag"
+        headers["mapped_aux_conc_cnumber_txt"] = "Mapped_Aux_Conc_CNumber"
+        headers["mapped_aux_di_cnumber_txt"] = "Mapped_Aux_DI_CNumber"
+        headers["mapped_aux_do_cnumber_txt"] = "Mapped_Aux_DO_CNumber"
+        headers["mapped_aux_flow_rate_cnumber_txt"] = "Mapped_Aux_Flow_Rate_CNumber"
+        headers["mapped_aux_ph_number_txt"] = "Mapped_Aux_PH_Number"
+        headers["mapped_aux_pressure_number_txt"] = "Mapped_Aux_Pressure_Number"
+        headers["mapped_aux_temperature_number_txt"] = "Mapped_Aux_Temperature_Number"
+        headers["mapped_aux_voltage_number_txt"] = "Mapped_Aux_Voltage_Number"
+        headers[
+            "schedule_file_name_txt"
+        ] = "Schedule_File_Name"  # KEEP FOR CELLPY FILE FORMAT
+        headers["start_datetime_txt"] = "Start_DateTime"
+        headers["test_id_txt"] = "Test_ID"  # KEEP FOR CELLPY FILE FORMAT
+        headers["test_name_txt"] = "Test_Name"  # KEEP FOR CELLPY FILE FORMAT
+        return headers
+
+    @staticmethod
+    def get_raw_limits():
+        raw_limits = dict()
+        raw_limits["current_hard"] = 0.000_000_000_000_1
+        raw_limits["current_soft"] = 0.000_01
+        raw_limits["stable_current_hard"] = 2.0
+        raw_limits["stable_current_soft"] = 4.0
+        raw_limits["stable_voltage_hard"] = 2.0
+        raw_limits["stable_voltage_soft"] = 4.0
+        raw_limits["stable_charge_hard"] = 0.001
+        raw_limits["stable_charge_soft"] = 5.0
+        raw_limits["ir_change"] = 0.00001
+        return raw_limits
+
+    def _get_res_connector(self, temp_filename):
+        """Returns a connection to the .res-file"""
+
+        if dbloader is None:
+            txt = f"{ODBC=}\n"
+            txt += f"{SEARCH_FOR_ODBC_DRIVERS=}\n"
+            txt += f"{use_subprocess=}\n"
+            txt += f"{detect_subprocess_need=}\n"
+            txt += f"{current_platform=}\n"
+            raise ValueError(
+                f"Something went seriously wrong." f"dbloader is None.\n{txt}"
+            )
+
+        if SEARCH_FOR_ODBC_DRIVERS:
+            logging.debug("Searching for odbc drivers")
+            try:
+                drivers = [
+                    driver
+                    for driver in dbloader.drivers()
+                    if "Microsoft Access Driver" in driver
+                ]
+                logging.debug(f"Found these: {drivers}")
+                driver = drivers[0]
+
+            except AttributeError as e:
+                print("ODBC drivers not found.")
+
+            except IndexError as e:
+                logging.debug("Unfortunately, it seems the list of drivers is emtpy.")
+                logging.debug("Use driver-name from config (if existing).")
+                driver = driver_dll
+                if is_macos:
+                    driver = "/usr/local/lib/libmdbodbc.dylib"
+                else:
+                    if not driver:
+                        print(
+                            "\nCould not find any odbc-drivers suitable "
+                            "for .res-type files. "
+                            "Check out the homepage of pydobc for info on "
+                            "installing drivers"
+                        )
+                        print(
+                            "One solution that might work is downloading "
+                            "the Microsoft Access database engine (in correct"
+                            " bytes (32 or 64)) "
+                            "from:\n"
+                            "https://www.microsoft.com/en-us/download/"
+                            "details.aspx?id=13255"
+                        )
+                        print(
+                            "Or install mdbtools and set it up "
+                            "(check the cellpy docs for help)"
+                        )
+                        print("\n")
+                    else:
+                        logging.debug("Using driver dll from config file")
+                        logging.debug(f"driver dll: {driver}")
+
+            self.logger.debug(f"odbc constr: {driver}")
+
+        else:
+            is64bit_python = check64bit(current_system="python")
+            if is64bit_python:
+                driver = "{Microsoft Access Driver (*.mdb, *.accdb)}"
+            else:
+                driver = "Microsoft Access Driver (*.mdb)"
+            self.logger.debug(f"odbc constr: {driver}")
+
+        constr = f"Driver={driver};Dbq={temp_filename};ExtendedAnsiSQL=1;"
+        logging.debug(f"constr: {constr}")
+
+        return constr
+
+    def _get_connection_or_engine(self, temp_filename):
+        # updated to use sqlalchemy - needs sqlalchemy-access
+        constr = self._get_res_connector(temp_filename)
+        self.logger.debug(f"constr str: {constr}")
+        connection_url = sa.engine.URL.create(
+            "access+pyodbc", query={"odbc_connect": constr}
+        )
+        engine = sa.create_engine(connection_url)
+        return engine
+
+    def _clean_up_loadres(self, cur, conn, filename):
+        if cur is not None:
+            cur.close()  # adodbapi
+        if conn is not None:
+            conn.close()  # adodbapi
+        if os.path.isfile(filename):
+            try:
+                os.remove(filename)
+            except WindowsError as e:
+                self.logger.warning("could not remove tmp-file\n%s %s" % (filename, e))
+
+    def _post_process(self, data):
+        fix_datetime = True
+        set_index = True
+        rename_headers = True
+
+        # TODO:  insert post-processing and div tests here
+        #    - check dtypes
+
+        # Remark that we also set index during saving the file to hdf5 if
+        #   it is not set.
+
+        if rename_headers:
+            columns = {}
+            for key in self.arbin_headers_normal:
+                old_header = normal_headers_renaming_dict[key]
+                new_header = self.cellpy_headers_normal[key]
+                columns[old_header] = new_header
+
+            data.raw.rename(index=str, columns=columns, inplace=True)
+            try:
+                # TODO: check if summary df is existing (to only check if it is
+                #  empty will give an error later!)
+                columns = {}
+                for key, old_header in summary_headers_renaming_dict.items():
+                    try:
+                        columns[old_header] = self.cellpy_headers_normal[key]
+                    except KeyError:
+                        columns[old_header] = old_header.lower()
+                data.summary.rename(index=str, columns=columns, inplace=True)
+            except Exception as e:
+                txt = (
+                    f"Exception raised ({e})\n"
+                    f"key: {key} old_header: {old_header}"
+                    f"cellpy headers normal type {type(self.cellpy_headers_normal)}"
+                )
+                raise Exception(txt)
+
+        if fix_datetime:
+            h_datetime = self.cellpy_headers_normal.datetime_txt
+            logging.debug("converting to datetime format")
+            # print(data.raw.columns)
+            data.raw[h_datetime] = data.raw[h_datetime].apply(
+                xldate_as_datetime, option="to_datetime"
+            )
+
+            h_datetime = h_datetime
+            if h_datetime in data.summary:
+                data.summary[h_datetime] = data.summary[h_datetime].apply(
+                    xldate_as_datetime, option="to_datetime"
+                )
+
+        if set_index:
+            hdr_data_point = self.cellpy_headers_normal.data_point_txt
+            if data.raw.index.name != hdr_data_point:
+                data.raw = data.raw.set_index(hdr_data_point, drop=False)
+
+        return data
+
+    def _inspect(self, run_data):
+        """Inspect the file -> reports to log (debug)"""
+
+        if not any([DEBUG_MODE]):
+            return run_data
+
+        if DEBUG_MODE:
+            new_cols = run_data.raw.columns
+            for col in self.arbin_headers_normal:
+                if col not in new_cols:
+                    logging.debug(f"Missing col: {col}")
+                    # data.raw[col] = np.nan
+            return run_data
+
+    def repair(self, file_name):
+        """try to repair a broken/corrupted file"""
+        raise NotImplemented
+
+    def _query_table(self, table_name, conn, sql=None):
+        from sqlalchemy import create_engine, text
+
+        self.logger.debug(f"reading {table_name}")
+        if sql is None:
+            sql = f"select * from {table_name}"
+        self.logger.debug(f"sql statement: {sql}")
+        df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
+        return df
+
+    def _make_name_from_frame(self, df, aux_index, data_type, dx_dt=False):
+        df_names = df.loc[
+            (df[self.arbin_headers_aux_global.aux_index_txt] == aux_index)
+            & (df[self.arbin_headers_aux_global.data_type_txt] == data_type),
+            :,
+        ]
+        unit = df_names[self.arbin_headers_aux_global.aux_unit_txt].values[0]
+        nick = (
+            df_names[self.arbin_headers_aux_global.aux_name_txt].values[0] or aux_index
+        )
+        if dx_dt:
+            name = f"aux_d_{nick}_dt_u_d{unit}_dt"
+        else:
+            name = f"aux_{nick}_u_{unit}"
+        return name
+
+    def _loader_win(
+        self,
+        file_name,
+        temp_filename,
+        *args,
+        bad_steps=None,
+        dataset_number=None,
+        data_points=None,
+        **kwargs,
+    ):
+        conn = None
+
+        table_name_global = TABLE_NAMES["global"]
+        table_name_aux_global = TABLE_NAMES["aux_global"]
+        table_name_aux = TABLE_NAMES["aux"]
+
+        table_name_normal = TABLE_NAMES["normal"]
+
+        if DEBUG_MODE:
+            time_0 = time.time()
+
+        conn = self._get_connection_or_engine(temp_filename)
+
+        self.logger.debug("reading global data table")
+
+        global_data_df = self._query_table(table_name=table_name_global, conn=conn)
+        tests = global_data_df[self.arbin_headers_normal.test_id_txt]
+        number_of_sets = len(tests)
+        self.logger.debug(f"number of datasets: {number_of_sets}")
+
+        if dataset_number is not None:
+            self.logger.info(f"Dataset number given: {dataset_number}")
+            self.logger.info(f"Available dataset numbers: {tests}")
+            # check if dataset_number is valid
+            #
+
+        else:
+            dataset_number = None
+
+        data = self._init_data(file_name, global_data_df, dataset_number)
+        self.logger.debug("reading raw-data")
+        test_id = data._internal_test_number
+
+        # --------- read stats-data (summary-data) ---------------------
+
+        # --------- read raw-data (normal-data) ------------------------
+        length_of_test, normal_df = self._load_res_normal_table(
+            conn, test_id, bad_steps, data_points
+        )
+        # --------- read auxiliary data (aux-data) ---------------------
+        normal_df = self._load_win_res_auxiliary_table(
+            conn, normal_df, table_name_aux, table_name_aux_global, test_id
+        )
+        # FIX: error in order by since datetime is not accurate enough (also need sorting on test-time)
+        #   sorting dataframe:
+        normal_df = normal_df.sort_values(
+            by=[self.arbin_headers_normal.datetime_txt, self.arbin_headers_normal.test_time_txt], ascending=True
+        )
+        # TODO 216: add order by on test_time as well in sql query
+        summary_df = self._load_res_summary_table(conn, test_id)
+        if summary_df.empty and prms.Reader.use_cellpy_stat_file:
+            txt = "\nCould not find any summary (stats-file)!"
+            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
+            logging.debug(txt)
+            # TODO: Enforce creating a summary df or modify renaming summary df (post process part)
+        # normal_df = normal_df.set_index("Data_Point")
+
+        data.summary = summary_df
+        if DEBUG_MODE:
+            mem_usage = normal_df.memory_usage()
+            logging.debug(
+                f"memory usage for "
+                f"loaded data: \n{mem_usage}"
+                f"\ntotal: {humanize_bytes(mem_usage.sum())}"
+            )
+            logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+
+        data.raw = normal_df
+        data.raw_data_files_length.append(length_of_test)
+        return data
+
+    def _load_win_res_auxiliary_table(
+        self, conn, normal_df, table_name_aux, table_name_aux_global, test_id
+    ):
+        aux_global_data_df = self._query_table(table_name_aux_global, conn)
+        if not aux_global_data_df.empty:
+            aux_df = self._get_aux_df(conn, test_id, table_name_aux)
+            aux_df, aux_global_data_df = self._aux_to_wide(aux_df, aux_global_data_df)
+            aux_df = self._rename_aux_cols(aux_df, aux_global_data_df)
+
+            if not aux_df.empty:
+                normal_df = self._join_aux_to_normal(aux_df, normal_df)
+        return normal_df
+
+    def _load_posix_res_auxiliary_table(self, aux_global_data_df, aux_df, normal_df):
+        if not aux_global_data_df.empty:
+            aux_df, aux_global_data_df = self._aux_to_wide(aux_df, aux_global_data_df)
+            aux_df = self._rename_aux_cols(aux_df, aux_global_data_df)
+
+            if not aux_df.empty:
+                normal_df = self._join_aux_to_normal(aux_df, normal_df)
+        return normal_df
+
+    def _join_aux_to_normal(self, aux_df, normal_df):
+        # TODO: clean up setting index (Data_Point). This is currently done in _post_process after
+        #    the column names are changed to cellpy-column names ("data_point").
+        #    It also keeps a copy of the "data_point"
+        #    column. And is that really necessary.
+        normal_df.set_index(self.arbin_headers_normal.data_point_txt, inplace=True)
+        normal_df = normal_df.join(aux_df, how="left")
+        normal_df.reset_index(inplace=True)
+        return normal_df
+
+    def _rename_aux_cols(self, aux_df, aux_global_data_df):
+        aux_dfs = []
+        if self.arbin_headers_aux.x_value_txt in aux_df.columns:
+            aux_df_x = aux_df[self.arbin_headers_aux.x_value_txt].copy()
+            aux_df_x.columns = [
+                self._make_name_from_frame(aux_global_data_df, z[1], z[0])
+                for z in aux_df_x.columns
+            ]
+            aux_dfs.append(aux_df_x)
+        if self.arbin_headers_aux.x_dt_value in aux_df.columns:
+            aux_df_dx_dt = aux_df[self.arbin_headers_aux.x_dt_value].copy()
+            aux_df_dx_dt.columns = [
+                self._make_name_from_frame(aux_global_data_df, z[1], z[0], True)
+                for z in aux_df_dx_dt.columns
+            ]
+            aux_dfs.append(aux_df_dx_dt)
+        aux_df = pd.concat(aux_dfs, axis=1)
+        return aux_df
+
+    def _aux_to_wide(self, aux_df, aux_global_data_df):
+        aux_df = aux_df.drop(self.arbin_headers_aux.test_id_txt, axis=1)
+        keys = [
+            self.arbin_headers_aux.data_point_txt,
+            self.arbin_headers_aux.aux_index_txt,
+            self.arbin_headers_aux.data_type_txt,
+        ]
+        aux_df = aux_df.set_index(keys=keys)
+        aux_df = aux_df.unstack(2).unstack(1).dropna(axis=1)
+        aux_global_data_df = aux_global_data_df.fillna(0)
+        return aux_df, aux_global_data_df
+
+    def _get_aux_df(self, conn, test_id, table_name_aux):
+        columns_txt = "*"
+        test_numbers = "(" + ",".join([str(tn) for tn in test_id]) + ")"
+        sql_1 = "select %s " % columns_txt
+        sql_2 = "from %s " % table_name_aux
+        sql_3 = f"where {self.arbin_headers_normal.test_id_txt} in {test_numbers}"
+        sql_4 = ""
+        sql_aux = sql_1 + sql_2 + sql_3 + sql_4
+        aux_df = self._query_table(table_name_aux, conn, sql=sql_aux)
+        return aux_df
+
+    def _loader_posix(
+        self,
+        file_name,
+        temp_filename,
+        temp_dir,
+        *args,
+        bad_steps=None,
+        dataset_number=None,
+        data_points=None,
+        **kwargs,
+    ):
+        # TODO: auxiliary channels (table)
+
+        table_name_global = TABLE_NAMES["global"]
+        table_name_stats = TABLE_NAMES["statistic"]
+        table_name_normal = TABLE_NAMES["normal"]
+        table_name_aux_global = TABLE_NAMES["aux_global"]
+        table_name_aux = TABLE_NAMES["aux"]
+
+        if is_posix:
+            if is_macos:
+                self.logger.debug("\nMAC OSX USING MDBTOOLS")
+            else:
+                self.logger.debug("\nPOSIX USING MDBTOOLS")
+        else:
+            self.logger.debug("\nWINDOWS USING MDBTOOLS-WIN")
+
+        if DEBUG_MODE:
+            time_0 = time.time()
+
+        (
+            tmp_name_global,
+            tmp_name_raw,
+            tmp_name_stats,
+            tmp_name_aux_global,
+            tmp_name_aux,
+        ) = self._create_tmp_files(
+            table_name_global,
+            table_name_normal,
+            table_name_stats,
+            table_name_aux_global,
+            table_name_aux,
+            temp_dir,
+            temp_filename,
+        )
+
+        # use pandas to load in the data
+        global_data_df = pd.read_csv(tmp_name_global)
+        tests = global_data_df[self.arbin_headers_normal.test_id_txt]
+        number_of_sets = len(tests)
+        self.logger.debug("number of datasets: %i" % number_of_sets)
+
+        if dataset_number is not None:
+            self.logger.info(f"Dataset number given: {dataset_number}")
+            self.logger.info(f"Available dataset numbers: {tests}")
+        else:
+            dataset_number = None
+
+        data = self._init_data(file_name, global_data_df, dataset_number)
+
+        self.logger.debug("reading raw-data")
+
+        (
+            length_of_test,
+            normal_df,
+            summary_df,
+            aux_global_data_df,
+            aux_df,
+        ) = self._load_from_tmp_files(
+            data,
+            tmp_name_global,
+            tmp_name_raw,
+            tmp_name_stats,
+            tmp_name_aux_global,
+            tmp_name_aux,
+            temp_filename,
+            bad_steps,
+            data_points,
+        )
+
+        # --------- read auxiliary data (aux-data) ---------------------
+        normal_df = self._load_posix_res_auxiliary_table(
+            aux_global_data_df, aux_df, normal_df
+        )
+
+        if summary_df.empty and prms.Reader.use_cellpy_stat_file:
+            txt = "\nCould not find any summary (stats-file)!"
+            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
+            logging.debug(txt)
+        # normal_df = normal_df.set_index("Data_Point")
+
+        data.summary = summary_df
+        if DEBUG_MODE:
+            mem_usage = normal_df.memory_usage()
+            logging.debug(
+                f"memory usage for "
+                f"loaded data: \n{mem_usage}"
+                f"\ntotal: {humanize_bytes(mem_usage.sum())}"
+            )
+            logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+
+        data.raw = normal_df
+        data.raw_data_files_length.append(length_of_test)
+        return data
+
+    def _check_size(self):
+        file_size = os.path.getsize(self.temp_file_path)
+        hfilesize = humanize_bytes(file_size)
+        txt = f"File size: {file_size} ({hfilesize})"
+        self.logger.debug(txt)
+        if file_size > prms.Instruments.Arbin.max_res_filesize:
+            error_message = "\nERROR (loader):\n"
+            error_message += (
+                f"{hfilesize} > {humanize_bytes(prms.Instruments.Arbin.max_res_filesize)} "
+                f"- File is too big!\n"
+            )
+            error_message += "(edit prms.Instruments.Arbin ['max_res_filesize'])\n"
+            logging.critical(error_message)
+            return False
+        return True
+
+    def loader(
+        self,
+        name,
+        *args,
+        bad_steps=None,
+        dataset_number=None,
+        data_points=None,
+        increment_cycle_index=True,
+        **kwargs,
+    ):
+        """Loads data from arbin .res files.
+
+        Args:
+            name (str): path to .res file.
+            bad_steps (list of tuples): (c, s) tuples of steps s (in cycle c)
+                to skip loading.
+            dataset_number (int): the data set number ('Test-ID') to select if you are dealing
+                with arbin files with more than one data-set.
+                Defaults to selecting all data-sets and merging them.
+            data_points (tuple of ints): load only data from data_point[0] to
+                    data_point[1] (use None for infinite).
+            increment_cycle_index (bool): increment the cycle index if merging several datasets (default True).
+
+        Returns:
+            new data (Data)
+        """
+        # TODO: @jepe - insert kwargs - current chunk, only normal data, etc
+        if dataset_number is not None:
+            self.logger.info(f"Dataset number given: {dataset_number}")
+            merge = False
+        else:
+            merge = True
+
+        try:
+            not_too_big = self._check_size()
+            if not not_too_big:
+                return None
+        except Exception as e:
+            self.logger.debug(f"could not get file size: {e}")
+
+        use_mdbtools = False
+        if use_subprocess:
+            use_mdbtools = True
+        if is_posix:
+            use_mdbtools = True
+
+        if use_mdbtools:
+            new_data = self._loader_posix(
+                self.name,
+                self.temp_file_path,
+                self.temp_file_path.parent,
+                *args,
+                bad_steps=bad_steps,
+                dataset_number=dataset_number,
+                data_points=data_points,
+                **kwargs,
+            )
+        else:
+            new_data = self._loader_win(
+                self.name,
+                self.temp_file_path,
+                *args,
+                bad_steps=bad_steps,
+                dataset_number=dataset_number,
+                data_points=data_points,
+                **kwargs,
+            )
+
+        new_data = self._post_process(new_data)
+        if merge:
+            new_data = self._merge(
+                new_data, increment_cycle_index=increment_cycle_index
+            )
+
+        new_data = self.identify_last_data_point(new_data)
+        new_data = self._inspect(new_data)
+
+        return new_data
+
+    def _merge(self, data, increment_cycle_index=True):
+        """Merge data from different data-sets (Test-ID) into one data-set."""
+        test_ids = data._internal_test_number
+        if len(test_ids) == 1:
+            logging.debug("Only one data-set - no need to merge")
+            return data
+        if data.raw.empty:
+            raise ValueError("No data to merge")
+
+        logging.debug("Merging data (only the normal/raw data)")
+        grouped = data.raw.groupby(self.cellpy_headers_normal.test_id_txt)
+        groups = []
+        last_data_point = 0
+        last_test_time = 0.0
+        last_cycle_index = 0
+        for test_id, df in grouped:
+            last = df.iloc[-1]
+            df[self.cellpy_headers_normal.data_point_txt] += last_data_point
+            df[self.cellpy_headers_normal.test_time_txt] += last_test_time
+            if increment_cycle_index:
+                df[self.cellpy_headers_normal.cycle_index_txt] += last_cycle_index
+            last_data_point = last[self.cellpy_headers_normal.data_point_txt]
+            last_test_time = last[self.cellpy_headers_normal.test_time_txt]
+            last_cycle_index = last[self.cellpy_headers_normal.cycle_index_txt]
+            groups.append(df)
+        data.raw = pd.concat(groups, ignore_index=True)
+        return data
+
+    @staticmethod
+    def _create_tmp_files(
+        table_name_global,
+        table_name_normal,
+        table_name_stats,
+        table_name_aux_global,
+        table_name_aux,
+        temp_dir,
+        temp_filename,
+    ):
+        import subprocess
+
+        # creating tmp-filenames
+        temp_csv_filename_global = os.path.join(temp_dir, "global_tmp.csv")
+        temp_csv_filename_normal = os.path.join(temp_dir, "normal_tmp.csv")
+        temp_csv_filename_stats = os.path.join(temp_dir, "stats_tmp.csv")
+        temp_csv_filename_aux_global = os.path.join(temp_dir, "aux_global_tmp.csv")
+        temp_csv_filename_aux = os.path.join(temp_dir, "aux_tmp.csv")
+        # making the cmds
+        mdb_prms = [
+            (table_name_global, temp_csv_filename_global),
+            (table_name_normal, temp_csv_filename_normal),
+            (table_name_stats, temp_csv_filename_stats),
+            (table_name_aux_global, temp_csv_filename_aux_global),
+            (table_name_aux, temp_csv_filename_aux),
+        ]
+        # executing cmds
+        for table_name, tmp_file in mdb_prms:
+            with open(tmp_file, "w") as f:
+                try:
+                    subprocess.call(
+                        [sub_process_path, temp_filename, table_name], stdout=f
+                    )
+                    logging.debug(f"ran mdb-export {str(f)} {table_name}")
+                except FileNotFoundError as e:
+                    logging.critical(
+                        f"Could not run {sub_process_path} on {temp_filename}"
+                    )
+                    logging.critical(f"Possible work-around: install mdbtools")
+                    raise e
+        return (
+            temp_csv_filename_global,
+            temp_csv_filename_normal,
+            temp_csv_filename_stats,
+            temp_csv_filename_aux_global,
+            temp_csv_filename_aux,
+        )
+
+    def _load_from_tmp_files(
+        self,
+        data,
+        temp_csv_filename_global,
+        temp_csv_filename_normal,
+        temp_csv_filename_stats,
+        temp_csv_filename_aux_global,
+        temp_csv_filename_aux,
+        temp_filename,
+        bad_steps,
+        data_points,
+    ):
+        """
+        if bad_steps is not None:
+            if not isinstance(bad_steps, (list, tuple)):
+                bad_steps = [bad_steps]
+            for bad_cycle, bad_step in bad_steps:
+                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
+                sql_4 += "AND NOT (%s=%i " % (
+                    self.headers_normal.cycle_index_txt,
+                    bad_cycle,
+                )
+                sql_4 += "AND %s=%i) " % (self.headers_normal.step_index_txt, bad_step)
+
+        """
+        # should include a more efficient to load the csv (maybe a loop where
+        #   we load only chunks and only keep the parts that fulfill the
+        #   filters (e.g. bad_steps, data_points,...)
+        normal_df = pd.read_csv(temp_csv_filename_normal)
+        # filter on test ID
+        if data._internal_test_number is not None:
+            normal_df = normal_df[
+                normal_df[self.arbin_headers_normal.test_id_txt].isin(
+                    data._internal_test_number
+                )
+            ]
+        # sort on data point
+        if prms._sort_if_subprocess:
+            normal_df = normal_df.sort_values(self.arbin_headers_normal.data_point_txt)
+
+        if bad_steps is not None:
+            logging.debug("removing bad steps")
+            if not isinstance(bad_steps, (list, tuple)):
+                bad_steps = [bad_steps]
+            if not isinstance(bad_steps[0], (list, tuple)):
+                bad_steps = [bad_steps]
+            for bad_cycle, bad_step in bad_steps:
+                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
+
+                selector = (
+                    normal_df[self.arbin_headers_normal.cycle_index_txt] == bad_cycle
+                ) & (normal_df[self.arbin_headers_normal.step_index_txt] == bad_step)
+
+                normal_df = normal_df.loc[~selector, :]
+
+        if prms.Reader.limit_loaded_cycles:
+            logging.debug("Not yet tested for aux data")
+            if len(prms.Reader.limit_loaded_cycles) > 1:
+                c1, c2 = prms.Reader.limit_loaded_cycles
+                selector = (
+                    normal_df[self.arbin_headers_normal.cycle_index_txt] > c1
+                ) & (normal_df[self.arbin_headers_normal.cycle_index_txt] < c2)
+
+            else:
+                c1 = prms.Reader.limit_loaded_cycles[0]
+                selector = normal_df[self.arbin_headers_normal.cycle_index_txt] == c1
+
+            normal_df = normal_df.loc[selector, :]
+
+        if data_points is not None:
+            logging.debug("selecting data-point range")
+            logging.debug("Not yet tested for aux data")
+            d1, d2 = data_points
+
+            if d1 is not None:
+                selector = normal_df[self.arbin_headers_normal.data_point_txt] >= d1
+                normal_df = normal_df.loc[selector, :]
+
+            if d2 is not None:
+                selector = normal_df[self.arbin_headers_normal.data_point_txt] <= d2
+                normal_df = normal_df.loc[selector, :]
+
+        length_of_test = normal_df.shape[0]
+        summary_df = pd.read_csv(temp_csv_filename_stats)
+        aux_global_df = pd.read_csv(temp_csv_filename_aux_global)
+        aux_df = pd.read_csv(temp_csv_filename_aux)
+
+        # clean up
+        for f in [
+            temp_filename,
+            temp_csv_filename_stats,
+            temp_csv_filename_normal,
+            temp_csv_filename_global,
+            temp_csv_filename_aux_global,
+            temp_csv_filename_aux,
+        ]:
+            if os.path.isfile(f):
+                try:
+                    os.remove(f)
+                except WindowsError as e:
+                    logging.warning(f"could not remove tmp-file\n{f} {e}")
+        return length_of_test, normal_df, summary_df, aux_global_df, aux_df
+
+    def _init_data(self, file_name, global_data_df, test_no=None):
+        data = Data()
+        data.loaded_from = file_name
+        self.generate_fid()
+        # name of the .res file it is loaded from:
+        # data.parent_filename = os.path.basename(file_name)
+
+        if test_no is None:
+            selected_global_data_df = global_data_df
+            data._internal_test_number = selected_global_data_df[
+                self.arbin_headers_global.test_id_txt
+            ].values
+        else:
+            if not isinstance(test_no, (tuple, list)):
+                test_no = [test_no]
+
+            selector = global_data_df[self.arbin_headers_global.test_id_txt].isin(
+                test_no
+            )
+            selected_global_data_df = global_data_df.loc[selector, :]
+            if selected_global_data_df.empty:
+                raise NoDataFound(f"Could not find any test with test-ID(s) {test_no}")
+            data._internal_test_number = test_no
+
+        # only picking the first entry (assuming only one cell pr file and channel)
+        data.channel_index = int(
+            selected_global_data_df[self.arbin_headers_global.channel_index_txt].values[
+                0
+            ]
+        )
+        data.creator = selected_global_data_df[
+            self.arbin_headers_global.creator_txt
+        ].values[0]
+        data.test_ID = global_data_df[self.arbin_headers_global.item_id_txt].values[0]
+        data.schedule_file_name = selected_global_data_df[
+            self.arbin_headers_global.schedule_file_name_txt
+        ].values[0]
+        data.start_datetime = selected_global_data_df[
+            self.arbin_headers_global.start_datetime_txt
+        ].values[0]
+        data.test_name = selected_global_data_df[
+            self.arbin_headers_global.test_name_txt
+        ].values[0]
+
+        data.raw_data_files.append(self.fid)
+        return data
+
+    def _normal_table_generator(self, **kwargs):
+        pass
+
+    def _load_res_summary_table(self, conn, test_ids):
+        table_name_stats = TABLE_NAMES["statistic"]
+        test_numbers = "(" + ",".join([str(tn) for tn in test_ids]) + ")"
+        sql = (
+            f"select * from {table_name_stats} "
+            f"where {self.arbin_headers_normal.test_id_txt} in {test_numbers} "
+            f"order by {self.arbin_headers_normal.test_id_txt}, {self.arbin_headers_normal.data_point_txt}"
+        )
+        summary_df = self._query_table(table_name_stats, conn, sql=sql)
+        return summary_df
+
+    def _load_res_normal_table(self, conn, test_ids, bad_steps, data_points):
+        self.logger.debug("starting loading raw-data")
+        self.logger.debug(f"connection: {conn} internal test-ID: {test_ids}")
+        self.logger.debug(f"bad steps:  {bad_steps}")
+
+        table_name_normal = TABLE_NAMES["normal"]
+
+        if prms.Reader.select_minimal:  # SETTING
+            columns = MINIMUM_SELECTION
+            columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
+        else:
+            columns_txt = "*"
+
+        sql_1 = f"select {columns_txt} "
+        sql_2 = f"from {table_name_normal} "
+        test_numbers = "(" + ",".join([str(tn) for tn in test_ids]) + ")"
+        sql_3 = f"where {self.arbin_headers_normal.test_id_txt} in {test_numbers}"
+        sql_4 = " "
+
+        if bad_steps is not None:
+            if not isinstance(bad_steps, (list, tuple)):
+                bad_steps = [bad_steps]
+            if not isinstance(bad_steps[0], (list, tuple)):
+                bad_steps = [bad_steps]
+            for bad_cycle, bad_step in bad_steps:
+                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
+                sql_4 += (
+                    f"AND NOT ({self.arbin_headers_normal.cycle_index_txt}={bad_cycle} "
+                )
+                sql_4 += f"AND {self.arbin_headers_normal.step_index_txt}={bad_step}) "
+
+        if prms.Reader.limit_loaded_cycles:
+            if len(prms.Reader.limit_loaded_cycles) > 1:
+                sql_4 += "AND %s>%i " % (
+                    self.arbin_headers_normal.cycle_index_txt,
+                    prms.Reader.limit_loaded_cycles[0],
+                )
+                sql_4 += "AND %s<%i " % (
+                    self.arbin_headers_normal.cycle_index_txt,
+                    prms.Reader.limit_loaded_cycles[-1],
+                )
+            else:
+                sql_4 = "AND %s=%i " % (
+                    self.arbin_headers_normal.cycle_index_txt,
+                    prms.Reader.limit_loaded_cycles[0],
+                )
+
+        if data_points is not None:
+            d1, d2 = data_points
+            if d1 is not None:
+                sql_4 += "AND %s>=%i " % (self.arbin_headers_normal.data_point_txt, d1)
+            if d2 is not None:
+                sql_4 += "AND %s<=%i " % (self.arbin_headers_normal.data_point_txt, d2)
+
+        sql_5 = f"order by {self.arbin_headers_normal.datetime_txt}"
+        sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
+
+        self.logger.debug("INFO ABOUT LOAD RES NORMAL")
+        self.logger.debug("sql statement: %s" % sql)
+
+        if DEBUG_MODE:
+            current_memory_usage = sys.getsizeof(self)
+            self.logger.debug(f"current memory usage: {current_memory_usage}")
+
+        if not prms.Instruments.Arbin.chunk_size:
+            self.logger.debug("no chunk-size given")
+            # memory here
+            normal_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
+            # memory here
+            length_of_test = normal_df.shape[0]
+        else:
+            self.logger.debug(f"chunk-size: {prms.Instruments.Arbin.chunk_size}")
+            self.logger.debug("creating a pd.read_sql_query generator")
+
+            normal_df_reader = pd.read_sql_query(
+                sql=sa.text(sql),
+                con=conn.connect(),
+                chunksize=prms.Instruments.Arbin.chunk_size,
+            )
+            normal_df = None
+            chunk_number = 0
+            self.logger.debug("created pandas sql reader")
+            self.logger.debug("iterating chunk-wise")
+            for i, chunk in enumerate(normal_df_reader):
+                self.logger.debug(f"iteration number {i}")
+                if prms.Instruments.Arbin.max_chunks:
+                    self.logger.debug(
+                        f"max number of chunks mode "
+                        f"({prms.Instruments.Arbin.max_chunks})"
+                    )
+                    if chunk_number < prms.Instruments.Arbin.max_chunks:
+                        normal_df = pd.concat([normal_df, chunk], ignore_index=True)
+                        self.logger.debug(
+                            f"chunk {i} of {prms.Instruments.Arbin.max_chunks}"
+                        )
+                    else:
+                        break
+                else:
+                    try:
+                        normal_df = pd.concat([normal_df, chunk], ignore_index=True)
+                        self.logger.debug("concatenated new chunk")
+                    except MemoryError:
+                        self.logger.error(
+                            " - Could not read complete file (MemoryError)."
+                        )
+                        self.logger.error(
+                            f"Last successfully loaded chunk " f"number: {chunk_number}"
+                        )
+                        self.logger.error(
+                            f"Chunk size: {prms.Instruments.Arbin.chunk_size}"
+                        )
+                        break
+                chunk_number += 1
+            length_of_test = normal_df.shape[0]
+            self.logger.debug(f"finished iterating (#rows: {length_of_test})")
+
+        self.logger.debug(f"loaded to normal_df (length =  {length_of_test})")
+        self.logger.debug(f"Headers:\n{normal_df.columns}")
+        if normal_df is None:
+            default_headers = [v for v in self.arbin_headers_normal.values()]
+            normal_df = pd.DataFrame(columns=default_headers)
+        return length_of_test, normal_df
+
+
+def check_loader_aux():
+    from pathlib import Path
+
+    from cellpy import log
+
+    log.setup_logging(default_level="CRITICAL")
+    p = Path(r"C:\scripts\cellpy_dev_resources\2020_jinpeng_aux_temperature")
+    f1 = p / "BIT_LFP5p12s_Pack02_CAP_Cyc200_T25_Nov23.res"
+    f2 = p / "BIT_LFP50_12S1P_SOP_0_97_T5_cyc200_3500W_20191231.res"
+    f3 = p / "TJP_LR1865SZ_OCV_19_Cyc150_T25_201105.res"
+
+    n = DataLoader().loader(f1)
+    print(n[0].raw.tail())
+
+
+def check_loader_empty_normal():
+    from cellpy import log
+
+    log.setup_logging(default_level="CRITICAL")
+
+    a = DataLoader()
+    cols = a.arbin_headers_normal
+    df = pd.DataFrame(columns=cols.values())
+    print(df)
+    print(df.empty)
+
+
+def check_multi():
+    import pathlib
+    import cellpy
+
+    f = r"C:\scripting\cellpy_dev_resources\dev_data\arbin_multi\20230531_NG27_02_cc_01.res"
+    out = r"C:\scripting\cellpy_dev_resources\dev_data\arbin_multi\20230531_NG27_02_cc_01.xlsx"
+    p = pathlib.Path(f)
+    c = cellpy.get(p)
+    c.to_excel(out, raw=True)
+
+
+def noodle():
+    import pandas as pd
+    df = pd.DataFrame({
+        "a": [1, 2, 3, 4, 5],
+        "b": [1, 2, 3, 4, 5],
+        "c": [1, 1, 1, 1, 2],
+    })
+    print(df)
+
+    df2 = df[df["c"].isin([1])]
+    print(" new ".center(80, "-"))
+    print(df2)
+
+
+if __name__ == "__main__":
+    print(" arbin-res-py ".center(80, "="))
+    noodle()
+    print(" finished ".center(80, "="))
```

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_7.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_7.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_csv.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_csv.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_h5.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_h5.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/arbin_sql_xlsx.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/arbin_sql_xlsx.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/base.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/base.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,747 +1,747 @@
-"""
-When you make a new loader you have to subclass the Loader class.
-Remember also to register it in cellpy.cellreader.
-
-(for future development, not used very efficiently yet).
-"""
-
-import abc
-import logging
-import pathlib
-import shutil
-import tempfile
-from abc import ABC
-from typing import List, Union
-
-import pandas as pd
-
-import cellpy.internals.core
-import cellpy.readers.core as core
-from cellpy.parameters.internal_settings import headers_normal
-from cellpy.readers.instruments.configurations import (
-    ModelParameters,
-    register_configuration_from_module,
-)
-from cellpy.readers.instruments.processors import post_processors, pre_processors
-from cellpy.readers.instruments.processors.post_processors import (
-    ORDERED_POST_PROCESSING_STEPS,
-)
-
-MINIMUM_SELECTION = [
-    "Data_Point",
-    "Test_Time",
-    "Step_Time",
-    "DateTime",
-    "Step_Index",
-    "Cycle_Index",
-    "Current",
-    "Voltage",
-    "Charge_Capacity",
-    "Discharge_Capacity",
-    "Internal_Resistance",
-]
-
-
-# TODO: move this to another module (e.g. inside processors):
-def find_delimiter_and_start(
-    file_name,
-    separators=None,
-    checking_length_header=30,
-    checking_length_whole=200,
-):
-    """function to automatically detect the delimiter and what line the first data appears on.
-
-    Remark! This function is rather simple, it splits the data into to parts
-        (possible header part (checking_length_header) and the rest of the data). Then it counts the appearances of
-        the different possible delimiters in the rest of the data part, and then selects a delimiter if it has unique
-        counts for all the lines.
-
-        The first line is defined as where the delimiter is used same number of times (probably a header line).
-    """
-
-    if separators is None:
-        separators = [";", "\t", "|", ","]
-    logging.debug(f"checking internals of the file {file_name}")
-
-    empty_lines = 0
-    with open(file_name, "r") as fin:
-        lines = []
-        for j in range(checking_length_whole):
-            line = fin.readline()
-            if not line:
-                break
-            if len(line.strip()):
-                lines.append(line)
-            else:
-                empty_lines += 1
-
-    checking_length_whole -= empty_lines
-    if checking_length_header - empty_lines < 1:
-        checking_length_header = checking_length_whole // 2
-    separator, number_of_hits = _find_separator(
-        checking_length_whole - checking_length_header, lines, separators
-    )
-
-    if separator is None:
-        raise IOError(f"could not decide delimiter in {file_name}")
-
-    if separator == "\t":
-        logging.debug("seperator = TAB")
-    elif separator == " ":
-        logging.debug("seperator = SPACE")
-    else:
-        logging.debug(f"seperator = {separator}")
-
-    first_index = _find_first_line_whit_delimiter(
-        checking_length_header, lines, number_of_hits, separator
-    )
-    logging.debug(f"First line with delimiter: {first_index}")
-    return separator, first_index
-
-
-def _find_first_line_whit_delimiter(
-    checking_length_header, lines, number_of_hits, separator
-):
-    first_part = lines[:checking_length_header]
-    if number_of_hits is None:
-        # remark! if number of hits (i.e. how many separators pr line) is not given, we set it to the amount of
-        # separators we find in the third last line.
-        number_of_hits = lines[-3].count(separator)
-    return [
-        line_number
-        for line_number, line in enumerate(first_part)
-        if line.count(separator) == number_of_hits
-    ][0]
-
-
-def _find_separator(checking_length, lines, separators):
-    logging.debug("searching for separators")
-    separator = None
-    number_of_hits = None
-    last_part = lines[
-        checking_length:-1
-    ]  # don't include last line since it might be corrupted
-    check_sep = dict()
-
-    for i, v in enumerate(separators):
-        check_sep[i] = [line.count(v) for line in last_part]
-
-    unique_sep_counts = {i: set(v) for i, v in check_sep.items()}
-
-    for index, value in unique_sep_counts.items():
-        value_as_list = list(value)
-        number_of_hits = value_as_list[0]
-        if len(value_as_list) == 1 and number_of_hits > 0:
-            separator = separators[index]
-            break
-
-    return separator, number_of_hits
-
-
-def query_csv(
-    self,
-    name,
-    sep=None,
-    skiprows=None,
-    header=None,
-    encoding=None,
-    decimal=None,
-    thousands=None,
-):
-    logging.debug(f"parsing with pandas.read_csv: {name}")
-    sep = sep or self.sep
-    skiprows = skiprows or self.skiprows
-    header = header or self.header
-    encoding = encoding or self.encoding
-    decimal = decimal or self.decimal
-    thousands = thousands or self.thousands
-    logging.critical(f"{sep=}, {skiprows=}, {header=}, {encoding=}, {decimal=}")
-    data_df = pd.read_csv(
-        name,
-        sep=sep,
-        skiprows=skiprows,
-        header=header,
-        encoding=encoding,
-        decimal=decimal,
-        thousands=thousands,
-    )
-    return data_df
-
-
-class AtomicLoad:
-    """Atomic loading class"""
-
-    instrument_name = "atomic_loader"
-
-    _name = None
-    _temp_file_path = None
-    _fid = None
-    _is_db: bool = False
-    _copy_also_local: bool = True
-    _refuse_copying: bool = False
-
-    @property
-    def is_db(self):
-        """Is the file stored in the database"""
-        return self._is_db
-
-    @is_db.setter
-    def is_db(self, value: bool):
-        """Is the file stored in the database"""
-        self._is_db = value
-
-    @property
-    def refuse_copying(self):
-        """Should the file be copied to a temporary file"""
-        return self._refuse_copying
-
-    @refuse_copying.setter
-    def refuse_copying(self, value: bool):
-        """Should the file be copied to a temporary file"""
-        self._refuse_copying = value
-
-    @property
-    def name(self):
-        """The name of the file to be loaded"""
-        return self._name
-
-    @name.setter
-    def name(self, value):
-        """The name of the file to be loaded"""
-        if not self.is_db and not isinstance(value, cellpy.internals.core.OtherPath):
-            logging.debug("converting to OtherPath")
-            value = cellpy.internals.core.OtherPath(value)
-        self._name = value
-
-    @property
-    def temp_file_path(self):
-        """The name of the file to be loaded if copied to a temporary file"""
-        return self._temp_file_path
-
-    @temp_file_path.setter
-    def temp_file_path(self, value):
-        """The name of the file to be loaded if copied to a temporary file"""
-        self._temp_file_path = value
-
-    @property
-    def fid(self):
-        """The unique file id"""
-        if self._fid is None:
-            self.generate_fid()
-        return self._fid
-
-    def generate_fid(self, value=None):
-        """Generate a unique file id"""
-        if self.is_db:
-            self._fid = core.FileID(self.name, is_db=True)
-        elif self._temp_file_path is not None:
-            self._fid = core.FileID(self.name)
-        elif self._name is not None:
-            self._fid = core.FileID(self.name)
-        elif value is not None:
-            self._fid = core.FileID(value)
-        else:
-            raise ValueError("could not generate fid")
-
-    def copy_to_temporary(self):
-        """Copy file to a temporary file"""
-
-        logging.debug(f"external file received? {self.name.is_external=}")
-        if self.name is None:
-            raise ValueError("no file name given to loader class (self.name is None)")
-
-        if self._refuse_copying:
-            logging.debug("refusing copying")
-            self._temp_file_path = self.name
-            return
-
-        if not self._copy_also_local and not self.name.is_external:
-            self._temp_file_path = self.name
-            return
-
-        self._temp_file_path = self.name.copy()
-
-    def loader_executor(self, *args, **kwargs):
-        """Load the file"""
-        name = args[0]
-        self.refuse_copying = kwargs.pop("refuse_copying", False)
-        self.name = name
-        if not self.is_db:
-            self.copy_to_temporary()
-        cellpy_data = self.loader(*args, **kwargs)
-        return cellpy_data
-
-    def loader(self, *args, **kwargs):
-        """The method that does the actual loading.
-
-        This method should be overwritten by the specific loader class.
-        """
-        ...
-
-
-class BaseLoader(AtomicLoad, metaclass=abc.ABCMeta):
-    """Main loading class"""
-
-    instrument_name = "base_loader"
-
-    # TODO: should also include the functions for getting cellpy headers etc
-    #  here
-
-    @staticmethod
-    @abc.abstractmethod
-    def get_raw_units() -> dict:
-        """Include the settings for the units used by the instrument.
-
-        This is needed for example when converting the capacity to a specific capacity.
-        So far, it has been difficult to get any kind of consensus on what the most optimal
-        units are for storing cycling data. Therefore, cellpy implements three levels of units:
-        1) the raw units that the data is loaded in already has and 2) the cellpy units used by cellpy
-        when generating summaries and related information, and 3) output units that can be set to get the data
-        in a specif unit when exporting or creating specific outputs such as ICA.
-
-        Comment 2022.09.11::
-
-            still not sure if we should use raw units or cellpy units in the cellpy-files (.h5/ .cellpy).
-            Currently, the summary is in cellpy units and the raw and step data is in raw units. If
-            you have any input on this topic, let us know.
-
-        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
-        Si units can be used). For example, for current defined in mA, the value for the
-        current unit-fraction will be 0.001.
-
-        The internal cellpy units are given in the ``cellpy_units`` attribute.
-
-        Returns:
-            dictionary of units (str)
-
-        Example:
-            A minimum viable implementation::
-
-                @staticmethod
-                def get_raw_units():
-                    raw_units = dict()
-                    raw_units["current"] = "A"
-                    raw_units["charge"] = "Ah"
-                    raw_units["mass"] = "g"
-                    raw_units["voltage"] = "V"
-                    return raw_units
-
-        """
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def get_raw_limits(self) -> dict:
-        """Include the settings for how to decide what kind of step you are examining here.
-
-        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
-        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
-        If the (accumulated) change is less than 'epsilon', then cellpy interpret it to be stable.
-        It is expected that different instruments (with different resolution etc.) have different
-        resolutions and noice levels, thus different 'epsilons'.
-
-        Returns: the raw limits (dict)
-
-        """
-        raise NotImplementedError
-
-    @classmethod
-    def get_params(cls, parameter: Union[str, None]) -> dict:
-        """Retrieves parameters needed for facilitating working with the
-        instrument without registering it.
-
-        Typically, it should include the name and raw_ext.
-
-        Return: parameters or a selected parameter
-        """
-
-        return getattr(cls, parameter)
-
-    @abc.abstractmethod
-    def loader(self, *args, **kwargs) -> list:
-        """Loads data into a Data object and returns it"""
-        # This method is used by cellreader through the AtomicLoad.loader_executor method.
-        # It should be overwritten by the specific loader class.
-        #
-        # Notice that it is highly recommended that you don't try to implement .loader_executor yourself
-        # in your subclass!
-        pass
-
-    @staticmethod
-    def identify_last_data_point(data: core.Data) -> core.Data:
-        """This method is used to find the last record in the data."""
-        return core.identify_last_data_point(data)
-
-
-class AutoLoader(BaseLoader):
-    """Main autoload class.
-
-    This class can be sub-classed if you want to make a data-reader for different type of "easily parsed" files
-    (for example csv-files). The subclass needs to have at least one
-    associated CONFIGURATION_MODULE defined and must have the following attributes as minimum::
-
-        default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
-        supported_models: dict = SUPPORTED_MODELS
-
-    where SUPPORTED_MODELS is a dictionary with {NICK_NAME : CONFIGURATION_MODULE_NAME}  key-value pairs.
-    Remark! the NICK_NAME must be in upper-case!
-
-    It is also possible to set these in a custom pre_init method::
-
-        @classmethod
-        def pre_init(cls):
-            cls.default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
-            cls.supported_models: dict = SUPPORTED_MODELS
-
-    or turn off automatic registering of configuration::
-
-        @classmethod
-        def pre_init(cls):
-            cls.auto_register_config = False  # defaults to True
-
-    During initialisation of the class, if ``auto_register_config == True``,  it will dynamically load the definitions
-    provided in the CONFIGURATION_MODULE.py located in the ``cellpy.readers.instruments.configurations``
-    folder/package.
-
-    Attributes can be set during initialisation of the class as **kwargs that are then handled by the
-    ``parse_formatter_parameters`` method.
-
-    Remark that some also can be provided as arguments to the ``loader`` method and will then automatically
-    be "transparent" to the ``cellpy.get`` function. So if you would like to give the user access to modify
-    these arguments, you should implement them in the ``parse_loader_parameters`` method.
-
-    """
-
-    instrument_name = "auto_loader"
-
-    def __init__(self, *args, **kwargs):
-        self.auto_register_config = True
-        self.pre_init()
-
-        if not hasattr(self, "supported_models"):
-            raise AttributeError(
-                f"missing attribute in sub-class of TxtLoader: supported_models"
-            )
-        if not hasattr(self, "default_model"):
-            raise AttributeError(
-                f"missing attribute in sub-class of TxtLoader: default_model"
-            )
-
-        # in case model is given as argument
-        self.model = kwargs.pop("model", self.default_model)
-        if self.auto_register_config:
-            self.config_params = self.register_configuration()
-
-        self.parse_formatter_parameters(**kwargs)
-
-        self.pre_processors = self.config_params.pre_processors
-        additional_pre_processor_args = kwargs.pop(
-            "pre_processors", None
-        )  # could replace None with an empty dict to get rid of the if-clause:
-        if additional_pre_processor_args:
-            for key in additional_pre_processor_args:
-                self.pre_processors[key] = additional_pre_processor_args[key]
-
-        self.post_processors = self.config_params.post_processors
-        additional_post_processor_args = kwargs.pop(
-            "post_processors", None
-        )  # could replace None with an empty dict to get rid of the if-clause:
-        if additional_post_processor_args:
-            for key in additional_post_processor_args:
-                self.post_processors[key] = additional_post_processor_args[key]
-
-        self.include_aux = kwargs.pop("include_aux", False)
-        self.keep_all_columns = kwargs.pop("keep_all_columns", False)
-        self.cellpy_headers_normal = (
-            headers_normal  # the column headers defined by cellpy
-        )
-
-    @abc.abstractmethod
-    def parse_formatter_parameters(self, **kwargs) -> None:
-        ...
-
-    @abc.abstractmethod
-    def parse_loader_parameters(self, **kwargs):
-        ...
-
-    @abc.abstractmethod
-    def query_file(self, file_path: Union[str, pathlib.Path]) -> pd.DataFrame:
-        ...
-
-    def pre_init(self) -> None:
-        ...
-
-    def register_configuration(self) -> ModelParameters:
-        """Register and load model configuration"""
-        if (
-            self.model is None
-        ):  # in case None was given as argument (model=None in initialisation)
-            self.model = self.default_model
-        model_module_name = self.supported_models.get(self.model.upper(), None)
-        if model_module_name is None:
-            raise Exception(
-                f"The model {self.model} does not have any defined configuration."
-                f"\nCurrent supported models are {[*self.supported_models.keys()]}"
-            )
-        return register_configuration_from_module(self.model, model_module_name)
-
-    def get_raw_units(self):
-        """Include the settings for the units used by the instrument.
-
-        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
-        Si units can be used). For example, for current defined in mA, the value for the
-        current unit-fraction will be 0.001.
-
-        Returns:
-            dictionary containing the unit-fractions for current, charge, and mass
-
-        """
-        return self.config_params.raw_units
-
-    def get_raw_limits(self):
-        """Include the settings for how to decide what kind of step you are examining here.
-
-        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
-        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
-        It is expected that different instruments (with different resolution etc.) have different
-        'epsilons'.
-
-        Returns:
-            the raw limits (dict)
-
-        """
-        return self.config_params.raw_limits
-
-    @staticmethod
-    def get_headers_aux(raw: pd.DataFrame) -> dict:
-        raise NotImplementedError(
-            f"missing method in sub-class of TxtLoader: get_headers_aux"
-        )
-
-    def _pre_process(self):
-        for processor_name in self.pre_processors:
-            if self.pre_processors[processor_name]:
-                if hasattr(pre_processors, processor_name):
-                    logging.critical(f"running pre-processor: {processor_name}")
-                    processor = getattr(pre_processors, processor_name)
-                    self.temp_file_path = processor(self.temp_file_path)
-                else:
-                    raise NotImplementedError(
-                        f"{processor_name} is not currently supported - aborting!"
-                    )
-
-    def loader(self, name: Union[str, pathlib.Path], **kwargs: str) -> core.Data:
-        """returns a Data object with loaded data.
-
-        Loads data from a txt file (csv-ish).
-
-        Args:
-            name (str, pathlib.Path): name of the file.
-            kwargs (dict): key-word arguments from raw_loader.
-
-        Returns:
-            new_tests (list of data objects)
-
-        """
-        pre_processor_hook = kwargs.pop("pre_processor_hook", None)
-
-        if self.pre_processors:
-            self._pre_process()
-
-        self.parse_loader_parameters(**kwargs)
-
-        data_df = self.query_file(self.temp_file_path)
-
-        if pre_processor_hook is not None:
-            logging.debug("running pre-processing-hook")
-            data_df = pre_processor_hook(data_df)
-
-        data = core.Data()
-
-        # metadata
-        meta = self.parse_meta()
-        data.loaded_from = name
-        data.channel_index = meta.get("channel_index", None)
-        data.test_ID = meta.get("test_ID", None)
-        data.test_name = meta.get("test_name", None)
-        data.creator = meta.get("creator", None)
-        data.schedule_file_name = meta.get("schedule_file_name", None)
-        data.start_datetime = meta.get("start_datetime", None)
-
-        # Generating a FileID project:
-        self.generate_fid()
-        data.raw_data_files.append(self.fid)
-
-        data.raw = data_df
-        data.raw_data_files_length.append(len(data_df))
-        data.summary = (
-            pd.DataFrame()
-        )  # creating an empty frame - loading summary is not implemented
-        data = self._post_process(data)
-        data = self.identify_last_data_point(data)
-        if data.start_datetime is None:
-            data.start_datetime = data.raw[headers_normal.datetime_txt].iat[0]
-
-        data = self.validate(data)
-        return data
-
-    def validate(self, data: core.Data) -> core.Data:
-        """validation of the loaded data, should raise an appropriate exception if it fails."""
-
-        logging.debug(f"no validation of defined in this sub-class of TxtLoader")
-        return data
-
-    def parse_meta(self) -> dict:
-        """method that parses the data for meta-data (e.g. start-time, channel number, ...)"""
-
-        logging.debug(
-            f"no parsing method for meta-data defined in this sub-class of TxtLoader"
-        )
-        return dict()
-
-    def _post_rename_headers(self, data):
-        if self.include_aux:
-            new_aux_headers = self.get_headers_aux(data.raw)
-            data.raw.rename(index=str, columns=new_aux_headers, inplace=True)
-        return data
-
-    def _post_process(self, data):
-        # ordered post-processing steps:
-        for processor_name in ORDERED_POST_PROCESSING_STEPS:
-            if processor_name in self.post_processors:
-                data = self._perform_post_process_step(data, processor_name)
-
-        # non-ordered post-processing steps
-        for processor_name in self.post_processors:
-            if processor_name not in ORDERED_POST_PROCESSING_STEPS:
-                data = self._perform_post_process_step(data, processor_name)
-        return data
-
-    def _perform_post_process_step(self, data, processor_name):
-        if self.post_processors[processor_name]:
-            if hasattr(post_processors, processor_name):
-                logging.critical(f"running post-processor: {processor_name}")
-                processor = getattr(post_processors, processor_name)
-                data = processor(data, self.config_params)
-                if hasattr(self, f"_post_{processor_name}"):  # internal addon-function
-                    _processor = getattr(self, f"_post_{processor_name}")
-                    data = _processor(data)
-            else:
-                raise NotImplementedError(
-                    f"{processor_name} is not currently supported - aborting!"
-                )
-        return data
-
-
-class TxtLoader(AutoLoader, ABC):
-    """Main txt loading class (for sub-classing).
-
-    The subclass of a ``TxtLoader`` gets its information by loading model specifications from its respective module
-    (``cellpy.readers.instruments.configurations.<module>``) or configuration file (yaml).
-
-    Remark that if you implement automatic loading of the formatter, the module / yaml-file must include all
-    the required formatter parameters (sep, skiprows, header, encoding, decimal, thousands).
-
-    If you need more flexibility, try using the ``CustomTxtLoader`` or subclass directly
-    from ``AutoLoader`` or ``Loader``.
-
-    Constructor:
-        model (str): short name of the (already implemented) sub-model.
-        sep (str): delimiter.
-        skiprows (int): number of lines to skip.
-        header (int): number of the header lines.
-        encoding (str): encoding.
-        decimal (str): character used for decimal in the raw data, defaults to '.'.
-        processors (dict): pre-processing steps to take (before loading with pandas).
-        post_processors (dict): post-processing steps to make after loading the data, but before
-        returning them to the caller.
-        include_aux (bool): also parse so-called auxiliary columns / data. Defaults to False.
-        keep_all_columns (bool): load all columns, also columns that are not 100% necessary for ``cellpy`` to work.
-        Remark that the configuration settings for the sub-model must include a list of column header names
-        that should be kept if keep_all_columns is False (default).
-
-    Module:
-        sep (str): the delimiter (also works as a switch to turn on/off automatic detection of delimiter and
-        start of data (skiprows)).
-
-    """
-
-    instrument_name = "txt_loader"
-    raw_ext = "*"
-
-    # override this if needed
-    def parse_loader_parameters(self, **kwargs):
-        sep = kwargs.get("sep", None)
-        if sep is not None:
-            self.sep = sep
-        if self.sep is None:
-            self._auto_formatter()
-
-    # override this if needed
-    def parse_formatter_parameters(self, **kwargs):
-        logging.debug(f"model: {self.model}")
-        if not self.config_params.formatters:
-            # Setting defaults if formatter is not loaded
-            logging.debug("No formatter given - using default values.")
-            self.sep = kwargs.pop("sep", None)
-            self.skiprows = kwargs.pop("skiprows", 0)
-            self.header = kwargs.pop("header", 0)
-            self.encoding = kwargs.pop("encoding", "utf-8")
-            self.decimal = kwargs.pop("decimal", ".")
-            self.thousands = kwargs.pop("thousands", None)
-
-        else:
-            # Remark! This will break if one of these parameters are missing
-            # (not a keyword argument and not within the configuration):
-            self.sep = kwargs.pop("sep", self.config_params.formatters["sep"])
-            self.skiprows = kwargs.pop(
-                "skiprows", self.config_params.formatters["skiprows"]
-            )
-            self.header = kwargs.pop("header", self.config_params.formatters["header"])
-            self.encoding = kwargs.pop(
-                "encoding", self.config_params.formatters["encoding"]
-            )
-            self.decimal = kwargs.pop(
-                "decimal", self.config_params.formatters["decimal"]
-            )
-            self.thousands = kwargs.pop(
-                "thousands", self.config_params.formatters["thousands"]
-            )
-        logging.debug(
-            f"Formatters: self.sep={self.sep} self.skiprows={self.skiprows} self.header={self.header} self.encoding={self.encoding}"
-        )
-        logging.debug(
-            f"Formatters (cont.): self.decimal={self.decimal} self.thousands={self.thousands}"
-        )
-
-    def _auto_formatter(self):
-        separator, first_index = find_delimiter_and_start(
-            self.name,
-            separators=None,
-            checking_length_header=100,
-            checking_length_whole=200,
-        )
-        self.encoding = "UTF-8"  # consider adding a find_encoding function
-        self.sep = separator
-        self.skiprows = first_index - 1
-        self.header = 0
-
-        logging.critical(
-            f"auto-formatting:\n  {self.sep=}\n  {self.skiprows=}\n  {self.header=}\n  {self.encoding=}\n"
-        )
-
-    # override this if using other query functions
-    def query_file(self, name):
-        logging.debug(f"parsing with pandas.read_csv: {name}")
-        logging.critical(
-            f"{self.sep=}, {self.skiprows=}, {self.header=}, {self.encoding=}, {self.decimal=}"
-        )
-        data_df = pd.read_csv(
-            name,
-            sep=self.sep,
-            skiprows=self.skiprows,
-            header=self.header,
-            encoding=self.encoding,
-            decimal=self.decimal,
-            thousands=self.thousands,
-        )
-        return data_df
+"""
+When you make a new loader you have to subclass the Loader class.
+Remember also to register it in cellpy.cellreader.
+
+(for future development, not used very efficiently yet).
+"""
+
+import abc
+import logging
+import pathlib
+import shutil
+import tempfile
+from abc import ABC
+from typing import List, Union
+
+import pandas as pd
+
+import cellpy.internals.core
+import cellpy.readers.core as core
+from cellpy.parameters.internal_settings import headers_normal
+from cellpy.readers.instruments.configurations import (
+    ModelParameters,
+    register_configuration_from_module,
+)
+from cellpy.readers.instruments.processors import post_processors, pre_processors
+from cellpy.readers.instruments.processors.post_processors import (
+    ORDERED_POST_PROCESSING_STEPS,
+)
+
+MINIMUM_SELECTION = [
+    "Data_Point",
+    "Test_Time",
+    "Step_Time",
+    "DateTime",
+    "Step_Index",
+    "Cycle_Index",
+    "Current",
+    "Voltage",
+    "Charge_Capacity",
+    "Discharge_Capacity",
+    "Internal_Resistance",
+]
+
+
+# TODO: move this to another module (e.g. inside processors):
+def find_delimiter_and_start(
+    file_name,
+    separators=None,
+    checking_length_header=30,
+    checking_length_whole=200,
+):
+    """function to automatically detect the delimiter and what line the first data appears on.
+
+    Remark! This function is rather simple, it splits the data into to parts
+        (possible header part (checking_length_header) and the rest of the data). Then it counts the appearances of
+        the different possible delimiters in the rest of the data part, and then selects a delimiter if it has unique
+        counts for all the lines.
+
+        The first line is defined as where the delimiter is used same number of times (probably a header line).
+    """
+
+    if separators is None:
+        separators = [";", "\t", "|", ","]
+    logging.debug(f"checking internals of the file {file_name}")
+
+    empty_lines = 0
+    with open(file_name, "r") as fin:
+        lines = []
+        for j in range(checking_length_whole):
+            line = fin.readline()
+            if not line:
+                break
+            if len(line.strip()):
+                lines.append(line)
+            else:
+                empty_lines += 1
+
+    checking_length_whole -= empty_lines
+    if checking_length_header - empty_lines < 1:
+        checking_length_header = checking_length_whole // 2
+    separator, number_of_hits = _find_separator(
+        checking_length_whole - checking_length_header, lines, separators
+    )
+
+    if separator is None:
+        raise IOError(f"could not decide delimiter in {file_name}")
+
+    if separator == "\t":
+        logging.debug("seperator = TAB")
+    elif separator == " ":
+        logging.debug("seperator = SPACE")
+    else:
+        logging.debug(f"seperator = {separator}")
+
+    first_index = _find_first_line_whit_delimiter(
+        checking_length_header, lines, number_of_hits, separator
+    )
+    logging.debug(f"First line with delimiter: {first_index}")
+    return separator, first_index
+
+
+def _find_first_line_whit_delimiter(
+    checking_length_header, lines, number_of_hits, separator
+):
+    first_part = lines[:checking_length_header]
+    if number_of_hits is None:
+        # remark! if number of hits (i.e. how many separators pr line) is not given, we set it to the amount of
+        # separators we find in the third last line.
+        number_of_hits = lines[-3].count(separator)
+    return [
+        line_number
+        for line_number, line in enumerate(first_part)
+        if line.count(separator) == number_of_hits
+    ][0]
+
+
+def _find_separator(checking_length, lines, separators):
+    logging.debug("searching for separators")
+    separator = None
+    number_of_hits = None
+    last_part = lines[
+        checking_length:-1
+    ]  # don't include last line since it might be corrupted
+    check_sep = dict()
+
+    for i, v in enumerate(separators):
+        check_sep[i] = [line.count(v) for line in last_part]
+
+    unique_sep_counts = {i: set(v) for i, v in check_sep.items()}
+
+    for index, value in unique_sep_counts.items():
+        value_as_list = list(value)
+        number_of_hits = value_as_list[0]
+        if len(value_as_list) == 1 and number_of_hits > 0:
+            separator = separators[index]
+            break
+
+    return separator, number_of_hits
+
+
+def query_csv(
+    self,
+    name,
+    sep=None,
+    skiprows=None,
+    header=None,
+    encoding=None,
+    decimal=None,
+    thousands=None,
+):
+    logging.debug(f"parsing with pandas.read_csv: {name}")
+    sep = sep or self.sep
+    skiprows = skiprows or self.skiprows
+    header = header or self.header
+    encoding = encoding or self.encoding
+    decimal = decimal or self.decimal
+    thousands = thousands or self.thousands
+    logging.critical(f"{sep=}, {skiprows=}, {header=}, {encoding=}, {decimal=}")
+    data_df = pd.read_csv(
+        name,
+        sep=sep,
+        skiprows=skiprows,
+        header=header,
+        encoding=encoding,
+        decimal=decimal,
+        thousands=thousands,
+    )
+    return data_df
+
+
+class AtomicLoad:
+    """Atomic loading class"""
+
+    instrument_name = "atomic_loader"
+
+    _name = None
+    _temp_file_path = None
+    _fid = None
+    _is_db: bool = False
+    _copy_also_local: bool = True
+    _refuse_copying: bool = False
+
+    @property
+    def is_db(self):
+        """Is the file stored in the database"""
+        return self._is_db
+
+    @is_db.setter
+    def is_db(self, value: bool):
+        """Is the file stored in the database"""
+        self._is_db = value
+
+    @property
+    def refuse_copying(self):
+        """Should the file be copied to a temporary file"""
+        return self._refuse_copying
+
+    @refuse_copying.setter
+    def refuse_copying(self, value: bool):
+        """Should the file be copied to a temporary file"""
+        self._refuse_copying = value
+
+    @property
+    def name(self):
+        """The name of the file to be loaded"""
+        return self._name
+
+    @name.setter
+    def name(self, value):
+        """The name of the file to be loaded"""
+        if not self.is_db and not isinstance(value, cellpy.internals.core.OtherPath):
+            logging.debug("converting to OtherPath")
+            value = cellpy.internals.core.OtherPath(value)
+        self._name = value
+
+    @property
+    def temp_file_path(self):
+        """The name of the file to be loaded if copied to a temporary file"""
+        return self._temp_file_path
+
+    @temp_file_path.setter
+    def temp_file_path(self, value):
+        """The name of the file to be loaded if copied to a temporary file"""
+        self._temp_file_path = value
+
+    @property
+    def fid(self):
+        """The unique file id"""
+        if self._fid is None:
+            self.generate_fid()
+        return self._fid
+
+    def generate_fid(self, value=None):
+        """Generate a unique file id"""
+        if self.is_db:
+            self._fid = core.FileID(self.name, is_db=True)
+        elif self._temp_file_path is not None:
+            self._fid = core.FileID(self.name)
+        elif self._name is not None:
+            self._fid = core.FileID(self.name)
+        elif value is not None:
+            self._fid = core.FileID(value)
+        else:
+            raise ValueError("could not generate fid")
+
+    def copy_to_temporary(self):
+        """Copy file to a temporary file"""
+
+        logging.debug(f"external file received? {self.name.is_external=}")
+        if self.name is None:
+            raise ValueError("no file name given to loader class (self.name is None)")
+
+        if self._refuse_copying:
+            logging.debug("refusing copying")
+            self._temp_file_path = self.name
+            return
+
+        if not self._copy_also_local and not self.name.is_external:
+            self._temp_file_path = self.name
+            return
+
+        self._temp_file_path = self.name.copy()
+
+    def loader_executor(self, *args, **kwargs):
+        """Load the file"""
+        name = args[0]
+        self.refuse_copying = kwargs.pop("refuse_copying", False)
+        self.name = name
+        if not self.is_db:
+            self.copy_to_temporary()
+        cellpy_data = self.loader(*args, **kwargs)
+        return cellpy_data
+
+    def loader(self, *args, **kwargs):
+        """The method that does the actual loading.
+
+        This method should be overwritten by the specific loader class.
+        """
+        ...
+
+
+class BaseLoader(AtomicLoad, metaclass=abc.ABCMeta):
+    """Main loading class"""
+
+    instrument_name = "base_loader"
+
+    # TODO: should also include the functions for getting cellpy headers etc
+    #  here
+
+    @staticmethod
+    @abc.abstractmethod
+    def get_raw_units() -> dict:
+        """Include the settings for the units used by the instrument.
+
+        This is needed for example when converting the capacity to a specific capacity.
+        So far, it has been difficult to get any kind of consensus on what the most optimal
+        units are for storing cycling data. Therefore, cellpy implements three levels of units:
+        1) the raw units that the data is loaded in already has and 2) the cellpy units used by cellpy
+        when generating summaries and related information, and 3) output units that can be set to get the data
+        in a specif unit when exporting or creating specific outputs such as ICA.
+
+        Comment 2022.09.11::
+
+            still not sure if we should use raw units or cellpy units in the cellpy-files (.h5/ .cellpy).
+            Currently, the summary is in cellpy units and the raw and step data is in raw units. If
+            you have any input on this topic, let us know.
+
+        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
+        Si units can be used). For example, for current defined in mA, the value for the
+        current unit-fraction will be 0.001.
+
+        The internal cellpy units are given in the ``cellpy_units`` attribute.
+
+        Returns:
+            dictionary of units (str)
+
+        Example:
+            A minimum viable implementation::
+
+                @staticmethod
+                def get_raw_units():
+                    raw_units = dict()
+                    raw_units["current"] = "A"
+                    raw_units["charge"] = "Ah"
+                    raw_units["mass"] = "g"
+                    raw_units["voltage"] = "V"
+                    return raw_units
+
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def get_raw_limits(self) -> dict:
+        """Include the settings for how to decide what kind of step you are examining here.
+
+        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
+        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
+        If the (accumulated) change is less than 'epsilon', then cellpy interpret it to be stable.
+        It is expected that different instruments (with different resolution etc.) have different
+        resolutions and noice levels, thus different 'epsilons'.
+
+        Returns: the raw limits (dict)
+
+        """
+        raise NotImplementedError
+
+    @classmethod
+    def get_params(cls, parameter: Union[str, None]) -> dict:
+        """Retrieves parameters needed for facilitating working with the
+        instrument without registering it.
+
+        Typically, it should include the name and raw_ext.
+
+        Return: parameters or a selected parameter
+        """
+
+        return getattr(cls, parameter)
+
+    @abc.abstractmethod
+    def loader(self, *args, **kwargs) -> list:
+        """Loads data into a Data object and returns it"""
+        # This method is used by cellreader through the AtomicLoad.loader_executor method.
+        # It should be overwritten by the specific loader class.
+        #
+        # Notice that it is highly recommended that you don't try to implement .loader_executor yourself
+        # in your subclass!
+        pass
+
+    @staticmethod
+    def identify_last_data_point(data: core.Data) -> core.Data:
+        """This method is used to find the last record in the data."""
+        return core.identify_last_data_point(data)
+
+
+class AutoLoader(BaseLoader):
+    """Main autoload class.
+
+    This class can be sub-classed if you want to make a data-reader for different type of "easily parsed" files
+    (for example csv-files). The subclass needs to have at least one
+    associated CONFIGURATION_MODULE defined and must have the following attributes as minimum::
+
+        default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
+        supported_models: dict = SUPPORTED_MODELS
+
+    where SUPPORTED_MODELS is a dictionary with {NICK_NAME : CONFIGURATION_MODULE_NAME}  key-value pairs.
+    Remark! the NICK_NAME must be in upper-case!
+
+    It is also possible to set these in a custom pre_init method::
+
+        @classmethod
+        def pre_init(cls):
+            cls.default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
+            cls.supported_models: dict = SUPPORTED_MODELS
+
+    or turn off automatic registering of configuration::
+
+        @classmethod
+        def pre_init(cls):
+            cls.auto_register_config = False  # defaults to True
+
+    During initialisation of the class, if ``auto_register_config == True``,  it will dynamically load the definitions
+    provided in the CONFIGURATION_MODULE.py located in the ``cellpy.readers.instruments.configurations``
+    folder/package.
+
+    Attributes can be set during initialisation of the class as **kwargs that are then handled by the
+    ``parse_formatter_parameters`` method.
+
+    Remark that some also can be provided as arguments to the ``loader`` method and will then automatically
+    be "transparent" to the ``cellpy.get`` function. So if you would like to give the user access to modify
+    these arguments, you should implement them in the ``parse_loader_parameters`` method.
+
+    """
+
+    instrument_name = "auto_loader"
+
+    def __init__(self, *args, **kwargs):
+        self.auto_register_config = True
+        self.pre_init()
+
+        if not hasattr(self, "supported_models"):
+            raise AttributeError(
+                f"missing attribute in sub-class of TxtLoader: supported_models"
+            )
+        if not hasattr(self, "default_model"):
+            raise AttributeError(
+                f"missing attribute in sub-class of TxtLoader: default_model"
+            )
+
+        # in case model is given as argument
+        self.model = kwargs.pop("model", self.default_model)
+        if self.auto_register_config:
+            self.config_params = self.register_configuration()
+
+        self.parse_formatter_parameters(**kwargs)
+
+        self.pre_processors = self.config_params.pre_processors
+        additional_pre_processor_args = kwargs.pop(
+            "pre_processors", None
+        )  # could replace None with an empty dict to get rid of the if-clause:
+        if additional_pre_processor_args:
+            for key in additional_pre_processor_args:
+                self.pre_processors[key] = additional_pre_processor_args[key]
+
+        self.post_processors = self.config_params.post_processors
+        additional_post_processor_args = kwargs.pop(
+            "post_processors", None
+        )  # could replace None with an empty dict to get rid of the if-clause:
+        if additional_post_processor_args:
+            for key in additional_post_processor_args:
+                self.post_processors[key] = additional_post_processor_args[key]
+
+        self.include_aux = kwargs.pop("include_aux", False)
+        self.keep_all_columns = kwargs.pop("keep_all_columns", False)
+        self.cellpy_headers_normal = (
+            headers_normal  # the column headers defined by cellpy
+        )
+
+    @abc.abstractmethod
+    def parse_formatter_parameters(self, **kwargs) -> None:
+        ...
+
+    @abc.abstractmethod
+    def parse_loader_parameters(self, **kwargs):
+        ...
+
+    @abc.abstractmethod
+    def query_file(self, file_path: Union[str, pathlib.Path]) -> pd.DataFrame:
+        ...
+
+    def pre_init(self) -> None:
+        ...
+
+    def register_configuration(self) -> ModelParameters:
+        """Register and load model configuration"""
+        if (
+            self.model is None
+        ):  # in case None was given as argument (model=None in initialisation)
+            self.model = self.default_model
+        model_module_name = self.supported_models.get(self.model.upper(), None)
+        if model_module_name is None:
+            raise Exception(
+                f"The model {self.model} does not have any defined configuration."
+                f"\nCurrent supported models are {[*self.supported_models.keys()]}"
+            )
+        return register_configuration_from_module(self.model, model_module_name)
+
+    def get_raw_units(self):
+        """Include the settings for the units used by the instrument.
+
+        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
+        Si units can be used). For example, for current defined in mA, the value for the
+        current unit-fraction will be 0.001.
+
+        Returns:
+            dictionary containing the unit-fractions for current, charge, and mass
+
+        """
+        return self.config_params.raw_units
+
+    def get_raw_limits(self):
+        """Include the settings for how to decide what kind of step you are examining here.
+
+        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
+        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
+        It is expected that different instruments (with different resolution etc.) have different
+        'epsilons'.
+
+        Returns:
+            the raw limits (dict)
+
+        """
+        return self.config_params.raw_limits
+
+    @staticmethod
+    def get_headers_aux(raw: pd.DataFrame) -> dict:
+        raise NotImplementedError(
+            f"missing method in sub-class of TxtLoader: get_headers_aux"
+        )
+
+    def _pre_process(self):
+        for processor_name in self.pre_processors:
+            if self.pre_processors[processor_name]:
+                if hasattr(pre_processors, processor_name):
+                    logging.critical(f"running pre-processor: {processor_name}")
+                    processor = getattr(pre_processors, processor_name)
+                    self.temp_file_path = processor(self.temp_file_path)
+                else:
+                    raise NotImplementedError(
+                        f"{processor_name} is not currently supported - aborting!"
+                    )
+
+    def loader(self, name: Union[str, pathlib.Path], **kwargs: str) -> core.Data:
+        """returns a Data object with loaded data.
+
+        Loads data from a txt file (csv-ish).
+
+        Args:
+            name (str, pathlib.Path): name of the file.
+            kwargs (dict): key-word arguments from raw_loader.
+
+        Returns:
+            new_tests (list of data objects)
+
+        """
+        pre_processor_hook = kwargs.pop("pre_processor_hook", None)
+
+        if self.pre_processors:
+            self._pre_process()
+
+        self.parse_loader_parameters(**kwargs)
+
+        data_df = self.query_file(self.temp_file_path)
+
+        if pre_processor_hook is not None:
+            logging.debug("running pre-processing-hook")
+            data_df = pre_processor_hook(data_df)
+
+        data = core.Data()
+
+        # metadata
+        meta = self.parse_meta()
+        data.loaded_from = name
+        data.channel_index = meta.get("channel_index", None)
+        data.test_ID = meta.get("test_ID", None)
+        data.test_name = meta.get("test_name", None)
+        data.creator = meta.get("creator", None)
+        data.schedule_file_name = meta.get("schedule_file_name", None)
+        data.start_datetime = meta.get("start_datetime", None)
+
+        # Generating a FileID project:
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
+
+        data.raw = data_df
+        data.raw_data_files_length.append(len(data_df))
+        data.summary = (
+            pd.DataFrame()
+        )  # creating an empty frame - loading summary is not implemented
+        data = self._post_process(data)
+        data = self.identify_last_data_point(data)
+        if data.start_datetime is None:
+            data.start_datetime = data.raw[headers_normal.datetime_txt].iat[0]
+
+        data = self.validate(data)
+        return data
+
+    def validate(self, data: core.Data) -> core.Data:
+        """validation of the loaded data, should raise an appropriate exception if it fails."""
+
+        logging.debug(f"no validation of defined in this sub-class of TxtLoader")
+        return data
+
+    def parse_meta(self) -> dict:
+        """method that parses the data for meta-data (e.g. start-time, channel number, ...)"""
+
+        logging.debug(
+            f"no parsing method for meta-data defined in this sub-class of TxtLoader"
+        )
+        return dict()
+
+    def _post_rename_headers(self, data):
+        if self.include_aux:
+            new_aux_headers = self.get_headers_aux(data.raw)
+            data.raw.rename(index=str, columns=new_aux_headers, inplace=True)
+        return data
+
+    def _post_process(self, data):
+        # ordered post-processing steps:
+        for processor_name in ORDERED_POST_PROCESSING_STEPS:
+            if processor_name in self.post_processors:
+                data = self._perform_post_process_step(data, processor_name)
+
+        # non-ordered post-processing steps
+        for processor_name in self.post_processors:
+            if processor_name not in ORDERED_POST_PROCESSING_STEPS:
+                data = self._perform_post_process_step(data, processor_name)
+        return data
+
+    def _perform_post_process_step(self, data, processor_name):
+        if self.post_processors[processor_name]:
+            if hasattr(post_processors, processor_name):
+                logging.critical(f"running post-processor: {processor_name}")
+                processor = getattr(post_processors, processor_name)
+                data = processor(data, self.config_params)
+                if hasattr(self, f"_post_{processor_name}"):  # internal addon-function
+                    _processor = getattr(self, f"_post_{processor_name}")
+                    data = _processor(data)
+            else:
+                raise NotImplementedError(
+                    f"{processor_name} is not currently supported - aborting!"
+                )
+        return data
+
+
+class TxtLoader(AutoLoader, ABC):
+    """Main txt loading class (for sub-classing).
+
+    The subclass of a ``TxtLoader`` gets its information by loading model specifications from its respective module
+    (``cellpy.readers.instruments.configurations.<module>``) or configuration file (yaml).
+
+    Remark that if you implement automatic loading of the formatter, the module / yaml-file must include all
+    the required formatter parameters (sep, skiprows, header, encoding, decimal, thousands).
+
+    If you need more flexibility, try using the ``CustomTxtLoader`` or subclass directly
+    from ``AutoLoader`` or ``Loader``.
+
+    Constructor:
+        model (str): short name of the (already implemented) sub-model.
+        sep (str): delimiter.
+        skiprows (int): number of lines to skip.
+        header (int): number of the header lines.
+        encoding (str): encoding.
+        decimal (str): character used for decimal in the raw data, defaults to '.'.
+        processors (dict): pre-processing steps to take (before loading with pandas).
+        post_processors (dict): post-processing steps to make after loading the data, but before
+        returning them to the caller.
+        include_aux (bool): also parse so-called auxiliary columns / data. Defaults to False.
+        keep_all_columns (bool): load all columns, also columns that are not 100% necessary for ``cellpy`` to work.
+        Remark that the configuration settings for the sub-model must include a list of column header names
+        that should be kept if keep_all_columns is False (default).
+
+    Module:
+        sep (str): the delimiter (also works as a switch to turn on/off automatic detection of delimiter and
+        start of data (skiprows)).
+
+    """
+
+    instrument_name = "txt_loader"
+    raw_ext = "*"
+
+    # override this if needed
+    def parse_loader_parameters(self, **kwargs):
+        sep = kwargs.get("sep", None)
+        if sep is not None:
+            self.sep = sep
+        if self.sep is None:
+            self._auto_formatter()
+
+    # override this if needed
+    def parse_formatter_parameters(self, **kwargs):
+        logging.debug(f"model: {self.model}")
+        if not self.config_params.formatters:
+            # Setting defaults if formatter is not loaded
+            logging.debug("No formatter given - using default values.")
+            self.sep = kwargs.pop("sep", None)
+            self.skiprows = kwargs.pop("skiprows", 0)
+            self.header = kwargs.pop("header", 0)
+            self.encoding = kwargs.pop("encoding", "utf-8")
+            self.decimal = kwargs.pop("decimal", ".")
+            self.thousands = kwargs.pop("thousands", None)
+
+        else:
+            # Remark! This will break if one of these parameters are missing
+            # (not a keyword argument and not within the configuration):
+            self.sep = kwargs.pop("sep", self.config_params.formatters["sep"])
+            self.skiprows = kwargs.pop(
+                "skiprows", self.config_params.formatters["skiprows"]
+            )
+            self.header = kwargs.pop("header", self.config_params.formatters["header"])
+            self.encoding = kwargs.pop(
+                "encoding", self.config_params.formatters["encoding"]
+            )
+            self.decimal = kwargs.pop(
+                "decimal", self.config_params.formatters["decimal"]
+            )
+            self.thousands = kwargs.pop(
+                "thousands", self.config_params.formatters["thousands"]
+            )
+        logging.debug(
+            f"Formatters: self.sep={self.sep} self.skiprows={self.skiprows} self.header={self.header} self.encoding={self.encoding}"
+        )
+        logging.debug(
+            f"Formatters (cont.): self.decimal={self.decimal} self.thousands={self.thousands}"
+        )
+
+    def _auto_formatter(self):
+        separator, first_index = find_delimiter_and_start(
+            self.name,
+            separators=None,
+            checking_length_header=100,
+            checking_length_whole=200,
+        )
+        self.encoding = "UTF-8"  # consider adding a find_encoding function
+        self.sep = separator
+        self.skiprows = first_index - 1
+        self.header = 0
+
+        logging.critical(
+            f"auto-formatting:\n  {self.sep=}\n  {self.skiprows=}\n  {self.header=}\n  {self.encoding=}\n"
+        )
+
+    # override this if using other query functions
+    def query_file(self, name):
+        logging.debug(f"parsing with pandas.read_csv: {name}")
+        logging.critical(
+            f"{self.sep=}, {self.skiprows=}, {self.header=}, {self.encoding=}, {self.decimal=}"
+        )
+        data_df = pd.read_csv(
+            name,
+            sep=self.sep,
+            skiprows=self.skiprows,
+            header=self.header,
+            encoding=self.encoding,
+            decimal=self.decimal,
+            thousands=self.thousands,
+        )
+        return data_df
```

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/biologics_mpr.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/biologics_mpr.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/__init__.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/__init__.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_four.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_four.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_one.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_one.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_three.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_three.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_two.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_two.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/maccor_txt_zero.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/maccor_txt_zero.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/configurations/neware_txt_zero.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/configurations/neware_txt_zero.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/custom.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/custom.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/ext_nda_reader.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/ext_nda_reader.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/local_instrument.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/local_instrument.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/maccor_txt.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/maccor_txt.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/neware_txt.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/neware_txt.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/pec_csv.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/pec_csv.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/processors/post_processors.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/processors/post_processors.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/instruments/processors/pre_processors.py` & `cellpy-1.0.0b1/cellpy/readers/instruments/processors/pre_processors.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/readers/sql_dbreader.py` & `cellpy-1.0.0b1/cellpy/readers/sql_dbreader.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch.py` & `cellpy-1.0.0b1/cellpy/utils/batch.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,1252 +1,1252 @@
-"""Routines for batch processing of cells (v2)."""
-
-import logging
-import os
-import pathlib
-import shutil
-import sys
-import warnings
-
-import pandas as pd
-from pandas import Index
-from tqdm.auto import tqdm
-
-import cellpy.exceptions
-from cellpy import log, prms
-from cellpy.parameters.internal_settings import (
-    headers_journal,
-    headers_step_table,
-    headers_summary,
-)
-from cellpy.internals.core import OtherPath
-from cellpy.utils.batch_tools.batch_analyzers import (
-    BaseSummaryAnalyzer,
-    OCVRelaxationAnalyzer,
-)
-from cellpy.utils.batch_tools.batch_core import Data
-from cellpy.utils.batch_tools.batch_experiments import CyclingExperiment
-from cellpy.utils.batch_tools.batch_exporters import CSVExporter
-from cellpy.utils.batch_tools.batch_journals import LabJournal
-from cellpy.utils.batch_tools.batch_plotters import CyclingSummaryPlotter
-from cellpy.utils.batch_tools.dumpers import ram_dumper
-
-# logger = logging.getLogger(__name__)
-logging.captureWarnings(True)
-
-COLUMNS_SELECTED_FOR_VIEW = [
-    headers_journal.mass,
-    headers_journal.total_mass,
-    headers_journal.loading,
-    headers_journal.nom_cap,
-]
-
-
-class Batch:
-    """A convenient class for running batch procedures.
-
-    The Batch class contains among other things:
-        - iterator protocol
-        - a journal with info about the different cells where the
-        main information is accessible as a pandas.DataFrame through the `.pages` attribute
-        - a data lookup accessor `.data` that behaves similarly as a dict.
-    """
-
-    def __init__(self, *args, **kwargs):
-        """Initialize the Batch class.
-
-        The initialization accepts arbitrary arguments and keyword arguments.
-        It first looks for the file_name and db_reader keyword arguments.
-
-        Usage:
-            b = Batch((name, (project)), **kwargs)
-
-        Examples:
-            >>> b = Batch("experiment001", "main_project")
-            >>> b = Batch("experiment001", "main_project", batch_col="b02")
-            >>> b = Batch(name="experiment001", project="main_project", batch_col="b02")
-            >>> b = Batch(file_name="cellpydata/batchfiles/cellpy_batch_experiment001.json")
-
-        Keyword Args (priority):
-            file_name (str or pathlib.Path): journal file name to load.
-            db_reader (str): data-base reader to use (defaults to "default" as given
-                in the config-file or prm-class).
-            frame (pandas.DataFrame): load from given dataframe.
-        Args:
-            *args: name (str) (project (str))
-
-        Keyword Args (other):
-            default_log_level (str): custom log-level (defaults to None (i.e. default log-level in cellpy)).
-            custom_log_dir (str or pathlib.Path): custom folder for putting the log-files.
-            force_raw_file (bool): load from raw regardless (defaults to False).
-            force_cellpy (bool): load cellpy-files regardless (defaults to False).
-            force_recalc (bool): Always recalculate (defaults to False).
-            export_cycles (bool): Extract and export individual cycles to csv (defaults to True).
-            export_raw (bool): Extract and export raw-data to csv (defaults to True).
-            export_ica (bool): Extract and export individual dQ/dV data to csv (defaults to True).
-            accept_errors (bool): Continue automatically to next file if error is raised (defaults to False).
-            nom_cap (float): give a nominal capacity if you want to use another value than
-                the one given in the config-file or prm-class.
-        """
-        # TODO: add option for setting max cycle number
-        #   use self.experiment.last_cycle = xxx
-        default_log_level = kwargs.pop("default_log_level", None)
-        custom_log_dir = kwargs.pop("custom_log_dir", None)
-        if default_log_level is not None or custom_log_dir is not None:
-            log.setup_logging(
-                custom_log_dir=custom_log_dir,
-                default_level=default_log_level,
-                reset_big_log=True,
-            )
-
-        db_reader = kwargs.pop("db_reader", "default")
-
-        file_name = kwargs.pop("file_name", None)
-        frame = kwargs.pop("frame", None)
-
-        logging.debug("creating CyclingExperiment")
-        self.experiment = CyclingExperiment(db_reader=db_reader)
-        logging.info("created CyclingExperiment")
-
-        self.experiment.force_cellpy = kwargs.pop("force_cellpy", False)
-        self.experiment.force_raw = kwargs.pop("force_raw_file", False)
-        self.experiment.force_recalc = kwargs.pop("force_recalc", False)
-        self.experiment.export_cycles = kwargs.pop("export_cycles", True)
-        self.experiment.export_raw = kwargs.pop("export_raw", True)
-        self.experiment.export_ica = kwargs.pop("export_ica", False)
-        self.experiment.accept_errors = kwargs.pop("accept_errors", False)
-        self.experiment.nom_cap = kwargs.pop("nom_cap", None)
-
-        if not file_name:
-            if frame is not None:
-                self.experiment.journal.from_frame(frame, **kwargs)
-            else:
-                if len(args) > 0:
-                    self.experiment.journal.name = args[0]
-
-                if len(args) > 1:
-                    self.experiment.journal.project = args[1]
-
-                for key in kwargs:
-                    if key == "name":
-                        self.experiment.journal.name = kwargs[key]
-                    elif key == "project":
-                        self.experiment.journal.project = kwargs[key]
-                    elif key == "batch_col":
-                        self.experiment.journal.batch_col = kwargs[key]
-        else:
-            self.experiment.journal.from_file(file_name=file_name, **kwargs)
-
-        self.exporter = CSVExporter()
-        self.exporter._assign_dumper(ram_dumper)
-        self.exporter.assign(self.experiment)
-
-        self.summary_collector = BaseSummaryAnalyzer()
-        self.summary_collector.assign(self.experiment)
-
-        self.plotter = CyclingSummaryPlotter()
-        self.plotter.assign(self.experiment)
-        self._journal_name = self.journal_name
-        self.headers_step_table = headers_step_table
-
-    def __str__(self):
-        return str(self.experiment)
-
-    def _repr_html_(self):
-        txt = f"<h2>Batch-object</h2> id={hex(id(self))}"
-        txt += f"<h3>batch.journal</h3>"
-        txt += f"<blockquote>{self.journal._repr_html_()}</blockquote>"
-        txt += f"<h3>batch.experiment</h3>"
-        txt += f"<blockquote>{self.experiment._repr_html_()}</blockquote>"
-
-        return txt
-
-    def __len__(self):
-        return len(self.experiment)
-
-    def __iter__(self):
-        return self.experiment.__iter__()
-
-    def show_pages(self, number_of_rows=5):
-        warnings.warn("Deprecated - use pages.head() instead", DeprecationWarning)
-        return self.experiment.journal.pages.head(number_of_rows)
-
-    @property
-    def view(self):
-        warnings.warn("Deprecated - use report instead", DeprecationWarning)
-        pages = self.experiment.journal.pages
-        pages = pages[COLUMNS_SELECTED_FOR_VIEW]
-        return pages
-
-    @property
-    def name(self):
-        return self.experiment.journal.name
-
-    def _check_cell_raw(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            return len(c.data.raw)
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_steps(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            return len(c.data.steps)
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_summary(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            return len(c.data.summary)
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_max_cap(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            s = c.data.summary
-            return s[headers_summary["charge_capacity_gravimetric"]].max()
-
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_min_cap(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            s = c.data.summary
-            return s[headers_summary["charge_capacity_gravimetric"]].min()
-
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_avg_cap(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            s = c.data.summary
-            return s[headers_summary["charge_capacity_gravimetric"]].mean()
-
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_std_cap(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            s = c.data.summary
-            return s[headers_summary["charge_capacity_gravimetric"]].std()
-
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_empty(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            return c.empty
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def _check_cell_cycles(self, cell_id):
-        try:
-            c = self.experiment.data[cell_id]
-            return c.data.steps[self.headers_step_table.cycle].max()
-        except Exception as e:
-            logging.debug(f"Exception ignored: {e}")
-            return None
-
-    def drop(self, cell_label=None):
-        """Drop cells from the journal.
-
-        If ``cell_label`` is not given, ``cellpy`` will look into the journal for session
-        info about bad cells, and if it finds it, it will remove those from the
-        journal.
-
-        Note! remember to save your journal again after modifying it.
-
-        Note! this method has not been properly tested yet.
-
-        Args:
-            cell_label (str): the cell label of the cell you would like to remove.
-
-        Returns:
-            ``cellpy.utils.batch`` object (returns a copy if `keep_old` is ``True``).
-
-        """
-        if cell_label is None:
-            try:
-                cell_labels = self.journal.session["bad_cells"]
-            except AttributeError:
-                logging.critical(
-                    "session info about bad cells is missing - cannot drop"
-                )
-                return
-        else:
-            cell_labels = [cell_label]
-
-        for cell_label in cell_labels:
-            if cell_label not in self.pages.index:
-                logging.critical(f"could not find {cell_label}")
-            else:
-                self.pages = self.pages.drop(cell_label)
-
-    def report(self, stylize=True):
-        """Create a report on all the cells in the batch object.
-
-        Remark! To perform a reporting, cellpy needs to access all the data (and it might take some time).
-
-        Returns:
-            ``pandas.DataFrame``
-        """
-        pages = self.experiment.journal.pages
-        pages = pages[COLUMNS_SELECTED_FOR_VIEW].copy()
-        # pages["empty"] = pages.index.map(self._check_cell_empty)
-        pages["raw_rows"] = pages.index.map(self._check_cell_raw)
-        pages["steps_rows"] = pages.index.map(self._check_cell_steps)
-        pages["summary_rows"] = pages.index.map(self._check_cell_summary)
-        pages["last_cycle"] = pages.index.map(self._check_cell_cycles)
-        pages["average_capacity"] = pages.index.map(self._check_cell_avg_cap)
-        pages["max_capacity"] = pages.index.map(self._check_cell_max_cap)
-        pages["min_capacity"] = pages.index.map(self._check_cell_min_cap)
-        pages["std_capacity"] = pages.index.map(self._check_cell_std_cap)
-
-        avg_last_cycle = pages.last_cycle.mean()
-        avg_max_capacity = pages.max_capacity.mean()
-
-        if stylize:
-
-            def highlight_outlier(s):
-                average = s.mean()
-                outlier = (s < average / 2) | (s > 2 * average)
-                return ["background-color: #f09223" if v else "" for v in outlier]
-
-            def highlight_small(s):
-                average = s.mean()
-                outlier = s < average / 4
-                return ["background-color: #41A1D8" if v else "" for v in outlier]
-
-            def highlight_very_small(s):
-                outlier = s <= 3
-                return ["background-color: #416CD8" if v else "" for v in outlier]
-
-            def highlight_big(s):
-                average = s.mean()
-                outlier = s > 2 * average
-                return ["background-color: #D85F41" if v else "" for v in outlier]
-
-            styled_pages = (
-                pages.style.apply(highlight_small, subset=["last_cycle"])
-                .apply(
-                    highlight_outlier,
-                    subset=["min_capacity", "max_capacity", "average_capacity"],
-                )
-                .apply(
-                    highlight_big,
-                    subset=["min_capacity", "max_capacity", "average_capacity"],
-                )
-                .apply(
-                    highlight_very_small,
-                    subset=["max_capacity", "average_capacity", "last_cycle"],
-                )
-                # .format({'min_capacity': "{:.2f}",
-                #        'max_capacity': "{:.2f}",
-                #        'average_capacity': "{:.2f}",
-                #        'std_capacity': '{:.2f}'})
-            )
-            pages = styled_pages
-
-        return pages
-
-    @property
-    def info_file(self):
-        # renamed to journal_name
-        warnings.warn("Deprecated - use journal_name instead", DeprecationWarning)
-        return self.experiment.journal.file_name
-
-    @property
-    def journal_name(self):
-        return self.experiment.journal.file_name
-
-    def _concat_memory_dumped(self, engine_name):
-        keys = [df.name for df in self.experiment.memory_dumped[engine_name]]
-        return pd.concat(self.experiment.memory_dumped[engine_name], keys=keys, axis=1)
-
-    @property
-    def summaries(self):
-        """Concatenated summaries from all cells (multiindex dataframe)."""
-        try:
-            return self._concat_memory_dumped("summary_engine")
-        except KeyError:
-            logging.critical("no summaries exists (dumping to ram first)")
-            self.summary_collector.do()
-            return self._concat_memory_dumped("summary_engine")
-
-    @property
-    def summary_headers(self):
-        """The column names of the concatenated summaries"""
-        try:
-            return self.summaries.columns.get_level_values(0)
-        except AttributeError:
-            logging.info("can't get any columns")
-
-    @property
-    def cell_names(self) -> list:
-        return self.experiment.cell_names
-
-    @property
-    def labels(self):
-        # Plan: allow cells to both have a label and a cell_name, where that latter should be a unique
-        # identifier. Consider also to allow for a group-name.
-        # The label and cell name can be the same. Consider allowing several cells to share the same label
-        # thus returning several cellpy cell objects. Our use "group" for this purpose.
-        print(
-            "Label-based look-up is not supported yet. Performing cell-name based look-up instead."
-        )
-        return self.experiment.cell_names
-
-    @property
-    def cells(self) -> Data:
-        """Access cells as a Data object (attribute lookup and automatic loading)
-
-        Usage (at least in jupyter notebook):
-            Write `b.cells.x` and press <TAB>. Then a pop-up will appear, and you can choose the
-            cell you would like to retrieve.
-        """
-        return self.experiment.data
-
-    @property
-    def cell_summary_headers(self) -> Index:
-        return self.experiment.data[self.experiment.cell_names[0]].data.summary.columns
-
-    @property
-    def cell_raw_headers(self) -> Index:
-        return self.experiment.data[self.experiment.cell_names[0]].data.raw.columns
-
-    @property
-    def cell_step_headers(self) -> Index:
-        return self.experiment.data[self.experiment.cell_names[0]].data.steps.columns
-
-    @property
-    def pages(self) -> pd.DataFrame:
-        return self.experiment.journal.pages
-
-    @pages.setter
-    def pages(self, df: pd.DataFrame):
-        self.experiment.journal.pages = df
-        all_cell_labels = set(self.experiment.cell_data_frames.keys())
-        cell_labels_to_keep = set(self.journal.pages.index)
-        cell_labels_to_remove = all_cell_labels - cell_labels_to_keep
-        for cell_label in cell_labels_to_remove:
-            del self.experiment.cell_data_frames[cell_label]
-
-    @property
-    def journal(self) -> LabJournal:
-        return self.experiment.journal
-
-    @journal.setter
-    def journal(self, new):
-        # self.experiment.journal = new
-        raise NotImplementedError(
-            "Setting a new journal object directly on a "
-            "batch object is not allowed at the moment. Try modifying "
-            "the journal.pages instead."
-        )
-
-    def old_duplicate_journal(self, folder=None) -> None:
-        """Copy the journal to folder.
-
-        Args:
-            folder (str or pathlib.Path): folder to copy to (defaults to the
-            current folder).
-        """
-
-        logging.debug(f"duplicating journal to folder {folder}")
-        journal_name = pathlib.Path(self.experiment.journal.file_name)
-        if not journal_name.is_file():
-            logging.info("No journal saved")
-            return
-        new_journal_name = journal_name.name
-        if folder is not None:
-            new_journal_name = pathlib.Path(folder) / new_journal_name
-        try:
-            shutil.copy(journal_name, new_journal_name)
-        except shutil.SameFileError:
-            logging.debug("same file exception encountered")
-
-    def duplicate_journal(self, folder=None) -> None:
-        """Copy the journal to folder.
-
-        Args:
-            folder (str or pathlib.Path): folder to copy to (defaults to the
-            current folder).
-        """
-        self.experiment.journal.duplicate_journal(folder)
-        #
-        # logging.debug(f"duplicating journal to folder {folder}")
-        # journal_name = pathlib.Path(self.experiment.journal.file_name)
-        # if not journal_name.is_file():
-        #     logging.info("No journal saved")
-        #     return
-        # new_journal_name = journal_name.name
-        # if folder is not None:
-        #     new_journal_name = pathlib.Path(folder) / new_journal_name
-        # try:
-        #     shutil.copy(journal_name, new_journal_name)
-        # except shutil.SameFileError:
-        #     logging.debug("same file exception encountered")
-
-    def create_journal(self, description=None, from_db=True, **kwargs):
-        """Create journal pages.
-
-            This method is a wrapper for the different Journal methods for making
-            journal pages (Batch.experiment.journal.xxx). It is under development. If you
-            want to use 'advanced' options (i.e. not loading from a db), please consider
-            using the methods available in Journal for now.
-
-            Args:
-                description: the information and meta-data needed to generate the journal pages ('empty': create an
-                    empty journal; ``dict``: create journal pages from a dictionary; ``pd.DataFrame``: create journal pages
-                    from a ``pandas.DataFrame``: 'filename.json': load cellpy batch file; 'filename.xlsx': create journal
-                    pages from an Excel file).
-                from_db (bool): Deprecation Warning: this parameter will be removed as it is
-                    the default anyway. Generate the pages from a db (the default option).
-                    This will be over-ridden if description is given.
-
-                **kwargs: sent to sub-function(s) (*e.g.* from_db -> simple_db_reader -> find_files ->
-                    filefinder.search_for_files).
-
-            kwargs -> from_db:
-                project=None, name=None, batch_col=None
-
-
-            kwargs -> simple_db_reader:
-                reader: a reader object (defaults to dbreader.Reader)
-                cell_ids: keys (cell IDs)
-                file_list: file list to send to filefinder (instead of searching in folders for files).
-                pre_path: prepended path to send to filefinder.
-                include_key: include the key col in the pages (the cell IDs).
-                include_individual_arguments: include the argument column in the pages.
-                additional_column_names: list of additional column names to include in the pages.
-
-            kwargs -> filefinder.search_for_files:
-                run_name(str): run-file identification.
-                raw_extension(str): optional, extension of run-files (without the '.').
-                cellpy_file_extension(str): optional, extension for cellpy files
-                    (without the '.').
-                raw_file_dir(path): optional, directory where to look for run-files
-                    (default: read prm-file)
-                cellpy_file_dir(path): optional, directory where to look for
-                    cellpy-files (default: read prm-file)
-                prm_filename(path): optional parameter file can be given.
-                file_name_format(str): format of raw-file names or a glob pattern
-                    (default: YYYYMMDD_[name]EEE_CC_TT_RR).
-                reg_exp(str): use regular expression instead (defaults to None).
-                sub_folders (bool): perform search also in sub-folders.
-                file_list (list of str): perform the search within a given list
-                    of filenames instead of searching the folder(s). The list should
-                    not contain the full filepath (only the actual file names). If
-                    you want to provide the full path, you will have to modify the
-                    file_name_format or reg_exp accordingly.
-                pre_path (path or str): path to prepend the list of files selected
-                     from the file_list.
-            kwargs -> journal.to_file:
-                duplicate_to_local_folder (bool): default True.
-
-
-        Returns:
-            info_dict
-
-        """
-
-        # TODO (jepe): create option to update journal without looking for files
-
-        logging.debug("Creating a journal")
-        logging.debug(f"description: {description}")
-        logging.debug(f"from_db: {from_db}")
-        logging.info(f"name: {self.experiment.journal.name}")
-        logging.info(f"project: {self.experiment.journal.project}")
-        to_project_folder = kwargs.pop("to_project_folder", True)
-        duplicate_to_local_folder = kwargs.pop("duplicate_to_local_folder", True)
-
-        if description is not None:
-            from_db = False
-        else:
-            if self.experiment.journal.pages is not None:
-                warnings.warn(
-                    "You created a journal - but you already have a "
-                    "journal. Hope you know what you are doing!"
-                )
-
-        if from_db:
-            self.experiment.journal.from_db(**kwargs)
-            self.experiment.journal.to_file(
-                duplicate_to_local_folder=duplicate_to_local_folder
-            )
-
-            # TODO: remove these:
-            if duplicate_to_local_folder:
-                self.experiment.journal.duplicate_journal()
-            if to_project_folder:
-                self.duplicate_journal(prms.Paths.batchfiledir)
-
-        else:
-            is_str = isinstance(description, str)
-            is_file = False
-
-            if is_str and pathlib.Path(description).is_file():
-                description = pathlib.Path(description)
-                is_file = True
-
-            if isinstance(description, pathlib.Path):
-                logging.debug("pathlib.Path object given")
-                is_file = True
-
-            if is_file:
-                logging.info(f"loading file {description}")
-                if description.suffix in [".json", ".xlsx"]:
-                    self.experiment.journal.from_file(description)
-                else:
-                    warnings.warn("unknown file extension")
-
-            else:
-                if is_str and description.lower() == "empty":
-                    logging.debug("creating empty journal pages")
-
-                    self.experiment.journal.pages = (
-                        self.experiment.journal.create_empty_pages()
-                    )
-
-                elif isinstance(description, pd.DataFrame):
-                    logging.debug("pandas DataFrame given")
-
-                    p = self.experiment.journal.create_empty_pages()
-                    columns = p.columns
-
-                    for column in columns:
-                        try:
-                            p[column] = description[column]
-                        except KeyError:
-                            logging.debug(f"missing key: {column}")
-
-                    # checking if filenames is a column
-                    if "filenames" in description.columns:
-                        indexes = description["filenames"]
-                    else:
-                        indexes = description.index
-
-                    p.index = indexes
-                    self.experiment.journal.pages = p
-
-                elif isinstance(description, dict):
-                    logging.debug("dictionary given")
-                    self.experiment.journal.pages = (
-                        self.experiment.journal.create_empty_pages()
-                    )
-                    for k in self.experiment.journal.pages.columns:
-                        try:
-                            value = description[k]
-                        except KeyError:
-                            warnings.warn(f"missing key: {k}")
-                        else:
-                            if not isinstance(value, list):
-                                warnings.warn("encountered item that is not a list")
-                                logging.debug(f"converting '{k}' to list-type")
-                                value = [value]
-                            if k == "raw_file_names":
-                                if not isinstance(value[0], list):
-                                    warnings.warn(
-                                        "encountered raw file description"
-                                        "that is not of list-type"
-                                    )
-                                    logging.debug(
-                                        "converting raw file description to a"
-                                        "list of lists"
-                                    )
-                                    value = [value]
-                            self.experiment.journal.pages[k] = value
-
-                    try:
-                        value = description["filenames"]
-                        if not isinstance(value, list):
-                            warnings.warn("encountered item that is not a list")
-                            logging.debug(f"converting '{k}' to list-type")
-                            value = [value]
-                        self.experiment.journal.pages.index = value
-                    except KeyError:
-                        logging.debug("could not interpret the index")
-
-                else:
-                    logging.debug(
-                        "the option you provided seems to be either of "
-                        "an unknown type or a file not found"
-                    )
-                    logging.info(
-                        "did not understand the option - creating empty journal pages"
-                    )
-
-            # finally
-            self.experiment.journal.to_file(
-                duplicate_to_local_folder=duplicate_to_local_folder
-            )
-            self.experiment.journal.generate_folder_names()
-            self.experiment.journal.paginate()
-            self.duplicate_journal(prms.Paths.batchfiledir)
-
-    def create_folder_structure(self) -> None:
-        warnings.warn("Deprecated - use paginate instead.", DeprecationWarning)
-        self.experiment.journal.paginate()
-        logging.info("created folders")
-
-    def paginate(self) -> None:
-        """Create the folders where cellpy will put its output."""
-
-        self.experiment.journal.paginate()
-        logging.info("created folders")
-
-    def save_journal(self) -> None:
-        """Save the journal (json-format).
-
-        The journal file will be saved in the project directory and in the
-        batch-file-directory (prms.Paths.batchfiledir). The latter is useful
-        for processing several batches using the iterate_batches functionality.
-        """
-
-        # Remark! Got an recursive error when running on mac.
-        self.experiment.journal.to_file(to_project_folder=True, paginate=False)
-        logging.info("saved journal pages to project folder")
-        self.duplicate_journal(prms.Paths.batchfiledir)
-        logging.info("duplicated journal pages to batch dir")
-        self.duplicate_journal()
-        logging.info("duplicated journal pages to current dir")
-
-    def export_journal(self, filename=None) -> None:
-        """Export the journal to xlsx."""
-        if filename is None:
-            filename = self.experiment.journal.file_name
-        filename = pathlib.Path(filename).with_suffix(".xlsx")
-        self.experiment.journal.to_file(
-            file_name=filename, to_project_folder=False, paginate=False
-        )
-
-    def duplicate_cellpy_files(
-        self, location: str = "standard", selector: dict = None, **kwargs
-    ) -> None:
-        """Copy the cellpy files and make a journal with the new names available in
-        the current folder.
-
-        Args:
-            location: where to copy the files. Either choose among the following
-                options:
-                "standard": data/interim folder
-                "here": current directory
-                "cellpydatadir": the stated cellpy data dir in your settings (prms)
-            or if the location is not one of the above, use the actual value of the
-                location argument.
-            selector (dict): if given, the cellpy files are reloaded after duplicating and
-                modified based on the given selector(s).
-
-            kwargs: sent to update if selector is provided
-
-        Returns:
-            The updated journal pages.
-        """
-
-        pages = self.experiment.journal.pages
-        cellpy_file_dir = OtherPath(prms.Paths.cellpydatadir)
-
-        if location == "standard":
-            batch_data_dir = pathlib.Path("data") / "interim"
-
-        elif location == "here":
-            batch_data_dir = pathlib.Path(".")
-
-        elif location == "cellpydatadir":
-            batch_data_dir = cellpy_file_dir
-
-        else:
-            batch_data_dir = location
-
-        def _new_file_path(x):
-            return str(batch_data_dir / pathlib.Path(x).name)
-
-        # update the journal pages
-        columns = pages.columns
-        pages["new_cellpy_file_name"] = pages.cellpy_file_name.apply(_new_file_path)
-
-        # copy the cellpy files
-        for n, row in pages.iterrows():
-            logging.info(f"{row.cellpy_file_name} -> {row.new_cellpy_file_name}")
-            try:
-                from_file = row.cellpy_file_name
-                to_file = row.new_cellpy_file_name
-                os.makedirs(os.path.dirname(to_file), exist_ok=True)
-                shutil.copy(from_file, to_file)
-            except shutil.SameFileError:
-                logging.info("Same file! No point in copying")
-            except FileNotFoundError:
-                logging.info("File not found! Cannot copy it!")
-
-        # save the journal pages
-        pages["cellpy_file_name"] = pages["new_cellpy_file_name"]
-        self.experiment.journal.pages = pages[columns]
-        journal_file_name = pathlib.Path(self.experiment.journal.file_name).name
-        self.experiment.journal.to_file(
-            journal_file_name, paginate=False, to_project_folder=False
-        )
-        if selector is not None:
-            logging.info("Modifying the cellpy-files.")
-            logging.info(f"selector: {selector}")
-            self.experiment.force_cellpy = True
-            self.update(selector=selector, **kwargs)
-
-    # TODO: list_journals?
-
-    def link(self, max_cycle=None, force_combine_summaries=False) -> None:
-        """Link journal content to the cellpy-files and load the step information.
-
-        Args:
-            max_cycle (int): set maximum cycle number to link to.
-            force_combine_summaries (bool): automatically run combine_summaries (set this to True
-                if you are re-linking without max_cycle for a batch that previously were linked
-                with max_cycle)
-
-        """
-
-        self.experiment.link(max_cycle=max_cycle)
-        if force_combine_summaries or max_cycle:
-            self.summary_collector.do(reset=True)
-
-    def load(self) -> None:
-        # does the same as update
-        warnings.warn("Deprecated - use update instead.", DeprecationWarning)
-        self.experiment.update()
-
-    def update(self, pool=False, **kwargs) -> None:
-        """Updates the selected datasets.
-
-        Keyword Args (to experiment-instance):
-            all_in_memory (bool): store the `cellpydata` in memory (default
-                False)
-            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
-                dictionary will override the **kwargs and the parameters from the journal pages
-                for the indicated cell.
-            logging_mode (str): sets the logging mode for the loader(s).
-            accept_errors (bool): if True, the loader will continue even if it encounters errors.
-
-
-        Additional kwargs:
-            transferred all the way to the instrument loader, if not
-            picked up earlier. Remark that you can obtain the same pr. cell by
-            providing a `cellspecs` dictionary. The kwargs have precedence over the
-            parameters given in the journal pages, but will be overridden by parameters
-            given by `cellspecs`.
-
-            Merging:
-                recalc (Bool): set to False if you don't want automatic "recalc" of
-                    cycle numbers etc. when merging several data-sets.
-            Loading:
-                selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
-                loading from raw is not necessary (or turned off).
-
-        """
-        self.experiment.errors["update"] = []
-        if pool:
-            self.experiment.parallel_update(**kwargs)
-        else:
-            self.experiment.update(**kwargs)
-
-    def export_cellpy_files(self, path=None, **kwargs) -> None:
-        if path is None:
-            path = pathlib.Path(".").resolve()
-        self.experiment.errors["export_cellpy_files"] = []
-        self.experiment.export_cellpy_files(path=path, **kwargs)
-
-    def recalc(self, **kwargs) -> None:
-        """Run make_step_table and make_summary on all cells.
-
-        Keyword Args:
-            save (bool): Save updated cellpy-files if True (defaults to True).
-            step_opts (dict): parameters to inject to make_steps (defaults to None).
-            summary_opts (dict): parameters to inject to make_summary (defaults to None).
-            indexes (list): Only recalculate for given indexes (i.e. list of cell-names) (defaults to None).
-            calc_steps (bool): Run make_steps before making the summary (defaults to True).
-            testing (bool): Only for testing purposes (defaults to False).
-
-        Returns:
-            None
-        """
-        self.experiment.errors["recalc"] = []
-        self.experiment.recalc(**kwargs)
-
-    def make_summaries(self) -> None:
-        warnings.warn("Deprecated - use combine_summaries instead.", DeprecationWarning)
-
-        self.exporter.do()
-
-    def combine_summaries(self, export_to_csv=True, **kwargs) -> None:
-        """Combine selected columns from each of the cells into single frames"""
-
-        if export_to_csv:
-            self.exporter.do()
-        else:
-            self.summary_collector.do(**kwargs)
-
-    def plot_summaries(
-        self, output_filename=None, backend=None, reload_data=False, **kwargs
-    ) -> None:
-        """Plot the summaries"""
-
-        if reload_data or ("summary_engine" not in self.experiment.memory_dumped):
-            logging.debug("running summary_collector")
-            self.summary_collector.do(reset=True)
-
-        if backend is None:
-            backend = prms.Batch.backend
-
-        if backend in ["bokeh", "matplotlib"]:
-            prms.Batch.backend = backend
-
-        if backend == "bokeh":
-            try:
-                import bokeh.plotting
-
-                prms.Batch.backend = "bokeh"
-
-                if output_filename is not None:
-                    bokeh.plotting.output_file(output_filename)
-                else:
-                    if prms.Batch.notebook:
-                        bokeh.plotting.output_notebook()
-
-            except ModuleNotFoundError:
-                prms.Batch.backend = "matplotlib"
-                logging.warning(
-                    "could not find the bokeh module -> using matplotlib instead"
-                )
-
-        self.plotter.do(**kwargs)
-
-
-def init(*args, **kwargs) -> Batch:
-    """Returns an initialized instance of the Batch class.
-
-    Args:
-        *args: passed directly to Batch()
-            **name**: name of batch;
-            **project**: name of project;
-            **batch_col**: batch column identifier.
-        **kwargs:
-            **file_name**: json file if loading from pages;
-            **default_log_level**: "INFO" or "DEBUG";
-            the rest is passed directly to Batch().
-
-    Examples:
-        >>> empty_batch = Batch.init(db_reader=None)
-        >>> batch_from_file = Batch.init(file_name="cellpy_batch_my_experiment.json")
-        >>> normal_init_of_batch = Batch.init()
-    """
-    # TODO: add option for setting max cycle number (experiment.last_cycle)
-    # set up cellpy logger
-    default_log_level = kwargs.pop("default_log_level", None)
-    testing = kwargs.pop("testing", False)
-    file_name = kwargs.pop("file_name", None)
-    frame = kwargs.pop("frame", None)
-    log.setup_logging(
-        default_level=default_log_level, testing=testing, reset_big_log=True
-    )
-
-    logging.debug(f"returning Batch(kwargs: {kwargs})")
-    if file_name is not None:
-        kwargs.pop("db_reader", None)
-        return Batch(*args, file_name=file_name, db_reader=None, **kwargs)
-    if frame is not None:
-        kwargs.pop("db_reader", None)
-        return Batch(*args, file_name=None, db_reader=None, frame=frame, **kwargs)
-
-    return Batch(*args, **kwargs)
-
-
-def from_journal(journal_file, autolink=True, testing=False) -> Batch:
-    """Create a Batch from a journal file"""
-    # TODO: add option for setting max cycle number (experiment.last_cycle)
-    b = init(db_reader=None, file_name=journal_file, testing=testing)
-    if autolink:
-        b.link()
-    return b
-
-
-def load_pages(file_name) -> pd.DataFrame:
-    """Retrieve pages from a Journal file.
-
-    This function is here to let you easily inspect a Journal file without
-    starting up the full batch-functionality.
-
-    Examples:
-        >>> from cellpy.utils import batch
-        >>> journal_file_name = 'cellpy_journal_one.json'
-        >>> pages = batch.load_pages(journal_file_name)
-
-    Returns:
-        pandas.DataFrame
-
-    """
-
-    logging.info(f"Loading pages from {file_name}")
-    try:
-        pages, *_ = LabJournal.read_journal_jason_file(file_name)
-        return pages
-    except cellpy.exceptions.UnderDefined:
-        logging.critical("could not find any pages.")
-
-
-def process_batch(*args, **kwargs) -> Batch:
-    """Execute a batch run, either from a given file_name or by giving the name and project as input.
-
-    Examples:
-        >>> process_batch(file_name | (name, project), **kwargs)
-
-    Args:
-        *args: file_name or name and project (both string)
-
-    Keyword Args:
-        backend (str): what backend to use when plotting ('bokeh' or 'matplotlib').
-            Defaults to 'matplotlib'.
-        dpi (int): resolution used when saving matplotlib plot(s). Defaults to 300 dpi.
-        default_log_level (str): What log-level to use for console output. Chose between
-            'CRITICAL', 'DEBUG', or 'INFO'. The default is 'CRITICAL' (i.e. usually no log output to console).
-
-    Returns:
-        ``cellpy.batch.Batch`` object
-    """
-    silent = kwargs.pop("silent", False)
-    backend = kwargs.pop("backend", None)
-
-    if backend is not None:
-        prms.Batch.backend = backend
-    else:
-        prms.Batch.backend = "matplotlib"
-
-    dpi = kwargs.pop("dpi", 300)
-
-    default_log_level = kwargs.pop("default_log_level", "CRITICAL")
-    if len(args) == 1:
-        file_name = args[0]
-    else:
-        file_name = kwargs.pop("file_name", None)
-    testing = kwargs.pop("testing", False)
-    log.setup_logging(
-        default_level=default_log_level, reset_big_log=True, testing=testing
-    )
-    logging.debug(f"creating Batch(kwargs: {kwargs})")
-
-    if file_name is not None:
-        kwargs.pop("db_reader", None)
-        b = Batch(*args, file_name=file_name, db_reader=None, **kwargs)
-        b.create_journal(file_name)
-    else:
-        b = Batch(*args, **kwargs)
-        b.create_journal()
-
-    steps = {
-        "paginate": (b.paginate,),
-        "update": (b.update,),
-        "combine": (b.combine_summaries,),
-        "plot": (b.plot_summaries,),
-        "save": (_pb_save_plot, b, dpi),
-    }
-
-    with tqdm(total=(100 * len(steps) + 20), leave=False, file=sys.stdout) as pbar:
-        pbar.update(10)
-        for description in steps:
-            func, *args = steps[description]
-            pbar.set_description(description)
-            pbar.update(10)
-            try:
-                func(*args)
-            except cellpy.exceptions.NullData as e:
-                if not silent:
-                    tqdm.write(f"\nEXCEPTION (NullData): {str(e)}")
-                    tqdm.write("...aborting")
-                    return
-                else:
-                    raise e
-
-        pbar.set_description(f"final")
-        pbar.update(10)
-
-    return b
-
-
-def _pb_save_plot(b, dpi):
-    name = b.experiment.journal.name
-    out_dir = pathlib.Path(b.experiment.journal.batch_dir)
-
-    for n, farm in enumerate(b.plotter.farms):
-        if len(b.plotter.farms) > 1:
-            file_name = f"summary_plot_{name}_{str(n + 1).zfill(3)}.png"
-        else:
-            file_name = f"summary_plot_{name}.png"
-        out = out_dir / file_name
-        logging.info(f"saving file {file_name} in\n{out}")
-        farm.savefig(out, dpi=dpi)
-    # and other stuff
-
-
-def iterate_batches(folder, extension=".json", glob_pattern=None, **kwargs):
-    """Iterate through all journals in given folder.
-
-    Args:
-        folder (str or pathlib.Path): folder containing the journal files.
-        extension (str): extension for the journal files (used when creating a default glob-pattern).
-        glob_pattern (str): optional glob pattern.
-        **kwargs: keyword arguments passed to ``batch.process_batch``.
-    """
-
-    folder = pathlib.Path(folder)
-    logging.info(f"Folder for batches to be iterated: {folder}")
-    if not folder.is_dir():
-        print(f"Could not find the folder ({folder})")
-        print("Aborting...")
-        logging.info("ABORTING - folder not found.")
-        return
-    print(" Iterating through the folder ".center(80, "="))
-    print(f"Folder name: {folder}")
-    if not glob_pattern:
-        glob_pattern = f"*{extension}"
-    print(f"Glob pattern: {glob_pattern}")
-    files = sorted(folder.glob(glob_pattern))
-    if not files:
-        print("No files found! Aborting...")
-        logging.info("ABORTING - no files detected.")
-        return
-    print("Found the following files:")
-    for n, file in enumerate(files):
-        logging.debug(f"file: {file}")
-        print(f"  {str(n).zfill(4)} - {file}")
-
-    print(" Processing ".center(80, "-"))
-    output = []
-    failed = []
-    with tqdm(files, file=sys.stdout) as pbar:
-        for n, file in enumerate(pbar):
-            output_str = f"[{str(n).zfill(4)}]"
-            pbar.set_description(output_str)
-            output_str += f"({file.name})"
-            logging.debug(f"processing file: {file.name}")
-            try:
-                process_batch(file, **kwargs)
-                output_str += " [OK]"
-                logging.debug(f"No errors detected.")
-            except Exception as e:
-                output_str += " [FAILED!]"
-                failed.append(str(file))
-                logging.debug("Error detected.")
-                logging.debug(e)
-
-            output.append(output_str)
-
-    print(" Result ".center(80, "-"))
-    print("\n".join(output))
-    if failed:
-        print("\nFailed:")
-        failed_txt = "\n".join(failed)
-        print(failed_txt)
-        logging.info(failed_txt)
-    print("\n...Finished ")
-
-
-def check_standard():
-    from pathlib import Path
-
-    # Use these when working on my work PC:
-    test_data_path = r"C:\Scripting\MyFiles\development_cellpy\testdata"
-    out_data_path = r"C:\Scripting\Processing\Test\out"
-
-    # Use these when working on my MacBook:
-    # test_data_path = "/Users/jepe/scripting/cellpy/testdata"
-    # out_data_path = "/Users/jepe/cellpy_data"
-
-    test_data_path = Path(test_data_path)
-    out_data_path = Path(out_data_path)
-
-    logging.info("---SETTING SOME PRMS---")
-    prms.Paths.db_filename = "cellpy_db.xlsx"
-    prms.Paths.cellpydatadir = test_data_path / "hdf5"
-    prms.Paths.outdatadir = out_data_path
-    prms.Paths.rawdatadir = test_data_path / "data"
-    prms.Paths.db_path = test_data_path / "db"
-    prms.Paths.filelogdir = test_data_path / "log"
-
-    project = "prebens_experiment"
-    name = "test"
-    batch_col = "b01"
-
-    logging.info("---INITIALISATION OF BATCH---")
-    b = init(name, project, batch_col=batch_col)
-    b.experiment.export_raw = True
-    b.experiment.export_cycles = True
-    logging.info("*creating info df*")
-    b.create_journal()
-    logging.info("*creating folder structure*")
-    b.paginate()
-    logging.info("*load and save*")
-    b.update()
-    logging.info("*make summaries*")
-    try:
-        b.combine_summaries()
-        summaries = b.experiment.memory_dumped
-    except cellpy.exeptions.NullData:
-        print("NO DATA")
-        return
-    # except cellpy.exceptions.NullData:
-    #     print("NOTHING")
-    #     return
-
-    logging.info("*plotting summaries*")
-    b.plot_summaries("tmp_bokeh_plot.html")
-
-    # logging.info("*using special features*")
-    # logging.info(" - select_ocv_points")
-    # analyzer = OCVRelaxationAnalyzer()
-    # analyzer.assign(b.experiment)
-    # analyzer.do()
-    # ocv_df_list = analyzer.farms[0]
-    # for df in ocv_df_list:
-    #     df_up = df.loc[df.type == "ocvrlx_up", :]
-    #     df_down = df.loc[df.type == "ocvrlx_down", :]
-    #     logging.info(df_up)
-    logging.info("---FINISHED---")
-
-
-def check_new():
-    use_db = False
-    f = r"C:\scripts\processing_cellpy\out\SecondLife\cellpy_batch_embla_002.json"
-    # f = r"C:\Scripting\Processing\Celldata\outdata\SilcRoad\cellpy_batch_uio66.json"
-    # f = r"C:\Scripting\Processing\Celldata\outdata\MoZEES\cellpy_batch_round_robin_001.json"
-    name = "embla_test"
-    project = "cellpy_test"
-    batch_col = "b02"
-    if use_db:
-        process_batch(name, project, batch_col=batch_col, nom_cap=372)
-    else:
-        # process_batch(f, nom_cap=372)
-        process_batch(f, force_raw_file=False, force_cellpy=True, nom_cap=372)
-
-
-# TODO: allow exporting html when processing batch instead of just png
-
-
-def check_iterate():
-    folder_name = r"C:\Scripting\Processing\Celldata\live"
-    iterate_batches(folder_name, export_cycles=False, export_raw=False)
-
-
-if __name__ == "__main__":
-    print("---IN BATCH 2 MAIN---")
-    check_new()
+"""Routines for batch processing of cells (v2)."""
+
+import logging
+import os
+import pathlib
+import shutil
+import sys
+import warnings
+
+import pandas as pd
+from pandas import Index
+from tqdm.auto import tqdm
+
+import cellpy.exceptions
+from cellpy import log, prms
+from cellpy.parameters.internal_settings import (
+    headers_journal,
+    headers_step_table,
+    headers_summary,
+)
+from cellpy.internals.core import OtherPath
+from cellpy.utils.batch_tools.batch_analyzers import (
+    BaseSummaryAnalyzer,
+    OCVRelaxationAnalyzer,
+)
+from cellpy.utils.batch_tools.batch_core import Data
+from cellpy.utils.batch_tools.batch_experiments import CyclingExperiment
+from cellpy.utils.batch_tools.batch_exporters import CSVExporter
+from cellpy.utils.batch_tools.batch_journals import LabJournal
+from cellpy.utils.batch_tools.batch_plotters import CyclingSummaryPlotter
+from cellpy.utils.batch_tools.dumpers import ram_dumper
+
+# logger = logging.getLogger(__name__)
+logging.captureWarnings(True)
+
+COLUMNS_SELECTED_FOR_VIEW = [
+    headers_journal.mass,
+    headers_journal.total_mass,
+    headers_journal.loading,
+    headers_journal.nom_cap,
+]
+
+
+class Batch:
+    """A convenient class for running batch procedures.
+
+    The Batch class contains among other things:
+        - iterator protocol
+        - a journal with info about the different cells where the
+        main information is accessible as a pandas.DataFrame through the `.pages` attribute
+        - a data lookup accessor `.data` that behaves similarly as a dict.
+    """
+
+    def __init__(self, *args, **kwargs):
+        """Initialize the Batch class.
+
+        The initialization accepts arbitrary arguments and keyword arguments.
+        It first looks for the file_name and db_reader keyword arguments.
+
+        Usage:
+            b = Batch((name, (project)), **kwargs)
+
+        Examples:
+            >>> b = Batch("experiment001", "main_project")
+            >>> b = Batch("experiment001", "main_project", batch_col="b02")
+            >>> b = Batch(name="experiment001", project="main_project", batch_col="b02")
+            >>> b = Batch(file_name="cellpydata/batchfiles/cellpy_batch_experiment001.json")
+
+        Keyword Args (priority):
+            file_name (str or pathlib.Path): journal file name to load.
+            db_reader (str): data-base reader to use (defaults to "default" as given
+                in the config-file or prm-class).
+            frame (pandas.DataFrame): load from given dataframe.
+        Args:
+            *args: name (str) (project (str))
+
+        Keyword Args (other):
+            default_log_level (str): custom log-level (defaults to None (i.e. default log-level in cellpy)).
+            custom_log_dir (str or pathlib.Path): custom folder for putting the log-files.
+            force_raw_file (bool): load from raw regardless (defaults to False).
+            force_cellpy (bool): load cellpy-files regardless (defaults to False).
+            force_recalc (bool): Always recalculate (defaults to False).
+            export_cycles (bool): Extract and export individual cycles to csv (defaults to True).
+            export_raw (bool): Extract and export raw-data to csv (defaults to True).
+            export_ica (bool): Extract and export individual dQ/dV data to csv (defaults to True).
+            accept_errors (bool): Continue automatically to next file if error is raised (defaults to False).
+            nom_cap (float): give a nominal capacity if you want to use another value than
+                the one given in the config-file or prm-class.
+        """
+        # TODO: add option for setting max cycle number
+        #   use self.experiment.last_cycle = xxx
+        default_log_level = kwargs.pop("default_log_level", None)
+        custom_log_dir = kwargs.pop("custom_log_dir", None)
+        if default_log_level is not None or custom_log_dir is not None:
+            log.setup_logging(
+                custom_log_dir=custom_log_dir,
+                default_level=default_log_level,
+                reset_big_log=True,
+            )
+
+        db_reader = kwargs.pop("db_reader", "default")
+
+        file_name = kwargs.pop("file_name", None)
+        frame = kwargs.pop("frame", None)
+
+        logging.debug("creating CyclingExperiment")
+        self.experiment = CyclingExperiment(db_reader=db_reader)
+        logging.info("created CyclingExperiment")
+
+        self.experiment.force_cellpy = kwargs.pop("force_cellpy", False)
+        self.experiment.force_raw = kwargs.pop("force_raw_file", False)
+        self.experiment.force_recalc = kwargs.pop("force_recalc", False)
+        self.experiment.export_cycles = kwargs.pop("export_cycles", True)
+        self.experiment.export_raw = kwargs.pop("export_raw", True)
+        self.experiment.export_ica = kwargs.pop("export_ica", False)
+        self.experiment.accept_errors = kwargs.pop("accept_errors", False)
+        self.experiment.nom_cap = kwargs.pop("nom_cap", None)
+
+        if not file_name:
+            if frame is not None:
+                self.experiment.journal.from_frame(frame, **kwargs)
+            else:
+                if len(args) > 0:
+                    self.experiment.journal.name = args[0]
+
+                if len(args) > 1:
+                    self.experiment.journal.project = args[1]
+
+                for key in kwargs:
+                    if key == "name":
+                        self.experiment.journal.name = kwargs[key]
+                    elif key == "project":
+                        self.experiment.journal.project = kwargs[key]
+                    elif key == "batch_col":
+                        self.experiment.journal.batch_col = kwargs[key]
+        else:
+            self.experiment.journal.from_file(file_name=file_name, **kwargs)
+
+        self.exporter = CSVExporter()
+        self.exporter._assign_dumper(ram_dumper)
+        self.exporter.assign(self.experiment)
+
+        self.summary_collector = BaseSummaryAnalyzer()
+        self.summary_collector.assign(self.experiment)
+
+        self.plotter = CyclingSummaryPlotter()
+        self.plotter.assign(self.experiment)
+        self._journal_name = self.journal_name
+        self.headers_step_table = headers_step_table
+
+    def __str__(self):
+        return str(self.experiment)
+
+    def _repr_html_(self):
+        txt = f"<h2>Batch-object</h2> id={hex(id(self))}"
+        txt += f"<h3>batch.journal</h3>"
+        txt += f"<blockquote>{self.journal._repr_html_()}</blockquote>"
+        txt += f"<h3>batch.experiment</h3>"
+        txt += f"<blockquote>{self.experiment._repr_html_()}</blockquote>"
+
+        return txt
+
+    def __len__(self):
+        return len(self.experiment)
+
+    def __iter__(self):
+        return self.experiment.__iter__()
+
+    def show_pages(self, number_of_rows=5):
+        warnings.warn("Deprecated - use pages.head() instead", DeprecationWarning)
+        return self.experiment.journal.pages.head(number_of_rows)
+
+    @property
+    def view(self):
+        warnings.warn("Deprecated - use report instead", DeprecationWarning)
+        pages = self.experiment.journal.pages
+        pages = pages[COLUMNS_SELECTED_FOR_VIEW]
+        return pages
+
+    @property
+    def name(self):
+        return self.experiment.journal.name
+
+    def _check_cell_raw(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            return len(c.data.raw)
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_steps(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            return len(c.data.steps)
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_summary(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            return len(c.data.summary)
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_max_cap(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].max()
+
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_min_cap(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].min()
+
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_avg_cap(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].mean()
+
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_std_cap(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].std()
+
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_empty(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            return c.empty
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def _check_cell_cycles(self, cell_id):
+        try:
+            c = self.experiment.data[cell_id]
+            return c.data.steps[self.headers_step_table.cycle].max()
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
+            return None
+
+    def drop(self, cell_label=None):
+        """Drop cells from the journal.
+
+        If ``cell_label`` is not given, ``cellpy`` will look into the journal for session
+        info about bad cells, and if it finds it, it will remove those from the
+        journal.
+
+        Note! remember to save your journal again after modifying it.
+
+        Note! this method has not been properly tested yet.
+
+        Args:
+            cell_label (str): the cell label of the cell you would like to remove.
+
+        Returns:
+            ``cellpy.utils.batch`` object (returns a copy if `keep_old` is ``True``).
+
+        """
+        if cell_label is None:
+            try:
+                cell_labels = self.journal.session["bad_cells"]
+            except AttributeError:
+                logging.critical(
+                    "session info about bad cells is missing - cannot drop"
+                )
+                return
+        else:
+            cell_labels = [cell_label]
+
+        for cell_label in cell_labels:
+            if cell_label not in self.pages.index:
+                logging.critical(f"could not find {cell_label}")
+            else:
+                self.pages = self.pages.drop(cell_label)
+
+    def report(self, stylize=True):
+        """Create a report on all the cells in the batch object.
+
+        Remark! To perform a reporting, cellpy needs to access all the data (and it might take some time).
+
+        Returns:
+            ``pandas.DataFrame``
+        """
+        pages = self.experiment.journal.pages
+        pages = pages[COLUMNS_SELECTED_FOR_VIEW].copy()
+        # pages["empty"] = pages.index.map(self._check_cell_empty)
+        pages["raw_rows"] = pages.index.map(self._check_cell_raw)
+        pages["steps_rows"] = pages.index.map(self._check_cell_steps)
+        pages["summary_rows"] = pages.index.map(self._check_cell_summary)
+        pages["last_cycle"] = pages.index.map(self._check_cell_cycles)
+        pages["average_capacity"] = pages.index.map(self._check_cell_avg_cap)
+        pages["max_capacity"] = pages.index.map(self._check_cell_max_cap)
+        pages["min_capacity"] = pages.index.map(self._check_cell_min_cap)
+        pages["std_capacity"] = pages.index.map(self._check_cell_std_cap)
+
+        avg_last_cycle = pages.last_cycle.mean()
+        avg_max_capacity = pages.max_capacity.mean()
+
+        if stylize:
+
+            def highlight_outlier(s):
+                average = s.mean()
+                outlier = (s < average / 2) | (s > 2 * average)
+                return ["background-color: #f09223" if v else "" for v in outlier]
+
+            def highlight_small(s):
+                average = s.mean()
+                outlier = s < average / 4
+                return ["background-color: #41A1D8" if v else "" for v in outlier]
+
+            def highlight_very_small(s):
+                outlier = s <= 3
+                return ["background-color: #416CD8" if v else "" for v in outlier]
+
+            def highlight_big(s):
+                average = s.mean()
+                outlier = s > 2 * average
+                return ["background-color: #D85F41" if v else "" for v in outlier]
+
+            styled_pages = (
+                pages.style.apply(highlight_small, subset=["last_cycle"])
+                .apply(
+                    highlight_outlier,
+                    subset=["min_capacity", "max_capacity", "average_capacity"],
+                )
+                .apply(
+                    highlight_big,
+                    subset=["min_capacity", "max_capacity", "average_capacity"],
+                )
+                .apply(
+                    highlight_very_small,
+                    subset=["max_capacity", "average_capacity", "last_cycle"],
+                )
+                # .format({'min_capacity': "{:.2f}",
+                #        'max_capacity': "{:.2f}",
+                #        'average_capacity': "{:.2f}",
+                #        'std_capacity': '{:.2f}'})
+            )
+            pages = styled_pages
+
+        return pages
+
+    @property
+    def info_file(self):
+        # renamed to journal_name
+        warnings.warn("Deprecated - use journal_name instead", DeprecationWarning)
+        return self.experiment.journal.file_name
+
+    @property
+    def journal_name(self):
+        return self.experiment.journal.file_name
+
+    def _concat_memory_dumped(self, engine_name):
+        keys = [df.name for df in self.experiment.memory_dumped[engine_name]]
+        return pd.concat(self.experiment.memory_dumped[engine_name], keys=keys, axis=1)
+
+    @property
+    def summaries(self):
+        """Concatenated summaries from all cells (multiindex dataframe)."""
+        try:
+            return self._concat_memory_dumped("summary_engine")
+        except KeyError:
+            logging.critical("no summaries exists (dumping to ram first)")
+            self.summary_collector.do()
+            return self._concat_memory_dumped("summary_engine")
+
+    @property
+    def summary_headers(self):
+        """The column names of the concatenated summaries"""
+        try:
+            return self.summaries.columns.get_level_values(0)
+        except AttributeError:
+            logging.info("can't get any columns")
+
+    @property
+    def cell_names(self) -> list:
+        return self.experiment.cell_names
+
+    @property
+    def labels(self):
+        # Plan: allow cells to both have a label and a cell_name, where that latter should be a unique
+        # identifier. Consider also to allow for a group-name.
+        # The label and cell name can be the same. Consider allowing several cells to share the same label
+        # thus returning several cellpy cell objects. Our use "group" for this purpose.
+        print(
+            "Label-based look-up is not supported yet. Performing cell-name based look-up instead."
+        )
+        return self.experiment.cell_names
+
+    @property
+    def cells(self) -> Data:
+        """Access cells as a Data object (attribute lookup and automatic loading)
+
+        Usage (at least in jupyter notebook):
+            Write `b.cells.x` and press <TAB>. Then a pop-up will appear, and you can choose the
+            cell you would like to retrieve.
+        """
+        return self.experiment.data
+
+    @property
+    def cell_summary_headers(self) -> Index:
+        return self.experiment.data[self.experiment.cell_names[0]].data.summary.columns
+
+    @property
+    def cell_raw_headers(self) -> Index:
+        return self.experiment.data[self.experiment.cell_names[0]].data.raw.columns
+
+    @property
+    def cell_step_headers(self) -> Index:
+        return self.experiment.data[self.experiment.cell_names[0]].data.steps.columns
+
+    @property
+    def pages(self) -> pd.DataFrame:
+        return self.experiment.journal.pages
+
+    @pages.setter
+    def pages(self, df: pd.DataFrame):
+        self.experiment.journal.pages = df
+        all_cell_labels = set(self.experiment.cell_data_frames.keys())
+        cell_labels_to_keep = set(self.journal.pages.index)
+        cell_labels_to_remove = all_cell_labels - cell_labels_to_keep
+        for cell_label in cell_labels_to_remove:
+            del self.experiment.cell_data_frames[cell_label]
+
+    @property
+    def journal(self) -> LabJournal:
+        return self.experiment.journal
+
+    @journal.setter
+    def journal(self, new):
+        # self.experiment.journal = new
+        raise NotImplementedError(
+            "Setting a new journal object directly on a "
+            "batch object is not allowed at the moment. Try modifying "
+            "the journal.pages instead."
+        )
+
+    def old_duplicate_journal(self, folder=None) -> None:
+        """Copy the journal to folder.
+
+        Args:
+            folder (str or pathlib.Path): folder to copy to (defaults to the
+            current folder).
+        """
+
+        logging.debug(f"duplicating journal to folder {folder}")
+        journal_name = pathlib.Path(self.experiment.journal.file_name)
+        if not journal_name.is_file():
+            logging.info("No journal saved")
+            return
+        new_journal_name = journal_name.name
+        if folder is not None:
+            new_journal_name = pathlib.Path(folder) / new_journal_name
+        try:
+            shutil.copy(journal_name, new_journal_name)
+        except shutil.SameFileError:
+            logging.debug("same file exception encountered")
+
+    def duplicate_journal(self, folder=None) -> None:
+        """Copy the journal to folder.
+
+        Args:
+            folder (str or pathlib.Path): folder to copy to (defaults to the
+            current folder).
+        """
+        self.experiment.journal.duplicate_journal(folder)
+        #
+        # logging.debug(f"duplicating journal to folder {folder}")
+        # journal_name = pathlib.Path(self.experiment.journal.file_name)
+        # if not journal_name.is_file():
+        #     logging.info("No journal saved")
+        #     return
+        # new_journal_name = journal_name.name
+        # if folder is not None:
+        #     new_journal_name = pathlib.Path(folder) / new_journal_name
+        # try:
+        #     shutil.copy(journal_name, new_journal_name)
+        # except shutil.SameFileError:
+        #     logging.debug("same file exception encountered")
+
+    def create_journal(self, description=None, from_db=True, **kwargs):
+        """Create journal pages.
+
+            This method is a wrapper for the different Journal methods for making
+            journal pages (Batch.experiment.journal.xxx). It is under development. If you
+            want to use 'advanced' options (i.e. not loading from a db), please consider
+            using the methods available in Journal for now.
+
+            Args:
+                description: the information and meta-data needed to generate the journal pages ('empty': create an
+                    empty journal; ``dict``: create journal pages from a dictionary; ``pd.DataFrame``: create journal pages
+                    from a ``pandas.DataFrame``: 'filename.json': load cellpy batch file; 'filename.xlsx': create journal
+                    pages from an Excel file).
+                from_db (bool): Deprecation Warning: this parameter will be removed as it is
+                    the default anyway. Generate the pages from a db (the default option).
+                    This will be over-ridden if description is given.
+
+                **kwargs: sent to sub-function(s) (*e.g.* from_db -> simple_db_reader -> find_files ->
+                    filefinder.search_for_files).
+
+            kwargs -> from_db:
+                project=None, name=None, batch_col=None
+
+
+            kwargs -> simple_db_reader:
+                reader: a reader object (defaults to dbreader.Reader)
+                cell_ids: keys (cell IDs)
+                file_list: file list to send to filefinder (instead of searching in folders for files).
+                pre_path: prepended path to send to filefinder.
+                include_key: include the key col in the pages (the cell IDs).
+                include_individual_arguments: include the argument column in the pages.
+                additional_column_names: list of additional column names to include in the pages.
+
+            kwargs -> filefinder.search_for_files:
+                run_name(str): run-file identification.
+                raw_extension(str): optional, extension of run-files (without the '.').
+                cellpy_file_extension(str): optional, extension for cellpy files
+                    (without the '.').
+                raw_file_dir(path): optional, directory where to look for run-files
+                    (default: read prm-file)
+                cellpy_file_dir(path): optional, directory where to look for
+                    cellpy-files (default: read prm-file)
+                prm_filename(path): optional parameter file can be given.
+                file_name_format(str): format of raw-file names or a glob pattern
+                    (default: YYYYMMDD_[name]EEE_CC_TT_RR).
+                reg_exp(str): use regular expression instead (defaults to None).
+                sub_folders (bool): perform search also in sub-folders.
+                file_list (list of str): perform the search within a given list
+                    of filenames instead of searching the folder(s). The list should
+                    not contain the full filepath (only the actual file names). If
+                    you want to provide the full path, you will have to modify the
+                    file_name_format or reg_exp accordingly.
+                pre_path (path or str): path to prepend the list of files selected
+                     from the file_list.
+            kwargs -> journal.to_file:
+                duplicate_to_local_folder (bool): default True.
+
+
+        Returns:
+            info_dict
+
+        """
+
+        # TODO (jepe): create option to update journal without looking for files
+
+        logging.debug("Creating a journal")
+        logging.debug(f"description: {description}")
+        logging.debug(f"from_db: {from_db}")
+        logging.info(f"name: {self.experiment.journal.name}")
+        logging.info(f"project: {self.experiment.journal.project}")
+        to_project_folder = kwargs.pop("to_project_folder", True)
+        duplicate_to_local_folder = kwargs.pop("duplicate_to_local_folder", True)
+
+        if description is not None:
+            from_db = False
+        else:
+            if self.experiment.journal.pages is not None:
+                warnings.warn(
+                    "You created a journal - but you already have a "
+                    "journal. Hope you know what you are doing!"
+                )
+
+        if from_db:
+            self.experiment.journal.from_db(**kwargs)
+            self.experiment.journal.to_file(
+                duplicate_to_local_folder=duplicate_to_local_folder
+            )
+
+            # TODO: remove these:
+            if duplicate_to_local_folder:
+                self.experiment.journal.duplicate_journal()
+            if to_project_folder:
+                self.duplicate_journal(prms.Paths.batchfiledir)
+
+        else:
+            is_str = isinstance(description, str)
+            is_file = False
+
+            if is_str and pathlib.Path(description).is_file():
+                description = pathlib.Path(description)
+                is_file = True
+
+            if isinstance(description, pathlib.Path):
+                logging.debug("pathlib.Path object given")
+                is_file = True
+
+            if is_file:
+                logging.info(f"loading file {description}")
+                if description.suffix in [".json", ".xlsx"]:
+                    self.experiment.journal.from_file(description)
+                else:
+                    warnings.warn("unknown file extension")
+
+            else:
+                if is_str and description.lower() == "empty":
+                    logging.debug("creating empty journal pages")
+
+                    self.experiment.journal.pages = (
+                        self.experiment.journal.create_empty_pages()
+                    )
+
+                elif isinstance(description, pd.DataFrame):
+                    logging.debug("pandas DataFrame given")
+
+                    p = self.experiment.journal.create_empty_pages()
+                    columns = p.columns
+
+                    for column in columns:
+                        try:
+                            p[column] = description[column]
+                        except KeyError:
+                            logging.debug(f"missing key: {column}")
+
+                    # checking if filenames is a column
+                    if "filenames" in description.columns:
+                        indexes = description["filenames"]
+                    else:
+                        indexes = description.index
+
+                    p.index = indexes
+                    self.experiment.journal.pages = p
+
+                elif isinstance(description, dict):
+                    logging.debug("dictionary given")
+                    self.experiment.journal.pages = (
+                        self.experiment.journal.create_empty_pages()
+                    )
+                    for k in self.experiment.journal.pages.columns:
+                        try:
+                            value = description[k]
+                        except KeyError:
+                            warnings.warn(f"missing key: {k}")
+                        else:
+                            if not isinstance(value, list):
+                                warnings.warn("encountered item that is not a list")
+                                logging.debug(f"converting '{k}' to list-type")
+                                value = [value]
+                            if k == "raw_file_names":
+                                if not isinstance(value[0], list):
+                                    warnings.warn(
+                                        "encountered raw file description"
+                                        "that is not of list-type"
+                                    )
+                                    logging.debug(
+                                        "converting raw file description to a"
+                                        "list of lists"
+                                    )
+                                    value = [value]
+                            self.experiment.journal.pages[k] = value
+
+                    try:
+                        value = description["filenames"]
+                        if not isinstance(value, list):
+                            warnings.warn("encountered item that is not a list")
+                            logging.debug(f"converting '{k}' to list-type")
+                            value = [value]
+                        self.experiment.journal.pages.index = value
+                    except KeyError:
+                        logging.debug("could not interpret the index")
+
+                else:
+                    logging.debug(
+                        "the option you provided seems to be either of "
+                        "an unknown type or a file not found"
+                    )
+                    logging.info(
+                        "did not understand the option - creating empty journal pages"
+                    )
+
+            # finally
+            self.experiment.journal.to_file(
+                duplicate_to_local_folder=duplicate_to_local_folder
+            )
+            self.experiment.journal.generate_folder_names()
+            self.experiment.journal.paginate()
+            self.duplicate_journal(prms.Paths.batchfiledir)
+
+    def create_folder_structure(self) -> None:
+        warnings.warn("Deprecated - use paginate instead.", DeprecationWarning)
+        self.experiment.journal.paginate()
+        logging.info("created folders")
+
+    def paginate(self) -> None:
+        """Create the folders where cellpy will put its output."""
+
+        self.experiment.journal.paginate()
+        logging.info("created folders")
+
+    def save_journal(self) -> None:
+        """Save the journal (json-format).
+
+        The journal file will be saved in the project directory and in the
+        batch-file-directory (prms.Paths.batchfiledir). The latter is useful
+        for processing several batches using the iterate_batches functionality.
+        """
+
+        # Remark! Got an recursive error when running on mac.
+        self.experiment.journal.to_file(to_project_folder=True, paginate=False)
+        logging.info("saved journal pages to project folder")
+        self.duplicate_journal(prms.Paths.batchfiledir)
+        logging.info("duplicated journal pages to batch dir")
+        self.duplicate_journal()
+        logging.info("duplicated journal pages to current dir")
+
+    def export_journal(self, filename=None) -> None:
+        """Export the journal to xlsx."""
+        if filename is None:
+            filename = self.experiment.journal.file_name
+        filename = pathlib.Path(filename).with_suffix(".xlsx")
+        self.experiment.journal.to_file(
+            file_name=filename, to_project_folder=False, paginate=False
+        )
+
+    def duplicate_cellpy_files(
+        self, location: str = "standard", selector: dict = None, **kwargs
+    ) -> None:
+        """Copy the cellpy files and make a journal with the new names available in
+        the current folder.
+
+        Args:
+            location: where to copy the files. Either choose among the following
+                options:
+                "standard": data/interim folder
+                "here": current directory
+                "cellpydatadir": the stated cellpy data dir in your settings (prms)
+            or if the location is not one of the above, use the actual value of the
+                location argument.
+            selector (dict): if given, the cellpy files are reloaded after duplicating and
+                modified based on the given selector(s).
+
+            kwargs: sent to update if selector is provided
+
+        Returns:
+            The updated journal pages.
+        """
+
+        pages = self.experiment.journal.pages
+        cellpy_file_dir = OtherPath(prms.Paths.cellpydatadir)
+
+        if location == "standard":
+            batch_data_dir = pathlib.Path("data") / "interim"
+
+        elif location == "here":
+            batch_data_dir = pathlib.Path(".")
+
+        elif location == "cellpydatadir":
+            batch_data_dir = cellpy_file_dir
+
+        else:
+            batch_data_dir = location
+
+        def _new_file_path(x):
+            return str(batch_data_dir / pathlib.Path(x).name)
+
+        # update the journal pages
+        columns = pages.columns
+        pages["new_cellpy_file_name"] = pages.cellpy_file_name.apply(_new_file_path)
+
+        # copy the cellpy files
+        for n, row in pages.iterrows():
+            logging.info(f"{row.cellpy_file_name} -> {row.new_cellpy_file_name}")
+            try:
+                from_file = row.cellpy_file_name
+                to_file = row.new_cellpy_file_name
+                os.makedirs(os.path.dirname(to_file), exist_ok=True)
+                shutil.copy(from_file, to_file)
+            except shutil.SameFileError:
+                logging.info("Same file! No point in copying")
+            except FileNotFoundError:
+                logging.info("File not found! Cannot copy it!")
+
+        # save the journal pages
+        pages["cellpy_file_name"] = pages["new_cellpy_file_name"]
+        self.experiment.journal.pages = pages[columns]
+        journal_file_name = pathlib.Path(self.experiment.journal.file_name).name
+        self.experiment.journal.to_file(
+            journal_file_name, paginate=False, to_project_folder=False
+        )
+        if selector is not None:
+            logging.info("Modifying the cellpy-files.")
+            logging.info(f"selector: {selector}")
+            self.experiment.force_cellpy = True
+            self.update(selector=selector, **kwargs)
+
+    # TODO: list_journals?
+
+    def link(self, max_cycle=None, force_combine_summaries=False) -> None:
+        """Link journal content to the cellpy-files and load the step information.
+
+        Args:
+            max_cycle (int): set maximum cycle number to link to.
+            force_combine_summaries (bool): automatically run combine_summaries (set this to True
+                if you are re-linking without max_cycle for a batch that previously were linked
+                with max_cycle)
+
+        """
+
+        self.experiment.link(max_cycle=max_cycle)
+        if force_combine_summaries or max_cycle:
+            self.summary_collector.do(reset=True)
+
+    def load(self) -> None:
+        # does the same as update
+        warnings.warn("Deprecated - use update instead.", DeprecationWarning)
+        self.experiment.update()
+
+    def update(self, pool=False, **kwargs) -> None:
+        """Updates the selected datasets.
+
+        Keyword Args (to experiment-instance):
+            all_in_memory (bool): store the `cellpydata` in memory (default
+                False)
+            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
+                dictionary will override the **kwargs and the parameters from the journal pages
+                for the indicated cell.
+            logging_mode (str): sets the logging mode for the loader(s).
+            accept_errors (bool): if True, the loader will continue even if it encounters errors.
+
+
+        Additional kwargs:
+            transferred all the way to the instrument loader, if not
+            picked up earlier. Remark that you can obtain the same pr. cell by
+            providing a `cellspecs` dictionary. The kwargs have precedence over the
+            parameters given in the journal pages, but will be overridden by parameters
+            given by `cellspecs`.
+
+            Merging:
+                recalc (Bool): set to False if you don't want automatic "recalc" of
+                    cycle numbers etc. when merging several data-sets.
+            Loading:
+                selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
+                loading from raw is not necessary (or turned off).
+
+        """
+        self.experiment.errors["update"] = []
+        if pool:
+            self.experiment.parallel_update(**kwargs)
+        else:
+            self.experiment.update(**kwargs)
+
+    def export_cellpy_files(self, path=None, **kwargs) -> None:
+        if path is None:
+            path = pathlib.Path(".").resolve()
+        self.experiment.errors["export_cellpy_files"] = []
+        self.experiment.export_cellpy_files(path=path, **kwargs)
+
+    def recalc(self, **kwargs) -> None:
+        """Run make_step_table and make_summary on all cells.
+
+        Keyword Args:
+            save (bool): Save updated cellpy-files if True (defaults to True).
+            step_opts (dict): parameters to inject to make_steps (defaults to None).
+            summary_opts (dict): parameters to inject to make_summary (defaults to None).
+            indexes (list): Only recalculate for given indexes (i.e. list of cell-names) (defaults to None).
+            calc_steps (bool): Run make_steps before making the summary (defaults to True).
+            testing (bool): Only for testing purposes (defaults to False).
+
+        Returns:
+            None
+        """
+        self.experiment.errors["recalc"] = []
+        self.experiment.recalc(**kwargs)
+
+    def make_summaries(self) -> None:
+        warnings.warn("Deprecated - use combine_summaries instead.", DeprecationWarning)
+
+        self.exporter.do()
+
+    def combine_summaries(self, export_to_csv=True, **kwargs) -> None:
+        """Combine selected columns from each of the cells into single frames"""
+
+        if export_to_csv:
+            self.exporter.do()
+        else:
+            self.summary_collector.do(**kwargs)
+
+    def plot_summaries(
+        self, output_filename=None, backend=None, reload_data=False, **kwargs
+    ) -> None:
+        """Plot the summaries"""
+
+        if reload_data or ("summary_engine" not in self.experiment.memory_dumped):
+            logging.debug("running summary_collector")
+            self.summary_collector.do(reset=True)
+
+        if backend is None:
+            backend = prms.Batch.backend
+
+        if backend in ["bokeh", "matplotlib"]:
+            prms.Batch.backend = backend
+
+        if backend == "bokeh":
+            try:
+                import bokeh.plotting
+
+                prms.Batch.backend = "bokeh"
+
+                if output_filename is not None:
+                    bokeh.plotting.output_file(output_filename)
+                else:
+                    if prms.Batch.notebook:
+                        bokeh.plotting.output_notebook()
+
+            except ModuleNotFoundError:
+                prms.Batch.backend = "matplotlib"
+                logging.warning(
+                    "could not find the bokeh module -> using matplotlib instead"
+                )
+
+        self.plotter.do(**kwargs)
+
+
+def init(*args, **kwargs) -> Batch:
+    """Returns an initialized instance of the Batch class.
+
+    Args:
+        *args: passed directly to Batch()
+            **name**: name of batch;
+            **project**: name of project;
+            **batch_col**: batch column identifier.
+        **kwargs:
+            **file_name**: json file if loading from pages;
+            **default_log_level**: "INFO" or "DEBUG";
+            the rest is passed directly to Batch().
+
+    Examples:
+        >>> empty_batch = Batch.init(db_reader=None)
+        >>> batch_from_file = Batch.init(file_name="cellpy_batch_my_experiment.json")
+        >>> normal_init_of_batch = Batch.init()
+    """
+    # TODO: add option for setting max cycle number (experiment.last_cycle)
+    # set up cellpy logger
+    default_log_level = kwargs.pop("default_log_level", None)
+    testing = kwargs.pop("testing", False)
+    file_name = kwargs.pop("file_name", None)
+    frame = kwargs.pop("frame", None)
+    log.setup_logging(
+        default_level=default_log_level, testing=testing, reset_big_log=True
+    )
+
+    logging.debug(f"returning Batch(kwargs: {kwargs})")
+    if file_name is not None:
+        kwargs.pop("db_reader", None)
+        return Batch(*args, file_name=file_name, db_reader=None, **kwargs)
+    if frame is not None:
+        kwargs.pop("db_reader", None)
+        return Batch(*args, file_name=None, db_reader=None, frame=frame, **kwargs)
+
+    return Batch(*args, **kwargs)
+
+
+def from_journal(journal_file, autolink=True, testing=False) -> Batch:
+    """Create a Batch from a journal file"""
+    # TODO: add option for setting max cycle number (experiment.last_cycle)
+    b = init(db_reader=None, file_name=journal_file, testing=testing)
+    if autolink:
+        b.link()
+    return b
+
+
+def load_pages(file_name) -> pd.DataFrame:
+    """Retrieve pages from a Journal file.
+
+    This function is here to let you easily inspect a Journal file without
+    starting up the full batch-functionality.
+
+    Examples:
+        >>> from cellpy.utils import batch
+        >>> journal_file_name = 'cellpy_journal_one.json'
+        >>> pages = batch.load_pages(journal_file_name)
+
+    Returns:
+        pandas.DataFrame
+
+    """
+
+    logging.info(f"Loading pages from {file_name}")
+    try:
+        pages, *_ = LabJournal.read_journal_jason_file(file_name)
+        return pages
+    except cellpy.exceptions.UnderDefined:
+        logging.critical("could not find any pages.")
+
+
+def process_batch(*args, **kwargs) -> Batch:
+    """Execute a batch run, either from a given file_name or by giving the name and project as input.
+
+    Examples:
+        >>> process_batch(file_name | (name, project), **kwargs)
+
+    Args:
+        *args: file_name or name and project (both string)
+
+    Keyword Args:
+        backend (str): what backend to use when plotting ('bokeh' or 'matplotlib').
+            Defaults to 'matplotlib'.
+        dpi (int): resolution used when saving matplotlib plot(s). Defaults to 300 dpi.
+        default_log_level (str): What log-level to use for console output. Chose between
+            'CRITICAL', 'DEBUG', or 'INFO'. The default is 'CRITICAL' (i.e. usually no log output to console).
+
+    Returns:
+        ``cellpy.batch.Batch`` object
+    """
+    silent = kwargs.pop("silent", False)
+    backend = kwargs.pop("backend", None)
+
+    if backend is not None:
+        prms.Batch.backend = backend
+    else:
+        prms.Batch.backend = "matplotlib"
+
+    dpi = kwargs.pop("dpi", 300)
+
+    default_log_level = kwargs.pop("default_log_level", "CRITICAL")
+    if len(args) == 1:
+        file_name = args[0]
+    else:
+        file_name = kwargs.pop("file_name", None)
+    testing = kwargs.pop("testing", False)
+    log.setup_logging(
+        default_level=default_log_level, reset_big_log=True, testing=testing
+    )
+    logging.debug(f"creating Batch(kwargs: {kwargs})")
+
+    if file_name is not None:
+        kwargs.pop("db_reader", None)
+        b = Batch(*args, file_name=file_name, db_reader=None, **kwargs)
+        b.create_journal(file_name)
+    else:
+        b = Batch(*args, **kwargs)
+        b.create_journal()
+
+    steps = {
+        "paginate": (b.paginate,),
+        "update": (b.update,),
+        "combine": (b.combine_summaries,),
+        "plot": (b.plot_summaries,),
+        "save": (_pb_save_plot, b, dpi),
+    }
+
+    with tqdm(total=(100 * len(steps) + 20), leave=False, file=sys.stdout) as pbar:
+        pbar.update(10)
+        for description in steps:
+            func, *args = steps[description]
+            pbar.set_description(description)
+            pbar.update(10)
+            try:
+                func(*args)
+            except cellpy.exceptions.NullData as e:
+                if not silent:
+                    tqdm.write(f"\nEXCEPTION (NullData): {str(e)}")
+                    tqdm.write("...aborting")
+                    return
+                else:
+                    raise e
+
+        pbar.set_description(f"final")
+        pbar.update(10)
+
+    return b
+
+
+def _pb_save_plot(b, dpi):
+    name = b.experiment.journal.name
+    out_dir = pathlib.Path(b.experiment.journal.batch_dir)
+
+    for n, farm in enumerate(b.plotter.farms):
+        if len(b.plotter.farms) > 1:
+            file_name = f"summary_plot_{name}_{str(n + 1).zfill(3)}.png"
+        else:
+            file_name = f"summary_plot_{name}.png"
+        out = out_dir / file_name
+        logging.info(f"saving file {file_name} in\n{out}")
+        farm.savefig(out, dpi=dpi)
+    # and other stuff
+
+
+def iterate_batches(folder, extension=".json", glob_pattern=None, **kwargs):
+    """Iterate through all journals in given folder.
+
+    Args:
+        folder (str or pathlib.Path): folder containing the journal files.
+        extension (str): extension for the journal files (used when creating a default glob-pattern).
+        glob_pattern (str): optional glob pattern.
+        **kwargs: keyword arguments passed to ``batch.process_batch``.
+    """
+
+    folder = pathlib.Path(folder)
+    logging.info(f"Folder for batches to be iterated: {folder}")
+    if not folder.is_dir():
+        print(f"Could not find the folder ({folder})")
+        print("Aborting...")
+        logging.info("ABORTING - folder not found.")
+        return
+    print(" Iterating through the folder ".center(80, "="))
+    print(f"Folder name: {folder}")
+    if not glob_pattern:
+        glob_pattern = f"*{extension}"
+    print(f"Glob pattern: {glob_pattern}")
+    files = sorted(folder.glob(glob_pattern))
+    if not files:
+        print("No files found! Aborting...")
+        logging.info("ABORTING - no files detected.")
+        return
+    print("Found the following files:")
+    for n, file in enumerate(files):
+        logging.debug(f"file: {file}")
+        print(f"  {str(n).zfill(4)} - {file}")
+
+    print(" Processing ".center(80, "-"))
+    output = []
+    failed = []
+    with tqdm(files, file=sys.stdout) as pbar:
+        for n, file in enumerate(pbar):
+            output_str = f"[{str(n).zfill(4)}]"
+            pbar.set_description(output_str)
+            output_str += f"({file.name})"
+            logging.debug(f"processing file: {file.name}")
+            try:
+                process_batch(file, **kwargs)
+                output_str += " [OK]"
+                logging.debug(f"No errors detected.")
+            except Exception as e:
+                output_str += " [FAILED!]"
+                failed.append(str(file))
+                logging.debug("Error detected.")
+                logging.debug(e)
+
+            output.append(output_str)
+
+    print(" Result ".center(80, "-"))
+    print("\n".join(output))
+    if failed:
+        print("\nFailed:")
+        failed_txt = "\n".join(failed)
+        print(failed_txt)
+        logging.info(failed_txt)
+    print("\n...Finished ")
+
+
+def check_standard():
+    from pathlib import Path
+
+    # Use these when working on my work PC:
+    test_data_path = r"C:\Scripting\MyFiles\development_cellpy\testdata"
+    out_data_path = r"C:\Scripting\Processing\Test\out"
+
+    # Use these when working on my MacBook:
+    # test_data_path = "/Users/jepe/scripting/cellpy/testdata"
+    # out_data_path = "/Users/jepe/cellpy_data"
+
+    test_data_path = Path(test_data_path)
+    out_data_path = Path(out_data_path)
+
+    logging.info("---SETTING SOME PRMS---")
+    prms.Paths.db_filename = "cellpy_db.xlsx"
+    prms.Paths.cellpydatadir = test_data_path / "hdf5"
+    prms.Paths.outdatadir = out_data_path
+    prms.Paths.rawdatadir = test_data_path / "data"
+    prms.Paths.db_path = test_data_path / "db"
+    prms.Paths.filelogdir = test_data_path / "log"
+
+    project = "prebens_experiment"
+    name = "test"
+    batch_col = "b01"
+
+    logging.info("---INITIALISATION OF BATCH---")
+    b = init(name, project, batch_col=batch_col)
+    b.experiment.export_raw = True
+    b.experiment.export_cycles = True
+    logging.info("*creating info df*")
+    b.create_journal()
+    logging.info("*creating folder structure*")
+    b.paginate()
+    logging.info("*load and save*")
+    b.update()
+    logging.info("*make summaries*")
+    try:
+        b.combine_summaries()
+        summaries = b.experiment.memory_dumped
+    except cellpy.exeptions.NullData:
+        print("NO DATA")
+        return
+    # except cellpy.exceptions.NullData:
+    #     print("NOTHING")
+    #     return
+
+    logging.info("*plotting summaries*")
+    b.plot_summaries("tmp_bokeh_plot.html")
+
+    # logging.info("*using special features*")
+    # logging.info(" - select_ocv_points")
+    # analyzer = OCVRelaxationAnalyzer()
+    # analyzer.assign(b.experiment)
+    # analyzer.do()
+    # ocv_df_list = analyzer.farms[0]
+    # for df in ocv_df_list:
+    #     df_up = df.loc[df.type == "ocvrlx_up", :]
+    #     df_down = df.loc[df.type == "ocvrlx_down", :]
+    #     logging.info(df_up)
+    logging.info("---FINISHED---")
+
+
+def check_new():
+    use_db = False
+    f = r"C:\scripts\processing_cellpy\out\SecondLife\cellpy_batch_embla_002.json"
+    # f = r"C:\Scripting\Processing\Celldata\outdata\SilcRoad\cellpy_batch_uio66.json"
+    # f = r"C:\Scripting\Processing\Celldata\outdata\MoZEES\cellpy_batch_round_robin_001.json"
+    name = "embla_test"
+    project = "cellpy_test"
+    batch_col = "b02"
+    if use_db:
+        process_batch(name, project, batch_col=batch_col, nom_cap=372)
+    else:
+        # process_batch(f, nom_cap=372)
+        process_batch(f, force_raw_file=False, force_cellpy=True, nom_cap=372)
+
+
+# TODO: allow exporting html when processing batch instead of just png
+
+
+def check_iterate():
+    folder_name = r"C:\Scripting\Processing\Celldata\live"
+    iterate_batches(folder_name, export_cycles=False, export_raw=False)
+
+
+if __name__ == "__main__":
+    print("---IN BATCH 2 MAIN---")
+    check_new()
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_analyzers.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_analyzers.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_core.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_core.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_experiments.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_experiments.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,1002 +1,1002 @@
-import ast
-import logging
-import os
-import pathlib
-import sys
-import warnings
-
-import pandas as pd
-from tqdm.auto import tqdm
-
-import cellpy
-from cellpy import prms
-from cellpy.parameters.internal_settings import get_headers_journal, get_headers_summary
-from cellpy.readers import cellreader
-from cellpy.internals.core import OtherPath
-from cellpy.utils.batch_tools import batch_helpers as helper
-from cellpy.utils.batch_tools.batch_core import BaseExperiment
-from cellpy.utils.batch_tools.batch_journals import LabJournal
-
-hdr_journal = get_headers_journal()
-hdr_summary = get_headers_summary()
-
-
-class CyclingExperiment(BaseExperiment):
-    """Load experimental data into memory.
-
-    This is a re-implementation of the old batch behaviour where
-    all the data-files are processed sequentially (and optionally exported)
-    while the summary tables are kept and processed. This implementation
-    also saves the step tables (for later use when using look-up
-    functionality).
-
-
-    Attributes:
-        journal (:obj: LabJournal): information about the experiment.
-        force_cellpy (bool): tries only to load the cellpy-file if True.
-        force_raw (bool): loads raw-file(s) even though appropriate cellpy-file
-           exists if True.
-        save_cellpy (bool): saves a cellpy-file for each cell if True.
-        accept_errors (bool): in case of error, dont raise an exception, but
-           continue to the next file if True.
-        all_in_memory (bool): store the cellpydata-objects in memory if True.
-        export_cycles (bool): export voltage-capacity curves if True.
-        shifted_cycles (bool): set this to True if you want to export the
-           voltage-capacity curves using the shifted-cycles option (only valid
-           if you set export_cycles to True).
-        export_raw (bool): export the raw-data if True.
-        export_ica (bool): export dq-dv curves if True.
-        last_cycle (int): sets the last cycle (i.e. the highest cycle number)
-           that you would like to process dq-dv on). Use all if None (the
-           default value).
-        selected_summaries (list): a list of summary labels defining what
-           summary columns to make joint summaries from (optional).
-        errors (dict): contains a dictionary listing all the errors encountered.
-
-    Args:
-        db_reader (str or object): custom db_reader (see doc on db_reader).
-
-    Example:
-
-
-    """
-
-    def __init__(self, *args, **kwargs):
-        db_reader = kwargs.pop("db_reader", "default")
-        super().__init__(*args)
-        self.journal = LabJournal(db_reader=db_reader)
-        self.errors = dict()
-        self.log = dict()
-
-        self.force_cellpy = False
-        self.force_raw = False
-        self.force_recalc = False
-        self.save_cellpy = True
-        self.accept_errors = False
-        self.all_in_memory = False
-
-        self.export_cycles = False
-        self.shifted_cycles = False
-        self.export_raw = True
-        self.export_ica = False
-        self.last_cycle = None
-        self.nom_cap = None
-        self.instrument = None
-        self.custom_data_folder = None
-
-        self.selected_summaries = None
-
-    def _repr_html_(self):
-        txt = f"<h2>CyclingExperiment-object</h2> id={hex(id(self))}"
-        txt += "<h3>Main attributes</h3>"
-        txt += f"""
-        <table>
-            <thead>
-                <tr>
-                    <th>Attribute</th>
-                    <th>Value</th>
-                </tr>
-            </thead>
-            <tbody>
-                <tr><td><b>force_cellpy</b></td><td>{self.force_cellpy}</td></tr>
-                <tr><td><b>force_raw</b></td><td>{self.force_raw}</td></tr>
-                <tr><td><b>force_recalc</b></td><td>{self.force_recalc}</td></tr>
-                <tr><td><b>save_cellpy</b></td><td>{self.save_cellpy}</td></tr>
-                <tr><td><b>accept_errors</b></td><td>{self.accept_errors}</td></tr>
-                <tr><td><b>all_in_memory</b></td><td>{self.all_in_memory}</td></tr>
-                <tr><td><b>export_cycles</b></td><td>{self.export_cycles}</td></tr>
-                <tr><td><b>shifted_cycles</b></td><td>{self.shifted_cycles}</td></tr>
-                <tr><td><b>export_raw</b></td><td>{self.export_raw}</td></tr>
-                <tr><td><b>export_ica</b></td><td>{self.export_ica}</td></tr>
-                <tr><td><b>last_cycle</b></td><td>{self.last_cycle}</td></tr>
-                <tr><td><b>nom_cap</b></td><td>{self.nom_cap}</td></tr>
-                <tr><td><b>instrument</b></td><td>{self.instrument}</td></tr>
-                <tr><td><b>custom_data_folder</b></td><td>{self.custom_data_folder}</td></tr>
-                <tr><td><b>selected_summaries</b></td><td>{self.selected_summaries}</td></tr>
-            </tbody>
-        </table>
-        """
-        txt += "<h3>Cells</h3>"
-        txt += f"<p><b>data</b>: contains {len(self)} cells.</p>"
-        return txt
-
-    @staticmethod
-    def _get_cell_spec_from_page(indx: int, row: pd.Series) -> dict:
-        # Edit this if we decide to make "argument families", e.g. loader_split or merger_recalc.
-
-        PRM_SPLITTER = ";"
-        EQUAL_SIGN = "="
-
-        def _arg_parser(text: str) -> None:
-            individual_specs = text.split(PRM_SPLITTER)
-            for p in individual_specs:
-                p, a = p.split(EQUAL_SIGN)
-
-        logging.debug(f"getting cell_spec from journal pages ({indx})")
-        try:
-            cell_spec = row[hdr_journal.argument]
-            logging.debug(cell_spec)
-            if not isinstance(cell_spec, dict):
-                raise TypeError("the cell spec argument is not a dictionary")
-        except Exception as e:
-            logging.warning(f"could not get cell spec for {indx}")
-            logging.warning(f"error message: {e}")
-            return {}
-
-        # converting from str if needed
-        for spec in cell_spec:
-            if isinstance(cell_spec[spec], str):
-                if cell_spec[spec].lower() == "true":
-                    cell_spec[spec] = True
-                elif cell_spec[spec].lower() == "false":
-                    cell_spec[spec] = False
-                elif cell_spec[spec].lower() == "none":
-                    cell_spec[spec] = None
-                else:
-                    try:
-                        logging.debug(
-                            f"Using ast.literal_eval to convert cell-spec value from str '{cell_spec[spec]}'"
-                        )
-                        cell_spec[spec] = ast.literal_eval(cell_spec[spec])
-                    except ValueError as e:
-                        logging.warning(
-                            f"ERROR! Could not convert from str to python object!"
-                        )
-                        logging.debug(e)
-        return cell_spec
-
-    def update(
-        self,
-        all_in_memory=None,
-        cell_specs=None,
-        logging_mode=None,
-        accept_errors=None,
-        **kwargs,
-    ):
-        """Updates the selected datasets.
-
-        Args:
-            all_in_memory (bool): store the `cellpydata` in memory (default
-                False)
-            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
-                dictionary will override the **kwargs and the parameters from the journal pages
-                for the indicated cell.
-            logging_mode (str): sets the logging mode for the loader(s).
-            accept_errors (bool): if True, the loader will continue even if it encounters errors.
-
-            kwargs:
-                transferred all the way to the instrument loader, if not
-                picked up earlier. Remark that you can obtain the same pr. cell by
-                providing a `cellspecs` dictionary. The kwargs have precedence over the
-                parameters given in the journal pages, but will be overridden by parameters
-                given by `cellspecs`.
-
-                Merging:
-                    recalc (Bool): set to False if you don't want automatic "recalc" of
-                        cycle numbers etc. when merging several data-sets.
-                Loading:
-                    selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
-                    loading from raw is not necessary (or turned off).
-
-                Debugging:
-                    debug (Bool): set to True if you want to run in debug mode (should never be used by non-developers).
-
-        Debug-mode:
-                 - runs only for the first item in your journal
-
-        Examples:
-            >>> # Don't perform recalculation of cycle numbers etc. when merging
-            >>> # All cells:
-            >>> b.update(recalc=False)
-            >>> # For specific cell(s):
-            >>> cell_specs_cell_01 = {"name_of_cell_01": {"recalc": False}}
-            >>> b.update(cell_specs=cell_specs_cell_01)
-
-        """
-
-        # TODO: implement experiment.last_cycle
-        accept_errors = accept_errors or self.accept_errors
-
-        debugging = kwargs.pop("debug", False)
-        testing = kwargs.pop("testing", False)
-
-        # --- cleaning up attributes / arguments etc ---
-        force_cellpy = kwargs.pop("force_cellpy", self.force_cellpy)
-        force_raw = kwargs.pop("force_raw", self.force_raw)
-
-        logging.info("[update experiment]")
-        if all_in_memory is not None:
-            self.all_in_memory = all_in_memory
-
-        logging.info(f"Additional keyword arguments: {kwargs}")
-        selector = kwargs.get("selector", None)
-
-        pages = self.journal.pages
-        if self.nom_cap:
-            warnings.warn(
-                "Setting nominal capacity through attributes will be deprecated soon since it modifies "
-                "the journal pages."
-            )
-            pages[hdr_journal.nom_cap] = self.nom_cap
-
-        if self.instrument:
-            warnings.warn(
-                "Setting instrument through attributes will be deprecated soon since it modifies the journal pages."
-            )
-            pages[hdr_journal.instrument] = self.instrument
-
-        if x := kwargs.pop("instrument", None):
-            warnings.warn(
-                "Setting instrument through params will be deprecated soon since it modifies the journal pages."
-                "Future version will require instrument in the journal pages."
-            )
-            pages[hdr_journal.instrument] = x
-
-        if pages.empty:
-            raise Exception("your journal is empty")
-
-        # --- init ---
-        summary_frames = dict()
-        cell_data_frames = dict()
-        number_of_runs = len(pages)
-        logging.debug(f"You have {number_of_runs} cells in your journal")
-        counter = 0
-        errors = []
-
-        pbar = tqdm(list(pages.iterrows()), file=sys.stdout, leave=False)
-
-        if debugging:
-            pbar = tqdm(list(pages.iterrows())[0:1], file=sys.stdout, leave=False)
-
-        # --- iterating ---
-        # TODO: create a multiprocessing pool and get one statusbar pr cell
-        for index, row in pbar:
-            counter += 1
-            h_txt = f"{index}"
-            n_txt = f"loading {counter}"
-            l_txt = f"starting to process file # {counter} ({index})"
-            pbar.set_description(n_txt, refresh=True)
-            cell_spec_page = self._get_cell_spec_from_page(index, row)
-
-            if cell_specs is not None:
-                cell_spec = cell_specs.get(index, dict())
-            else:
-                cell_spec = dict()
-
-            cell_spec = {**cell_spec_page, **kwargs, **cell_spec}
-            l_txt += f" cell_spec: {cell_spec}"
-            logging.debug(l_txt)
-
-            # --- UPDATING ARGUMENTS ---
-            filename = None
-            instrument = None
-            cellpy_file = OtherPath(row[hdr_journal.cellpy_file_name])
-            _cellpy_file = None
-            if not force_raw and cellpy_file.is_file():
-                _cellpy_file = cellpy_file
-                logging.debug(f"Got cellpy file: {_cellpy_file}")
-            if not force_cellpy:
-                filename = row[hdr_journal.raw_file_names]
-                instrument = row[hdr_journal.instrument]
-
-            cycle_mode = row[hdr_journal.cell_type]
-            mass = row[hdr_journal.mass]
-            nom_cap = row[hdr_journal.nom_cap]
-
-            loading = None
-            area = None
-            if hdr_journal.loading in row:
-                loading = row[hdr_journal.loading]
-            if hdr_journal.area in row:
-                area = row[hdr_journal.area]
-
-            summary_kwargs = {
-                "use_cellpy_stat_file": prms.Reader.use_cellpy_stat_file,
-            }
-
-            step_kwargs = {}
-            if not filename and not force_cellpy:
-                logging.info(
-                    f"Raw file(s) not given in the journal.pages for index={index}"
-                )
-                errors.append(index)
-                continue
-
-            elif not cellpy_file and force_cellpy:
-                logging.info(
-                    f"Cellpy file not given in the journal.pages for index={index}"
-                )
-                errors.append(index)
-                continue
-
-            else:
-                logging.info(f"Processing {index}")
-
-            logging.info("loading cell")
-            try:
-                logging.debug("inside try: running cellpy.get")
-                cell_data = cellpy.get(
-                    filename=filename,
-                    instrument=instrument,
-                    cellpy_file=_cellpy_file,
-                    cycle_mode=cycle_mode,
-                    mass=mass,
-                    nom_cap=nom_cap,
-                    loading=loading,
-                    area=area,
-                    step_kwargs=step_kwargs,
-                    summary_kwargs=summary_kwargs,
-                    selector=selector,
-                    logging_mode=logging_mode,
-                    testing=testing,
-                    **cell_spec,
-                )
-                logging.info("loaded cell")
-
-            except Exception as e:
-                logging.info("Failed to load: " + str(e))
-                errors.append("update:" + str(index))
-                h_txt += " [-]"
-                pbar.set_postfix_str(s=h_txt, refresh=True)
-                if not accept_errors:
-                    raise e
-                continue
-
-            if cell_data.empty:
-                logging.info("...not loaded...")
-                logging.debug("Data is empty. Could not load cell!")
-                errors.append("check:" + str(index))
-                h_txt += " [-]"
-                pbar.set_postfix_str(s=h_txt, refresh=True)
-                continue
-
-            logging.info("...loaded successfully...")
-            h_txt += " [OK]"
-            pbar.set_postfix_str(s=h_txt, refresh=True)
-            summary_tmp = cell_data.data.summary
-            logging.info("Trying to get summary_data")
-
-            if cell_data.data.steps is None or self.force_recalc:
-                logging.info("Running make_step_table")
-                n_txt = f"steps {counter}"
-                pbar.set_description(n_txt, refresh=True)
-                cell_data.make_step_table()
-
-            if summary_tmp is None or self.force_recalc:
-                logging.info("Running make_summary")
-                n_txt = f"summary {counter}"
-                pbar.set_description(n_txt, refresh=True)
-                cell_data.make_summary(find_end_voltage=True, find_ir=True)
-
-            # some clean-ups (might not be needed anymore):
-            if not summary_tmp.index.name == hdr_summary.cycle_index:
-                logging.debug("Setting index to Cycle_Index")
-                # check if it is a byte-string
-                if b"Cycle_Index" in summary_tmp.columns:
-                    logging.debug("Seems to be a byte-string in the column-headers")
-                    summary_tmp.rename(
-                        columns={b"Cycle_Index": "Cycle_Index"}, inplace=True
-                    )
-                try:
-                    summary_tmp.set_index("cycle_index", inplace=True)
-                except KeyError:
-                    logging.debug("cycle_index already an index")
-
-            summary_frames[index] = summary_tmp
-
-            if self.all_in_memory:
-                cell_data_frames[index] = cell_data
-            else:
-                cell_data_frames[index] = cellreader.CellpyCell(initialize=True)
-                cell_data_frames[index].data.steps = cell_data.data.steps
-
-            if self.save_cellpy:
-                logging.info("saving to cellpy-format")
-                n_txt = f"saving {counter}"
-                pbar.set_description(n_txt, refresh=True)
-                if self.custom_data_folder is not None:
-                    print("Save to custom data-folder not implemented yet")
-                    print(f"Saving to {row.cellpy_file_name} instead")
-                if not row.fixed:
-                    logging.info("saving cell to %s" % row.cellpy_file_name)
-                    cell_data.ensure_step_table = True
-                    try:
-                        cell_data.save(row.cellpy_file_name)
-                    except Exception as e:
-                        logging.error("saving file failed")
-                        logging.error(e)
-
-                else:
-                    logging.debug("saving cell skipped (set to 'fixed' in info_df)")
-            else:
-                logging.info("You opted to not save to cellpy-format")
-                logging.info("It is usually recommended to save to cellpy-format:")
-                logging.info(" >>> b.experiment.save_cellpy = True")
-                logging.info(
-                    "Without the cellpy-files, you cannot select specific cells"
-                )
-                logging.info("if you did not opt to store all in memory")
-
-            if self.export_raw or self.export_cycles:
-                export_text = "exporting"
-                if self.export_raw:
-                    export_text += " [raw]"
-                if self.export_cycles:
-                    export_text += " [cycles]"
-                logging.info(export_text)
-                n_txt = f"{export_text} {counter}"
-                pbar.set_description(n_txt, refresh=True)
-                cell_data.to_csv(
-                    self.journal.raw_dir,
-                    sep=prms.Reader.sep,
-                    cycles=self.export_cycles,
-                    shifted=self.shifted_cycles,
-                    raw=self.export_raw,
-                    last_cycle=self.last_cycle,
-                )
-
-            if self.export_ica:
-                logging.info("exporting [ica]")
-                try:
-                    helper.export_dqdv(
-                        cell_data,
-                        savedir=self.journal.raw_dir,
-                        sep=prms.Reader.sep,
-                        last_cycle=self.last_cycle,
-                    )
-                except Exception as e:
-                    logging.error("Could not make/export dq/dv data")
-                    logging.debug(
-                        "Failed to make/export " "dq/dv data (%s): %s" % (index, str(e))
-                    )
-                    errors.append("ica:" + str(index))
-
-        self.errors["update"] = errors
-        self.summary_frames = summary_frames
-        self.cell_data_frames = cell_data_frames
-
-    def parallel_update(
-        self, all_in_memory=None, cell_specs=None, logging_mode=None, **kwargs
-    ):
-        """Updates the selected datasets in parallel.
-
-        Args:
-            all_in_memory (bool): store the `cellpydata` in memory (default
-                False)
-            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
-                dictionary will override the **kwargs and the parameters from the journal pages
-                for the indicated cell.
-            logging_mode (str): sets the logging mode for the loader(s).
-
-            kwargs:
-                transferred all the way to the instrument loader, if not
-                picked up earlier. Remark that you can obtain the same pr. cell by
-                providing a `cellspecs` dictionary. The kwargs have precedence over the
-                parameters given in the journal pages, but will be overridden by parameters
-                given by `cellspecs`.
-
-                Merging:
-                    recalc (Bool): set to False if you don't want automatic "recalc" of
-                        cycle numbers etc. when merging several data-sets.
-                Loading:
-                    selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
-                    loading from raw is not necessary (or turned off).
-
-                Debugging:
-                    debug (Bool): set to True if you want to run in debug mode (should never be used by non-developers).
-
-        Debug-mode:
-                 - runs only for the first item in your journal
-
-        Examples:
-            >>> # Don't perform recalculation of cycle numbers etc. when merging
-            >>> # All cells:
-            >>> b.update(recalc=False)
-            >>> # For specific cell(s):
-            >>> cell_specs_cell_01 = {"name_of_cell_01": {"recalc": False}}
-            >>> b.update(cell_specs=cell_specs_cell_01)
-
-        """
-        status = "PROD"  # set this to DEV when developing this
-        async_mode = "threading"
-        logging.debug("PARALLEL UPDATE")
-        # TODO: implement experiment.last_cycle
-        if status != "DEV":
-            print("SORRY - MULTIPROCESSING IS NOT IMPLEMENTED PROPERLY YET")
-            return self.update(
-                all_in_memory=all_in_memory,
-                cell_specs=cell_specs,
-                logging_mode=logging_mode,
-                **kwargs,
-            )
-
-        import concurrent.futures
-        import multiprocessing
-
-        max_number_processes = multiprocessing.cpu_count()
-
-        if async_mode == "threading":
-            pool_executor = concurrent.futures.ThreadPoolExecutor
-        else:
-            pool_executor = concurrent.futures.ProcessPoolExecutor
-
-        debugging = kwargs.pop("debug", False)
-
-        # --- cleaning up attributes / arguments etc ---
-        force_cellpy = kwargs.pop("force_cellpy", self.force_cellpy)
-        force_raw = kwargs.pop("force_raw", self.force_raw)
-
-        logging.info("[update experiment]")
-        if all_in_memory is not None:
-            self.all_in_memory = all_in_memory
-
-        logging.info(f"Additional keyword arguments: {kwargs}")
-        selector = kwargs.get("selector", None)
-
-        pages = self.journal.pages
-        if self.nom_cap:
-            warnings.warn(
-                "Setting nominal capacity through attributes will be deprecated soon since it modifies "
-                "the journal pages."
-            )
-            pages[hdr_journal.nom_cap] = self.nom_cap
-
-        if self.instrument:
-            warnings.warn(
-                "Setting instrument through attributes will be deprecated soon since it modifies the journal pages."
-            )
-            pages[hdr_journal.instrument] = self.instrument
-
-        if x := kwargs.pop("instrument", None):
-            warnings.warn(
-                "Setting instrument through params will be deprecated soon since it modifies the journal pages."
-                "Future version will require instrument in the journal pages."
-            )
-            pages[hdr_journal.instrument] = x
-
-        if pages.empty:
-            raise Exception("your journal is empty")
-
-        # --- init ---
-        summary_frames = dict()
-        cell_data_frames = dict()
-        number_of_runs = len(pages)
-        logging.debug(f"You have {number_of_runs} cells in your journal")
-        counter = 0
-        errors = []
-
-        pbar = tqdm(list(pages.iterrows()), file=sys.stdout, leave=False)
-
-        if debugging:
-            pbar = tqdm(list(pages.iterrows())[0:1], file=sys.stdout, leave=False)
-
-        # --- iterating ---
-        # TODO: create a multiprocessing pool and get one statusbar pr cell
-        params = []
-        with pool_executor(max_number_processes) as executor:
-            for index, row in pages.iterrows():
-                counter += 1
-                cell_spec_page = self._get_cell_spec_from_page(index, row)
-
-                if cell_specs is not None:
-                    cell_spec = cell_specs.get(index, dict())
-                else:
-                    cell_spec = dict()
-
-                cell_spec = {**cell_spec_page, **kwargs, **cell_spec}
-
-                # --- UPDATING ARGUMENTS ---
-                filename = None
-                instrument = None
-                cellpy_file = OtherPath(row[hdr_journal.cellpy_file_name])
-                _cellpy_file = None
-                if not force_raw and cellpy_file.is_file():
-                    _cellpy_file = cellpy_file
-                    logging.debug(f"Got cellpy file: {_cellpy_file}")
-                if not force_cellpy:
-                    filename = row[hdr_journal.raw_file_names]
-                    instrument = row[hdr_journal.instrument]
-
-                cycle_mode = row[hdr_journal.cell_type]
-                mass = row[hdr_journal.mass]
-                nom_cap = row[hdr_journal.nom_cap]
-
-                loading = None
-                area = None
-                if hdr_journal.loading in row:
-                    loading = row[hdr_journal.loading]
-                if hdr_journal.area in row:
-                    area = row[hdr_journal.area]
-
-                summary_kwargs = {
-                    "use_cellpy_stat_file": prms.Reader.use_cellpy_stat_file,
-                }
-
-                step_kwargs = {}
-                if not filename and not force_cellpy:
-                    logging.info(
-                        f"Raw file(s) not given in the journal.pages for index={index}"
-                    )
-                    errors.append(index)
-                    continue
-
-                elif not cellpy_file and force_cellpy:
-                    logging.info(
-                        f"Cellpy file not given in the journal.pages for index={index}"
-                    )
-                    errors.append(index)
-                    continue
-
-                else:
-                    logging.info(f"Processing {index}")
-
-                logging.info("loading cell")
-                params.append(
-                    dict(
-                        filename=filename,
-                        instrument=instrument,
-                        cellpy_file=_cellpy_file,
-                        cycle_mode=cycle_mode,
-                        mass=mass,
-                        nom_cap=nom_cap,
-                        loading=loading,
-                        area=area,
-                        step_kwargs=step_kwargs,
-                        summary_kwargs=summary_kwargs,
-                        selector=selector,
-                        logging_mode=logging_mode,
-                        testing=False,
-                        **cell_spec,
-                    )
-                )
-
-            pool = [executor.submit(cellpy.get, **param) for param in params]
-            for i in concurrent.futures.as_completed(pool):
-                cell_data = i.result()
-                if cell_data.empty:
-                    logging.info("...not loaded...")
-                    logging.debug("Data is empty. Could not load cell!")
-                    errors.append("check:" + str(index))
-
-                logging.info("...loaded successfully...")
-                summary_tmp = cell_data.data.summary
-                logging.info("Trying to get summary_data")
-
-                if cell_data.data.steps is None or self.force_recalc:
-                    logging.info("Running make_step_table")
-                    cell_data.make_step_table()
-
-                if summary_tmp is None or self.force_recalc:
-                    logging.info("Running make_summary")
-                    cell_data.make_summary(find_end_voltage=True, find_ir=True)
-
-                # some clean-ups (might not be needed anymore):
-                if not summary_tmp.index.name == hdr_summary.cycle_index:
-                    logging.debug("Setting index to Cycle_Index")
-                    # check if it is a byte-string
-                    if b"Cycle_Index" in summary_tmp.columns:
-                        logging.debug("Seems to be a byte-string in the column-headers")
-                        summary_tmp.rename(
-                            columns={b"Cycle_Index": "Cycle_Index"}, inplace=True
-                        )
-                    try:
-                        summary_tmp.set_index("cycle_index", inplace=True)
-                    except KeyError:
-                        logging.debug("cycle_index already an index")
-
-                summary_frames[index] = summary_tmp
-
-                if self.all_in_memory:
-                    cell_data_frames[index] = cell_data
-                else:
-                    cell_data_frames[index] = cellreader.CellpyCell(initialize=True)
-                    cell_data_frames[index].data.steps = cell_data.data.steps
-
-                if self.save_cellpy:
-                    logging.info("saving to cellpy-format")
-                    n_txt = f"saving {counter}"
-                    pbar.set_description(n_txt, refresh=True)
-                    if self.custom_data_folder is not None:
-                        print("Save to custom data-folder not implemented yet")
-                        print(f"Saving to {row.cellpy_file_name} instead")
-                    if not row.fixed:
-                        logging.info("saving cell to %s" % row.cellpy_file_name)
-                        cell_data.ensure_step_table = True
-                        try:
-                            cell_data.save(row.cellpy_file_name)
-                        except Exception as e:
-                            logging.error("saving file failed")
-                            logging.error(e)
-
-                    else:
-                        logging.debug("saving cell skipped (set to 'fixed' in info_df)")
-                else:
-                    logging.info("You opted to not save to cellpy-format")
-                    logging.info("It is usually recommended to save to cellpy-format:")
-                    logging.info(" >>> b.experiment.save_cellpy = True")
-                    logging.info(
-                        "Without the cellpy-files, you cannot select specific cells"
-                    )
-                    logging.info("if you did not opt to store all in memory")
-
-                if self.export_raw or self.export_cycles:
-                    export_text = "exporting"
-                    if self.export_raw:
-                        export_text += " [raw]"
-                    if self.export_cycles:
-                        export_text += " [cycles]"
-                    logging.info(export_text)
-                    n_txt = f"{export_text} {counter}"
-                    pbar.set_description(n_txt, refresh=True)
-                    cell_data.to_csv(
-                        self.journal.raw_dir,
-                        sep=prms.Reader.sep,
-                        cycles=self.export_cycles,
-                        shifted=self.shifted_cycles,
-                        raw=self.export_raw,
-                        last_cycle=self.last_cycle,
-                    )
-
-                if self.export_ica:
-                    logging.info("exporting [ica]")
-                    try:
-                        helper.export_dqdv(
-                            cell_data,
-                            savedir=self.journal.raw_dir,
-                            sep=prms.Reader.sep,
-                            last_cycle=self.last_cycle,
-                        )
-                    except Exception as e:
-                        logging.error("Could not make/export dq/dv data")
-                        logging.debug(
-                            "Failed to make/export "
-                            "dq/dv data (%s): %s" % (index, str(e))
-                        )
-                        errors.append("ica:" + str(index))
-
-        self.errors["update"] = errors
-        self.summary_frames = summary_frames
-        self.cell_data_frames = cell_data_frames
-
-    def export_cellpy_files(self, path=None, **kwargs):
-        """Export all cellpy-files to a given path.
-
-        Remarks:
-            This method can only export to local folders
-            (OtherPath objects are not formally supported, but
-            might still work if the path is local).
-
-        Args:
-            path (str, pathlib.Path): path to export to (default: current working directory)
-        """
-        if path is None:
-            path = "."
-        errors = []
-        path = pathlib.Path(path)
-        cell_names = self.cell_names
-        for cell_name in cell_names:
-            cellpy_file_name = self.journal.pages.loc[
-                cell_name, hdr_journal.cellpy_file_name
-            ]
-            cellpy_file_name = path / pathlib.Path(cellpy_file_name).name
-            print(f"Exporting {cell_name} to {cellpy_file_name}")
-            try:
-                c = self.data[cell_name]
-            except TypeError as e:
-                errors.append(f"could not extract data for {cell_name} - linking")
-                self._link_cellpy_file(cell_name)
-
-            c.save(cellpy_file_name, **kwargs)
-        self.errors["export_cellpy_files"] = errors
-
-    @property
-    def cell_names(self):
-        """Returns a list of cell-names (strings)"""
-        try:
-            return [key for key in self.cell_data_frames]
-        except TypeError:
-            return None
-
-    def status(self):
-        print("\n")
-        print(" STATUS ".center(80, "="))
-        print(self)
-        print(" summary frames ".center(80, "-"))
-        if self.summary_frames is not None:
-            for key in self.summary_frames:
-                print(f" {{{key}}}")
-        print(" memory dumped ".center(80, "-"))
-        if self.memory_dumped is not None:
-            for key in self.memory_dumped:
-                print(f"{key}: {type(self.memory_dumped[key])}")
-        print(80 * "=")
-
-    def link(self, **kwargs):
-        """Ensure that an appropriate link to the cellpy-files exists for
-        each cell.
-
-        The experiment will then contain a CellpyCell object for each cell
-        (in the cell_data_frames attribute) with only the step-table stored.
-
-        Remark that running update persists the summary frames instead (or
-        everything in case you specify all_in_memory=True).
-        This might be considered "a strange and unexpected behaviour". Sorry
-        for that (but the authors of this package is also a bit strange...).
-
-        (OK, I will change it. Soon.)
-
-        **kwargs: passed to _link_cellpy_file
-            max_cycle (int): maximum cycle number to link/load (remark that the
-                cellpy objects will get the property overwrite_able set to False
-                if you give a max_cycle to prevent accidentally saving a "truncated"
-                file (use c.save(filename, overwrite=True) to force overwrite))
-
-
-        """
-        logging.info("[establishing links]")
-        logging.debug("checking and establishing link to data")
-
-        errors = []
-
-        for cell_label in self.journal.pages.index:
-            logging.debug(f"trying to link {cell_label}")
-            try:
-                self._link_cellpy_file(cell_label, **kwargs)
-            except IOError as e:
-                logging.warning(e)
-                e_txt = f"{cell_label}: links not established - try update instead"
-                logging.warning(e_txt)
-                errors.append(e_txt)
-
-        self.errors["link"] = errors
-
-    def recalc(
-        self,
-        save=True,
-        step_opts=None,
-        summary_opts=None,
-        indexes=None,
-        calc_steps=True,
-        testing=False,
-    ):
-        """Run make_step_table and make_summary on all cells.
-
-        Args:
-            save (bool): Save updated cellpy-files if True.
-            step_opts (dict): parameters to inject to make_steps.
-            summary_opts (dict): parameters to inject to make_summary.
-            indexes (list): Only recalculate for given indexes (i.e. list of cell-names).
-            calc_steps (bool): Run make_steps before making the summary.
-            testing (bool): Only for testing purposes.
-
-        Returns:
-            None
-        """
-
-        # TODO: option (default) to only recalc if the values (mass, nom_cap,...) have changed
-        errors = []
-        log = []
-        if testing:
-            pbar = tqdm(
-                list(self.journal.pages.iloc[0:2, :].iterrows()),
-                file=sys.stdout,
-                leave=False,
-            )
-        elif indexes is not None:
-            pbar = tqdm(
-                list(self.journal.pages.loc[indexes, :].iterrows()),
-                file=sys.stdout,
-                leave=False,
-            )
-        else:
-            pbar = tqdm(
-                list(self.journal.pages.iterrows()), file=sys.stdout, leave=False
-            )
-        for indx, row in pbar:
-            nom_cap = row[hdr_journal.nom_cap]
-            mass = row[hdr_journal.mass]
-            pbar.set_description(indx)
-            try:
-                c = self.data[indx]
-            except TypeError as e:
-                e_txt = (
-                    f"could not extract data for {indx} - have you forgotten to link?"
-                )
-                errors.append(e_txt)
-                warnings.warn(e_txt)
-
-            else:
-                if nom_cap:
-                    c.set_nom_cap(nom_cap)
-                if mass:
-                    c.set_mass(mass)
-                try:
-                    if calc_steps:
-                        pbar.set_postfix_str(s="steps", refresh=True)
-                        if step_opts is not None:
-                            c.make_step_table(**step_opts)
-                        else:
-                            c.make_step_table()
-
-                    pbar.set_postfix_str(s="summary", refresh=True)
-                    if summary_opts is not None:
-                        c.make_summary(**summary_opts)
-                    else:
-                        c.make_summary(find_end_voltage=True, find_ir=True)
-
-                except Exception as e:
-                    e_txt = f"recalculating for {indx} failed!"
-                    errors.append(e_txt)
-                    warnings.warn(e_txt)
-                else:
-                    if save:
-                        # remark! got a win error when trying to save (hdf5-file in use) (must fix this)
-                        pbar.set_postfix_str(s="save", refresh=True)
-                        try:
-                            c.save(row.cellpy_file_name)
-                            log.append(f"saved {indx} to {row.cellpy_file_name}")
-                        except Exception as e:
-                            e_txt = f"saving {indx} to {row.cellpy_file_name} failed!"
-                            errors.append(e_txt)
-                            warnings.warn(e_txt)
-        self.errors["recalc"] = errors
-        self.log["recalc"] = log
-
-
-class ImpedanceExperiment(BaseExperiment):
-    def __init__(self):
-        super().__init__()
-
-
-class LifeTimeExperiment(BaseExperiment):
-    def __init__(self):
-        super().__init__()
-
-
-if __name__ == "__main__":
-    from pathlib import Path
-    import os
-    import pandas as pd
-    import numpy as np
-    import seaborn as sns
-    import plotly.express as px
-
-    import cellpy
-    from cellpy.utils import batch, helpers, plotutils
-
-    project_dir = Path("../../../testdata/batch_project")
-    print(f"{project_dir.resolve()=}")
-    journal = project_dir / "test_project.json"
-    journal = journal.resolve()
-    print(f"{journal=}")
-    assert project_dir.is_dir()
-    assert journal.is_file()
-    os.chdir(project_dir)
-
-    print(f"cellpy version: {cellpy.__version__}")
-    cellpy.log.setup_logging("INFO")
-
-    b = batch.from_journal(journal)
-    b.update()
-
-    print("Ended OK")
+import ast
+import logging
+import os
+import pathlib
+import sys
+import warnings
+
+import pandas as pd
+from tqdm.auto import tqdm
+
+import cellpy
+from cellpy import prms
+from cellpy.parameters.internal_settings import get_headers_journal, get_headers_summary
+from cellpy.readers import cellreader
+from cellpy.internals.core import OtherPath
+from cellpy.utils.batch_tools import batch_helpers as helper
+from cellpy.utils.batch_tools.batch_core import BaseExperiment
+from cellpy.utils.batch_tools.batch_journals import LabJournal
+
+hdr_journal = get_headers_journal()
+hdr_summary = get_headers_summary()
+
+
+class CyclingExperiment(BaseExperiment):
+    """Load experimental data into memory.
+
+    This is a re-implementation of the old batch behaviour where
+    all the data-files are processed sequentially (and optionally exported)
+    while the summary tables are kept and processed. This implementation
+    also saves the step tables (for later use when using look-up
+    functionality).
+
+
+    Attributes:
+        journal (:obj: LabJournal): information about the experiment.
+        force_cellpy (bool): tries only to load the cellpy-file if True.
+        force_raw (bool): loads raw-file(s) even though appropriate cellpy-file
+           exists if True.
+        save_cellpy (bool): saves a cellpy-file for each cell if True.
+        accept_errors (bool): in case of error, dont raise an exception, but
+           continue to the next file if True.
+        all_in_memory (bool): store the cellpydata-objects in memory if True.
+        export_cycles (bool): export voltage-capacity curves if True.
+        shifted_cycles (bool): set this to True if you want to export the
+           voltage-capacity curves using the shifted-cycles option (only valid
+           if you set export_cycles to True).
+        export_raw (bool): export the raw-data if True.
+        export_ica (bool): export dq-dv curves if True.
+        last_cycle (int): sets the last cycle (i.e. the highest cycle number)
+           that you would like to process dq-dv on). Use all if None (the
+           default value).
+        selected_summaries (list): a list of summary labels defining what
+           summary columns to make joint summaries from (optional).
+        errors (dict): contains a dictionary listing all the errors encountered.
+
+    Args:
+        db_reader (str or object): custom db_reader (see doc on db_reader).
+
+    Example:
+
+
+    """
+
+    def __init__(self, *args, **kwargs):
+        db_reader = kwargs.pop("db_reader", "default")
+        super().__init__(*args)
+        self.journal = LabJournal(db_reader=db_reader)
+        self.errors = dict()
+        self.log = dict()
+
+        self.force_cellpy = False
+        self.force_raw = False
+        self.force_recalc = False
+        self.save_cellpy = True
+        self.accept_errors = False
+        self.all_in_memory = False
+
+        self.export_cycles = False
+        self.shifted_cycles = False
+        self.export_raw = True
+        self.export_ica = False
+        self.last_cycle = None
+        self.nom_cap = None
+        self.instrument = None
+        self.custom_data_folder = None
+
+        self.selected_summaries = None
+
+    def _repr_html_(self):
+        txt = f"<h2>CyclingExperiment-object</h2> id={hex(id(self))}"
+        txt += "<h3>Main attributes</h3>"
+        txt += f"""
+        <table>
+            <thead>
+                <tr>
+                    <th>Attribute</th>
+                    <th>Value</th>
+                </tr>
+            </thead>
+            <tbody>
+                <tr><td><b>force_cellpy</b></td><td>{self.force_cellpy}</td></tr>
+                <tr><td><b>force_raw</b></td><td>{self.force_raw}</td></tr>
+                <tr><td><b>force_recalc</b></td><td>{self.force_recalc}</td></tr>
+                <tr><td><b>save_cellpy</b></td><td>{self.save_cellpy}</td></tr>
+                <tr><td><b>accept_errors</b></td><td>{self.accept_errors}</td></tr>
+                <tr><td><b>all_in_memory</b></td><td>{self.all_in_memory}</td></tr>
+                <tr><td><b>export_cycles</b></td><td>{self.export_cycles}</td></tr>
+                <tr><td><b>shifted_cycles</b></td><td>{self.shifted_cycles}</td></tr>
+                <tr><td><b>export_raw</b></td><td>{self.export_raw}</td></tr>
+                <tr><td><b>export_ica</b></td><td>{self.export_ica}</td></tr>
+                <tr><td><b>last_cycle</b></td><td>{self.last_cycle}</td></tr>
+                <tr><td><b>nom_cap</b></td><td>{self.nom_cap}</td></tr>
+                <tr><td><b>instrument</b></td><td>{self.instrument}</td></tr>
+                <tr><td><b>custom_data_folder</b></td><td>{self.custom_data_folder}</td></tr>
+                <tr><td><b>selected_summaries</b></td><td>{self.selected_summaries}</td></tr>
+            </tbody>
+        </table>
+        """
+        txt += "<h3>Cells</h3>"
+        txt += f"<p><b>data</b>: contains {len(self)} cells.</p>"
+        return txt
+
+    @staticmethod
+    def _get_cell_spec_from_page(indx: int, row: pd.Series) -> dict:
+        # Edit this if we decide to make "argument families", e.g. loader_split or merger_recalc.
+
+        PRM_SPLITTER = ";"
+        EQUAL_SIGN = "="
+
+        def _arg_parser(text: str) -> None:
+            individual_specs = text.split(PRM_SPLITTER)
+            for p in individual_specs:
+                p, a = p.split(EQUAL_SIGN)
+
+        logging.debug(f"getting cell_spec from journal pages ({indx})")
+        try:
+            cell_spec = row[hdr_journal.argument]
+            logging.debug(cell_spec)
+            if not isinstance(cell_spec, dict):
+                raise TypeError("the cell spec argument is not a dictionary")
+        except Exception as e:
+            logging.warning(f"could not get cell spec for {indx}")
+            logging.warning(f"error message: {e}")
+            return {}
+
+        # converting from str if needed
+        for spec in cell_spec:
+            if isinstance(cell_spec[spec], str):
+                if cell_spec[spec].lower() == "true":
+                    cell_spec[spec] = True
+                elif cell_spec[spec].lower() == "false":
+                    cell_spec[spec] = False
+                elif cell_spec[spec].lower() == "none":
+                    cell_spec[spec] = None
+                else:
+                    try:
+                        logging.debug(
+                            f"Using ast.literal_eval to convert cell-spec value from str '{cell_spec[spec]}'"
+                        )
+                        cell_spec[spec] = ast.literal_eval(cell_spec[spec])
+                    except ValueError as e:
+                        logging.warning(
+                            f"ERROR! Could not convert from str to python object!"
+                        )
+                        logging.debug(e)
+        return cell_spec
+
+    def update(
+        self,
+        all_in_memory=None,
+        cell_specs=None,
+        logging_mode=None,
+        accept_errors=None,
+        **kwargs,
+    ):
+        """Updates the selected datasets.
+
+        Args:
+            all_in_memory (bool): store the `cellpydata` in memory (default
+                False)
+            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
+                dictionary will override the **kwargs and the parameters from the journal pages
+                for the indicated cell.
+            logging_mode (str): sets the logging mode for the loader(s).
+            accept_errors (bool): if True, the loader will continue even if it encounters errors.
+
+            kwargs:
+                transferred all the way to the instrument loader, if not
+                picked up earlier. Remark that you can obtain the same pr. cell by
+                providing a `cellspecs` dictionary. The kwargs have precedence over the
+                parameters given in the journal pages, but will be overridden by parameters
+                given by `cellspecs`.
+
+                Merging:
+                    recalc (Bool): set to False if you don't want automatic "recalc" of
+                        cycle numbers etc. when merging several data-sets.
+                Loading:
+                    selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
+                    loading from raw is not necessary (or turned off).
+
+                Debugging:
+                    debug (Bool): set to True if you want to run in debug mode (should never be used by non-developers).
+
+        Debug-mode:
+                 - runs only for the first item in your journal
+
+        Examples:
+            >>> # Don't perform recalculation of cycle numbers etc. when merging
+            >>> # All cells:
+            >>> b.update(recalc=False)
+            >>> # For specific cell(s):
+            >>> cell_specs_cell_01 = {"name_of_cell_01": {"recalc": False}}
+            >>> b.update(cell_specs=cell_specs_cell_01)
+
+        """
+
+        # TODO: implement experiment.last_cycle
+        accept_errors = accept_errors or self.accept_errors
+
+        debugging = kwargs.pop("debug", False)
+        testing = kwargs.pop("testing", False)
+
+        # --- cleaning up attributes / arguments etc ---
+        force_cellpy = kwargs.pop("force_cellpy", self.force_cellpy)
+        force_raw = kwargs.pop("force_raw", self.force_raw)
+
+        logging.info("[update experiment]")
+        if all_in_memory is not None:
+            self.all_in_memory = all_in_memory
+
+        logging.info(f"Additional keyword arguments: {kwargs}")
+        selector = kwargs.get("selector", None)
+
+        pages = self.journal.pages
+        if self.nom_cap:
+            warnings.warn(
+                "Setting nominal capacity through attributes will be deprecated soon since it modifies "
+                "the journal pages."
+            )
+            pages[hdr_journal.nom_cap] = self.nom_cap
+
+        if self.instrument:
+            warnings.warn(
+                "Setting instrument through attributes will be deprecated soon since it modifies the journal pages."
+            )
+            pages[hdr_journal.instrument] = self.instrument
+
+        if x := kwargs.pop("instrument", None):
+            warnings.warn(
+                "Setting instrument through params will be deprecated soon since it modifies the journal pages."
+                "Future version will require instrument in the journal pages."
+            )
+            pages[hdr_journal.instrument] = x
+
+        if pages.empty:
+            raise Exception("your journal is empty")
+
+        # --- init ---
+        summary_frames = dict()
+        cell_data_frames = dict()
+        number_of_runs = len(pages)
+        logging.debug(f"You have {number_of_runs} cells in your journal")
+        counter = 0
+        errors = []
+
+        pbar = tqdm(list(pages.iterrows()), file=sys.stdout, leave=False)
+
+        if debugging:
+            pbar = tqdm(list(pages.iterrows())[0:1], file=sys.stdout, leave=False)
+
+        # --- iterating ---
+        # TODO: create a multiprocessing pool and get one statusbar pr cell
+        for index, row in pbar:
+            counter += 1
+            h_txt = f"{index}"
+            n_txt = f"loading {counter}"
+            l_txt = f"starting to process file # {counter} ({index})"
+            pbar.set_description(n_txt, refresh=True)
+            cell_spec_page = self._get_cell_spec_from_page(index, row)
+
+            if cell_specs is not None:
+                cell_spec = cell_specs.get(index, dict())
+            else:
+                cell_spec = dict()
+
+            cell_spec = {**cell_spec_page, **kwargs, **cell_spec}
+            l_txt += f" cell_spec: {cell_spec}"
+            logging.debug(l_txt)
+
+            # --- UPDATING ARGUMENTS ---
+            filename = None
+            instrument = None
+            cellpy_file = OtherPath(row[hdr_journal.cellpy_file_name])
+            _cellpy_file = None
+            if not force_raw and cellpy_file.is_file():
+                _cellpy_file = cellpy_file
+                logging.debug(f"Got cellpy file: {_cellpy_file}")
+            if not force_cellpy:
+                filename = row[hdr_journal.raw_file_names]
+                instrument = row[hdr_journal.instrument]
+
+            cycle_mode = row[hdr_journal.cell_type]
+            mass = row[hdr_journal.mass]
+            nom_cap = row[hdr_journal.nom_cap]
+
+            loading = None
+            area = None
+            if hdr_journal.loading in row:
+                loading = row[hdr_journal.loading]
+            if hdr_journal.area in row:
+                area = row[hdr_journal.area]
+
+            summary_kwargs = {
+                "use_cellpy_stat_file": prms.Reader.use_cellpy_stat_file,
+            }
+
+            step_kwargs = {}
+            if not filename and not force_cellpy:
+                logging.info(
+                    f"Raw file(s) not given in the journal.pages for index={index}"
+                )
+                errors.append(index)
+                continue
+
+            elif not cellpy_file and force_cellpy:
+                logging.info(
+                    f"Cellpy file not given in the journal.pages for index={index}"
+                )
+                errors.append(index)
+                continue
+
+            else:
+                logging.info(f"Processing {index}")
+
+            logging.info("loading cell")
+            try:
+                logging.debug("inside try: running cellpy.get")
+                cell_data = cellpy.get(
+                    filename=filename,
+                    instrument=instrument,
+                    cellpy_file=_cellpy_file,
+                    cycle_mode=cycle_mode,
+                    mass=mass,
+                    nom_cap=nom_cap,
+                    loading=loading,
+                    area=area,
+                    step_kwargs=step_kwargs,
+                    summary_kwargs=summary_kwargs,
+                    selector=selector,
+                    logging_mode=logging_mode,
+                    testing=testing,
+                    **cell_spec,
+                )
+                logging.info("loaded cell")
+
+            except Exception as e:
+                logging.info("Failed to load: " + str(e))
+                errors.append("update:" + str(index))
+                h_txt += " [-]"
+                pbar.set_postfix_str(s=h_txt, refresh=True)
+                if not accept_errors:
+                    raise e
+                continue
+
+            if cell_data.empty:
+                logging.info("...not loaded...")
+                logging.debug("Data is empty. Could not load cell!")
+                errors.append("check:" + str(index))
+                h_txt += " [-]"
+                pbar.set_postfix_str(s=h_txt, refresh=True)
+                continue
+
+            logging.info("...loaded successfully...")
+            h_txt += " [OK]"
+            pbar.set_postfix_str(s=h_txt, refresh=True)
+            summary_tmp = cell_data.data.summary
+            logging.info("Trying to get summary_data")
+
+            if cell_data.data.steps is None or self.force_recalc:
+                logging.info("Running make_step_table")
+                n_txt = f"steps {counter}"
+                pbar.set_description(n_txt, refresh=True)
+                cell_data.make_step_table()
+
+            if summary_tmp is None or self.force_recalc:
+                logging.info("Running make_summary")
+                n_txt = f"summary {counter}"
+                pbar.set_description(n_txt, refresh=True)
+                cell_data.make_summary(find_end_voltage=True, find_ir=True)
+
+            # some clean-ups (might not be needed anymore):
+            if not summary_tmp.index.name == hdr_summary.cycle_index:
+                logging.debug("Setting index to Cycle_Index")
+                # check if it is a byte-string
+                if b"Cycle_Index" in summary_tmp.columns:
+                    logging.debug("Seems to be a byte-string in the column-headers")
+                    summary_tmp.rename(
+                        columns={b"Cycle_Index": "Cycle_Index"}, inplace=True
+                    )
+                try:
+                    summary_tmp.set_index("cycle_index", inplace=True)
+                except KeyError:
+                    logging.debug("cycle_index already an index")
+
+            summary_frames[index] = summary_tmp
+
+            if self.all_in_memory:
+                cell_data_frames[index] = cell_data
+            else:
+                cell_data_frames[index] = cellreader.CellpyCell(initialize=True)
+                cell_data_frames[index].data.steps = cell_data.data.steps
+
+            if self.save_cellpy:
+                logging.info("saving to cellpy-format")
+                n_txt = f"saving {counter}"
+                pbar.set_description(n_txt, refresh=True)
+                if self.custom_data_folder is not None:
+                    print("Save to custom data-folder not implemented yet")
+                    print(f"Saving to {row.cellpy_file_name} instead")
+                if not row.fixed:
+                    logging.info("saving cell to %s" % row.cellpy_file_name)
+                    cell_data.ensure_step_table = True
+                    try:
+                        cell_data.save(row.cellpy_file_name)
+                    except Exception as e:
+                        logging.error("saving file failed")
+                        logging.error(e)
+
+                else:
+                    logging.debug("saving cell skipped (set to 'fixed' in info_df)")
+            else:
+                logging.info("You opted to not save to cellpy-format")
+                logging.info("It is usually recommended to save to cellpy-format:")
+                logging.info(" >>> b.experiment.save_cellpy = True")
+                logging.info(
+                    "Without the cellpy-files, you cannot select specific cells"
+                )
+                logging.info("if you did not opt to store all in memory")
+
+            if self.export_raw or self.export_cycles:
+                export_text = "exporting"
+                if self.export_raw:
+                    export_text += " [raw]"
+                if self.export_cycles:
+                    export_text += " [cycles]"
+                logging.info(export_text)
+                n_txt = f"{export_text} {counter}"
+                pbar.set_description(n_txt, refresh=True)
+                cell_data.to_csv(
+                    self.journal.raw_dir,
+                    sep=prms.Reader.sep,
+                    cycles=self.export_cycles,
+                    shifted=self.shifted_cycles,
+                    raw=self.export_raw,
+                    last_cycle=self.last_cycle,
+                )
+
+            if self.export_ica:
+                logging.info("exporting [ica]")
+                try:
+                    helper.export_dqdv(
+                        cell_data,
+                        savedir=self.journal.raw_dir,
+                        sep=prms.Reader.sep,
+                        last_cycle=self.last_cycle,
+                    )
+                except Exception as e:
+                    logging.error("Could not make/export dq/dv data")
+                    logging.debug(
+                        "Failed to make/export " "dq/dv data (%s): %s" % (index, str(e))
+                    )
+                    errors.append("ica:" + str(index))
+
+        self.errors["update"] = errors
+        self.summary_frames = summary_frames
+        self.cell_data_frames = cell_data_frames
+
+    def parallel_update(
+        self, all_in_memory=None, cell_specs=None, logging_mode=None, **kwargs
+    ):
+        """Updates the selected datasets in parallel.
+
+        Args:
+            all_in_memory (bool): store the `cellpydata` in memory (default
+                False)
+            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
+                dictionary will override the **kwargs and the parameters from the journal pages
+                for the indicated cell.
+            logging_mode (str): sets the logging mode for the loader(s).
+
+            kwargs:
+                transferred all the way to the instrument loader, if not
+                picked up earlier. Remark that you can obtain the same pr. cell by
+                providing a `cellspecs` dictionary. The kwargs have precedence over the
+                parameters given in the journal pages, but will be overridden by parameters
+                given by `cellspecs`.
+
+                Merging:
+                    recalc (Bool): set to False if you don't want automatic "recalc" of
+                        cycle numbers etc. when merging several data-sets.
+                Loading:
+                    selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
+                    loading from raw is not necessary (or turned off).
+
+                Debugging:
+                    debug (Bool): set to True if you want to run in debug mode (should never be used by non-developers).
+
+        Debug-mode:
+                 - runs only for the first item in your journal
+
+        Examples:
+            >>> # Don't perform recalculation of cycle numbers etc. when merging
+            >>> # All cells:
+            >>> b.update(recalc=False)
+            >>> # For specific cell(s):
+            >>> cell_specs_cell_01 = {"name_of_cell_01": {"recalc": False}}
+            >>> b.update(cell_specs=cell_specs_cell_01)
+
+        """
+        status = "PROD"  # set this to DEV when developing this
+        async_mode = "threading"
+        logging.debug("PARALLEL UPDATE")
+        # TODO: implement experiment.last_cycle
+        if status != "DEV":
+            print("SORRY - MULTIPROCESSING IS NOT IMPLEMENTED PROPERLY YET")
+            return self.update(
+                all_in_memory=all_in_memory,
+                cell_specs=cell_specs,
+                logging_mode=logging_mode,
+                **kwargs,
+            )
+
+        import concurrent.futures
+        import multiprocessing
+
+        max_number_processes = multiprocessing.cpu_count()
+
+        if async_mode == "threading":
+            pool_executor = concurrent.futures.ThreadPoolExecutor
+        else:
+            pool_executor = concurrent.futures.ProcessPoolExecutor
+
+        debugging = kwargs.pop("debug", False)
+
+        # --- cleaning up attributes / arguments etc ---
+        force_cellpy = kwargs.pop("force_cellpy", self.force_cellpy)
+        force_raw = kwargs.pop("force_raw", self.force_raw)
+
+        logging.info("[update experiment]")
+        if all_in_memory is not None:
+            self.all_in_memory = all_in_memory
+
+        logging.info(f"Additional keyword arguments: {kwargs}")
+        selector = kwargs.get("selector", None)
+
+        pages = self.journal.pages
+        if self.nom_cap:
+            warnings.warn(
+                "Setting nominal capacity through attributes will be deprecated soon since it modifies "
+                "the journal pages."
+            )
+            pages[hdr_journal.nom_cap] = self.nom_cap
+
+        if self.instrument:
+            warnings.warn(
+                "Setting instrument through attributes will be deprecated soon since it modifies the journal pages."
+            )
+            pages[hdr_journal.instrument] = self.instrument
+
+        if x := kwargs.pop("instrument", None):
+            warnings.warn(
+                "Setting instrument through params will be deprecated soon since it modifies the journal pages."
+                "Future version will require instrument in the journal pages."
+            )
+            pages[hdr_journal.instrument] = x
+
+        if pages.empty:
+            raise Exception("your journal is empty")
+
+        # --- init ---
+        summary_frames = dict()
+        cell_data_frames = dict()
+        number_of_runs = len(pages)
+        logging.debug(f"You have {number_of_runs} cells in your journal")
+        counter = 0
+        errors = []
+
+        pbar = tqdm(list(pages.iterrows()), file=sys.stdout, leave=False)
+
+        if debugging:
+            pbar = tqdm(list(pages.iterrows())[0:1], file=sys.stdout, leave=False)
+
+        # --- iterating ---
+        # TODO: create a multiprocessing pool and get one statusbar pr cell
+        params = []
+        with pool_executor(max_number_processes) as executor:
+            for index, row in pages.iterrows():
+                counter += 1
+                cell_spec_page = self._get_cell_spec_from_page(index, row)
+
+                if cell_specs is not None:
+                    cell_spec = cell_specs.get(index, dict())
+                else:
+                    cell_spec = dict()
+
+                cell_spec = {**cell_spec_page, **kwargs, **cell_spec}
+
+                # --- UPDATING ARGUMENTS ---
+                filename = None
+                instrument = None
+                cellpy_file = OtherPath(row[hdr_journal.cellpy_file_name])
+                _cellpy_file = None
+                if not force_raw and cellpy_file.is_file():
+                    _cellpy_file = cellpy_file
+                    logging.debug(f"Got cellpy file: {_cellpy_file}")
+                if not force_cellpy:
+                    filename = row[hdr_journal.raw_file_names]
+                    instrument = row[hdr_journal.instrument]
+
+                cycle_mode = row[hdr_journal.cell_type]
+                mass = row[hdr_journal.mass]
+                nom_cap = row[hdr_journal.nom_cap]
+
+                loading = None
+                area = None
+                if hdr_journal.loading in row:
+                    loading = row[hdr_journal.loading]
+                if hdr_journal.area in row:
+                    area = row[hdr_journal.area]
+
+                summary_kwargs = {
+                    "use_cellpy_stat_file": prms.Reader.use_cellpy_stat_file,
+                }
+
+                step_kwargs = {}
+                if not filename and not force_cellpy:
+                    logging.info(
+                        f"Raw file(s) not given in the journal.pages for index={index}"
+                    )
+                    errors.append(index)
+                    continue
+
+                elif not cellpy_file and force_cellpy:
+                    logging.info(
+                        f"Cellpy file not given in the journal.pages for index={index}"
+                    )
+                    errors.append(index)
+                    continue
+
+                else:
+                    logging.info(f"Processing {index}")
+
+                logging.info("loading cell")
+                params.append(
+                    dict(
+                        filename=filename,
+                        instrument=instrument,
+                        cellpy_file=_cellpy_file,
+                        cycle_mode=cycle_mode,
+                        mass=mass,
+                        nom_cap=nom_cap,
+                        loading=loading,
+                        area=area,
+                        step_kwargs=step_kwargs,
+                        summary_kwargs=summary_kwargs,
+                        selector=selector,
+                        logging_mode=logging_mode,
+                        testing=False,
+                        **cell_spec,
+                    )
+                )
+
+            pool = [executor.submit(cellpy.get, **param) for param in params]
+            for i in concurrent.futures.as_completed(pool):
+                cell_data = i.result()
+                if cell_data.empty:
+                    logging.info("...not loaded...")
+                    logging.debug("Data is empty. Could not load cell!")
+                    errors.append("check:" + str(index))
+
+                logging.info("...loaded successfully...")
+                summary_tmp = cell_data.data.summary
+                logging.info("Trying to get summary_data")
+
+                if cell_data.data.steps is None or self.force_recalc:
+                    logging.info("Running make_step_table")
+                    cell_data.make_step_table()
+
+                if summary_tmp is None or self.force_recalc:
+                    logging.info("Running make_summary")
+                    cell_data.make_summary(find_end_voltage=True, find_ir=True)
+
+                # some clean-ups (might not be needed anymore):
+                if not summary_tmp.index.name == hdr_summary.cycle_index:
+                    logging.debug("Setting index to Cycle_Index")
+                    # check if it is a byte-string
+                    if b"Cycle_Index" in summary_tmp.columns:
+                        logging.debug("Seems to be a byte-string in the column-headers")
+                        summary_tmp.rename(
+                            columns={b"Cycle_Index": "Cycle_Index"}, inplace=True
+                        )
+                    try:
+                        summary_tmp.set_index("cycle_index", inplace=True)
+                    except KeyError:
+                        logging.debug("cycle_index already an index")
+
+                summary_frames[index] = summary_tmp
+
+                if self.all_in_memory:
+                    cell_data_frames[index] = cell_data
+                else:
+                    cell_data_frames[index] = cellreader.CellpyCell(initialize=True)
+                    cell_data_frames[index].data.steps = cell_data.data.steps
+
+                if self.save_cellpy:
+                    logging.info("saving to cellpy-format")
+                    n_txt = f"saving {counter}"
+                    pbar.set_description(n_txt, refresh=True)
+                    if self.custom_data_folder is not None:
+                        print("Save to custom data-folder not implemented yet")
+                        print(f"Saving to {row.cellpy_file_name} instead")
+                    if not row.fixed:
+                        logging.info("saving cell to %s" % row.cellpy_file_name)
+                        cell_data.ensure_step_table = True
+                        try:
+                            cell_data.save(row.cellpy_file_name)
+                        except Exception as e:
+                            logging.error("saving file failed")
+                            logging.error(e)
+
+                    else:
+                        logging.debug("saving cell skipped (set to 'fixed' in info_df)")
+                else:
+                    logging.info("You opted to not save to cellpy-format")
+                    logging.info("It is usually recommended to save to cellpy-format:")
+                    logging.info(" >>> b.experiment.save_cellpy = True")
+                    logging.info(
+                        "Without the cellpy-files, you cannot select specific cells"
+                    )
+                    logging.info("if you did not opt to store all in memory")
+
+                if self.export_raw or self.export_cycles:
+                    export_text = "exporting"
+                    if self.export_raw:
+                        export_text += " [raw]"
+                    if self.export_cycles:
+                        export_text += " [cycles]"
+                    logging.info(export_text)
+                    n_txt = f"{export_text} {counter}"
+                    pbar.set_description(n_txt, refresh=True)
+                    cell_data.to_csv(
+                        self.journal.raw_dir,
+                        sep=prms.Reader.sep,
+                        cycles=self.export_cycles,
+                        shifted=self.shifted_cycles,
+                        raw=self.export_raw,
+                        last_cycle=self.last_cycle,
+                    )
+
+                if self.export_ica:
+                    logging.info("exporting [ica]")
+                    try:
+                        helper.export_dqdv(
+                            cell_data,
+                            savedir=self.journal.raw_dir,
+                            sep=prms.Reader.sep,
+                            last_cycle=self.last_cycle,
+                        )
+                    except Exception as e:
+                        logging.error("Could not make/export dq/dv data")
+                        logging.debug(
+                            "Failed to make/export "
+                            "dq/dv data (%s): %s" % (index, str(e))
+                        )
+                        errors.append("ica:" + str(index))
+
+        self.errors["update"] = errors
+        self.summary_frames = summary_frames
+        self.cell_data_frames = cell_data_frames
+
+    def export_cellpy_files(self, path=None, **kwargs):
+        """Export all cellpy-files to a given path.
+
+        Remarks:
+            This method can only export to local folders
+            (OtherPath objects are not formally supported, but
+            might still work if the path is local).
+
+        Args:
+            path (str, pathlib.Path): path to export to (default: current working directory)
+        """
+        if path is None:
+            path = "."
+        errors = []
+        path = pathlib.Path(path)
+        cell_names = self.cell_names
+        for cell_name in cell_names:
+            cellpy_file_name = self.journal.pages.loc[
+                cell_name, hdr_journal.cellpy_file_name
+            ]
+            cellpy_file_name = path / pathlib.Path(cellpy_file_name).name
+            print(f"Exporting {cell_name} to {cellpy_file_name}")
+            try:
+                c = self.data[cell_name]
+            except TypeError as e:
+                errors.append(f"could not extract data for {cell_name} - linking")
+                self._link_cellpy_file(cell_name)
+
+            c.save(cellpy_file_name, **kwargs)
+        self.errors["export_cellpy_files"] = errors
+
+    @property
+    def cell_names(self):
+        """Returns a list of cell-names (strings)"""
+        try:
+            return [key for key in self.cell_data_frames]
+        except TypeError:
+            return None
+
+    def status(self):
+        print("\n")
+        print(" STATUS ".center(80, "="))
+        print(self)
+        print(" summary frames ".center(80, "-"))
+        if self.summary_frames is not None:
+            for key in self.summary_frames:
+                print(f" {{{key}}}")
+        print(" memory dumped ".center(80, "-"))
+        if self.memory_dumped is not None:
+            for key in self.memory_dumped:
+                print(f"{key}: {type(self.memory_dumped[key])}")
+        print(80 * "=")
+
+    def link(self, **kwargs):
+        """Ensure that an appropriate link to the cellpy-files exists for
+        each cell.
+
+        The experiment will then contain a CellpyCell object for each cell
+        (in the cell_data_frames attribute) with only the step-table stored.
+
+        Remark that running update persists the summary frames instead (or
+        everything in case you specify all_in_memory=True).
+        This might be considered "a strange and unexpected behaviour". Sorry
+        for that (but the authors of this package is also a bit strange...).
+
+        (OK, I will change it. Soon.)
+
+        **kwargs: passed to _link_cellpy_file
+            max_cycle (int): maximum cycle number to link/load (remark that the
+                cellpy objects will get the property overwrite_able set to False
+                if you give a max_cycle to prevent accidentally saving a "truncated"
+                file (use c.save(filename, overwrite=True) to force overwrite))
+
+
+        """
+        logging.info("[establishing links]")
+        logging.debug("checking and establishing link to data")
+
+        errors = []
+
+        for cell_label in self.journal.pages.index:
+            logging.debug(f"trying to link {cell_label}")
+            try:
+                self._link_cellpy_file(cell_label, **kwargs)
+            except IOError as e:
+                logging.warning(e)
+                e_txt = f"{cell_label}: links not established - try update instead"
+                logging.warning(e_txt)
+                errors.append(e_txt)
+
+        self.errors["link"] = errors
+
+    def recalc(
+        self,
+        save=True,
+        step_opts=None,
+        summary_opts=None,
+        indexes=None,
+        calc_steps=True,
+        testing=False,
+    ):
+        """Run make_step_table and make_summary on all cells.
+
+        Args:
+            save (bool): Save updated cellpy-files if True.
+            step_opts (dict): parameters to inject to make_steps.
+            summary_opts (dict): parameters to inject to make_summary.
+            indexes (list): Only recalculate for given indexes (i.e. list of cell-names).
+            calc_steps (bool): Run make_steps before making the summary.
+            testing (bool): Only for testing purposes.
+
+        Returns:
+            None
+        """
+
+        # TODO: option (default) to only recalc if the values (mass, nom_cap,...) have changed
+        errors = []
+        log = []
+        if testing:
+            pbar = tqdm(
+                list(self.journal.pages.iloc[0:2, :].iterrows()),
+                file=sys.stdout,
+                leave=False,
+            )
+        elif indexes is not None:
+            pbar = tqdm(
+                list(self.journal.pages.loc[indexes, :].iterrows()),
+                file=sys.stdout,
+                leave=False,
+            )
+        else:
+            pbar = tqdm(
+                list(self.journal.pages.iterrows()), file=sys.stdout, leave=False
+            )
+        for indx, row in pbar:
+            nom_cap = row[hdr_journal.nom_cap]
+            mass = row[hdr_journal.mass]
+            pbar.set_description(indx)
+            try:
+                c = self.data[indx]
+            except TypeError as e:
+                e_txt = (
+                    f"could not extract data for {indx} - have you forgotten to link?"
+                )
+                errors.append(e_txt)
+                warnings.warn(e_txt)
+
+            else:
+                if nom_cap:
+                    c.set_nom_cap(nom_cap)
+                if mass:
+                    c.set_mass(mass)
+                try:
+                    if calc_steps:
+                        pbar.set_postfix_str(s="steps", refresh=True)
+                        if step_opts is not None:
+                            c.make_step_table(**step_opts)
+                        else:
+                            c.make_step_table()
+
+                    pbar.set_postfix_str(s="summary", refresh=True)
+                    if summary_opts is not None:
+                        c.make_summary(**summary_opts)
+                    else:
+                        c.make_summary(find_end_voltage=True, find_ir=True)
+
+                except Exception as e:
+                    e_txt = f"recalculating for {indx} failed!"
+                    errors.append(e_txt)
+                    warnings.warn(e_txt)
+                else:
+                    if save:
+                        # remark! got a win error when trying to save (hdf5-file in use) (must fix this)
+                        pbar.set_postfix_str(s="save", refresh=True)
+                        try:
+                            c.save(row.cellpy_file_name)
+                            log.append(f"saved {indx} to {row.cellpy_file_name}")
+                        except Exception as e:
+                            e_txt = f"saving {indx} to {row.cellpy_file_name} failed!"
+                            errors.append(e_txt)
+                            warnings.warn(e_txt)
+        self.errors["recalc"] = errors
+        self.log["recalc"] = log
+
+
+class ImpedanceExperiment(BaseExperiment):
+    def __init__(self):
+        super().__init__()
+
+
+class LifeTimeExperiment(BaseExperiment):
+    def __init__(self):
+        super().__init__()
+
+
+if __name__ == "__main__":
+    from pathlib import Path
+    import os
+    import pandas as pd
+    import numpy as np
+    import seaborn as sns
+    import plotly.express as px
+
+    import cellpy
+    from cellpy.utils import batch, helpers, plotutils
+
+    project_dir = Path("../../../testdata/batch_project")
+    print(f"{project_dir.resolve()=}")
+    journal = project_dir / "test_project.json"
+    journal = journal.resolve()
+    print(f"{journal=}")
+    assert project_dir.is_dir()
+    assert journal.is_file()
+    os.chdir(project_dir)
+
+    print(f"cellpy version: {cellpy.__version__}")
+    cellpy.log.setup_logging("INFO")
+
+    b = batch.from_journal(journal)
+    b.update()
+
+    print("Ended OK")
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_exporters.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_exporters.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_helpers.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_helpers.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_journals.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_journals.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,789 +1,789 @@
-import json
-import logging
-import os
-import pathlib
-import platform
-import shutil
-import tempfile
-import warnings
-from abc import ABC
-
-import pandas as pd
-
-from cellpy.exceptions import UnderDefined
-from cellpy.parameters import prms
-from cellpy.parameters.internal_settings import (
-    get_headers_journal,
-    keys_journal_session,
-)
-from cellpy.parameters.legacy.update_headers import (
-    headers_journal_v0 as hdr_journal_old,
-)
-from cellpy.readers import dbreader
-from cellpy.utils.batch_tools.batch_core import BaseJournal
-from cellpy.utils.batch_tools.engines import simple_db_engine, sql_db_engine
-
-hdr_journal = get_headers_journal()
-
-trans_dict = {}
-missing_keys = []
-for key in hdr_journal:
-    if key in hdr_journal_old:
-        trans_dict[hdr_journal_old[key]] = hdr_journal[key]
-    else:
-        missing_keys.append(key)
-
-
-class LabJournal(BaseJournal, ABC):
-    def __init__(self, db_reader="default", engine=None, batch_col=None, **kwargs):
-        """Journal for selected batch.
-
-        The journal contains pages (pandas.DataFrame) with prms for
-        each cell (one cell pr row).
-
-        Args:
-            db_reader: either default (a simple excel reader already
-                implemented in cellpy) or other db readers that implement
-                the needed API.
-            engine: defaults to simple_db_engine for parsing db using the
-                db_reader
-                    self.pages = simple_db_engine(
-                        self.db_reader, id_keys, **kwargs
-                    )
-            batch_col: the column name for the batch column in the db (used by simple_db_engine).
-            **kwargs: passed to the db_reader
-        """
-
-        super().__init__()
-        if db_reader is None:
-            return
-
-        if isinstance(db_reader, str):
-            if db_reader == "off":
-                self.db_reader = None
-                return
-            if db_reader == "default":
-                db_reader = prms.Db.db_type
-            if db_reader == "simple_excel_reader":
-                self.db_reader = dbreader.Reader()
-                self.engine = simple_db_engine
-            elif db_reader == "sql_db_reader":
-                raise NotImplementedError("sql_db_reader is not implemented yet")
-                # self.db_reader = sql_dbreader.SqlReader()
-                # self.engine = sql_db_engine
-            else:
-                raise UnderDefined(f"The db-reader '{db_reader}' is not supported")
-        else:
-            logging.debug(f"Remark! db_reader: {db_reader}")
-            self.db_reader = db_reader
-
-        if engine is None:
-            self.engine = simple_db_engine
-
-        self.batch_col = batch_col or "b01"
-
-    def _repr_html_(self):
-        txt = f"<h2>LabJournal-object</h2> id={hex(id(self))}"
-        txt += "<h3>Main attributes</h3>"
-        txt += f"""
-        <table>
-            <thead>
-                <tr>
-                    <th>Attribute</th>
-                    <th>Value</th>
-                </tr>
-            </thead>
-            <tbody>
-                <tr><td><b>name</b></td><td>{self.name}</td></tr>
-                <tr><td><b>project</b></td><td>{self.project}</td></tr>
-                <tr><td><b>file_name</b></td><td>{self.file_name}</td></tr>
-                <tr><td><b>db_reader</b></td><td>{self.db_reader}</td></tr>
-        """
-        if self.db_reader == "default":
-            txt += f"<tr><td><b>batch_col</b></td><td>{self.batch_col}</td></tr>"
-        txt += f"""
-                <tr><td><b>time_stamp</b></td><td>{self.time_stamp}</td></tr>
-                <tr><td><b>project_dir</b></td><td>{self.project_dir}</td></tr>
-                <tr><td><b>raw_dir</b></td><td>{self.raw_dir}</td></tr>
-                <tr><td><b>batch_dir</b></td><td>{self.batch_dir}</td></tr>
-            </tbody>
-        </table>
-        """
-        txt += "<h3>Session info</h3>"
-        for key in self.session:
-            txt += f"<p><b>{key}</b>: {self.session[key]}</p>"
-
-        txt += "<h3>Pages</h3>"
-        try:
-            txt += self.pages._repr_html_()  # pylint: disable=protected-access
-        except AttributeError:
-            txt += "<p><b>pages</b><br> not found!</p>"
-        except ValueError:
-            txt += "<p><b>pages</b><br> not readable!</p>"
-        return txt
-
-    def _check_file_name(self, file_name, to_project_folder=False):
-        if file_name is None:
-            if not self.file_name:
-                self.generate_file_name()
-            file_name = pathlib.Path(self.file_name)
-
-        else:
-            file_name = pathlib.Path(file_name)
-        if to_project_folder:
-            file_name = file_name.with_suffix(".json").name
-            project_dir = pathlib.Path(self.project_dir)
-
-            file_name = project_dir / file_name
-        self.file_name = file_name  # updates object (maybe not smart)
-        return file_name
-
-    def from_db(self, project=None, name=None, batch_col=None, **kwargs):
-        """populate journal from db.
-
-        Args:
-            project (str): project name.
-            name (str): experiment name.
-            batch_col (int): batch column.
-
-        **kwargs: sent to engine.
-
-        simple_db-engine -> filefinder.search_for_files:
-            run_name(str): run-file identification.
-            raw_extension(str): optional, extension of run-files (without the '.').
-            cellpy_file_extension(str): optional, extension for cellpy files
-                (without the '.').
-            raw_file_dir(path): optional, directory where to look for run-files
-                (default: read prm-file)
-            cellpy_file_dir(path): optional, directory where to look for
-                cellpy-files (default: read prm-file)
-            prm_filename(path): optional parameter file can be given.
-            file_name_format(str): format of raw-file names or a glob pattern
-                (default: YYYYMMDD_[name]EEE_CC_TT_RR) [not finished yet].
-            reg_exp(str): use regular expression instead (defaults to None) [not finished yet].
-            sub_folders (bool): perform search also in sub-folders.
-            file_list (list of str): perform the search within a given list
-                of filenames instead of searching the folder(s). The list should
-                not contain the full filepath (only the actual file names). If
-                you want to provide the full path, you will have to modify the
-                file_name_format or reg_exp accordingly.
-            pre_path (path or str): path to prepend the list of files selected
-                 from the file_list.
-
-        Returns:
-            None
-        """
-        logging.debug("creating journal from db")
-        if batch_col is None:
-            batch_col = self.batch_col
-        if project is not None:
-            self.project = project
-        if name is None:
-            name = self.name
-        else:
-            self.name = name
-        logging.debug(f"batch_name, batch_col: {name}, {batch_col}")
-
-        if self.db_reader is not None:
-            if isinstance(self.db_reader, dbreader.Reader):  # Simple excel-db
-                id_keys = self.db_reader.select_batch(name, batch_col)
-                logging.debug(f"id_keys: {id_keys}")
-
-                self.pages = self.engine(self.db_reader, id_keys, **kwargs)
-            else:
-                logging.debug(
-                    "creating journal pages using advanced reader methods (not simple excel-db)"
-                )
-                self.pages = self.engine(self.db_reader, batch_name=name, **kwargs)
-
-            if self.pages.empty:
-                logging.critical(
-                    f"EMPTY JOURNAL: are you sure you have provided correct input to batch?"
-                )
-                logging.critical(f"name: {name}")
-                logging.critical(f"project: {self.project}")
-                logging.critical(f"batch_col: {batch_col}")
-        else:
-            logging.debug("creating empty journal pages")
-            self.pages = pd.DataFrame()
-
-        self.generate_empty_session()
-        self.generate_folder_names()
-        self.paginate()
-
-    def generate_empty_session(self):
-        self.session = {}
-        for item in keys_journal_session:
-            self.session[item] = None
-
-    @staticmethod
-    def _fix_cellpy_paths(p):
-        logging.debug("_fix_cellpy_paths does not work with OtherPaths yet")
-        # if platform.system() != "Windows":
-        #     if p.find("\\") >= 0:
-        #         # convert from win to posix
-        #         p = pathlib.PureWindowsPath(p)
-        # else:
-        #     if p.find("/") >= 0:
-        #         # convert from posix to win
-        #         p = pathlib.PurePosixPath(p)
-        # p = pathlib.Path(p)
-        return p
-
-    @classmethod
-    def read_journal_jason_file(cls, file_name, **kwargs):
-        logging.debug(f"json loader starting on {file_name}")
-        with open(file_name, "r") as infile:
-            top_level_dict = json.load(infile)
-        pages_dict = top_level_dict["info_df"]
-        meta = top_level_dict["metadata"]
-        session = top_level_dict.get("session", None)
-        pages = pd.DataFrame(pages_dict)
-        if pages.empty:
-            logging.critical("could not find any pages in the journal")
-            raise UnderDefined
-
-        pages = cls._clean_pages(pages)
-
-        if session is None:
-            logging.debug(f"no session - generating empty one")
-            session = dict()
-
-        session, pages = cls._clean_session(session, pages)
-
-        return pages, meta, session
-
-    @classmethod
-    def read_journal_excel_file(cls, file_name, **kwargs):
-        sheet_names = {"meta": "meta", "pages": "pages", "session": "session"}
-        project = kwargs.pop("project", "NaN")
-        name = kwargs.pop("batch", pathlib.Path(file_name).stem)
-        _meta = {
-            "name": name,
-            "project": project,
-            "project_dir": pathlib.Path("."),
-            "batch_dir": pathlib.Path("."),
-            "raw_dir": pathlib.Path("."),
-        }
-        logging.debug(f"xlsx loader starting on {file_name}")
-
-        meta_sheet_name = sheet_names["meta"]  # not tested yet
-        pages_sheet_name = sheet_names["pages"]
-        session_sheet_name = sheet_names["session"]  # not tested yet
-
-        temporary_directory = tempfile.mkdtemp()
-        temporary_file_name = shutil.copy(file_name, temporary_directory)
-        try:
-            pages = pd.read_excel(
-                temporary_file_name, engine="openpyxl", sheet_name=pages_sheet_name
-            )
-        except KeyError:
-            print(f"Worksheet '{pages_sheet_name}' does not exist.")
-            return None
-
-        try:
-            session = pd.read_excel(
-                temporary_file_name,
-                sheet_name=session_sheet_name,
-                engine="openpyxl",
-                header=[0, 1],
-            )
-        except (KeyError, ValueError):
-            print(f"Worksheet '{session_sheet_name}' does not exist.")
-            session = None
-
-        try:
-            meta = pd.read_excel(
-                temporary_file_name, sheet_name=meta_sheet_name, engine="openpyxl"
-            )
-
-        except (KeyError, ValueError):
-            print(f"Worksheet '{meta_sheet_name}' does not exist.")
-            meta = None
-
-        if pages.empty:
-            logging.critical("could not find any pages in the journal")
-            raise UnderDefined
-        pages = cls._clean_pages(pages)
-        pages = pages.set_index(hdr_journal.filename)
-
-        if meta is None:
-            meta = _meta
-        else:
-            meta = cls._unpack_meta(meta) or _meta
-
-        if session is None:
-            logging.debug(f"no session - generating empty one")
-            session = dict()
-        else:
-            session = cls._unpack_session(session)
-
-        session, pages = cls._clean_session(session, pages)
-
-        return pages, meta, session
-
-    @classmethod
-    def _unpack_session(cls, session):
-        try:
-            bcn2 = {
-                l: list(sb["cycle_index"].values)
-                for l, sb in session["bad_cycles"].groupby("cell_name")
-            }
-        except KeyError:
-            bcn2 = []
-
-        try:
-            bc2 = list(session["bad_cells"]["cell_name"].dropna().values.flatten())
-        except KeyError:
-            bc2 = []
-
-        try:
-            s2 = list(session["starred"]["cell_name"].dropna().values.flatten())
-        except KeyError:
-            s2 = []
-
-        try:
-            n2 = list(session["notes"]["txt"].dropna().values.flatten())
-        except KeyError:
-            n2 = []
-
-        session = {"bad_cycles": bcn2, "bad_cells": bc2, "starred": s2, "notes": n2}
-
-        return session
-
-    @classmethod
-    def _unpack_meta(cls, meta):
-        try:
-            meta = meta.loc[:, ["parameter", "value"]]
-        except KeyError:
-            return
-        meta = meta.set_index("parameter")
-        return meta.to_dict()["value"]
-
-    @classmethod
-    def _clean_session(cls, session, pages):
-        # include steps for cleaning up the session dict here
-        if not session:
-            logging.critical("no session found in your journal file")
-        for item in keys_journal_session:
-            session[item] = session.get(item, None)
-
-        return session, pages
-
-    @classmethod
-    def _clean_pages(cls, pages: pd.DataFrame) -> pd.DataFrame:
-        import ast
-
-        logging.debug("removing empty rows")
-        pages = pages.dropna(how="all")
-        logging.debug("checking path-names")
-        try:
-            p = pages[hdr_journal.raw_file_names]
-            new_p = []
-            for f in p:
-                if isinstance(f, str):
-                    try:
-                        new_f = ast.literal_eval(f"'{f}'")
-                        if isinstance(new_f, list):
-                            f = new_f
-                    except Exception as e:
-                        warnings.warn(e)
-                        warnings.warn(f"Could not evaluate {f}")
-
-                new_p.append(f)
-            pages[hdr_journal.raw_file_names] = new_p
-
-        except KeyError:
-            print(
-                "Tried but failed in converting raw_file_names into an appropriate list"
-            )
-        try:
-            pages[hdr_journal.cellpy_file_name] = pages[
-                hdr_journal.cellpy_file_name
-            ].apply(cls._fix_cellpy_paths)
-        except KeyError:
-            # assumes it is an old type journal file
-            print(f"The key '{hdr_journal.cellpy_file_name}' is missing!")
-            print(f"Assumes that this is an old-type journal file.")
-            try:
-                pages.rename(columns=trans_dict, inplace=True)
-                pages[hdr_journal.cellpy_file_name] = pages[
-                    hdr_journal.cellpy_file_name
-                ].apply(cls._fix_cellpy_paths)
-                logging.warning("old journal file - updating")
-            except KeyError:
-                print("Error! Could still not parse the pages.")
-                print(f"Missing key: {hdr_journal.cellpy_file_name}")
-                pages[hdr_journal.cellpy_file_name] = None
-
-        # only keep selected cells if keep column exists
-        if "keep" in pages.columns:
-            logging.debug("Journal contains 'keep' - selecting only 'keep' > 0.")
-            pages = pages.loc[pages.keep > 0, :]
-
-        for column_name in missing_keys:
-            if column_name not in pages.columns:
-                logging.debug(f"wrong journal format - missing: {column_name}")
-                pages[column_name] = None
-
-        for column_name in hdr_journal:
-            if column_name not in pages.columns:
-                if column_name != hdr_journal.filename:
-                    pages[column_name] = None
-
-        return pages
-
-    def from_file(self, file_name=None, paginate=True, **kwargs):
-        """Loads a DataFrame with all the needed info about the experiment"""
-        file_name = self._check_file_name(file_name)
-        logging.info(f"reading {file_name}")
-        if pathlib.Path(file_name).suffix.lower() == ".xlsx":
-            file_loader = self.read_journal_excel_file
-        else:
-            file_loader = self.read_journal_jason_file
-        try:
-            out = file_loader(file_name, **kwargs)
-            if out is None:
-                raise IOError(f"Error reading {file_name}.")
-            pages, meta_dict, session = out
-        except UnderDefined as e:
-            logging.critical(f"could not load {file_name}")
-            raise UnderDefined from e
-
-        logging.debug(f"got pages and meta_dict")
-
-        self.pages = pages
-        self.session = session
-        self.file_name = file_name
-        self._prm_packer(meta_dict)
-
-        if paginate:
-            self.generate_folder_names()
-            self.paginate()
-
-    def from_frame(self, frame, name=None, project=None, paginate=None, **kwargs):
-        if name is not None:
-            self.name = name
-        if project is not None:
-            self.project = project
-
-        self.pages = (
-            frame  # TODO: include a check here to see if the pages are appropriate
-        )
-        for hdr in hdr_journal.values():
-            if hdr not in self.pages.columns:
-                self.pages[hdr] = None
-
-        if hdr_journal.filename in self.pages.columns:
-            self.pages = self.pages.set_index(hdr_journal.filename)
-
-        if paginate is None:
-            if self.name and self.project:
-                paginate = True
-
-        if paginate:
-            logging.critical(f"paginating {project}/{name} ")
-            self.generate_folder_names()
-            self.paginate()
-
-    def from_file_old(self, file_name=None):
-        """Loads a DataFrame with all the needed info about the experiment"""
-
-        file_name = self._check_file_name(file_name)
-
-        with open(file_name, "r") as infile:
-            top_level_dict = json.load(infile)
-
-        pages_dict = top_level_dict["info_df"]
-        pages = pd.DataFrame(pages_dict)
-        pages[hdr_journal.cellpy_file_name] = pages[hdr_journal.cellpy_file_name].apply(
-            self._fix_cellpy_paths
-        )
-        self.pages = pages
-        self.file_name = file_name
-        self._prm_packer(top_level_dict["metadata"])
-        self.generate_folder_names()
-        self.paginate()
-
-    def create_empty_pages(self, description=None):
-        if description is not None:
-            print(f"Creating from {type(description)} is not implemented yet")
-
-        logging.debug("Creating an empty journal")
-        logging.debug(f"name: {self.name}")
-        logging.debug(f"project: {self.project}")
-
-        col_names = list(hdr_journal.values())
-        pages = pd.DataFrame(columns=col_names)
-        pages.set_index(hdr_journal.filename, inplace=True)
-        return pages
-
-    def duplicate_journal(self, folder=None) -> None:
-        """Copy the journal to folder.
-
-        Args:
-            folder (str or pathlib.Path): folder to copy to (defaults to the
-            current folder).
-        """
-
-        logging.debug(f"duplicating journal to folder {folder}")
-        journal_name = pathlib.Path(self.file_name)
-        if not journal_name.is_file():
-            logging.info("No journal saved")
-            return
-        new_journal_name = journal_name.name
-        if folder is not None:
-            new_journal_name = pathlib.Path(folder) / new_journal_name
-        try:
-            shutil.copy(journal_name, new_journal_name)
-        except shutil.SameFileError:
-            logging.debug("same file exception encountered")
-
-    def to_file(
-        self,
-        file_name=None,
-        paginate=True,
-        to_project_folder=True,
-        duplicate_to_local_folder=True,
-    ):
-        """Saves a DataFrame with all the needed info about the experiment.
-
-        Args:
-            file_name (str or pathlib.Path): journal file name (.json or .xlsx)
-            paginate (bool): make project folders
-            to_project_folder (bool): save journal file to the folder containing your cellpy projects
-            duplicate_to_local_folder (bool): save journal file to the folder you are in now also
-
-        Returns:
-            None
-        """
-        file_name = self._check_file_name(
-            file_name, to_project_folder=to_project_folder
-        )
-
-        pages = self.pages
-        session = self.session
-        meta = self._prm_packer()
-        top_level_dict = {"info_df": pages, "metadata": meta, "session": session}
-
-        is_json = False
-        is_xlsx = False
-
-        if file_name.suffix == ".xlsx":
-            is_xlsx = True
-
-        if file_name.suffix == ".json":
-            is_json = True
-
-        if is_xlsx:
-            df_session = self._pack_session(session)
-            df_meta = self._pack_meta(meta)
-
-            try:
-                pages.index.name = "filename"
-                with pd.ExcelWriter(file_name, mode="w", engine="openpyxl") as writer:
-                    pages.to_excel(writer, sheet_name="pages", engine="openpyxl")
-                    # no index is not supported for multi-index (update to index=False when pandas implement it):
-                    df_session.to_excel(writer, sheet_name="session", engine="openpyxl")
-                    df_meta.to_excel(
-                        writer, sheet_name="meta", engine="openpyxl", index=False
-                    )
-            except PermissionError as e:
-                print(f"Could not load journal to xlsx ({e})")
-
-        if is_json:
-            jason_string = json.dumps(
-                top_level_dict,
-                default=lambda info_df: json.loads(
-                    info_df.to_json(default_handler=str)
-                ),
-            )
-
-            with open(file_name, "w") as outfile:
-                outfile.write(jason_string)
-
-        self.file_name = file_name
-        logging.info(f"Saved file to {file_name}")
-
-        if paginate:
-            self.paginate()
-
-        if duplicate_to_local_folder:
-            self.duplicate_journal()
-
-    @staticmethod
-    def _pack_session(session):
-        frames = []
-        keys = []
-        try:
-            l_bad_cycle_numbers = []
-
-            for k, v in session["bad_cycles"].items():
-                l_bad_cycle_numbers.append(pd.DataFrame(data=v, columns=[k]))
-
-            df_bad_cycle_numbers = (
-                pd.concat(l_bad_cycle_numbers, axis=1)
-                .melt(var_name="cell_name", value_name="cycle_index")
-                .dropna()
-            )
-            frames.append(df_bad_cycle_numbers)
-            keys.append("bad_cycles")
-        except (KeyError, AttributeError):
-            logging.debug("missing bad cycle numbers")
-
-        df_bad_cells = pd.DataFrame(session["bad_cells"], columns=["cell_name"])
-        frames.append(df_bad_cells)
-        keys.append("bad_cells")
-
-        df_starred = pd.DataFrame(session["starred"], columns=["cell_name"])
-        frames.append(df_starred)
-        keys.append("starred")
-
-        df_notes = pd.DataFrame(session["notes"], columns=["txt"])
-        frames.append(df_notes)
-        keys.append("notes")
-
-        session = pd.concat(frames, axis=1, keys=keys)
-        return session
-
-    @staticmethod
-    def _pack_meta(meta):
-        meta = pd.DataFrame(meta, index=[0]).melt(
-            var_name="parameter", value_name="value"
-        )
-        return meta
-
-    def generate_folder_names(self):
-        """Set appropriate folder names."""
-        logging.debug("creating folder names")
-        if self.project and isinstance(self.project, (pathlib.Path, str)):
-            logging.debug("got project name")
-            logging.debug(self.project)
-            self.project_dir = os.path.join(prms.Paths.outdatadir, self.project)
-        else:
-            logging.critical(
-                "Could not create project dir (missing project definition)"
-            )
-        if self.name:
-            self.batch_dir = os.path.join(self.project_dir, self.name)
-            self.raw_dir = os.path.join(self.batch_dir, "raw_data")
-        else:
-            logging.critical(
-                "Could not create batch_dir and raw_dir", "(missing batch name)"
-            )
-        logging.debug(f"batch dir: {self.batch_dir}")
-        logging.debug(f"project dir: {self.project_dir}")
-        logging.debug(f"raw dir: {self.raw_dir}")
-
-    def paginate(self):
-        """Make folders where we would like to put results etc."""
-
-        project_dir = self.project_dir
-        raw_dir = self.raw_dir
-        batch_dir = self.batch_dir
-
-        if project_dir is None:
-            raise UnderDefined("no project directory defined")
-        if raw_dir is None:
-            raise UnderDefined("no raw directory defined")
-        if batch_dir is None:
-            raise UnderDefined("no batch directory defined")
-
-        # create the folders
-        if not os.path.isdir(project_dir):
-            os.mkdir(project_dir)
-            logging.info(f"created folder {project_dir}")
-        if not os.path.isdir(batch_dir):
-            os.mkdir(batch_dir)
-            logging.info(f"created folder {batch_dir}")
-        if not os.path.isdir(raw_dir):
-            os.mkdir(raw_dir)
-            logging.info(f"created folder {raw_dir}")
-
-        self.project_dir = project_dir
-        self.batch_dir = batch_dir
-        self.raw_dir = raw_dir
-
-        return project_dir, batch_dir, raw_dir
-
-    def generate_file_name(self):
-        """generate a suitable file name for the experiment"""
-        if not self.project:
-            raise UnderDefined("project name not given")
-        out_data_dir = prms.Paths.outdatadir
-        project_dir = os.path.join(out_data_dir, self.project)
-        file_name = f"cellpy_batch_{self.name}.json"
-        self.file_name = os.path.join(project_dir, file_name)
-
-    # v.1.0.0:
-    def look_for_file(self):
-        pass
-
-    def get_column(self, header):
-        """populate new column from db"""
-        pass
-
-    def get_cell(self, id_key):
-        """get additional cell info from db"""
-        pass
-
-    def add_comment(self, comment):
-        """add a comment (will be saved in the journal file)"""
-        pass
-
-    def remove_comment(self, comment_id):
-        pass
-
-    def view_comments(self):
-        pass
-
-    def remove_cell(self, cell_id):
-        pass
-
-    def add_cell(self, cell_id, **kwargs):
-        """Add a cell to the pages"""
-        pass
-
-
-def _dev_journal_loading():
-    from cellpy import log
-
-    log.setup_logging(default_level="DEBUG")
-    journal_file = pathlib.Path(
-        "../../../testdata/batch_project/test_project.json"
-    ).resolve()
-    assert journal_file.is_file()
-
-    logging.debug(f"reading journal file {journal_file}")
-    journal = LabJournal(db_reader=None)
-    journal.from_file(journal_file, paginate=False)
-    print(80 * "-")
-    print(journal.pages)
-    print(80 * "-")
-    print(journal.session)
-
-    # creating a mock session
-    bad_cycle_numbers = {
-        "20160805_test001_45_cc": [4, 337, 338],
-        "20160805_test001_47_cc": [7, 8, 9],
-    }
-    bad_cells = ["20160805_test001_45_cc"]
-
-    notes = {"date_stamp": "one comment for the road", "date_stamp2": "another comment"}
-    session = {
-        "bad_cycle_numbers": bad_cycle_numbers,
-        "bad_cells": bad_cells,
-        "notes": notes,
-    }
-
-    # journal.session = session
-
-    new_journal_name = journal_file.with_name(f"{journal_file.stem}_tmp.xlsx")
-    print(new_journal_name)
-    journal.to_file(file_name=new_journal_name, paginate=False, to_project_folder=False)
-
-
-if __name__ == "__main__":
-    print(" running journal ".center(80, "-"))
-    _dev_journal_loading()
-    print(" finished ".center(80, "-"))
+import json
+import logging
+import os
+import pathlib
+import platform
+import shutil
+import tempfile
+import warnings
+from abc import ABC
+
+import pandas as pd
+
+from cellpy.exceptions import UnderDefined
+from cellpy.parameters import prms
+from cellpy.parameters.internal_settings import (
+    get_headers_journal,
+    keys_journal_session,
+)
+from cellpy.parameters.legacy.update_headers import (
+    headers_journal_v0 as hdr_journal_old,
+)
+from cellpy.readers import dbreader
+from cellpy.utils.batch_tools.batch_core import BaseJournal
+from cellpy.utils.batch_tools.engines import simple_db_engine, sql_db_engine
+
+hdr_journal = get_headers_journal()
+
+trans_dict = {}
+missing_keys = []
+for key in hdr_journal:
+    if key in hdr_journal_old:
+        trans_dict[hdr_journal_old[key]] = hdr_journal[key]
+    else:
+        missing_keys.append(key)
+
+
+class LabJournal(BaseJournal, ABC):
+    def __init__(self, db_reader="default", engine=None, batch_col=None, **kwargs):
+        """Journal for selected batch.
+
+        The journal contains pages (pandas.DataFrame) with prms for
+        each cell (one cell pr row).
+
+        Args:
+            db_reader: either default (a simple excel reader already
+                implemented in cellpy) or other db readers that implement
+                the needed API.
+            engine: defaults to simple_db_engine for parsing db using the
+                db_reader
+                    self.pages = simple_db_engine(
+                        self.db_reader, id_keys, **kwargs
+                    )
+            batch_col: the column name for the batch column in the db (used by simple_db_engine).
+            **kwargs: passed to the db_reader
+        """
+
+        super().__init__()
+        if db_reader is None:
+            return
+
+        if isinstance(db_reader, str):
+            if db_reader == "off":
+                self.db_reader = None
+                return
+            if db_reader == "default":
+                db_reader = prms.Db.db_type
+            if db_reader == "simple_excel_reader":
+                self.db_reader = dbreader.Reader()
+                self.engine = simple_db_engine
+            elif db_reader == "sql_db_reader":
+                raise NotImplementedError("sql_db_reader is not implemented yet")
+                # self.db_reader = sql_dbreader.SqlReader()
+                # self.engine = sql_db_engine
+            else:
+                raise UnderDefined(f"The db-reader '{db_reader}' is not supported")
+        else:
+            logging.debug(f"Remark! db_reader: {db_reader}")
+            self.db_reader = db_reader
+
+        if engine is None:
+            self.engine = simple_db_engine
+
+        self.batch_col = batch_col or "b01"
+
+    def _repr_html_(self):
+        txt = f"<h2>LabJournal-object</h2> id={hex(id(self))}"
+        txt += "<h3>Main attributes</h3>"
+        txt += f"""
+        <table>
+            <thead>
+                <tr>
+                    <th>Attribute</th>
+                    <th>Value</th>
+                </tr>
+            </thead>
+            <tbody>
+                <tr><td><b>name</b></td><td>{self.name}</td></tr>
+                <tr><td><b>project</b></td><td>{self.project}</td></tr>
+                <tr><td><b>file_name</b></td><td>{self.file_name}</td></tr>
+                <tr><td><b>db_reader</b></td><td>{self.db_reader}</td></tr>
+        """
+        if self.db_reader == "default":
+            txt += f"<tr><td><b>batch_col</b></td><td>{self.batch_col}</td></tr>"
+        txt += f"""
+                <tr><td><b>time_stamp</b></td><td>{self.time_stamp}</td></tr>
+                <tr><td><b>project_dir</b></td><td>{self.project_dir}</td></tr>
+                <tr><td><b>raw_dir</b></td><td>{self.raw_dir}</td></tr>
+                <tr><td><b>batch_dir</b></td><td>{self.batch_dir}</td></tr>
+            </tbody>
+        </table>
+        """
+        txt += "<h3>Session info</h3>"
+        for key in self.session:
+            txt += f"<p><b>{key}</b>: {self.session[key]}</p>"
+
+        txt += "<h3>Pages</h3>"
+        try:
+            txt += self.pages._repr_html_()  # pylint: disable=protected-access
+        except AttributeError:
+            txt += "<p><b>pages</b><br> not found!</p>"
+        except ValueError:
+            txt += "<p><b>pages</b><br> not readable!</p>"
+        return txt
+
+    def _check_file_name(self, file_name, to_project_folder=False):
+        if file_name is None:
+            if not self.file_name:
+                self.generate_file_name()
+            file_name = pathlib.Path(self.file_name)
+
+        else:
+            file_name = pathlib.Path(file_name)
+        if to_project_folder:
+            file_name = file_name.with_suffix(".json").name
+            project_dir = pathlib.Path(self.project_dir)
+
+            file_name = project_dir / file_name
+        self.file_name = file_name  # updates object (maybe not smart)
+        return file_name
+
+    def from_db(self, project=None, name=None, batch_col=None, **kwargs):
+        """populate journal from db.
+
+        Args:
+            project (str): project name.
+            name (str): experiment name.
+            batch_col (int): batch column.
+
+        **kwargs: sent to engine.
+
+        simple_db-engine -> filefinder.search_for_files:
+            run_name(str): run-file identification.
+            raw_extension(str): optional, extension of run-files (without the '.').
+            cellpy_file_extension(str): optional, extension for cellpy files
+                (without the '.').
+            raw_file_dir(path): optional, directory where to look for run-files
+                (default: read prm-file)
+            cellpy_file_dir(path): optional, directory where to look for
+                cellpy-files (default: read prm-file)
+            prm_filename(path): optional parameter file can be given.
+            file_name_format(str): format of raw-file names or a glob pattern
+                (default: YYYYMMDD_[name]EEE_CC_TT_RR) [not finished yet].
+            reg_exp(str): use regular expression instead (defaults to None) [not finished yet].
+            sub_folders (bool): perform search also in sub-folders.
+            file_list (list of str): perform the search within a given list
+                of filenames instead of searching the folder(s). The list should
+                not contain the full filepath (only the actual file names). If
+                you want to provide the full path, you will have to modify the
+                file_name_format or reg_exp accordingly.
+            pre_path (path or str): path to prepend the list of files selected
+                 from the file_list.
+
+        Returns:
+            None
+        """
+        logging.debug("creating journal from db")
+        if batch_col is None:
+            batch_col = self.batch_col
+        if project is not None:
+            self.project = project
+        if name is None:
+            name = self.name
+        else:
+            self.name = name
+        logging.debug(f"batch_name, batch_col: {name}, {batch_col}")
+
+        if self.db_reader is not None:
+            if isinstance(self.db_reader, dbreader.Reader):  # Simple excel-db
+                id_keys = self.db_reader.select_batch(name, batch_col)
+                logging.debug(f"id_keys: {id_keys}")
+
+                self.pages = self.engine(self.db_reader, id_keys, **kwargs)
+            else:
+                logging.debug(
+                    "creating journal pages using advanced reader methods (not simple excel-db)"
+                )
+                self.pages = self.engine(self.db_reader, batch_name=name, **kwargs)
+
+            if self.pages.empty:
+                logging.critical(
+                    f"EMPTY JOURNAL: are you sure you have provided correct input to batch?"
+                )
+                logging.critical(f"name: {name}")
+                logging.critical(f"project: {self.project}")
+                logging.critical(f"batch_col: {batch_col}")
+        else:
+            logging.debug("creating empty journal pages")
+            self.pages = pd.DataFrame()
+
+        self.generate_empty_session()
+        self.generate_folder_names()
+        self.paginate()
+
+    def generate_empty_session(self):
+        self.session = {}
+        for item in keys_journal_session:
+            self.session[item] = None
+
+    @staticmethod
+    def _fix_cellpy_paths(p):
+        logging.debug("_fix_cellpy_paths does not work with OtherPaths yet")
+        # if platform.system() != "Windows":
+        #     if p.find("\\") >= 0:
+        #         # convert from win to posix
+        #         p = pathlib.PureWindowsPath(p)
+        # else:
+        #     if p.find("/") >= 0:
+        #         # convert from posix to win
+        #         p = pathlib.PurePosixPath(p)
+        # p = pathlib.Path(p)
+        return p
+
+    @classmethod
+    def read_journal_jason_file(cls, file_name, **kwargs):
+        logging.debug(f"json loader starting on {file_name}")
+        with open(file_name, "r") as infile:
+            top_level_dict = json.load(infile)
+        pages_dict = top_level_dict["info_df"]
+        meta = top_level_dict["metadata"]
+        session = top_level_dict.get("session", None)
+        pages = pd.DataFrame(pages_dict)
+        if pages.empty:
+            logging.critical("could not find any pages in the journal")
+            raise UnderDefined
+
+        pages = cls._clean_pages(pages)
+
+        if session is None:
+            logging.debug(f"no session - generating empty one")
+            session = dict()
+
+        session, pages = cls._clean_session(session, pages)
+
+        return pages, meta, session
+
+    @classmethod
+    def read_journal_excel_file(cls, file_name, **kwargs):
+        sheet_names = {"meta": "meta", "pages": "pages", "session": "session"}
+        project = kwargs.pop("project", "NaN")
+        name = kwargs.pop("batch", pathlib.Path(file_name).stem)
+        _meta = {
+            "name": name,
+            "project": project,
+            "project_dir": pathlib.Path("."),
+            "batch_dir": pathlib.Path("."),
+            "raw_dir": pathlib.Path("."),
+        }
+        logging.debug(f"xlsx loader starting on {file_name}")
+
+        meta_sheet_name = sheet_names["meta"]  # not tested yet
+        pages_sheet_name = sheet_names["pages"]
+        session_sheet_name = sheet_names["session"]  # not tested yet
+
+        temporary_directory = tempfile.mkdtemp()
+        temporary_file_name = shutil.copy(file_name, temporary_directory)
+        try:
+            pages = pd.read_excel(
+                temporary_file_name, engine="openpyxl", sheet_name=pages_sheet_name
+            )
+        except KeyError:
+            print(f"Worksheet '{pages_sheet_name}' does not exist.")
+            return None
+
+        try:
+            session = pd.read_excel(
+                temporary_file_name,
+                sheet_name=session_sheet_name,
+                engine="openpyxl",
+                header=[0, 1],
+            )
+        except (KeyError, ValueError):
+            print(f"Worksheet '{session_sheet_name}' does not exist.")
+            session = None
+
+        try:
+            meta = pd.read_excel(
+                temporary_file_name, sheet_name=meta_sheet_name, engine="openpyxl"
+            )
+
+        except (KeyError, ValueError):
+            print(f"Worksheet '{meta_sheet_name}' does not exist.")
+            meta = None
+
+        if pages.empty:
+            logging.critical("could not find any pages in the journal")
+            raise UnderDefined
+        pages = cls._clean_pages(pages)
+        pages = pages.set_index(hdr_journal.filename)
+
+        if meta is None:
+            meta = _meta
+        else:
+            meta = cls._unpack_meta(meta) or _meta
+
+        if session is None:
+            logging.debug(f"no session - generating empty one")
+            session = dict()
+        else:
+            session = cls._unpack_session(session)
+
+        session, pages = cls._clean_session(session, pages)
+
+        return pages, meta, session
+
+    @classmethod
+    def _unpack_session(cls, session):
+        try:
+            bcn2 = {
+                l: list(sb["cycle_index"].values)
+                for l, sb in session["bad_cycles"].groupby("cell_name")
+            }
+        except KeyError:
+            bcn2 = []
+
+        try:
+            bc2 = list(session["bad_cells"]["cell_name"].dropna().values.flatten())
+        except KeyError:
+            bc2 = []
+
+        try:
+            s2 = list(session["starred"]["cell_name"].dropna().values.flatten())
+        except KeyError:
+            s2 = []
+
+        try:
+            n2 = list(session["notes"]["txt"].dropna().values.flatten())
+        except KeyError:
+            n2 = []
+
+        session = {"bad_cycles": bcn2, "bad_cells": bc2, "starred": s2, "notes": n2}
+
+        return session
+
+    @classmethod
+    def _unpack_meta(cls, meta):
+        try:
+            meta = meta.loc[:, ["parameter", "value"]]
+        except KeyError:
+            return
+        meta = meta.set_index("parameter")
+        return meta.to_dict()["value"]
+
+    @classmethod
+    def _clean_session(cls, session, pages):
+        # include steps for cleaning up the session dict here
+        if not session:
+            logging.critical("no session found in your journal file")
+        for item in keys_journal_session:
+            session[item] = session.get(item, None)
+
+        return session, pages
+
+    @classmethod
+    def _clean_pages(cls, pages: pd.DataFrame) -> pd.DataFrame:
+        import ast
+
+        logging.debug("removing empty rows")
+        pages = pages.dropna(how="all")
+        logging.debug("checking path-names")
+        try:
+            p = pages[hdr_journal.raw_file_names]
+            new_p = []
+            for f in p:
+                if isinstance(f, str):
+                    try:
+                        new_f = ast.literal_eval(f"'{f}'")
+                        if isinstance(new_f, list):
+                            f = new_f
+                    except Exception as e:
+                        warnings.warn(e)
+                        warnings.warn(f"Could not evaluate {f}")
+
+                new_p.append(f)
+            pages[hdr_journal.raw_file_names] = new_p
+
+        except KeyError:
+            print(
+                "Tried but failed in converting raw_file_names into an appropriate list"
+            )
+        try:
+            pages[hdr_journal.cellpy_file_name] = pages[
+                hdr_journal.cellpy_file_name
+            ].apply(cls._fix_cellpy_paths)
+        except KeyError:
+            # assumes it is an old type journal file
+            print(f"The key '{hdr_journal.cellpy_file_name}' is missing!")
+            print(f"Assumes that this is an old-type journal file.")
+            try:
+                pages.rename(columns=trans_dict, inplace=True)
+                pages[hdr_journal.cellpy_file_name] = pages[
+                    hdr_journal.cellpy_file_name
+                ].apply(cls._fix_cellpy_paths)
+                logging.warning("old journal file - updating")
+            except KeyError:
+                print("Error! Could still not parse the pages.")
+                print(f"Missing key: {hdr_journal.cellpy_file_name}")
+                pages[hdr_journal.cellpy_file_name] = None
+
+        # only keep selected cells if keep column exists
+        if "keep" in pages.columns:
+            logging.debug("Journal contains 'keep' - selecting only 'keep' > 0.")
+            pages = pages.loc[pages.keep > 0, :]
+
+        for column_name in missing_keys:
+            if column_name not in pages.columns:
+                logging.debug(f"wrong journal format - missing: {column_name}")
+                pages[column_name] = None
+
+        for column_name in hdr_journal:
+            if column_name not in pages.columns:
+                if column_name != hdr_journal.filename:
+                    pages[column_name] = None
+
+        return pages
+
+    def from_file(self, file_name=None, paginate=True, **kwargs):
+        """Loads a DataFrame with all the needed info about the experiment"""
+        file_name = self._check_file_name(file_name)
+        logging.info(f"reading {file_name}")
+        if pathlib.Path(file_name).suffix.lower() == ".xlsx":
+            file_loader = self.read_journal_excel_file
+        else:
+            file_loader = self.read_journal_jason_file
+        try:
+            out = file_loader(file_name, **kwargs)
+            if out is None:
+                raise IOError(f"Error reading {file_name}.")
+            pages, meta_dict, session = out
+        except UnderDefined as e:
+            logging.critical(f"could not load {file_name}")
+            raise UnderDefined from e
+
+        logging.debug(f"got pages and meta_dict")
+
+        self.pages = pages
+        self.session = session
+        self.file_name = file_name
+        self._prm_packer(meta_dict)
+
+        if paginate:
+            self.generate_folder_names()
+            self.paginate()
+
+    def from_frame(self, frame, name=None, project=None, paginate=None, **kwargs):
+        if name is not None:
+            self.name = name
+        if project is not None:
+            self.project = project
+
+        self.pages = (
+            frame  # TODO: include a check here to see if the pages are appropriate
+        )
+        for hdr in hdr_journal.values():
+            if hdr not in self.pages.columns:
+                self.pages[hdr] = None
+
+        if hdr_journal.filename in self.pages.columns:
+            self.pages = self.pages.set_index(hdr_journal.filename)
+
+        if paginate is None:
+            if self.name and self.project:
+                paginate = True
+
+        if paginate:
+            logging.critical(f"paginating {project}/{name} ")
+            self.generate_folder_names()
+            self.paginate()
+
+    def from_file_old(self, file_name=None):
+        """Loads a DataFrame with all the needed info about the experiment"""
+
+        file_name = self._check_file_name(file_name)
+
+        with open(file_name, "r") as infile:
+            top_level_dict = json.load(infile)
+
+        pages_dict = top_level_dict["info_df"]
+        pages = pd.DataFrame(pages_dict)
+        pages[hdr_journal.cellpy_file_name] = pages[hdr_journal.cellpy_file_name].apply(
+            self._fix_cellpy_paths
+        )
+        self.pages = pages
+        self.file_name = file_name
+        self._prm_packer(top_level_dict["metadata"])
+        self.generate_folder_names()
+        self.paginate()
+
+    def create_empty_pages(self, description=None):
+        if description is not None:
+            print(f"Creating from {type(description)} is not implemented yet")
+
+        logging.debug("Creating an empty journal")
+        logging.debug(f"name: {self.name}")
+        logging.debug(f"project: {self.project}")
+
+        col_names = list(hdr_journal.values())
+        pages = pd.DataFrame(columns=col_names)
+        pages.set_index(hdr_journal.filename, inplace=True)
+        return pages
+
+    def duplicate_journal(self, folder=None) -> None:
+        """Copy the journal to folder.
+
+        Args:
+            folder (str or pathlib.Path): folder to copy to (defaults to the
+            current folder).
+        """
+
+        logging.debug(f"duplicating journal to folder {folder}")
+        journal_name = pathlib.Path(self.file_name)
+        if not journal_name.is_file():
+            logging.info("No journal saved")
+            return
+        new_journal_name = journal_name.name
+        if folder is not None:
+            new_journal_name = pathlib.Path(folder) / new_journal_name
+        try:
+            shutil.copy(journal_name, new_journal_name)
+        except shutil.SameFileError:
+            logging.debug("same file exception encountered")
+
+    def to_file(
+        self,
+        file_name=None,
+        paginate=True,
+        to_project_folder=True,
+        duplicate_to_local_folder=True,
+    ):
+        """Saves a DataFrame with all the needed info about the experiment.
+
+        Args:
+            file_name (str or pathlib.Path): journal file name (.json or .xlsx)
+            paginate (bool): make project folders
+            to_project_folder (bool): save journal file to the folder containing your cellpy projects
+            duplicate_to_local_folder (bool): save journal file to the folder you are in now also
+
+        Returns:
+            None
+        """
+        file_name = self._check_file_name(
+            file_name, to_project_folder=to_project_folder
+        )
+
+        pages = self.pages
+        session = self.session
+        meta = self._prm_packer()
+        top_level_dict = {"info_df": pages, "metadata": meta, "session": session}
+
+        is_json = False
+        is_xlsx = False
+
+        if file_name.suffix == ".xlsx":
+            is_xlsx = True
+
+        if file_name.suffix == ".json":
+            is_json = True
+
+        if is_xlsx:
+            df_session = self._pack_session(session)
+            df_meta = self._pack_meta(meta)
+
+            try:
+                pages.index.name = "filename"
+                with pd.ExcelWriter(file_name, mode="w", engine="openpyxl") as writer:
+                    pages.to_excel(writer, sheet_name="pages", engine="openpyxl")
+                    # no index is not supported for multi-index (update to index=False when pandas implement it):
+                    df_session.to_excel(writer, sheet_name="session", engine="openpyxl")
+                    df_meta.to_excel(
+                        writer, sheet_name="meta", engine="openpyxl", index=False
+                    )
+            except PermissionError as e:
+                print(f"Could not load journal to xlsx ({e})")
+
+        if is_json:
+            jason_string = json.dumps(
+                top_level_dict,
+                default=lambda info_df: json.loads(
+                    info_df.to_json(default_handler=str)
+                ),
+            )
+
+            with open(file_name, "w") as outfile:
+                outfile.write(jason_string)
+
+        self.file_name = file_name
+        logging.info(f"Saved file to {file_name}")
+
+        if paginate:
+            self.paginate()
+
+        if duplicate_to_local_folder:
+            self.duplicate_journal()
+
+    @staticmethod
+    def _pack_session(session):
+        frames = []
+        keys = []
+        try:
+            l_bad_cycle_numbers = []
+
+            for k, v in session["bad_cycles"].items():
+                l_bad_cycle_numbers.append(pd.DataFrame(data=v, columns=[k]))
+
+            df_bad_cycle_numbers = (
+                pd.concat(l_bad_cycle_numbers, axis=1)
+                .melt(var_name="cell_name", value_name="cycle_index")
+                .dropna()
+            )
+            frames.append(df_bad_cycle_numbers)
+            keys.append("bad_cycles")
+        except (KeyError, AttributeError):
+            logging.debug("missing bad cycle numbers")
+
+        df_bad_cells = pd.DataFrame(session["bad_cells"], columns=["cell_name"])
+        frames.append(df_bad_cells)
+        keys.append("bad_cells")
+
+        df_starred = pd.DataFrame(session["starred"], columns=["cell_name"])
+        frames.append(df_starred)
+        keys.append("starred")
+
+        df_notes = pd.DataFrame(session["notes"], columns=["txt"])
+        frames.append(df_notes)
+        keys.append("notes")
+
+        session = pd.concat(frames, axis=1, keys=keys)
+        return session
+
+    @staticmethod
+    def _pack_meta(meta):
+        meta = pd.DataFrame(meta, index=[0]).melt(
+            var_name="parameter", value_name="value"
+        )
+        return meta
+
+    def generate_folder_names(self):
+        """Set appropriate folder names."""
+        logging.debug("creating folder names")
+        if self.project and isinstance(self.project, (pathlib.Path, str)):
+            logging.debug("got project name")
+            logging.debug(self.project)
+            self.project_dir = os.path.join(prms.Paths.outdatadir, self.project)
+        else:
+            logging.critical(
+                "Could not create project dir (missing project definition)"
+            )
+        if self.name:
+            self.batch_dir = os.path.join(self.project_dir, self.name)
+            self.raw_dir = os.path.join(self.batch_dir, "raw_data")
+        else:
+            logging.critical(
+                "Could not create batch_dir and raw_dir", "(missing batch name)"
+            )
+        logging.debug(f"batch dir: {self.batch_dir}")
+        logging.debug(f"project dir: {self.project_dir}")
+        logging.debug(f"raw dir: {self.raw_dir}")
+
+    def paginate(self):
+        """Make folders where we would like to put results etc."""
+
+        project_dir = self.project_dir
+        raw_dir = self.raw_dir
+        batch_dir = self.batch_dir
+
+        if project_dir is None:
+            raise UnderDefined("no project directory defined")
+        if raw_dir is None:
+            raise UnderDefined("no raw directory defined")
+        if batch_dir is None:
+            raise UnderDefined("no batch directory defined")
+
+        # create the folders
+        if not os.path.isdir(project_dir):
+            os.mkdir(project_dir)
+            logging.info(f"created folder {project_dir}")
+        if not os.path.isdir(batch_dir):
+            os.mkdir(batch_dir)
+            logging.info(f"created folder {batch_dir}")
+        if not os.path.isdir(raw_dir):
+            os.mkdir(raw_dir)
+            logging.info(f"created folder {raw_dir}")
+
+        self.project_dir = project_dir
+        self.batch_dir = batch_dir
+        self.raw_dir = raw_dir
+
+        return project_dir, batch_dir, raw_dir
+
+    def generate_file_name(self):
+        """generate a suitable file name for the experiment"""
+        if not self.project:
+            raise UnderDefined("project name not given")
+        out_data_dir = prms.Paths.outdatadir
+        project_dir = os.path.join(out_data_dir, self.project)
+        file_name = f"cellpy_batch_{self.name}.json"
+        self.file_name = os.path.join(project_dir, file_name)
+
+    # v.1.0.0:
+    def look_for_file(self):
+        pass
+
+    def get_column(self, header):
+        """populate new column from db"""
+        pass
+
+    def get_cell(self, id_key):
+        """get additional cell info from db"""
+        pass
+
+    def add_comment(self, comment):
+        """add a comment (will be saved in the journal file)"""
+        pass
+
+    def remove_comment(self, comment_id):
+        pass
+
+    def view_comments(self):
+        pass
+
+    def remove_cell(self, cell_id):
+        pass
+
+    def add_cell(self, cell_id, **kwargs):
+        """Add a cell to the pages"""
+        pass
+
+
+def _dev_journal_loading():
+    from cellpy import log
+
+    log.setup_logging(default_level="DEBUG")
+    journal_file = pathlib.Path(
+        "../../../testdata/batch_project/test_project.json"
+    ).resolve()
+    assert journal_file.is_file()
+
+    logging.debug(f"reading journal file {journal_file}")
+    journal = LabJournal(db_reader=None)
+    journal.from_file(journal_file, paginate=False)
+    print(80 * "-")
+    print(journal.pages)
+    print(80 * "-")
+    print(journal.session)
+
+    # creating a mock session
+    bad_cycle_numbers = {
+        "20160805_test001_45_cc": [4, 337, 338],
+        "20160805_test001_47_cc": [7, 8, 9],
+    }
+    bad_cells = ["20160805_test001_45_cc"]
+
+    notes = {"date_stamp": "one comment for the road", "date_stamp2": "another comment"}
+    session = {
+        "bad_cycle_numbers": bad_cycle_numbers,
+        "bad_cells": bad_cells,
+        "notes": notes,
+    }
+
+    # journal.session = session
+
+    new_journal_name = journal_file.with_name(f"{journal_file.stem}_tmp.xlsx")
+    print(new_journal_name)
+    journal.to_file(file_name=new_journal_name, paginate=False, to_project_folder=False)
+
+
+if __name__ == "__main__":
+    print(" running journal ".center(80, "-"))
+    _dev_journal_loading()
+    print(" finished ".center(80, "-"))
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/batch_plotters.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/batch_plotters.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/dumpers.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/dumpers.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/engines.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/engines.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,284 +1,284 @@
-"""Engines are functions that are used by the Do-ers.
-
-    Keyword Args: experiments, farms, barn, optionals
-    Returns: farms, barn
-"""
-
-import logging
-import time
-import warnings
-
-import pandas as pd
-
-from cellpy import dbreader
-from cellpy.parameters.internal_settings import get_headers_journal, get_headers_summary
-from cellpy.utils.batch_tools import batch_helpers as helper
-
-hdr_journal = get_headers_journal()
-hdr_summary = get_headers_summary()
-
-SELECTED_SUMMARIES = [
-    hdr_summary["discharge_capacity_gravimetric"],
-    hdr_summary["charge_capacity_gravimetric"],
-    hdr_summary["coulombic_efficiency"],
-    hdr_summary["cumulated_coulombic_efficiency"],
-    hdr_summary["ir_discharge"],
-    hdr_summary["ir_charge"],
-    hdr_summary["end_voltage_discharge"],
-    hdr_summary["end_voltage_charge"],
-    hdr_summary["charge_c_rate"],
-    hdr_summary["discharge_c_rate"],
-]
-
-
-def cycles_engine(**kwargs):
-    """engine to extract cycles"""
-    logging.debug("cycles_engine::Not finished yet (sorry).")
-    warnings.warn(
-        "This utility function will be seriously changed soon and possibly removed",
-        category=DeprecationWarning,
-    )
-    # raise NotImplementedError
-
-    experiments = kwargs["experiments"]
-
-    farms = []
-    barn = "raw_dir"  # Its a murder in the red barn - murder in the red barn
-
-    for experiment in experiments:
-        farms.append([])
-        if experiment.all_in_memory:
-            logging.debug("all in memory")
-            for key in experiment.cell_data_frames:
-                logging.debug(f"extracting cycles from {key}")
-                # extract cycles here and send it to the farm
-        else:
-            logging.debug("dont have it in memory - need to lookup in the files")
-            for key in experiment.cell_data_frames:
-                logging.debug(f"looking up cellpyfile for {key}")
-                # extract cycles here and send it to the farm
-
-    return farms, barn
-
-
-def raw_data_engine(**kwargs):
-    """engine to extract raw data"""
-    warnings.warn(
-        "This utility function will be seriously changed soon and possibly removed",
-        category=DeprecationWarning,
-    )
-    logging.debug("cycles_engine")
-    farms = None
-    barn = "raw_dir"
-    raise NotImplementedError
-
-
-def summary_engine(**kwargs):
-    """engine to extract summary data"""
-    logging.debug("summary_engine")
-    # farms = kwargs["farms"]
-
-    farms = []
-    experiments = kwargs.pop("experiments")
-    reset = kwargs.pop("reset", False)
-
-    for experiment in experiments:
-        if experiment.selected_summaries is None:
-            selected_summaries = SELECTED_SUMMARIES
-        else:
-            selected_summaries = experiment.selected_summaries
-
-        if reset or experiment.summary_frames is None:
-            logging.debug("No summary frames found")
-            logging.debug("Re-loading")
-            experiment.summary_frames = _load_summaries(experiment)
-
-        farm = helper.join_summaries(experiment.summary_frames, selected_summaries)
-        farms.append(farm)
-    barn = "batch_dir"
-
-    return farms, barn
-
-
-def _load_summaries(experiment):
-    summary_frames = {}
-    for label in experiment.cell_names:
-        # TODO: replace this with direct lookup from hdf5?
-        summary_frames[label] = experiment.data[label].data.summary
-    return summary_frames
-
-
-def dq_dv_engine(**kwargs):
-    """engine that performs incremental analysis of the cycle-data"""
-    warnings.warn(
-        "This utility function will be seriously changed soon and possibly removed",
-        category=DeprecationWarning,
-    )
-    farms = None
-    barn = "raw_dir"
-    raise NotImplementedError
-
-
-def _query(reader_method, cell_ids, column_name=None):
-    if not any(cell_ids):
-        logging.debug("Received empty cell_ids")
-        return []
-
-    try:
-        if column_name is None:
-            result = [reader_method(cell_id) for cell_id in cell_ids]
-        else:
-            result = [reader_method(column_name, cell_id) for cell_id in cell_ids]
-    except Exception as e:
-        logging.debug(f"Error in querying db.")
-        logging.debug(e)
-        result = [None for _ in range(len(cell_ids))]
-    return result
-
-
-def sql_db_engine(*args, **kwargs) -> pd.DataFrame:
-    print("sql_db_engine")
-    print(f"args: {args}")
-    print(f"kwargs: {kwargs}")
-    return pd.DataFrame()
-
-
-# TODO-246: load area
-def simple_db_engine(
-    reader=None,
-    cell_ids=None,
-    file_list=None,
-    pre_path=None,
-    include_key=False,
-    include_individual_arguments=True,
-    additional_column_names=None,
-    batch_name=None,
-    **kwargs,
-):
-    """Engine that gets values from the db for given set of cell IDs.
-
-    The simple_db_engine looks up values for mass, names, etc. from
-    the db using the reader object. In addition, it searches for the
-    corresponding raw files / data.
-
-    Args:
-        reader: a reader object (defaults to dbreader.Reader)
-        cell_ids: keys (cell IDs) (assumes that the db has already been filtered, if not, use batch_name).
-        file_list: file list to send to filefinder (instead of searching in folders for files).
-        pre_path: prepended path to send to filefinder.
-        include_key: include the key col in the pages (the cell IDs).
-        include_individual_arguments: include the argument column in the pages.
-        additional_column_names: list of additional column names to include in the pages.
-        batch_name: name of the batch (used if cell_ids are not given)
-        **kwargs: sent to filefinder
-
-    Returns:
-        pages (pandas.DataFrame)
-    """
-
-    new_version = False
-
-    # This is not really a proper Do-er engine. But not sure where to put it.
-    logging.debug("simple_db_engine")
-    if reader is None:
-        reader = dbreader.Reader()
-        logging.debug("No reader provided. Creating one myself.")
-
-    if cell_ids is None:
-        pages_dict = reader.from_batch(
-            batch_name=batch_name,
-            include_key=include_key,
-            include_individual_arguments=include_individual_arguments,
-        )
-
-    else:
-        pages_dict = dict()
-        pages_dict[hdr_journal["filename"]] = _query(reader.get_cell_name, cell_ids)
-        if include_key:
-            pages_dict[hdr_journal["id_key"]] = cell_ids
-
-        if include_individual_arguments:
-            pages_dict[hdr_journal["argument"]] = _query(reader.get_args, cell_ids)
-
-        pages_dict[hdr_journal["mass"]] = _query(reader.get_mass, cell_ids)
-        pages_dict[hdr_journal["total_mass"]] = _query(reader.get_total_mass, cell_ids)
-        pages_dict[hdr_journal["loading"]] = _query(reader.get_loading, cell_ids)
-        pages_dict[hdr_journal["nom_cap"]] = _query(reader.get_nom_cap, cell_ids)
-        pages_dict[hdr_journal["area"]] = _query(reader.get_area, cell_ids)
-        pages_dict[hdr_journal["experiment"]] = _query(
-            reader.get_experiment_type, cell_ids
-        )
-        pages_dict[hdr_journal["fixed"]] = _query(reader.inspect_hd5f_fixed, cell_ids)
-        pages_dict[hdr_journal["label"]] = _query(reader.get_label, cell_ids)
-        pages_dict[hdr_journal["cell_type"]] = _query(reader.get_cell_type, cell_ids)
-        pages_dict[hdr_journal["instrument"]] = _query(reader.get_instrument, cell_ids)
-        pages_dict[hdr_journal["raw_file_names"]] = []
-        pages_dict[hdr_journal["cellpy_file_name"]] = []
-        pages_dict[hdr_journal["comment"]] = _query(reader.get_comment, cell_ids)
-        pages_dict[hdr_journal["group"]] = _query(reader.get_group, cell_ids)
-
-        if additional_column_names is not None:
-            for k in additional_column_names:
-                try:
-                    pages_dict[k] = _query(reader.get_by_column_label, cell_ids, k)
-                except Exception as e:
-                    logging.info(f"Could not retrieve from column {k} ({e})")
-
-        logging.debug(f"created info-dict from {reader.db_file}:")
-
-    for key in list(pages_dict.keys()):
-        logging.debug("%s: %s" % (key, str(pages_dict[key])))
-
-    _groups = pages_dict[hdr_journal["group"]]
-    groups = helper.fix_groups(_groups)
-    pages_dict[hdr_journal["group"]] = groups
-
-    my_timer_start = time.time()
-    pages_dict = helper.find_files(
-        pages_dict, file_list=file_list, pre_path=pre_path, **kwargs
-    )
-    my_timer_end = time.time()
-    if (my_timer_end - my_timer_start) > 5.0:
-        logging.critical(
-            "The function _find_files was very slow. "
-            "Save your journal so you don't have to run it again! "
-            "You can load it again using the from_journal(journal_name) method."
-        )
-
-    pages = pd.DataFrame(pages_dict)
-    try:
-        pages = pages.sort_values([hdr_journal.group, hdr_journal.filename])
-    except TypeError as e:
-        _report_suspected_duplicate_id(
-            e,
-            "sort the values",
-            pages[[hdr_journal.group, hdr_journal.filename]],
-        )
-
-    pages = helper.make_unique_groups(pages)
-
-    try:
-        pages[hdr_journal.label] = pages[hdr_journal.filename].apply(
-            helper.create_labels
-        )
-    except AttributeError as e:
-        _report_suspected_duplicate_id(
-            e, "make labels", pages[[hdr_journal.label, hdr_journal.filename]]
-        )
-
-    else:
-        # TODO: check if drop=False works [#index]
-        pages.set_index(hdr_journal["filename"], inplace=True)  # edit this to allow for
-        # non-numeric index-names (for tab completion and python-box)
-    return pages
-
-
-def _report_suspected_duplicate_id(e, what="do it", on=None):
-    logging.warning(f"could not {what}")
-    logging.warning(f"{on}")
-    logging.warning("maybe you have a corrupted db?")
-    logging.warning(
-        "typically happens if the cell_id is not unique (several rows or records in "
-        "your db has the same cell_id or key)"
-    )
-    logging.warning(e)
+"""Engines are functions that are used by the Do-ers.
+
+    Keyword Args: experiments, farms, barn, optionals
+    Returns: farms, barn
+"""
+
+import logging
+import time
+import warnings
+
+import pandas as pd
+
+from cellpy import dbreader
+from cellpy.parameters.internal_settings import get_headers_journal, get_headers_summary
+from cellpy.utils.batch_tools import batch_helpers as helper
+
+hdr_journal = get_headers_journal()
+hdr_summary = get_headers_summary()
+
+SELECTED_SUMMARIES = [
+    hdr_summary["discharge_capacity_gravimetric"],
+    hdr_summary["charge_capacity_gravimetric"],
+    hdr_summary["coulombic_efficiency"],
+    hdr_summary["cumulated_coulombic_efficiency"],
+    hdr_summary["ir_discharge"],
+    hdr_summary["ir_charge"],
+    hdr_summary["end_voltage_discharge"],
+    hdr_summary["end_voltage_charge"],
+    hdr_summary["charge_c_rate"],
+    hdr_summary["discharge_c_rate"],
+]
+
+
+def cycles_engine(**kwargs):
+    """engine to extract cycles"""
+    logging.debug("cycles_engine::Not finished yet (sorry).")
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
+    # raise NotImplementedError
+
+    experiments = kwargs["experiments"]
+
+    farms = []
+    barn = "raw_dir"  # Its a murder in the red barn - murder in the red barn
+
+    for experiment in experiments:
+        farms.append([])
+        if experiment.all_in_memory:
+            logging.debug("all in memory")
+            for key in experiment.cell_data_frames:
+                logging.debug(f"extracting cycles from {key}")
+                # extract cycles here and send it to the farm
+        else:
+            logging.debug("dont have it in memory - need to lookup in the files")
+            for key in experiment.cell_data_frames:
+                logging.debug(f"looking up cellpyfile for {key}")
+                # extract cycles here and send it to the farm
+
+    return farms, barn
+
+
+def raw_data_engine(**kwargs):
+    """engine to extract raw data"""
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
+    logging.debug("cycles_engine")
+    farms = None
+    barn = "raw_dir"
+    raise NotImplementedError
+
+
+def summary_engine(**kwargs):
+    """engine to extract summary data"""
+    logging.debug("summary_engine")
+    # farms = kwargs["farms"]
+
+    farms = []
+    experiments = kwargs.pop("experiments")
+    reset = kwargs.pop("reset", False)
+
+    for experiment in experiments:
+        if experiment.selected_summaries is None:
+            selected_summaries = SELECTED_SUMMARIES
+        else:
+            selected_summaries = experiment.selected_summaries
+
+        if reset or experiment.summary_frames is None:
+            logging.debug("No summary frames found")
+            logging.debug("Re-loading")
+            experiment.summary_frames = _load_summaries(experiment)
+
+        farm = helper.join_summaries(experiment.summary_frames, selected_summaries)
+        farms.append(farm)
+    barn = "batch_dir"
+
+    return farms, barn
+
+
+def _load_summaries(experiment):
+    summary_frames = {}
+    for label in experiment.cell_names:
+        # TODO: replace this with direct lookup from hdf5?
+        summary_frames[label] = experiment.data[label].data.summary
+    return summary_frames
+
+
+def dq_dv_engine(**kwargs):
+    """engine that performs incremental analysis of the cycle-data"""
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
+    farms = None
+    barn = "raw_dir"
+    raise NotImplementedError
+
+
+def _query(reader_method, cell_ids, column_name=None):
+    if not any(cell_ids):
+        logging.debug("Received empty cell_ids")
+        return []
+
+    try:
+        if column_name is None:
+            result = [reader_method(cell_id) for cell_id in cell_ids]
+        else:
+            result = [reader_method(column_name, cell_id) for cell_id in cell_ids]
+    except Exception as e:
+        logging.debug(f"Error in querying db.")
+        logging.debug(e)
+        result = [None for _ in range(len(cell_ids))]
+    return result
+
+
+def sql_db_engine(*args, **kwargs) -> pd.DataFrame:
+    print("sql_db_engine")
+    print(f"args: {args}")
+    print(f"kwargs: {kwargs}")
+    return pd.DataFrame()
+
+
+# TODO-246: load area
+def simple_db_engine(
+    reader=None,
+    cell_ids=None,
+    file_list=None,
+    pre_path=None,
+    include_key=False,
+    include_individual_arguments=True,
+    additional_column_names=None,
+    batch_name=None,
+    **kwargs,
+):
+    """Engine that gets values from the db for given set of cell IDs.
+
+    The simple_db_engine looks up values for mass, names, etc. from
+    the db using the reader object. In addition, it searches for the
+    corresponding raw files / data.
+
+    Args:
+        reader: a reader object (defaults to dbreader.Reader)
+        cell_ids: keys (cell IDs) (assumes that the db has already been filtered, if not, use batch_name).
+        file_list: file list to send to filefinder (instead of searching in folders for files).
+        pre_path: prepended path to send to filefinder.
+        include_key: include the key col in the pages (the cell IDs).
+        include_individual_arguments: include the argument column in the pages.
+        additional_column_names: list of additional column names to include in the pages.
+        batch_name: name of the batch (used if cell_ids are not given)
+        **kwargs: sent to filefinder
+
+    Returns:
+        pages (pandas.DataFrame)
+    """
+
+    new_version = False
+
+    # This is not really a proper Do-er engine. But not sure where to put it.
+    logging.debug("simple_db_engine")
+    if reader is None:
+        reader = dbreader.Reader()
+        logging.debug("No reader provided. Creating one myself.")
+
+    if cell_ids is None:
+        pages_dict = reader.from_batch(
+            batch_name=batch_name,
+            include_key=include_key,
+            include_individual_arguments=include_individual_arguments,
+        )
+
+    else:
+        pages_dict = dict()
+        pages_dict[hdr_journal["filename"]] = _query(reader.get_cell_name, cell_ids)
+        if include_key:
+            pages_dict[hdr_journal["id_key"]] = cell_ids
+
+        if include_individual_arguments:
+            pages_dict[hdr_journal["argument"]] = _query(reader.get_args, cell_ids)
+
+        pages_dict[hdr_journal["mass"]] = _query(reader.get_mass, cell_ids)
+        pages_dict[hdr_journal["total_mass"]] = _query(reader.get_total_mass, cell_ids)
+        pages_dict[hdr_journal["loading"]] = _query(reader.get_loading, cell_ids)
+        pages_dict[hdr_journal["nom_cap"]] = _query(reader.get_nom_cap, cell_ids)
+        pages_dict[hdr_journal["area"]] = _query(reader.get_area, cell_ids)
+        pages_dict[hdr_journal["experiment"]] = _query(
+            reader.get_experiment_type, cell_ids
+        )
+        pages_dict[hdr_journal["fixed"]] = _query(reader.inspect_hd5f_fixed, cell_ids)
+        pages_dict[hdr_journal["label"]] = _query(reader.get_label, cell_ids)
+        pages_dict[hdr_journal["cell_type"]] = _query(reader.get_cell_type, cell_ids)
+        pages_dict[hdr_journal["instrument"]] = _query(reader.get_instrument, cell_ids)
+        pages_dict[hdr_journal["raw_file_names"]] = []
+        pages_dict[hdr_journal["cellpy_file_name"]] = []
+        pages_dict[hdr_journal["comment"]] = _query(reader.get_comment, cell_ids)
+        pages_dict[hdr_journal["group"]] = _query(reader.get_group, cell_ids)
+
+        if additional_column_names is not None:
+            for k in additional_column_names:
+                try:
+                    pages_dict[k] = _query(reader.get_by_column_label, cell_ids, k)
+                except Exception as e:
+                    logging.info(f"Could not retrieve from column {k} ({e})")
+
+        logging.debug(f"created info-dict from {reader.db_file}:")
+
+    for key in list(pages_dict.keys()):
+        logging.debug("%s: %s" % (key, str(pages_dict[key])))
+
+    _groups = pages_dict[hdr_journal["group"]]
+    groups = helper.fix_groups(_groups)
+    pages_dict[hdr_journal["group"]] = groups
+
+    my_timer_start = time.time()
+    pages_dict = helper.find_files(
+        pages_dict, file_list=file_list, pre_path=pre_path, **kwargs
+    )
+    my_timer_end = time.time()
+    if (my_timer_end - my_timer_start) > 5.0:
+        logging.critical(
+            "The function _find_files was very slow. "
+            "Save your journal so you don't have to run it again! "
+            "You can load it again using the from_journal(journal_name) method."
+        )
+
+    pages = pd.DataFrame(pages_dict)
+    try:
+        pages = pages.sort_values([hdr_journal.group, hdr_journal.filename])
+    except TypeError as e:
+        _report_suspected_duplicate_id(
+            e,
+            "sort the values",
+            pages[[hdr_journal.group, hdr_journal.filename]],
+        )
+
+    pages = helper.make_unique_groups(pages)
+
+    try:
+        pages[hdr_journal.label] = pages[hdr_journal.filename].apply(
+            helper.create_labels
+        )
+    except AttributeError as e:
+        _report_suspected_duplicate_id(
+            e, "make labels", pages[[hdr_journal.label, hdr_journal.filename]]
+        )
+
+    else:
+        # TODO: check if drop=False works [#index]
+        pages.set_index(hdr_journal["filename"], inplace=True)  # edit this to allow for
+        # non-numeric index-names (for tab completion and python-box)
+    return pages
+
+
+def _report_suspected_duplicate_id(e, what="do it", on=None):
+    logging.warning(f"could not {what}")
+    logging.warning(f"{on}")
+    logging.warning("maybe you have a corrupted db?")
+    logging.warning(
+        "typically happens if the cell_id is not unique (several rows or records in "
+        "your db has the same cell_id or key)"
+    )
+    logging.warning(e)
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/batch_tools/sqlite_from_excel_db.py` & `cellpy-1.0.0b1/cellpy/utils/batch_tools/sqlite_from_excel_db.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/collectors.py` & `cellpy-1.0.0b1/cellpy/utils/collectors.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,1848 +1,1848 @@
-"""Collectors are used for simplifying plotting and exporting batch objects."""
-
-import functools
-import inspect
-import logging
-import math
-from pprint import pprint
-from pathlib import Path
-import textwrap
-from typing import Any
-import time
-from itertools import count
-from multiprocessing import Process
-
-import pandas as pd
-import plotly.express as px
-import plotly.io as pio
-import plotly.graph_objects as go
-import numpy as np
-
-import cellpy
-from cellpy.readers.core import group_by_interpolate
-from cellpy.utils.batch import Batch
-from cellpy.utils.helpers import concatenate_summaries
-from cellpy.utils.plotutils import plot_concatenated
-from cellpy.utils import ica
-
-DEFAULT_CYCLES = [1, 10, 20]
-
-CELLPY_MINIMUM_VERSION = "1.0.0"
-PLOTLY_BASE_TEMPLATE = "seaborn"
-IMAGE_TO_FILE_TIMEOUT = 30
-
-px_template_all_axis_shown = dict(
-    xaxis=dict(
-        linecolor="rgb(36,36,36)",
-        mirror=True,
-        showline=True,
-        zeroline=False,
-        title={"standoff": 15},
-    ),
-    yaxis=dict(
-        linecolor="rgb(36,36,36)",
-        mirror=True,
-        showline=True,
-        zeroline=False,
-        title={"standoff": 15},
-    ),
-)
-
-fig_pr_cell_template = go.layout.Template(
-    # layout=px_template_all_axis_shown
-)
-
-fig_pr_cycle_template = go.layout.Template(
-    # layout=px_template_all_axis_shown
-)
-
-film_template = go.layout.Template(
-    # layout=px_template_all_axis_shown
-)
-
-summary_template = go.layout.Template(
-    # layout=px_template_all_axis_shown
-)
-
-
-def _setup():
-    _welcome_message()
-
-
-def _welcome_message():
-    cellpy_version = cellpy.__version__
-    logging.info(f"cellpy version: {cellpy_version}")
-    logging.info(f"collectors need at least: {CELLPY_MINIMUM_VERSION}")
-
-
-_setup()
-
-
-class BatchCollector:
-    collector_name: str = None
-    data: pd.DataFrame = None
-    figure: Any = None
-    name: str = None
-    nick: str = None
-    autorun: bool = True
-    figure_directory: Path = Path("out")
-    data_directory: Path = Path("data/processed/")
-    renderer: Any = None
-    units: dict = None
-
-    # override default arguments:
-    elevated_data_collector_arguments: dict = None
-    elevated_plotter_arguments: dict = None
-
-    # defaults (and used also when resetting):
-    _default_data_collector_arguments = {}
-    _default_plotter_arguments = {}
-
-    def __init__(
-        self,
-        b,
-        data_collector,
-        plotter,
-        collector_name=None,
-        name=None,
-        nick=None,
-        autorun=True,
-        backend="plotly",
-        elevated_data_collector_arguments=None,
-        elevated_plotter_arguments=None,
-        data_collector_arguments: dict = None,
-        plotter_arguments: dict = None,
-        **kwargs,
-    ):
-        """Update both the collected data and the plot(s).
-        Args:
-            b (cellpy.utils.Batch): the batch object.
-            data_collector (callable): method that collects the data.
-            plotter (callable): method that crates the plots.
-            collector_name (str): name of collector.
-            name (str or bool): name used for auto-generating filenames etc.
-            autorun (bool): run collector and plotter immediately if True.
-            use_templates (bool): also apply template(s) in autorun mode if True.
-            backend (str): name of plotting backend to use ("plotly" or "matplotlib").
-            elevated_data_collector_arguments (dict): arguments picked up by the child class' initializer.
-            elevated_plotter_arguments (dict): arguments picked up by the child class' initializer.
-            data_collector_arguments (dict): keyword arguments sent to the data collector.
-            plotter_arguments (dict): keyword arguments sent to the plotter.
-            update_name (bool): update the name (using automatic name generation) based on new settings.
-            **kwargs: set Collector attributes.
-        """
-        self.b = b
-        self.data_collector = data_collector
-        self.plotter = plotter
-        self.nick = nick
-        self.backend = backend
-        self.collector_name = collector_name or "base"
-
-        # Arguments given as default arguments in the subclass have "low" priority (below elevated arguments at least):
-        self._data_collector_arguments = self._default_data_collector_arguments.copy()
-        self._plotter_arguments = self._default_plotter_arguments.copy()
-        self._update_arguments(data_collector_arguments, plotter_arguments)
-
-        # Elevated arguments have preference above the data_collector and plotter argument dicts:
-        self._parse_elevated_arguments(
-            elevated_data_collector_arguments, elevated_plotter_arguments
-        )
-
-        self._set_attributes(**kwargs)
-
-        self._set_plotly_templates()
-
-        if nick is None:
-            self.nick = b.name
-
-        if name is None:
-            name = self.generate_name()
-        self.name = name
-
-        self.parse_units()
-
-        if autorun:
-            self.update(update_name=False)
-
-    @staticmethod
-    def _set_plotly_templates():
-        pio.templates.default = PLOTLY_BASE_TEMPLATE
-        pio.templates["fig_pr_cell"] = fig_pr_cell_template
-        pio.templates["fig_pr_cycle"] = fig_pr_cycle_template
-        pio.templates["film"] = film_template
-        pio.templates["summary"] = summary_template
-
-    @property
-    def data_collector_arguments(self):
-        return self._data_collector_arguments
-
-    @data_collector_arguments.setter
-    def data_collector_arguments(self, argument_dict: dict):
-        if argument_dict is not None:
-            self._data_collector_arguments = {
-                **self._data_collector_arguments,
-                **argument_dict,
-            }
-
-    @property
-    def plotter_arguments(self):
-        return self._plotter_arguments
-
-    @plotter_arguments.setter
-    def plotter_arguments(self, argument_dict: dict):
-        if argument_dict is not None:
-            self._plotter_arguments = {**self._plotter_arguments, **argument_dict}
-
-    def _attr_text(self, bullet_start=" - ", sep="\n"):
-        txt = f"{bullet_start}collector_name: {self.collector_name}" + sep
-        txt += f"{bullet_start}autorun: {self.autorun}" + sep
-        txt += f"{bullet_start}name: {self.name}" + sep
-        txt += f"{bullet_start}nick: {self.nick}" + sep
-        txt += f"{bullet_start}csv_include_index: {self.csv_include_index}" + sep
-        txt += f"{bullet_start}csv_layout: {self.csv_layout}" + sep
-        txt += f"{bullet_start}sep: {self.sep}" + sep
-        txt += f"{bullet_start}backend: {self.backend}" + sep
-        txt += f"{bullet_start}toolbar: {self.toolbar}" + sep
-        txt += f"{bullet_start}figure_directory: {self.figure_directory}" + sep
-        txt += f"{bullet_start}data_directory: {self.data_directory}" + sep
-        txt += f"{bullet_start}batch-instance: {self.b.name}" + sep
-        txt += (
-            f"{bullet_start}data_collector_arguments: {self.data_collector_arguments}"
-            + sep
-        )
-        txt += f"{bullet_start}plotter_arguments: {self.plotter_arguments}" + sep
-        return txt
-
-    def _attr_data_collector(self, h1="", h2="", sep="\n"):
-        data_name = self.data_collector.__name__
-        data_sig = inspect.signature(self.data_collector)
-        data_doc = inspect.getdoc(self.data_collector)
-        txt = f"{h1}{data_name}"
-        txt = f"{txt}{data_sig}{h2}{sep}"
-        txt = f"{txt}{sep}{data_doc}{sep}"
-        return txt
-
-    def _attr_plotter(self, h1="", h2="", sep="\n"):
-        plotter_name = self.plotter.__name__
-        plotter_sig = inspect.signature(self.plotter)
-        plotter_doc = inspect.getdoc(self.plotter)
-        txt = f"{h1}{plotter_name}"
-        txt = f"{txt}{plotter_sig}{h2}{sep}"
-        txt = f"{txt}{sep}{plotter_doc}{sep}"
-        return txt
-
-    def __str__(self):
-        class_name = self.__class__.__name__
-        txt = f"{class_name}\n{len(class_name) * '='}\n\n"
-        txt += "Attributes:\n"
-        txt += "-----------\n"
-        txt += self._attr_text(sep="\n")
-
-        txt += "\nfigure:\n"
-        txt += ".......\n"
-        fig_txt = f"{self.figure}"
-        if isinstance(fig_txt, str) and len(fig_txt) > 500:
-            fig_txt = fig_txt[0:500] + "\n ..."
-        txt += f"{fig_txt}\n"
-
-        txt += "\ndata:\n"
-        txt += ".....\n"
-        txt += f"{self.data}\n"
-
-        txt += "\nData collector:\n"
-        txt += "---------------\n"
-        txt += self._attr_data_collector(sep="\n")
-
-        txt += "\nPlotter:\n"
-        txt += "--------\n"
-        txt += self._attr_plotter(sep="\n")
-        return txt
-
-    def _repr_html_(self):
-        class_name = self.__class__.__name__
-        txt = f"<h2>{class_name}</h2> id={hex(id(self))}"
-        txt += f"<h3>Attributes:</h3>"
-        txt += "<ul>"
-        txt += self._attr_text(bullet_start="<li><code>", sep="</code></li>")
-        txt += "</ul>"
-        txt += f"<h3>Figure:</h3><code>"
-
-        fig_txt = f"{self.figure}"
-        if isinstance(fig_txt, str) and len(fig_txt) > 500:
-            fig_txt = fig_txt[0:500] + "<br>..."
-        txt += f"{fig_txt}<br></code>"
-
-        txt += f"<h3>Data:</h3>"
-        if hasattr(self.data, "_repr_html_"):
-            txt += self.data._repr_html_()
-        else:
-            txt += "NONE"
-        txt += "<br>"
-        txt += f"<h3>Data Collector:</h3><blockquote>"
-        txt += self._attr_data_collector(h1="<b>", h2="</b>", sep="<br>")
-        txt += f"</blockquote><h3>Plotter:</h3><blockquote>"
-        txt += self._attr_plotter(h1="<b>", h2="</b>", sep="<br>")
-        txt += "</blockquote>"
-        return txt
-
-    def _set_attributes(self, **kwargs):
-        self.sep = kwargs.get("sep", ";")
-        self.csv_include_index = kwargs.get("csv_include_index", True)
-        self.csv_layout = kwargs.get("csv_layout", "long")
-        self.dpi = kwargs.get("dpi", 200)
-        self.toolbar = kwargs.get("toolbar", True)
-
-    def generate_name(self):
-        names = ["collector", self.collector_name]
-        if self.nick:
-            names.insert(0, self.nick)
-        name = "_".join(names)
-        return name
-
-    def render(self):
-        self.figure = self.plotter(
-            self.data,
-            journal=self.b.journal,
-            units=self.units,
-            **self.plotter_arguments,
-        )
-
-    def _parse_elevated_arguments(
-        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
-    ):
-        if data_collector_arguments is not None:
-            logging.info(f"Updating elevated arguments")
-            elevated_data_collector_arguments = {}
-            for k, v in data_collector_arguments.items():
-                if v is not None:
-                    elevated_data_collector_arguments[k] = v
-            self._update_arguments(
-                elevated_data_collector_arguments, None, set_as_defaults=True
-            )
-
-        if plotter_arguments is not None:
-            logging.info(f"Updating elevated arguments")
-            elevated_plotter_arguments = {}
-            for k, v in plotter_arguments.items():
-                if v is not None:
-                    elevated_plotter_arguments[k] = v
-
-            self._update_arguments(
-                None, elevated_plotter_arguments, set_as_defaults=True
-            )
-
-    def _update_arguments(
-        self,
-        data_collector_arguments: dict = None,
-        plotter_arguments: dict = None,
-        set_as_defaults=False,
-    ):
-        self.data_collector_arguments = data_collector_arguments
-        self.plotter_arguments = plotter_arguments
-        self._check_plotter_arguments()
-
-        logging.info(f"**data_collector_arguments: {self.data_collector_arguments}")
-        logging.info(f"**plotter_arguments: {self.plotter_arguments}")
-
-        # setting defaults also (py3.6 compatible):
-        if set_as_defaults:
-            logging.info("updating defaults for current instance")
-            if data_collector_arguments is not None:
-                self._default_data_collector_arguments = {
-                    **self._default_data_collector_arguments,
-                    **data_collector_arguments,
-                }
-            if plotter_arguments is not None:
-                self._default_plotter_arguments = {
-                    **self._default_plotter_arguments,
-                    **plotter_arguments,
-                }
-
-    def _check_plotter_arguments(self):
-        if "plot_type" in self.plotter_arguments:
-            print(
-                "WARNING - using possible difficult option (future versions will fix this)"
-            )
-            print("*** 'plot_type' TRANSLATED TO 'method'")
-            self.plotter_arguments["method"] = self.plotter_arguments.pop("plot_type")
-
-    def reset_arguments(
-        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
-    ):
-        """Reset the arguments to the defaults.
-        Args:
-            data_collector_arguments (dict): optional additional keyword arguments for the data collector.
-            plotter_arguments (dict): optional additional keyword arguments for the plotter.
-        """
-        self._data_collector_arguments = self._default_data_collector_arguments.copy()
-        self._plotter_arguments = self._default_plotter_arguments.copy()
-        self._update_arguments(data_collector_arguments, plotter_arguments)
-
-    def parse_units(self, **kwargs):
-        """Look through your cellpy objects and search for units."""
-        b = self.b
-        c_units = []
-        r_units = []
-        c_unit = None
-        r_unit = None
-        for c in b:
-            cu = c.cellpy_units
-            if cu != c_unit:
-                c_unit = cu
-                c_units.append(cu)
-
-            ru = c.raw_units
-            if ru != r_unit:
-                r_unit = ru
-                r_units.append(ru)
-        if len(c_units) > 1:
-            print("WARNING: non-homogenous units found: cellpy_units")
-        if len(r_units) > 1:
-            print("WARNING: non-homogenous units found: raw_units")
-        raw_units = r_units[0]
-        cellpy_units = c_units[0]
-        self.units = dict(raw_units=raw_units, cellpy_units=cellpy_units)
-
-    def update(
-        self,
-        data_collector_arguments: dict = None,
-        plotter_arguments: dict = None,
-        reset: bool = False,
-        update_data: bool = False,
-        update_name: bool = False,
-        update_plot: bool = True,
-    ):
-        """Update both the collected data and the plot(s).
-        Args:
-            data_collector_arguments (dict): keyword arguments sent to the data collector.
-            plotter_arguments (dict): keyword arguments sent to the plotter.
-            reset (bool): reset the arguments first.
-            update_data (bool): update the data before updating the plot even if data has been collected before.
-            update_name (bool): update the name (using automatic name generation) based on new settings.
-            update_plot (bool): update the plot.
-        """
-        if reset:
-            self.reset_arguments(data_collector_arguments, plotter_arguments)
-        else:
-            self._update_arguments(data_collector_arguments, plotter_arguments)
-        if update_data or self.data is None:
-            try:
-                self.data = self.data_collector(self.b, **self.data_collector_arguments)
-            except TypeError as e:
-                print("Type error:", e)
-                print("Registered data_collector_arguments:")
-                pprint(self.data_collector_arguments)
-                print("Hint: fix it and then re-run using reset=True")
-                return
-        if update_plot:
-            try:
-                self.render()
-            except TypeError as e:
-                print("Type error:", e)
-                print("Registered plotter_arguments:")
-                pprint(self.plotter_arguments)
-                print("Hint: fix it and then re-run using reset=True")
-                return
-
-        if update_name:
-            self.name = self.generate_name()
-
-    def _figure_valid(self):
-        # TODO: create a decorator
-        if self.figure is None:
-            print("No figure to show!")
-            return False
-        return True
-
-    def show(self, **kwargs):
-        """Show the figure.
-
-        Note that show returns the `figure` object and  if the `backend` used
-        does not provide automatic rendering in the editor / running environment you
-        are using, you might have to issue the rendering yourself. For example, if you
-        are using `plotly` and running it as a script in a typical command shell,
-        you will have to issue `.show()` on the returned `figure` object.
-
-        Args:
-            **kwargs: sent to the plotter.
-
-        Returns:
-            Figure object
-        """
-        if not self._figure_valid():
-            return
-
-        print(f"figure name: {self.name}")
-        if kwargs:
-            logging.info(f"updating figure with {kwargs}")
-            self._update_arguments(plotter_arguments=kwargs)
-            self.render()
-        return self.figure
-
-    def preprocess_data_for_csv(self):
-        logging.debug(f"the data layout {self.csv_layout} is not supported yet!")
-        return self.data
-
-    def to_csv(self, serial_number=None):
-        filename = self._output_path(serial_number)
-        filename = filename.with_suffix(".csv")
-        if self.csv_layout != "long":
-            data = self.preprocess_data_for_csv()
-        else:
-            data = self.data
-
-        data.to_csv(
-            filename,
-            sep=self.sep,
-            index=self.csv_include_index,
-        )
-        print(f"saved csv file: {filename}")
-
-    def _image_exporter_plotly(self, filename, timeout=IMAGE_TO_FILE_TIMEOUT, **kwargs):
-        p = Process(
-            target=self.figure.write_image,
-            args=(filename,),
-            name="save_plotly_image_to_file",
-            kwargs=kwargs,
-        )
-        p.start()
-        p.join(timeout=timeout)
-        p.terminate()
-        if p.exitcode is None:
-            print(f"Oops, {p} timeouts! Could not save {filename}")
-        if p.exitcode == 0:
-            print(f"saved image file: {filename}")
-
-    def to_image_files(self, serial_number=None):
-        if not self._figure_valid():
-            return
-        filename_pre = self._output_path(serial_number)
-        filename_png = filename_pre.with_suffix(".png")
-        filename_svg = filename_pre.with_suffix(".svg")
-        filename_json = filename_pre.with_suffix(".json")
-
-        if self.backend == "plotly":
-            self._image_exporter_plotly(filename_png, scale=3.0)
-            self._image_exporter_plotly(filename_svg)
-            self.figure.write_json(filename_json)
-            print(f"saved plotly json file: {filename_json}")
-        elif self.backend == "matplotlib":
-            print(f"TODO: implement saving {filename_png}")
-            print(f"TODO: implement saving {filename_svg}")
-            print(f"TODO: implement saving {filename_json}")
-        else:
-            print(f"TODO: implement saving {filename_png}")
-            print(f"TODO: implement saving {filename_svg}")
-            print(f"TODO: implement saving {filename_json}")
-
-    def save(self, serial_number=None):
-        self.to_csv(serial_number=serial_number)
-
-        if self._figure_valid():
-            self.to_image_files(serial_number=serial_number)
-
-    def _output_path(self, serial_number=None):
-        d = Path(self.figure_directory)
-        if not d.is_dir():
-            logging.debug(f"{d} does not exist")
-            d = Path().cwd()
-            logging.debug(f"using current directory ({d}) instead")
-        n = self.name
-        if serial_number is not None:
-            n = f"{n}_{serial_number:03}"
-        f = d / n
-        return f
-
-
-class BatchSummaryCollector(BatchCollector):
-    # Three main levels of arguments to the plotter and collector funcs is available:
-    #  - through dictionaries (`data_collector_arguments`, `plotter_arguments`) to init
-    #  - given as defaults in the subclass (`_default_data_collector_arguments`, `_default_plotter_arguments`)
-    #  - as elevated arguments (i.e. arguments normally given in the dictionaries elevated
-    #    to their own keyword parameters)
-
-    _default_data_collector_arguments = {
-        "columns": ["charge_capacity_gravimetric"],
-    }
-
-    def __init__(
-        self,
-        b,
-        max_cycle: int = None,
-        rate=None,
-        on=None,
-        columns=None,
-        column_names=None,
-        normalize_capacity_on=None,
-        scale_by=None,
-        nom_cap=None,
-        normalize_cycles=None,
-        group_it=None,
-        rate_std=None,
-        rate_column=None,
-        inverse=None,
-        inverted: bool = None,
-        key_index_bounds=None,
-        backend: str = None,
-        title: str = None,
-        points: bool = None,
-        line: bool = None,
-        width: int = None,
-        height: int = None,
-        legend_title: str = None,
-        marker_size: int = None,
-        cmap=None,
-        spread: bool = None,
-        *args,
-        **kwargs,
-    ):
-        """Collects and shows summaries.
-
-        Elevated data collector args:
-            max_cycle (int): drop all cycles above this value.
-            rate (float): filter on rate (C-rate)
-            on (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
-            columns (list): selected column(s) (using cellpy attribute name)
-                [defaults to "charge_capacity_gravimetric"]
-            column_names (list): selected column(s) (using exact column name)
-            normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the
-                normalization (typically the first few cycles after formation)
-            scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap",
-                or given value (defaults to one).
-            nom_cap (float): nominal capacity of the cell
-            normalize_cycles (bool): perform a normalization of the cycle numbers (also called equivalent cycle index)
-            group_it (bool): if True, average pr group.
-            rate_std (float): allow for this inaccuracy when selecting cycles based on rate
-            rate_column (str): name of the column containing the C-rates.
-            inverse (bool): select steps that do not have the given C-rate.
-            inverted (bool): select cycles that do not have the steps filtered by given C-rate.
-            key_index_bounds (list): used when creating a common label for the cells by splitting and combining from
-                key_index_bound[0] to key_index_bound[1].
-
-        Elevated plotter args:
-            backend (str): backend used (defaults to Bokeh)
-            points (bool): plot points if True
-            line (bool): plot line if True
-            width: width of plot
-            height: height of plot
-            legend_title: title to put over the legend
-            marker_size: size of the markers used
-            cmap: color-map to use
-            spread (bool): plot error-bands instead of error-bars if True
-        """
-
-        elevated_data_collector_arguments = dict(
-            max_cycle=max_cycle,
-            rate=rate,
-            on=on,
-            columns=columns,
-            column_names=column_names,
-            normalize_capacity_on=normalize_capacity_on,
-            scale_by=scale_by,
-            nom_cap=nom_cap,
-            normalize_cycles=normalize_cycles,
-            group_it=group_it,
-            rate_std=rate_std,
-            rate_column=rate_column,
-            inverse=inverse,
-            inverted=inverted,
-            key_index_bounds=key_index_bounds,
-        )
-
-        elevated_plotter_arguments = {
-            "backend": backend,
-            "title": title,
-            "points": points,
-            "line": line,
-            "width": width,
-            "height": height,
-            "legend_title": legend_title,
-            "marker_size": marker_size,
-            "cmap": cmap,
-            "spread": spread,
-        }
-
-        csv_layout = kwargs.pop("csv_layout", "wide")
-
-        super().__init__(
-            b,
-            plotter=summary_plotter,
-            data_collector=summary_collector,
-            collector_name="summary",
-            elevated_data_collector_arguments=elevated_data_collector_arguments,
-            elevated_plotter_arguments=elevated_plotter_arguments,
-            csv_layout=csv_layout,
-            *args,
-            **kwargs,
-        )
-
-    def generate_name(self):
-        names = ["collected_summaries"]
-        cols = self.data_collector_arguments.get("columns")
-        grouped = self.data_collector_arguments.get("group_it")
-        equivalent_cycles = self.data_collector_arguments.get("normalize_cycles")
-        normalized_cap = self.data_collector_arguments.get("normalize_capacity_on", [])
-
-        if isinstance(cols, str):
-            cols = [cols]
-        if self.nick:
-            names.insert(0, self.nick)
-        if cols:
-            names.extend(cols)
-        if grouped:
-            names.append("average")
-        if equivalent_cycles:
-            names.append("equivalents")
-        if len(normalized_cap):
-            names.append("norm")
-
-        name = "_".join(names)
-        return name
-
-    def preprocess_data_for_csv(self):
-        # TODO: check implementation long -> wide method here
-        cols = self.data.columns.to_list()
-        wide_cols = []
-        value_cols = []
-        sort_by = []
-        if "cycle" in cols:
-            index = "cycle"
-            cols.remove("cycle")
-        else:
-            print("Could not find index")
-            return self.data
-
-        if "sub_group" in cols:
-            cols.remove("sub_group")
-
-        if "group" in cols:
-            cols.remove("group")
-
-        for _col in cols:
-            if _col in ["cell", "variable"]:
-                wide_cols.append(_col)
-                if _col == "cell":
-                    sort_by.append(_col)
-            else:
-                value_cols.append(_col)
-                if _col == "variable":
-                    sort_by.append(_col)
-        try:
-            logging.debug("pivoting data")
-            logging.debug(f"index={index}")
-            logging.debug(f"columns={wide_cols}")
-            logging.debug(f"values={value_cols}")
-            data = pd.pivot(
-                self.data, index=index, columns=wide_cols, values=value_cols
-            )
-        except Exception as e:
-            print("Could not make wide:")
-            print(e)
-            return self.data
-
-        try:
-            data = data.sort_index(axis=1, level=sort_by)
-        except Exception as e:
-            logging.debug("-could not sort columns:")
-            logging.debug(e)
-        try:
-            if len(data.columns.names) == 3:
-                data = data.reorder_levels([1, 2, 0], axis=1)
-            else:
-                data = data.reorder_levels([1, 0], axis=1)
-        except Exception as e:
-            logging.debug("-could not reorder levels:")
-            logging.debug(e)
-        return data
-
-
-class BatchICACollector(BatchCollector):
-    def __init__(
-        self,
-        b,
-        plot_type="fig_pr_cell",
-        cycles=None,
-        max_cycle=None,
-        label_mapper=None,
-        backend=None,
-        cycles_to_plot=None,
-        width=None,
-        palette=None,
-        show_legend=None,
-        legend_position=None,
-        fig_title=None,
-        cols=None,
-        group_legend_muting=True,
-        *args,
-        **kwargs,
-    ):
-        """Create a collection of ica (dQ/dV) plots."""
-
-        self.plot_type = plot_type
-        self._default_plotter_arguments["method"] = plot_type
-
-        elevated_data_collector_arguments = dict(
-            cycles=cycles,
-            max_cycle=max_cycle,
-            label_mapper=label_mapper,
-        )
-        elevated_plotter_arguments = dict(
-            backend=backend,
-            cycles_to_plot=cycles_to_plot,
-            width=width,
-            palette=palette,
-            legend_position=legend_position,
-            show_legend=show_legend,
-            fig_title=fig_title,
-            cols=cols,
-            group_legend_muting=group_legend_muting,
-        )
-
-        super().__init__(
-            b,
-            plotter=ica_plotter,
-            data_collector=ica_collector,
-            collector_name="ica",
-            elevated_data_collector_arguments=elevated_data_collector_arguments,
-            elevated_plotter_arguments=elevated_plotter_arguments,
-            *args,
-            **kwargs,
-        )
-
-    def generate_name(self):
-        names = ["collected_ica"]
-
-        pm = self.plotter_arguments.get("method")
-        if pm == "fig_pr_cell":
-            names.append("pr_cell")
-        elif pm == "fig_pr_cycle":
-            names.append("pr_cyc")
-        elif pm == "film":
-            names.append("film")
-
-        if self.nick:
-            names.insert(0, self.nick)
-
-        name = "_".join(names)
-        return name
-
-
-class BatchCyclesCollector(BatchCollector):
-    _default_data_collector_arguments = {
-        "interpolated": True,
-        "number_of_points": 100,
-        "max_cycle": 50,
-        "abort_on_missing": False,
-        "method": "back-and-forth",
-    }
-
-    def __init__(
-        self,
-        b,
-        plot_type="fig_pr_cell",
-        collector_type="back-and-forth",
-        cycles=None,
-        max_cycle=None,
-        label_mapper=None,
-        backend=None,
-        cycles_to_plot=None,
-        width=None,
-        palette=None,
-        show_legend=None,
-        legend_position=None,
-        fig_title=None,
-        cols=None,
-        group_legend_muting=True,
-        *args,
-        **kwargs,
-    ):
-        """Create a collection of capacity plots.
-
-        Args:
-            b:
-            plot_type (str): either 'fig_pr_cell' or 'fig_pr_cycle'
-            collector_type (str): how the curves are given
-                "back-and-forth" - standard back and forth; discharge
-                    (or charge) reversed from where charge (or discharge) ends.
-                "forth" - discharge (or charge) continues along x-axis.
-                "forth-and-forth" - discharge (or charge) also starts at 0
-            data_collector_arguments (dict) - arguments transferred to the plotter
-            plotter_arguments (dict) - arguments transferred to the plotter
-
-        Elevated data collector args:
-            cycles (int): drop all cycles above this value.
-            max_cycle (float): filter on rate (C-rate)
-            label_mapper (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
-
-        Elevated plotter args:
-            backend (str): backend used (defaults to Bokeh)
-            cycles_to_plot (int): plot points if True
-            width (float): width of plot
-            legend_position (str): position of the legend
-            show_legend (bool): set to False if you don't want to show legend
-            fig_title (str): title (will be put above the figure)
-            palette (str): color-map to use
-            cols (int): number of columns
-        """
-
-        elevated_data_collector_arguments = dict(
-            cycles=cycles,
-            max_cycle=max_cycle,
-            label_mapper=label_mapper,
-        )
-        elevated_plotter_arguments = dict(
-            backend=backend,
-            cycles_to_plot=cycles_to_plot,
-            width=width,
-            palette=palette,
-            legend_position=legend_position,
-            show_legend=show_legend,
-            fig_title=fig_title,
-            cols=cols,
-            group_legend_muting=group_legend_muting,
-        )
-
-        # internal attribute to keep track of plot type:
-        self.plot_type = plot_type
-        self._max_letters_in_cell_names = max(len(x) for x in b.cell_names)
-        self._default_data_collector_arguments["method"] = collector_type
-        self._default_plotter_arguments["method"] = plot_type
-
-        super().__init__(
-            b,
-            plotter=cycles_plotter,
-            data_collector=cycles_collector,
-            collector_name="cycles",
-            elevated_data_collector_arguments=elevated_data_collector_arguments,
-            elevated_plotter_arguments=elevated_plotter_arguments,
-            *args,
-            **kwargs,
-        )
-
-    def _dynamic_update_template_parameter(self, hv_opt, backend, *args, **kwargs):
-        k = hv_opt.key
-        if k == "NdLayout" and backend == "matplotlib":
-            if self.plot_type != "fig_pr_cycle":
-                hv_opt.kwargs["fig_inches"] = self._max_letters_in_cell_names * 0.14
-        return hv_opt
-
-    def generate_name(self):
-        names = ["collected_cycles"]
-
-        if self.data_collector_arguments.get("interpolated"):
-            names.append("intp")
-            if n := self.data_collector_arguments.get("number_of_points"):
-                names.append(f"p{n}")
-        cm = self.data_collector_arguments.get("method")
-        if cm.startswith("b"):
-            names.append("bf")
-        else:
-            names.append("ff")
-
-        pm = self.plotter_arguments.get("method")
-        if pm == "fig_pr_cell":
-            names.append("pr_cell")
-        elif pm == "fig_pr_cycle":
-            names.append("pr_cyc")
-
-        if self.nick:
-            names.insert(0, self.nick)
-
-        name = "_".join(names)
-        return name
-
-
-def pick_named_cell(b, label_mapper=None):
-    """generator that picks a cell from the batch object, yields its label and the cell itself.
-
-    Args:
-        b (cellpy.batch object): your batch object
-        label_mapper (callable or dict): function (or dict) that changes the cell names.
-            The dictionary must have the cell labels as given in the `journal.pages` index and new label as values.
-            Similarly, if it is a function it takes the cell label as input and returns the new label.
-            Remark! No check are performed to ensure that the new cell labels are unique.
-
-    Yields:
-        label, group, subgroup, cell
-
-    Example:
-        def my_mapper(n):
-            return "_".join(n.split("_")[1:-1])
-
-        # outputs "nnn_x" etc., if cell-names are of the form "date_nnn_x_y":
-        for label, group, subgroup, cell in pick_named_cell(b, label_mapper=my_mapper):
-            print(label)
-    """
-
-    cell_names = b.cell_names
-    for n in cell_names:
-        group = b.pages.loc[n, "group"]
-        sub_group = b.pages.loc[n, "sub_group"]
-
-        if label_mapper is not None:
-            try:
-                if isinstance(label_mapper, dict):
-                    label = label_mapper[n]
-                else:
-                    label = label_mapper(n)
-            except Exception as e:
-                logging.info(f"label_mapper-error: could not rename cell {n}")
-                logging.debug(f"caught exception: {e}")
-                label = n
-        else:
-            try:
-                label = b.pages.loc[n, "label"]
-            except Exception as e:
-                logging.info(f"lookup in pages failed: could not rename cell {n}")
-                logging.debug(f"caught exception: {e}")
-                label = n
-
-        logging.info(f"renaming {n} -> {label} (group={group}, subgroup={sub_group})")
-        yield label, group, sub_group, b.experiment.data[n]
-
-
-def summary_collector(*args, **kwargs):
-    """See concatenate_summaries in helpers (summary_collector runs
-    concatenate_summaries with melt=True and mode='collector')"""
-    kwargs["melt"] = True
-    kwargs["mode"] = "collector"
-    return concatenate_summaries(*args, **kwargs)
-
-
-def cycles_collector(
-    b,
-    cycles=None,
-    interpolated=True,
-    number_of_points=100,
-    max_cycle=50,
-    abort_on_missing=False,
-    method="back-and-forth",
-    label_mapper=None,
-):
-    if cycles is None:
-        cycles = list(range(1, max_cycle + 1))
-    all_curves = []
-    keys = []
-    for n, g, sg, c in pick_named_cell(b, label_mapper):
-        curves = c.get_cap(
-            cycle=cycles,
-            label_cycle_number=True,
-            interpolated=interpolated,
-            number_of_points=number_of_points,
-            method=method,
-        )
-        logging.debug(f"processing {n} (cell name: {c.cell_name})")
-        if not curves.empty:
-            curves = curves.assign(group=g, sub_group=sg)
-            all_curves.append(curves)
-            keys.append(n)
-        else:
-            if abort_on_missing:
-                raise ValueError(f"{n} is empty - aborting!")
-            logging.critical(f"[{n} (cell name: {c.cell_name}) empty]")
-    collected_curves = pd.concat(
-        all_curves, keys=keys, axis=0, names=["cell", "point"]
-    ).reset_index(level="cell")
-    return collected_curves
-
-
-def ica_collector(
-    b,
-    cycles=None,
-    voltage_resolution=0.005,
-    max_cycle=50,
-    abort_on_missing=False,
-    label_direction=True,
-    number_of_points=None,
-    label_mapper=None,
-    **kwargs,
-):
-    if cycles is None:
-        cycles = list(range(1, max_cycle + 1))
-    all_curves = []
-    keys = []
-    for n, g, sg, c in pick_named_cell(b, label_mapper):
-        curves = ica.dqdv_frames(
-            c,
-            cycle=cycles,
-            voltage_resolution=voltage_resolution,
-            label_direction=label_direction,
-            number_of_points=number_of_points,
-            **kwargs,
-        )
-        logging.debug(f"processing {n} (cell name: {c.cell_name})")
-        if not curves.empty:
-            curves = curves.assign(group=g, sub_group=sg)
-            all_curves.append(curves)
-            keys.append(n)
-        else:
-            if abort_on_missing:
-                raise ValueError(f"{n} is empty - aborting!")
-            logging.critical(f"[{n} (cell name: {c.cell_name}) empty]")
-    collected_curves = pd.concat(
-        all_curves, keys=keys, axis=0, names=["cell", "point"]
-    ).reset_index(level="cell")
-    return collected_curves
-
-
-def remove_markers(trace):
-    trace.update(marker=None, mode="lines")
-    return trace
-
-
-def _hist_eq(trace):
-    z = histogram_equalization(trace.z)
-    trace.update(z=z)
-    return trace
-
-
-def y_axis_replacer(ax, label):
-    ax.update(title_text=label)
-    return ax
-
-
-def legend_replacer(trace, df, group_legends=True):
-    name = trace.name
-    parts = name.split(",")
-    if len(parts) == 2:
-        group = int(parts[0])
-        subgroup = int(parts[1])
-    else:
-        print(
-            "Have not implemented replacing legend labels that are not on the form a,b yet."
-        )
-        print(f"legend label: {name}")
-        return trace
-
-    cell_label = df.loc[
-        (df["group"] == group) & (df["sub_group"] == subgroup), "cell"
-    ].values[0]
-    if group_legends:
-        trace.update(
-            name=cell_label,
-            legendgroup=group,
-            hovertemplate=f"{cell_label}<br>{trace.hovertemplate}",
-        )
-    else:
-        trace.update(
-            name=cell_label,
-            legendgroup=cell_label,
-            hovertemplate=f"{cell_label}<br>{trace.hovertemplate}",
-        )
-
-
-def sequence_plotter(
-    collected_curves: pd.DataFrame,
-    x: str = "capacity",
-    y: str = "voltage",
-    z: str = "cycle",
-    g: str = "cell",
-    standard_deviation: str = None,
-    group: str = "group",
-    subgroup: str = "sub_group",
-    x_label: str = "Capacity",
-    x_unit: str = "mAh/g",
-    y_label: str = "Voltage",
-    y_unit: str = "V",
-    z_label: str = "Cycle",
-    z_unit: str = "n.",
-    y_label_mapper: dict = None,
-    nbinsx: int = 100,
-    histfunc: str = "avg",
-    histscale: str = "abs-log",
-    direction: str = "charge",
-    direction_col: str = "direction",
-    method: str = "fig_pr_cell",
-    markers: bool = False,
-    group_cells: bool = True,
-    group_legend_muting: bool = True,
-    backend: str = "plotly",
-    cycles: list = None,
-    facetplot: bool = False,
-    cols: int = 3,
-    palette_discrete: str = None,
-    palette_continuous: str = "Viridis",
-    palette_range: tuple = None,
-    height: float = None,
-    width: float = None,
-    **kwargs,
-) -> Any:
-    """create a plot made up of sequences of data (voltage curves, dQ/dV, etc).
-
-    This method contains the "common" operations done for all the sequence plots,
-    currently supporting filtering out the specific cycles, selecting either
-    dividing into subplots by cell or by cycle, and creating the (most basic) figure object.
-
-    Args:
-        collected_curves (pd.DataFrame): collected data in long format.
-        x: column name for x-values.
-        y: column name for y-values.
-        z: if method is 'fig_pr_cell', column name for color (legend), else for subplot.
-        g: if method is 'fig_pr_cell', column name for subplot, else for color.
-        standard_deviation: str = standard deviation column (skipped if None).
-        group: str = "group",
-        subgroup: str = "sub_group",
-        x_label: str = "",
-        x_unit:
-        y_label:
-        y_unit:
-        z_label: str = "Cycle"
-        z_unit: str = "n."
-        y_label_mapper: dict
-        nbinsx: int = 100
-        histfunc: str = "avg"
-        histscale (str) = "abs-log" used for scaling the z-values for 2D array plots (heatmaps and similar).
-        direction (str) = "charge", "discharge", or "both".
-        direction_col (str) = "direction",
-        method: 'fig_pr_cell' or 'fig_pr_cycle'.
-        markers: set to True if you want markers.
-        group_cells (bool):
-        group_legend_muting (bool):
-        backend: what backend to use.
-        cycles: what cycles to include in the plot.
-        palette_discrete:
-        palette_continuous:
-        palette_range (tuple):
-        facetplot (bool): square layout with group horizontally and subgroup vertically.
-        cols: number of columns for layout.
-        height:
-        width:
-
-        **kwargs: sent to backend (if `backend == "plotly"`, it will be
-            sent to `plotly.express` etc.)
-
-    Returns:
-        figure object
-    """
-    logging.debug("running sequence plotter")
-
-    for k in kwargs:
-        logging.debug(f"keyword argument sent to the backend: {k}")
-
-    curves = None
-    seaborn_arguments = dict()
-
-    if method == "film":
-        labels = {
-            f"{x}": f"{x_label} ({x_unit})",
-            f"{z}": f"{z_label} ({z_unit})",
-        }
-        plotly_arguments = dict(
-            x=x,
-            y=z,
-            z=y,
-            labels=labels,
-            facet_col_wrap=cols,
-            nbinsx=nbinsx,
-            histfunc=histfunc,
-        )
-
-    elif method == "summary":
-        labels = {
-            f"{x}": f"{x_label} ({x_unit})",
-        }
-        plotly_arguments = dict(x=x, y=y, labels=labels, markers=markers)
-        if g == "variable" and len(collected_curves[g].unique()) > 1:
-            plotly_arguments["facet_row"] = g
-        if standard_deviation:
-            plotly_arguments["error_y"] = standard_deviation
-
-    else:
-        labels = {
-            f"{x}": f"{x_label} ({x_unit})",
-            f"{y}": f"{y_label} ({y_unit})",
-        }
-        plotly_arguments = dict(x=x, y=y, labels=labels, facet_col_wrap=cols)
-
-    if method in ["fig_pr_cell", "film"]:
-        group_cells = False
-        if method == "fig_pr_cell":
-            plotly_arguments["markers"] = markers
-            plotly_arguments["color"] = z
-        if facetplot:
-            plotly_arguments["facet_col"] = group
-            plotly_arguments["facet_row"] = subgroup
-            plotly_arguments["hover_name"] = g
-        else:
-            plotly_arguments["facet_col"] = g
-
-        if cycles is not None:
-            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
-        else:
-            curves = collected_curves
-        logging.debug(f"filtered_curves:\n{curves}")
-
-        if method == "film":
-            # selecting direction
-            if direction == "charge":
-                curves = curves.query(f"{direction_col} < 0")
-            elif direction == "discharge":
-                curves = curves.query(f"{direction_col} > 0")
-            # scaling (assuming 'y' is the "value" axis):
-            if histscale == "abs-log":
-                curves[y] = curves[y].apply(np.abs).apply(np.log)
-            elif histscale == "abs":
-                curves[y] = curves[y].apply(np.abs)
-            elif histscale == "norm":
-                curves[y] = curves[y].apply(np.abs)
-
-    elif method == "fig_pr_cycle":
-        z, g = g, z
-        plotly_arguments["facet_col"] = g
-
-        if cycles is not None:
-            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
-        else:
-            curves = collected_curves
-
-        if group_cells:
-            plotly_arguments["color"] = group
-            plotly_arguments["symbol"] = subgroup
-        else:
-            plotly_arguments["markers"] = markers
-            plotly_arguments["color"] = z
-
-    elif method == "summary":
-        logging.info("sequence-plotter - summary")
-        if cycles is not None:
-            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
-        else:
-            curves = collected_curves
-
-        if group_cells:
-            plotly_arguments["color"] = group
-            plotly_arguments["symbol"] = subgroup
-        else:
-            plotly_arguments["color"] = z
-
-    if backend == "plotly":
-        if method == "fig_pr_cell":
-            start, end = 0.0, 1.0
-            if palette_range is not None:
-                start, end = palette_range
-            unique_cycle_numbers = curves[z].unique()
-            number_of_colors = len(unique_cycle_numbers)
-
-            selected_colors = px.colors.sample_colorscale(
-                palette_continuous, number_of_colors, low=start, high=end
-            )
-            plotly_arguments["color_discrete_sequence"] = selected_colors
-        elif method == "fig_pr_cycle":
-            if palette_discrete is not None:
-                # plotly_arguments["color_discrete_sequence"] = getattr(px.colors.sequential, palette_discrete)
-                logging.debug(
-                    f"palette_discrete is not implemented yet ({palette_discrete})"
-                )
-
-        elif method == "film":
-            number_of_colors = 10
-            start, end = 0.0, 1.0
-            if palette_range is not None:
-                start, end = palette_range
-            plotly_arguments["color_continuous_scale"] = px.colors.sample_colorscale(
-                palette_continuous, number_of_colors, low=start, high=end
-            )
-
-        elif method == "summary":
-            logging.info("sequence-plotter - summary plotly")
-
-        abs_facet_row_spacing = kwargs.pop("abs_facet_row_spacing", 20)
-        abs_facet_col_spacing = kwargs.pop("abs_facet_col_spacing", 20)
-        facet_row_spacing = kwargs.pop(
-            "facet_row_spacing", abs_facet_row_spacing / height if height else 0.1
-        )
-        facet_col_spacing = kwargs.pop(
-            "facet_col_spacing", abs_facet_col_spacing / (width or 1000)
-        )
-
-        plotly_arguments["facet_row_spacing"] = facet_row_spacing
-        plotly_arguments["facet_col_spacing"] = facet_col_spacing
-
-        logging.debug(f"{plotly_arguments=}")
-        logging.debug(f"{kwargs=}")
-
-        fig = None
-        if method in ["fig_pr_cycle", "fig_pr_cell"]:
-            fig = px.line(
-                curves,
-                **plotly_arguments,
-                **kwargs,
-            )
-
-            if method == "fig_pr_cycle" and group_cells:
-                try:
-                    fig.for_each_trace(
-                        functools.partial(
-                            legend_replacer,
-                            df=curves,
-                            group_legends=group_legend_muting,
-                        )
-                    )
-                    if markers is not True:
-                        fig.for_each_trace(remove_markers)
-                except Exception as e:
-                    print("failed")
-                    print(e)
-
-        elif method == "film":
-            fig = px.density_heatmap(curves, **plotly_arguments, **kwargs)
-            if histscale is None:
-                color_bar_txt = f"{y_label} ({y_unit})"
-            else:
-                color_bar_txt = f"{y_label} ({histscale})"
-
-            if histscale == "hist-eq":
-                fig = fig.for_each_trace(lambda _x: _hist_eq(_x))
-
-            fig.update_layout(coloraxis_colorbar_title_text=color_bar_txt)
-
-        elif method == "summary":
-            # print("TRYING...")
-            #
-            # print(f"{plotly_arguments=}")
-            # print(f"{kwargs=}")
-            # print(f"{curves.columns}")
-            fig = px.line(
-                curves,
-                **plotly_arguments,
-                **kwargs,
-            )
-            if group_cells:
-                try:
-                    fig.for_each_trace(
-                        functools.partial(
-                            legend_replacer,
-                            df=curves,
-                            group_legends=group_legend_muting,
-                        )
-                    )
-                    if markers is not True:
-                        fig.for_each_trace(remove_markers)
-                except Exception as e:
-                    print("failed")
-                    print(e)
-
-            if y_label_mapper:
-                annotations = fig.layout.annotations
-                if annotations:
-                    try:
-                        # might consider a more robust method here - currently
-                        # it assumes that the mapper is a list with same order
-                        # and length as number of rows
-                        for i, (a, la) in enumerate(zip(annotations, y_label_mapper)):
-                            row = i + 1
-                            fig.for_each_yaxis(
-                                functools.partial(y_axis_replacer, label=la),
-                                row=row,
-                            )
-                        fig.update_annotations(text="")
-                    except Exception as e:
-                        print("failed")
-                        print(e)
-                else:
-                    try:
-                        fig.for_each_yaxis(
-                            functools.partial(y_axis_replacer, label=y_label_mapper[0]),
-                        )
-                    except Exception as e:
-                        print("failed")
-                        print(e)
-
-        else:
-            print(f"method '{method}' is not supported by plotly")
-
-        return fig
-
-    elif backend == "matplotlib":
-        print(f"{backend} not implemented yet")
-
-    elif backend == "bokeh":
-        print(f"{backend} not implemented yet")
-
-
-def _cycles_plotter(
-    collected_curves,
-    cycles=None,
-    x="capacity",
-    y="voltage",
-    z="cycle",
-    g="cell",
-    standard_deviation=None,
-    default_title="Charge-Discharge Curves",
-    backend="plotly",
-    method="fig_pr_cell",
-    **kwargs,
-):
-    """Plot charge-discharge curves.
-
-    Args:
-        collected_curves(pd.DataFrame): collected data in long format.
-        backend (str): what backend to use.
-        method (str): 'fig_pr_cell' or 'fig_pr_cycle'.
-
-        **kwargs: consumed first in current function, rest sent to backend in sequence_plotter.
-
-    Returns:
-        styled figure object
-    """
-
-    # --- pre-processing ---
-    logging.debug("picking kwargs for current level - rest goes to sequence_plotter")
-    title = kwargs.pop("title", default_title)
-    width = kwargs.pop("width", None)
-    height = kwargs.pop("height", None)
-    palette = kwargs.pop("palette", None)
-    legend_position = kwargs.pop("legend_position", None)
-    legend_title = kwargs.pop("legend_title", None)
-    show_legend = kwargs.pop("show_legend", None)
-    cols = kwargs.pop("cols", 3)
-    sub_fig_min_height = kwargs.pop("sub_fig_min_height", 200)
-
-    # kwargs from default `BatchCollector.render` method not used by `sequence_plotter`:
-    journal = kwargs.pop("journal", None)
-    units = kwargs.pop("units", None)
-
-    if palette is not None:
-        kwargs["palette_continuous"] = palette
-        kwargs["palette_discrete"] = palette
-
-    if legend_title is None:
-        if method == "fig_pr_cell":
-            legend_title = "Cycle"
-        else:
-            legend_title = "Cell"
-
-    no_cols = cols
-
-    if method in ["fig_pr_cell", "film"]:
-        number_of_figs = len(collected_curves["cell"].unique())
-
-    elif method == "fig_pr_cycle":
-        if cycles is not None:
-            number_of_figs = len(cycles)
-        else:
-            number_of_figs = len(collected_curves["cycle"].unique())
-    elif method == "summary":
-        number_of_figs = len(collected_curves["variable"].unique())
-        sub_fig_min_height = 300
-    else:
-        number_of_figs = 1
-
-    no_rows = math.ceil(number_of_figs / no_cols)
-
-    if not height:
-        height = no_rows * sub_fig_min_height
-
-    fig = sequence_plotter(
-        collected_curves,
-        x=x,
-        y=y,
-        z=z,
-        g=g,
-        standard_deviation=standard_deviation,
-        backend=backend,
-        method=method,
-        cols=cols,
-        cycles=cycles,
-        width=width,
-        height=height,
-        **kwargs,
-    )
-    if fig is None:
-        print("Could not create figure")
-        return
-
-    # Rendering:
-    if backend == "plotly":
-        template = f"{PLOTLY_BASE_TEMPLATE}+{method}"
-
-        legend_orientation = "v"
-        if legend_position == "bottom":
-            legend_orientation = "h"
-
-        legend_dict = {
-            "title": legend_title,
-            "orientation": legend_orientation,
-        }
-        title_dict = {
-            "text": title,
-        }
-
-        fig.update_layout(
-            template=template,
-            title=title_dict,
-            legend=legend_dict,
-            showlegend=show_legend,
-            height=height,
-            width=width,
-        )
-
-    return fig
-
-
-def summary_plotter(collected_curves, cycles_to_plot=None, backend="plotly", **kwargs):
-    """Plot summaries (value vs cycle number).
-
-    Assuming data as pandas.DataFrame with either
-    1) long format (where variables, for example charge capacity, are in the column "variable") or
-    2) mixed long and wide format where the variables are own columns.
-    """
-    col_headers = collected_curves.columns.to_list()
-    possible_id_vars = [
-        "cell",
-        "cycle",
-        "equivalent_cycle",
-        "value",
-        "mean",
-        "std",
-        "group",
-        "sub_group",
-    ]
-
-    id_vars = []
-    for n in possible_id_vars:
-        if n in col_headers:
-            col_headers.remove(n)
-            id_vars.append(n)
-
-    if "variable" not in col_headers:
-        collected_curves = collected_curves.melt(
-            id_vars=id_vars, value_vars=col_headers
-        )
-
-    normalize_cycles = True if "equivalent_cycle" in id_vars else False
-    group_it = False if "group" in id_vars else True
-
-    cols = kwargs.pop("cols", 1)
-
-    z = "cell"
-    g = "variable"
-
-    if normalize_cycles:
-        x = "equivalent_cycle"
-        x_label = "Equivalent Cycle"
-        x_unit = "cum/nom.cap."
-    else:
-        x = "cycle"
-        x_label = "Cycle"
-        x_unit = "n."
-
-    if group_it:
-        group_cells = False
-        y = "mean"
-        standard_deviation = "std"
-
-    else:
-        y = "value"
-        standard_deviation = None
-        group_cells = kwargs.pop("group_cells", True)
-
-    units = kwargs.pop("units", None)
-    label_mapper = {
-        f"{y}": None,
-    }
-    if units:
-        label_mapper[y] = []
-        variables = list(collected_curves[g].unique())
-        for v in variables:
-            # extract units:
-            u_sub = None
-            if v.endswith("_areal"):
-                u_sub = units["cellpy_units"].specific_areal
-            elif v.endswith("_gravimetric"):
-                u_sub = units["cellpy_units"].specific_gravimetric
-            elif v.endswith("_volumetric"):
-                u_sub = units["cellpy_units"].specific_volumetric
-            u_top = None
-            if "_capacity" in v:
-                u_top = units["cellpy_units"].charge
-
-            # creating label:
-            u = u_top or "Value"
-            v = v.split("_")
-            if u_sub:
-                u_sub = u_sub.replace("**", "")
-                u = f"{u}/{u_sub}"
-                v = v[:-1]
-            v = " ".join(v).title()
-            label_mapper[y].append(f"{v} ({u})")
-
-    fig = _cycles_plotter(
-        collected_curves,
-        x=x,
-        y=y,
-        z=z,
-        g=g,
-        standard_deviation=standard_deviation,
-        x_label=x_label,
-        x_unit=x_unit,
-        y_label_mapper=label_mapper[y],
-        group_cells=group_cells,
-        default_title="Summary Plot",
-        backend=backend,
-        method="summary",
-        cycles=cycles_to_plot,
-        cols=cols,
-        **kwargs,
-    )
-
-    fig.update_yaxes(matches=None, showticklabels=True)
-    return fig
-
-
-def cycles_plotter(
-    collected_curves,
-    cycles_to_plot=None,
-    backend="plotly",
-    method="fig_pr_cell",
-    **kwargs,
-):
-    """Plot charge-discharge curves.
-
-    Args:
-        collected_curves(pd.DataFrame): collected data in long format.
-        cycles_to_plot (list): cycles to plot
-        backend (str): what backend to use.
-        method (str): 'fig_pr_cell' or 'fig_pr_cycle'.
-
-        **kwargs: consumed first in current function, rest sent to backend in sequence_plotter.
-
-    Returns:
-        styled figure object
-    """
-
-    if cycles_to_plot is not None:
-        unique_cycles = list(collected_curves.cycle.unique())
-        if len(unique_cycles) > 50:
-            cycles_to_plot = DEFAULT_CYCLES
-
-    return _cycles_plotter(
-        collected_curves,
-        x="capacity",
-        y="voltage",
-        z="cycle",
-        g="cell",
-        default_title="Charge-Discharge Curves",
-        backend=backend,
-        method=method,
-        cycles=cycles_to_plot,
-        **kwargs,
-    )
-
-
-def ica_plotter(
-    collected_curves,
-    cycles_to_plot=None,
-    backend="plotly",
-    method="fig_pr_cell",
-    direction="charge",
-    **kwargs,
-):
-    """Plot charge-discharge curves.
-
-    Args:
-        collected_curves(pd.DataFrame): collected data in long format.
-        cycles_to_plot (list): cycles to plot
-        backend (str): what backend to use.
-        method (str): 'fig_pr_cell' or 'fig_pr_cycle' or 'film'.
-        direction (str): 'charge' or 'discharge'.
-
-        **kwargs: consumed first in current function, rest sent to backend in sequence_plotter.
-
-    Returns:
-        styled figure object
-    """
-
-    if cycles_to_plot is None:
-        unique_cycles = list(collected_curves.cycle.unique())
-        max_cycle = max(unique_cycles)
-        if len(unique_cycles) > 50:
-            cycles_to_plot = DEFAULT_CYCLES
-            max_cycle = max(cycles_to_plot)
-    else:
-        max_cycle = max(cycles_to_plot)
-
-    if direction not in ["charge", "discharge"]:
-        print(f"direction='{direction}' not allowed - setting it to 'charge'")
-        direction = "charge"
-    if method in ["fig_pr_cell", "film"]:
-        kwargs["range_y"] = kwargs.pop("range_y", None) or (1, max_cycle)
-
-    return _cycles_plotter(
-        collected_curves,
-        x="voltage",
-        y="dq",
-        z="cycle",
-        g="cell",
-        x_label="Voltage",
-        x_unit="V",
-        y_label="dQ/dV",
-        y_unit="mAh/g/V.",
-        default_title=f"Incremental Analysis Plots ({direction.capitalize()})",
-        direction=direction,
-        backend=backend,
-        method=method,
-        cycles=cycles_to_plot,
-        **kwargs,
-    )
-
-
-def histogram_equalization(image: np.array) -> np.array:
-    """Perform histogram equalization on a numpy array.
-
-    # from http://www.janeriksolem.net/histogram-equalization-with-python-and.html
-    """
-    number_bins = 256
-    scale = 100
-    image[np.isnan(image)] = 0.0
-    image_histogram, bins = np.histogram(image.flatten(), number_bins, density=True)
-    cdf = image_histogram.cumsum()  # cumulative distribution function
-    cdf = (scale - 1) * cdf / cdf[-1]  # normalize
-    # use linear interpolation of cdf to find new pixel values
-    image_equalized = np.interp(image.flatten(), bins[:-1], cdf)
-
-    return image_equalized.reshape(image.shape)
-
-
-if __name__ == "__main__":
-    from pathlib import Path
-    import os
-
-    import matplotlib.pyplot as plt
-    import pandas as pd
-    import numpy as np
-    import seaborn as sns
-    import plotly.express as px
-
-    import cellpy
-    from cellpy.utils import batch, helpers, plotutils
-
-    project_dir = Path("../../testdata/batch_project")
-    journal = project_dir / "test_project.json"
-    assert project_dir.is_dir()
-    assert journal.is_file()
-    os.chdir(project_dir)
-    print(f"cellpy version: {cellpy.__version__}")
-    cellpy.log.setup_logging("INFO")
-
-    b = batch.from_journal(journal)
-    b.link()
-    c = b.cells.first()
-
-    summaries = BatchSummaryCollector(
-        b,
-        normalize_cycles=False,
-        group_it=False,
-        autorun=False,
-        columns=["charge_capacity_areal", "charge_capacity_gravimetric"],
-    )
-    summaries.update(update_data=True, update_plot=True)
-
-    # must use .figure.show() when not running in notebook:
-    summaries.figure.show()
-    summaries.save()
-
-    dqdvs = BatchICACollector(b, plot_type="film")
-    dqdvs.figure.show()
-
-    print("Ended OK")
+"""Collectors are used for simplifying plotting and exporting batch objects."""
+
+import functools
+import inspect
+import logging
+import math
+from pprint import pprint
+from pathlib import Path
+import textwrap
+from typing import Any
+import time
+from itertools import count
+from multiprocessing import Process
+
+import pandas as pd
+import plotly.express as px
+import plotly.io as pio
+import plotly.graph_objects as go
+import numpy as np
+
+import cellpy
+from cellpy.readers.core import group_by_interpolate
+from cellpy.utils.batch import Batch
+from cellpy.utils.helpers import concatenate_summaries
+from cellpy.utils.plotutils import plot_concatenated
+from cellpy.utils import ica
+
+DEFAULT_CYCLES = [1, 10, 20]
+
+CELLPY_MINIMUM_VERSION = "1.0.0"
+PLOTLY_BASE_TEMPLATE = "seaborn"
+IMAGE_TO_FILE_TIMEOUT = 30
+
+px_template_all_axis_shown = dict(
+    xaxis=dict(
+        linecolor="rgb(36,36,36)",
+        mirror=True,
+        showline=True,
+        zeroline=False,
+        title={"standoff": 15},
+    ),
+    yaxis=dict(
+        linecolor="rgb(36,36,36)",
+        mirror=True,
+        showline=True,
+        zeroline=False,
+        title={"standoff": 15},
+    ),
+)
+
+fig_pr_cell_template = go.layout.Template(
+    # layout=px_template_all_axis_shown
+)
+
+fig_pr_cycle_template = go.layout.Template(
+    # layout=px_template_all_axis_shown
+)
+
+film_template = go.layout.Template(
+    # layout=px_template_all_axis_shown
+)
+
+summary_template = go.layout.Template(
+    # layout=px_template_all_axis_shown
+)
+
+
+def _setup():
+    _welcome_message()
+
+
+def _welcome_message():
+    cellpy_version = cellpy.__version__
+    logging.info(f"cellpy version: {cellpy_version}")
+    logging.info(f"collectors need at least: {CELLPY_MINIMUM_VERSION}")
+
+
+_setup()
+
+
+class BatchCollector:
+    collector_name: str = None
+    data: pd.DataFrame = None
+    figure: Any = None
+    name: str = None
+    nick: str = None
+    autorun: bool = True
+    figure_directory: Path = Path("out")
+    data_directory: Path = Path("data/processed/")
+    renderer: Any = None
+    units: dict = None
+
+    # override default arguments:
+    elevated_data_collector_arguments: dict = None
+    elevated_plotter_arguments: dict = None
+
+    # defaults (and used also when resetting):
+    _default_data_collector_arguments = {}
+    _default_plotter_arguments = {}
+
+    def __init__(
+        self,
+        b,
+        data_collector,
+        plotter,
+        collector_name=None,
+        name=None,
+        nick=None,
+        autorun=True,
+        backend="plotly",
+        elevated_data_collector_arguments=None,
+        elevated_plotter_arguments=None,
+        data_collector_arguments: dict = None,
+        plotter_arguments: dict = None,
+        **kwargs,
+    ):
+        """Update both the collected data and the plot(s).
+        Args:
+            b (cellpy.utils.Batch): the batch object.
+            data_collector (callable): method that collects the data.
+            plotter (callable): method that crates the plots.
+            collector_name (str): name of collector.
+            name (str or bool): name used for auto-generating filenames etc.
+            autorun (bool): run collector and plotter immediately if True.
+            use_templates (bool): also apply template(s) in autorun mode if True.
+            backend (str): name of plotting backend to use ("plotly" or "matplotlib").
+            elevated_data_collector_arguments (dict): arguments picked up by the child class' initializer.
+            elevated_plotter_arguments (dict): arguments picked up by the child class' initializer.
+            data_collector_arguments (dict): keyword arguments sent to the data collector.
+            plotter_arguments (dict): keyword arguments sent to the plotter.
+            update_name (bool): update the name (using automatic name generation) based on new settings.
+            **kwargs: set Collector attributes.
+        """
+        self.b = b
+        self.data_collector = data_collector
+        self.plotter = plotter
+        self.nick = nick
+        self.backend = backend
+        self.collector_name = collector_name or "base"
+
+        # Arguments given as default arguments in the subclass have "low" priority (below elevated arguments at least):
+        self._data_collector_arguments = self._default_data_collector_arguments.copy()
+        self._plotter_arguments = self._default_plotter_arguments.copy()
+        self._update_arguments(data_collector_arguments, plotter_arguments)
+
+        # Elevated arguments have preference above the data_collector and plotter argument dicts:
+        self._parse_elevated_arguments(
+            elevated_data_collector_arguments, elevated_plotter_arguments
+        )
+
+        self._set_attributes(**kwargs)
+
+        self._set_plotly_templates()
+
+        if nick is None:
+            self.nick = b.name
+
+        if name is None:
+            name = self.generate_name()
+        self.name = name
+
+        self.parse_units()
+
+        if autorun:
+            self.update(update_name=False)
+
+    @staticmethod
+    def _set_plotly_templates():
+        pio.templates.default = PLOTLY_BASE_TEMPLATE
+        pio.templates["fig_pr_cell"] = fig_pr_cell_template
+        pio.templates["fig_pr_cycle"] = fig_pr_cycle_template
+        pio.templates["film"] = film_template
+        pio.templates["summary"] = summary_template
+
+    @property
+    def data_collector_arguments(self):
+        return self._data_collector_arguments
+
+    @data_collector_arguments.setter
+    def data_collector_arguments(self, argument_dict: dict):
+        if argument_dict is not None:
+            self._data_collector_arguments = {
+                **self._data_collector_arguments,
+                **argument_dict,
+            }
+
+    @property
+    def plotter_arguments(self):
+        return self._plotter_arguments
+
+    @plotter_arguments.setter
+    def plotter_arguments(self, argument_dict: dict):
+        if argument_dict is not None:
+            self._plotter_arguments = {**self._plotter_arguments, **argument_dict}
+
+    def _attr_text(self, bullet_start=" - ", sep="\n"):
+        txt = f"{bullet_start}collector_name: {self.collector_name}" + sep
+        txt += f"{bullet_start}autorun: {self.autorun}" + sep
+        txt += f"{bullet_start}name: {self.name}" + sep
+        txt += f"{bullet_start}nick: {self.nick}" + sep
+        txt += f"{bullet_start}csv_include_index: {self.csv_include_index}" + sep
+        txt += f"{bullet_start}csv_layout: {self.csv_layout}" + sep
+        txt += f"{bullet_start}sep: {self.sep}" + sep
+        txt += f"{bullet_start}backend: {self.backend}" + sep
+        txt += f"{bullet_start}toolbar: {self.toolbar}" + sep
+        txt += f"{bullet_start}figure_directory: {self.figure_directory}" + sep
+        txt += f"{bullet_start}data_directory: {self.data_directory}" + sep
+        txt += f"{bullet_start}batch-instance: {self.b.name}" + sep
+        txt += (
+            f"{bullet_start}data_collector_arguments: {self.data_collector_arguments}"
+            + sep
+        )
+        txt += f"{bullet_start}plotter_arguments: {self.plotter_arguments}" + sep
+        return txt
+
+    def _attr_data_collector(self, h1="", h2="", sep="\n"):
+        data_name = self.data_collector.__name__
+        data_sig = inspect.signature(self.data_collector)
+        data_doc = inspect.getdoc(self.data_collector)
+        txt = f"{h1}{data_name}"
+        txt = f"{txt}{data_sig}{h2}{sep}"
+        txt = f"{txt}{sep}{data_doc}{sep}"
+        return txt
+
+    def _attr_plotter(self, h1="", h2="", sep="\n"):
+        plotter_name = self.plotter.__name__
+        plotter_sig = inspect.signature(self.plotter)
+        plotter_doc = inspect.getdoc(self.plotter)
+        txt = f"{h1}{plotter_name}"
+        txt = f"{txt}{plotter_sig}{h2}{sep}"
+        txt = f"{txt}{sep}{plotter_doc}{sep}"
+        return txt
+
+    def __str__(self):
+        class_name = self.__class__.__name__
+        txt = f"{class_name}\n{len(class_name) * '='}\n\n"
+        txt += "Attributes:\n"
+        txt += "-----------\n"
+        txt += self._attr_text(sep="\n")
+
+        txt += "\nfigure:\n"
+        txt += ".......\n"
+        fig_txt = f"{self.figure}"
+        if isinstance(fig_txt, str) and len(fig_txt) > 500:
+            fig_txt = fig_txt[0:500] + "\n ..."
+        txt += f"{fig_txt}\n"
+
+        txt += "\ndata:\n"
+        txt += ".....\n"
+        txt += f"{self.data}\n"
+
+        txt += "\nData collector:\n"
+        txt += "---------------\n"
+        txt += self._attr_data_collector(sep="\n")
+
+        txt += "\nPlotter:\n"
+        txt += "--------\n"
+        txt += self._attr_plotter(sep="\n")
+        return txt
+
+    def _repr_html_(self):
+        class_name = self.__class__.__name__
+        txt = f"<h2>{class_name}</h2> id={hex(id(self))}"
+        txt += f"<h3>Attributes:</h3>"
+        txt += "<ul>"
+        txt += self._attr_text(bullet_start="<li><code>", sep="</code></li>")
+        txt += "</ul>"
+        txt += f"<h3>Figure:</h3><code>"
+
+        fig_txt = f"{self.figure}"
+        if isinstance(fig_txt, str) and len(fig_txt) > 500:
+            fig_txt = fig_txt[0:500] + "<br>..."
+        txt += f"{fig_txt}<br></code>"
+
+        txt += f"<h3>Data:</h3>"
+        if hasattr(self.data, "_repr_html_"):
+            txt += self.data._repr_html_()
+        else:
+            txt += "NONE"
+        txt += "<br>"
+        txt += f"<h3>Data Collector:</h3><blockquote>"
+        txt += self._attr_data_collector(h1="<b>", h2="</b>", sep="<br>")
+        txt += f"</blockquote><h3>Plotter:</h3><blockquote>"
+        txt += self._attr_plotter(h1="<b>", h2="</b>", sep="<br>")
+        txt += "</blockquote>"
+        return txt
+
+    def _set_attributes(self, **kwargs):
+        self.sep = kwargs.get("sep", ";")
+        self.csv_include_index = kwargs.get("csv_include_index", True)
+        self.csv_layout = kwargs.get("csv_layout", "long")
+        self.dpi = kwargs.get("dpi", 200)
+        self.toolbar = kwargs.get("toolbar", True)
+
+    def generate_name(self):
+        names = ["collector", self.collector_name]
+        if self.nick:
+            names.insert(0, self.nick)
+        name = "_".join(names)
+        return name
+
+    def render(self):
+        self.figure = self.plotter(
+            self.data,
+            journal=self.b.journal,
+            units=self.units,
+            **self.plotter_arguments,
+        )
+
+    def _parse_elevated_arguments(
+        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
+    ):
+        if data_collector_arguments is not None:
+            logging.info(f"Updating elevated arguments")
+            elevated_data_collector_arguments = {}
+            for k, v in data_collector_arguments.items():
+                if v is not None:
+                    elevated_data_collector_arguments[k] = v
+            self._update_arguments(
+                elevated_data_collector_arguments, None, set_as_defaults=True
+            )
+
+        if plotter_arguments is not None:
+            logging.info(f"Updating elevated arguments")
+            elevated_plotter_arguments = {}
+            for k, v in plotter_arguments.items():
+                if v is not None:
+                    elevated_plotter_arguments[k] = v
+
+            self._update_arguments(
+                None, elevated_plotter_arguments, set_as_defaults=True
+            )
+
+    def _update_arguments(
+        self,
+        data_collector_arguments: dict = None,
+        plotter_arguments: dict = None,
+        set_as_defaults=False,
+    ):
+        self.data_collector_arguments = data_collector_arguments
+        self.plotter_arguments = plotter_arguments
+        self._check_plotter_arguments()
+
+        logging.info(f"**data_collector_arguments: {self.data_collector_arguments}")
+        logging.info(f"**plotter_arguments: {self.plotter_arguments}")
+
+        # setting defaults also (py3.6 compatible):
+        if set_as_defaults:
+            logging.info("updating defaults for current instance")
+            if data_collector_arguments is not None:
+                self._default_data_collector_arguments = {
+                    **self._default_data_collector_arguments,
+                    **data_collector_arguments,
+                }
+            if plotter_arguments is not None:
+                self._default_plotter_arguments = {
+                    **self._default_plotter_arguments,
+                    **plotter_arguments,
+                }
+
+    def _check_plotter_arguments(self):
+        if "plot_type" in self.plotter_arguments:
+            print(
+                "WARNING - using possible difficult option (future versions will fix this)"
+            )
+            print("*** 'plot_type' TRANSLATED TO 'method'")
+            self.plotter_arguments["method"] = self.plotter_arguments.pop("plot_type")
+
+    def reset_arguments(
+        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
+    ):
+        """Reset the arguments to the defaults.
+        Args:
+            data_collector_arguments (dict): optional additional keyword arguments for the data collector.
+            plotter_arguments (dict): optional additional keyword arguments for the plotter.
+        """
+        self._data_collector_arguments = self._default_data_collector_arguments.copy()
+        self._plotter_arguments = self._default_plotter_arguments.copy()
+        self._update_arguments(data_collector_arguments, plotter_arguments)
+
+    def parse_units(self, **kwargs):
+        """Look through your cellpy objects and search for units."""
+        b = self.b
+        c_units = []
+        r_units = []
+        c_unit = None
+        r_unit = None
+        for c in b:
+            cu = c.cellpy_units
+            if cu != c_unit:
+                c_unit = cu
+                c_units.append(cu)
+
+            ru = c.raw_units
+            if ru != r_unit:
+                r_unit = ru
+                r_units.append(ru)
+        if len(c_units) > 1:
+            print("WARNING: non-homogenous units found: cellpy_units")
+        if len(r_units) > 1:
+            print("WARNING: non-homogenous units found: raw_units")
+        raw_units = r_units[0]
+        cellpy_units = c_units[0]
+        self.units = dict(raw_units=raw_units, cellpy_units=cellpy_units)
+
+    def update(
+        self,
+        data_collector_arguments: dict = None,
+        plotter_arguments: dict = None,
+        reset: bool = False,
+        update_data: bool = False,
+        update_name: bool = False,
+        update_plot: bool = True,
+    ):
+        """Update both the collected data and the plot(s).
+        Args:
+            data_collector_arguments (dict): keyword arguments sent to the data collector.
+            plotter_arguments (dict): keyword arguments sent to the plotter.
+            reset (bool): reset the arguments first.
+            update_data (bool): update the data before updating the plot even if data has been collected before.
+            update_name (bool): update the name (using automatic name generation) based on new settings.
+            update_plot (bool): update the plot.
+        """
+        if reset:
+            self.reset_arguments(data_collector_arguments, plotter_arguments)
+        else:
+            self._update_arguments(data_collector_arguments, plotter_arguments)
+        if update_data or self.data is None:
+            try:
+                self.data = self.data_collector(self.b, **self.data_collector_arguments)
+            except TypeError as e:
+                print("Type error:", e)
+                print("Registered data_collector_arguments:")
+                pprint(self.data_collector_arguments)
+                print("Hint: fix it and then re-run using reset=True")
+                return
+        if update_plot:
+            try:
+                self.render()
+            except TypeError as e:
+                print("Type error:", e)
+                print("Registered plotter_arguments:")
+                pprint(self.plotter_arguments)
+                print("Hint: fix it and then re-run using reset=True")
+                return
+
+        if update_name:
+            self.name = self.generate_name()
+
+    def _figure_valid(self):
+        # TODO: create a decorator
+        if self.figure is None:
+            print("No figure to show!")
+            return False
+        return True
+
+    def show(self, **kwargs):
+        """Show the figure.
+
+        Note that show returns the `figure` object and  if the `backend` used
+        does not provide automatic rendering in the editor / running environment you
+        are using, you might have to issue the rendering yourself. For example, if you
+        are using `plotly` and running it as a script in a typical command shell,
+        you will have to issue `.show()` on the returned `figure` object.
+
+        Args:
+            **kwargs: sent to the plotter.
+
+        Returns:
+            Figure object
+        """
+        if not self._figure_valid():
+            return
+
+        print(f"figure name: {self.name}")
+        if kwargs:
+            logging.info(f"updating figure with {kwargs}")
+            self._update_arguments(plotter_arguments=kwargs)
+            self.render()
+        return self.figure
+
+    def preprocess_data_for_csv(self):
+        logging.debug(f"the data layout {self.csv_layout} is not supported yet!")
+        return self.data
+
+    def to_csv(self, serial_number=None):
+        filename = self._output_path(serial_number)
+        filename = filename.with_suffix(".csv")
+        if self.csv_layout != "long":
+            data = self.preprocess_data_for_csv()
+        else:
+            data = self.data
+
+        data.to_csv(
+            filename,
+            sep=self.sep,
+            index=self.csv_include_index,
+        )
+        print(f"saved csv file: {filename}")
+
+    def _image_exporter_plotly(self, filename, timeout=IMAGE_TO_FILE_TIMEOUT, **kwargs):
+        p = Process(
+            target=self.figure.write_image,
+            args=(filename,),
+            name="save_plotly_image_to_file",
+            kwargs=kwargs,
+        )
+        p.start()
+        p.join(timeout=timeout)
+        p.terminate()
+        if p.exitcode is None:
+            print(f"Oops, {p} timeouts! Could not save {filename}")
+        if p.exitcode == 0:
+            print(f"saved image file: {filename}")
+
+    def to_image_files(self, serial_number=None):
+        if not self._figure_valid():
+            return
+        filename_pre = self._output_path(serial_number)
+        filename_png = filename_pre.with_suffix(".png")
+        filename_svg = filename_pre.with_suffix(".svg")
+        filename_json = filename_pre.with_suffix(".json")
+
+        if self.backend == "plotly":
+            self._image_exporter_plotly(filename_png, scale=3.0)
+            self._image_exporter_plotly(filename_svg)
+            self.figure.write_json(filename_json)
+            print(f"saved plotly json file: {filename_json}")
+        elif self.backend == "matplotlib":
+            print(f"TODO: implement saving {filename_png}")
+            print(f"TODO: implement saving {filename_svg}")
+            print(f"TODO: implement saving {filename_json}")
+        else:
+            print(f"TODO: implement saving {filename_png}")
+            print(f"TODO: implement saving {filename_svg}")
+            print(f"TODO: implement saving {filename_json}")
+
+    def save(self, serial_number=None):
+        self.to_csv(serial_number=serial_number)
+
+        if self._figure_valid():
+            self.to_image_files(serial_number=serial_number)
+
+    def _output_path(self, serial_number=None):
+        d = Path(self.figure_directory)
+        if not d.is_dir():
+            logging.debug(f"{d} does not exist")
+            d = Path().cwd()
+            logging.debug(f"using current directory ({d}) instead")
+        n = self.name
+        if serial_number is not None:
+            n = f"{n}_{serial_number:03}"
+        f = d / n
+        return f
+
+
+class BatchSummaryCollector(BatchCollector):
+    # Three main levels of arguments to the plotter and collector funcs is available:
+    #  - through dictionaries (`data_collector_arguments`, `plotter_arguments`) to init
+    #  - given as defaults in the subclass (`_default_data_collector_arguments`, `_default_plotter_arguments`)
+    #  - as elevated arguments (i.e. arguments normally given in the dictionaries elevated
+    #    to their own keyword parameters)
+
+    _default_data_collector_arguments = {
+        "columns": ["charge_capacity_gravimetric"],
+    }
+
+    def __init__(
+        self,
+        b,
+        max_cycle: int = None,
+        rate=None,
+        on=None,
+        columns=None,
+        column_names=None,
+        normalize_capacity_on=None,
+        scale_by=None,
+        nom_cap=None,
+        normalize_cycles=None,
+        group_it=None,
+        rate_std=None,
+        rate_column=None,
+        inverse=None,
+        inverted: bool = None,
+        key_index_bounds=None,
+        backend: str = None,
+        title: str = None,
+        points: bool = None,
+        line: bool = None,
+        width: int = None,
+        height: int = None,
+        legend_title: str = None,
+        marker_size: int = None,
+        cmap=None,
+        spread: bool = None,
+        *args,
+        **kwargs,
+    ):
+        """Collects and shows summaries.
+
+        Elevated data collector args:
+            max_cycle (int): drop all cycles above this value.
+            rate (float): filter on rate (C-rate)
+            on (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
+            columns (list): selected column(s) (using cellpy attribute name)
+                [defaults to "charge_capacity_gravimetric"]
+            column_names (list): selected column(s) (using exact column name)
+            normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the
+                normalization (typically the first few cycles after formation)
+            scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap",
+                or given value (defaults to one).
+            nom_cap (float): nominal capacity of the cell
+            normalize_cycles (bool): perform a normalization of the cycle numbers (also called equivalent cycle index)
+            group_it (bool): if True, average pr group.
+            rate_std (float): allow for this inaccuracy when selecting cycles based on rate
+            rate_column (str): name of the column containing the C-rates.
+            inverse (bool): select steps that do not have the given C-rate.
+            inverted (bool): select cycles that do not have the steps filtered by given C-rate.
+            key_index_bounds (list): used when creating a common label for the cells by splitting and combining from
+                key_index_bound[0] to key_index_bound[1].
+
+        Elevated plotter args:
+            backend (str): backend used (defaults to Bokeh)
+            points (bool): plot points if True
+            line (bool): plot line if True
+            width: width of plot
+            height: height of plot
+            legend_title: title to put over the legend
+            marker_size: size of the markers used
+            cmap: color-map to use
+            spread (bool): plot error-bands instead of error-bars if True
+        """
+
+        elevated_data_collector_arguments = dict(
+            max_cycle=max_cycle,
+            rate=rate,
+            on=on,
+            columns=columns,
+            column_names=column_names,
+            normalize_capacity_on=normalize_capacity_on,
+            scale_by=scale_by,
+            nom_cap=nom_cap,
+            normalize_cycles=normalize_cycles,
+            group_it=group_it,
+            rate_std=rate_std,
+            rate_column=rate_column,
+            inverse=inverse,
+            inverted=inverted,
+            key_index_bounds=key_index_bounds,
+        )
+
+        elevated_plotter_arguments = {
+            "backend": backend,
+            "title": title,
+            "points": points,
+            "line": line,
+            "width": width,
+            "height": height,
+            "legend_title": legend_title,
+            "marker_size": marker_size,
+            "cmap": cmap,
+            "spread": spread,
+        }
+
+        csv_layout = kwargs.pop("csv_layout", "wide")
+
+        super().__init__(
+            b,
+            plotter=summary_plotter,
+            data_collector=summary_collector,
+            collector_name="summary",
+            elevated_data_collector_arguments=elevated_data_collector_arguments,
+            elevated_plotter_arguments=elevated_plotter_arguments,
+            csv_layout=csv_layout,
+            *args,
+            **kwargs,
+        )
+
+    def generate_name(self):
+        names = ["collected_summaries"]
+        cols = self.data_collector_arguments.get("columns")
+        grouped = self.data_collector_arguments.get("group_it")
+        equivalent_cycles = self.data_collector_arguments.get("normalize_cycles")
+        normalized_cap = self.data_collector_arguments.get("normalize_capacity_on", [])
+
+        if isinstance(cols, str):
+            cols = [cols]
+        if self.nick:
+            names.insert(0, self.nick)
+        if cols:
+            names.extend(cols)
+        if grouped:
+            names.append("average")
+        if equivalent_cycles:
+            names.append("equivalents")
+        if len(normalized_cap):
+            names.append("norm")
+
+        name = "_".join(names)
+        return name
+
+    def preprocess_data_for_csv(self):
+        # TODO: check implementation long -> wide method here
+        cols = self.data.columns.to_list()
+        wide_cols = []
+        value_cols = []
+        sort_by = []
+        if "cycle" in cols:
+            index = "cycle"
+            cols.remove("cycle")
+        else:
+            print("Could not find index")
+            return self.data
+
+        if "sub_group" in cols:
+            cols.remove("sub_group")
+
+        if "group" in cols:
+            cols.remove("group")
+
+        for _col in cols:
+            if _col in ["cell", "variable"]:
+                wide_cols.append(_col)
+                if _col == "cell":
+                    sort_by.append(_col)
+            else:
+                value_cols.append(_col)
+                if _col == "variable":
+                    sort_by.append(_col)
+        try:
+            logging.debug("pivoting data")
+            logging.debug(f"index={index}")
+            logging.debug(f"columns={wide_cols}")
+            logging.debug(f"values={value_cols}")
+            data = pd.pivot(
+                self.data, index=index, columns=wide_cols, values=value_cols
+            )
+        except Exception as e:
+            print("Could not make wide:")
+            print(e)
+            return self.data
+
+        try:
+            data = data.sort_index(axis=1, level=sort_by)
+        except Exception as e:
+            logging.debug("-could not sort columns:")
+            logging.debug(e)
+        try:
+            if len(data.columns.names) == 3:
+                data = data.reorder_levels([1, 2, 0], axis=1)
+            else:
+                data = data.reorder_levels([1, 0], axis=1)
+        except Exception as e:
+            logging.debug("-could not reorder levels:")
+            logging.debug(e)
+        return data
+
+
+class BatchICACollector(BatchCollector):
+    def __init__(
+        self,
+        b,
+        plot_type="fig_pr_cell",
+        cycles=None,
+        max_cycle=None,
+        label_mapper=None,
+        backend=None,
+        cycles_to_plot=None,
+        width=None,
+        palette=None,
+        show_legend=None,
+        legend_position=None,
+        fig_title=None,
+        cols=None,
+        group_legend_muting=True,
+        *args,
+        **kwargs,
+    ):
+        """Create a collection of ica (dQ/dV) plots."""
+
+        self.plot_type = plot_type
+        self._default_plotter_arguments["method"] = plot_type
+
+        elevated_data_collector_arguments = dict(
+            cycles=cycles,
+            max_cycle=max_cycle,
+            label_mapper=label_mapper,
+        )
+        elevated_plotter_arguments = dict(
+            backend=backend,
+            cycles_to_plot=cycles_to_plot,
+            width=width,
+            palette=palette,
+            legend_position=legend_position,
+            show_legend=show_legend,
+            fig_title=fig_title,
+            cols=cols,
+            group_legend_muting=group_legend_muting,
+        )
+
+        super().__init__(
+            b,
+            plotter=ica_plotter,
+            data_collector=ica_collector,
+            collector_name="ica",
+            elevated_data_collector_arguments=elevated_data_collector_arguments,
+            elevated_plotter_arguments=elevated_plotter_arguments,
+            *args,
+            **kwargs,
+        )
+
+    def generate_name(self):
+        names = ["collected_ica"]
+
+        pm = self.plotter_arguments.get("method")
+        if pm == "fig_pr_cell":
+            names.append("pr_cell")
+        elif pm == "fig_pr_cycle":
+            names.append("pr_cyc")
+        elif pm == "film":
+            names.append("film")
+
+        if self.nick:
+            names.insert(0, self.nick)
+
+        name = "_".join(names)
+        return name
+
+
+class BatchCyclesCollector(BatchCollector):
+    _default_data_collector_arguments = {
+        "interpolated": True,
+        "number_of_points": 100,
+        "max_cycle": 50,
+        "abort_on_missing": False,
+        "method": "back-and-forth",
+    }
+
+    def __init__(
+        self,
+        b,
+        plot_type="fig_pr_cell",
+        collector_type="back-and-forth",
+        cycles=None,
+        max_cycle=None,
+        label_mapper=None,
+        backend=None,
+        cycles_to_plot=None,
+        width=None,
+        palette=None,
+        show_legend=None,
+        legend_position=None,
+        fig_title=None,
+        cols=None,
+        group_legend_muting=True,
+        *args,
+        **kwargs,
+    ):
+        """Create a collection of capacity plots.
+
+        Args:
+            b:
+            plot_type (str): either 'fig_pr_cell' or 'fig_pr_cycle'
+            collector_type (str): how the curves are given
+                "back-and-forth" - standard back and forth; discharge
+                    (or charge) reversed from where charge (or discharge) ends.
+                "forth" - discharge (or charge) continues along x-axis.
+                "forth-and-forth" - discharge (or charge) also starts at 0
+            data_collector_arguments (dict) - arguments transferred to the plotter
+            plotter_arguments (dict) - arguments transferred to the plotter
+
+        Elevated data collector args:
+            cycles (int): drop all cycles above this value.
+            max_cycle (float): filter on rate (C-rate)
+            label_mapper (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
+
+        Elevated plotter args:
+            backend (str): backend used (defaults to Bokeh)
+            cycles_to_plot (int): plot points if True
+            width (float): width of plot
+            legend_position (str): position of the legend
+            show_legend (bool): set to False if you don't want to show legend
+            fig_title (str): title (will be put above the figure)
+            palette (str): color-map to use
+            cols (int): number of columns
+        """
+
+        elevated_data_collector_arguments = dict(
+            cycles=cycles,
+            max_cycle=max_cycle,
+            label_mapper=label_mapper,
+        )
+        elevated_plotter_arguments = dict(
+            backend=backend,
+            cycles_to_plot=cycles_to_plot,
+            width=width,
+            palette=palette,
+            legend_position=legend_position,
+            show_legend=show_legend,
+            fig_title=fig_title,
+            cols=cols,
+            group_legend_muting=group_legend_muting,
+        )
+
+        # internal attribute to keep track of plot type:
+        self.plot_type = plot_type
+        self._max_letters_in_cell_names = max(len(x) for x in b.cell_names)
+        self._default_data_collector_arguments["method"] = collector_type
+        self._default_plotter_arguments["method"] = plot_type
+
+        super().__init__(
+            b,
+            plotter=cycles_plotter,
+            data_collector=cycles_collector,
+            collector_name="cycles",
+            elevated_data_collector_arguments=elevated_data_collector_arguments,
+            elevated_plotter_arguments=elevated_plotter_arguments,
+            *args,
+            **kwargs,
+        )
+
+    def _dynamic_update_template_parameter(self, hv_opt, backend, *args, **kwargs):
+        k = hv_opt.key
+        if k == "NdLayout" and backend == "matplotlib":
+            if self.plot_type != "fig_pr_cycle":
+                hv_opt.kwargs["fig_inches"] = self._max_letters_in_cell_names * 0.14
+        return hv_opt
+
+    def generate_name(self):
+        names = ["collected_cycles"]
+
+        if self.data_collector_arguments.get("interpolated"):
+            names.append("intp")
+            if n := self.data_collector_arguments.get("number_of_points"):
+                names.append(f"p{n}")
+        cm = self.data_collector_arguments.get("method")
+        if cm.startswith("b"):
+            names.append("bf")
+        else:
+            names.append("ff")
+
+        pm = self.plotter_arguments.get("method")
+        if pm == "fig_pr_cell":
+            names.append("pr_cell")
+        elif pm == "fig_pr_cycle":
+            names.append("pr_cyc")
+
+        if self.nick:
+            names.insert(0, self.nick)
+
+        name = "_".join(names)
+        return name
+
+
+def pick_named_cell(b, label_mapper=None):
+    """generator that picks a cell from the batch object, yields its label and the cell itself.
+
+    Args:
+        b (cellpy.batch object): your batch object
+        label_mapper (callable or dict): function (or dict) that changes the cell names.
+            The dictionary must have the cell labels as given in the `journal.pages` index and new label as values.
+            Similarly, if it is a function it takes the cell label as input and returns the new label.
+            Remark! No check are performed to ensure that the new cell labels are unique.
+
+    Yields:
+        label, group, subgroup, cell
+
+    Example:
+        def my_mapper(n):
+            return "_".join(n.split("_")[1:-1])
+
+        # outputs "nnn_x" etc., if cell-names are of the form "date_nnn_x_y":
+        for label, group, subgroup, cell in pick_named_cell(b, label_mapper=my_mapper):
+            print(label)
+    """
+
+    cell_names = b.cell_names
+    for n in cell_names:
+        group = b.pages.loc[n, "group"]
+        sub_group = b.pages.loc[n, "sub_group"]
+
+        if label_mapper is not None:
+            try:
+                if isinstance(label_mapper, dict):
+                    label = label_mapper[n]
+                else:
+                    label = label_mapper(n)
+            except Exception as e:
+                logging.info(f"label_mapper-error: could not rename cell {n}")
+                logging.debug(f"caught exception: {e}")
+                label = n
+        else:
+            try:
+                label = b.pages.loc[n, "label"]
+            except Exception as e:
+                logging.info(f"lookup in pages failed: could not rename cell {n}")
+                logging.debug(f"caught exception: {e}")
+                label = n
+
+        logging.info(f"renaming {n} -> {label} (group={group}, subgroup={sub_group})")
+        yield label, group, sub_group, b.experiment.data[n]
+
+
+def summary_collector(*args, **kwargs):
+    """See concatenate_summaries in helpers (summary_collector runs
+    concatenate_summaries with melt=True and mode='collector')"""
+    kwargs["melt"] = True
+    kwargs["mode"] = "collector"
+    return concatenate_summaries(*args, **kwargs)
+
+
+def cycles_collector(
+    b,
+    cycles=None,
+    interpolated=True,
+    number_of_points=100,
+    max_cycle=50,
+    abort_on_missing=False,
+    method="back-and-forth",
+    label_mapper=None,
+):
+    if cycles is None:
+        cycles = list(range(1, max_cycle + 1))
+    all_curves = []
+    keys = []
+    for n, g, sg, c in pick_named_cell(b, label_mapper):
+        curves = c.get_cap(
+            cycle=cycles,
+            label_cycle_number=True,
+            interpolated=interpolated,
+            number_of_points=number_of_points,
+            method=method,
+        )
+        logging.debug(f"processing {n} (cell name: {c.cell_name})")
+        if not curves.empty:
+            curves = curves.assign(group=g, sub_group=sg)
+            all_curves.append(curves)
+            keys.append(n)
+        else:
+            if abort_on_missing:
+                raise ValueError(f"{n} is empty - aborting!")
+            logging.critical(f"[{n} (cell name: {c.cell_name}) empty]")
+    collected_curves = pd.concat(
+        all_curves, keys=keys, axis=0, names=["cell", "point"]
+    ).reset_index(level="cell")
+    return collected_curves
+
+
+def ica_collector(
+    b,
+    cycles=None,
+    voltage_resolution=0.005,
+    max_cycle=50,
+    abort_on_missing=False,
+    label_direction=True,
+    number_of_points=None,
+    label_mapper=None,
+    **kwargs,
+):
+    if cycles is None:
+        cycles = list(range(1, max_cycle + 1))
+    all_curves = []
+    keys = []
+    for n, g, sg, c in pick_named_cell(b, label_mapper):
+        curves = ica.dqdv_frames(
+            c,
+            cycle=cycles,
+            voltage_resolution=voltage_resolution,
+            label_direction=label_direction,
+            number_of_points=number_of_points,
+            **kwargs,
+        )
+        logging.debug(f"processing {n} (cell name: {c.cell_name})")
+        if not curves.empty:
+            curves = curves.assign(group=g, sub_group=sg)
+            all_curves.append(curves)
+            keys.append(n)
+        else:
+            if abort_on_missing:
+                raise ValueError(f"{n} is empty - aborting!")
+            logging.critical(f"[{n} (cell name: {c.cell_name}) empty]")
+    collected_curves = pd.concat(
+        all_curves, keys=keys, axis=0, names=["cell", "point"]
+    ).reset_index(level="cell")
+    return collected_curves
+
+
+def remove_markers(trace):
+    trace.update(marker=None, mode="lines")
+    return trace
+
+
+def _hist_eq(trace):
+    z = histogram_equalization(trace.z)
+    trace.update(z=z)
+    return trace
+
+
+def y_axis_replacer(ax, label):
+    ax.update(title_text=label)
+    return ax
+
+
+def legend_replacer(trace, df, group_legends=True):
+    name = trace.name
+    parts = name.split(",")
+    if len(parts) == 2:
+        group = int(parts[0])
+        subgroup = int(parts[1])
+    else:
+        print(
+            "Have not implemented replacing legend labels that are not on the form a,b yet."
+        )
+        print(f"legend label: {name}")
+        return trace
+
+    cell_label = df.loc[
+        (df["group"] == group) & (df["sub_group"] == subgroup), "cell"
+    ].values[0]
+    if group_legends:
+        trace.update(
+            name=cell_label,
+            legendgroup=group,
+            hovertemplate=f"{cell_label}<br>{trace.hovertemplate}",
+        )
+    else:
+        trace.update(
+            name=cell_label,
+            legendgroup=cell_label,
+            hovertemplate=f"{cell_label}<br>{trace.hovertemplate}",
+        )
+
+
+def sequence_plotter(
+    collected_curves: pd.DataFrame,
+    x: str = "capacity",
+    y: str = "voltage",
+    z: str = "cycle",
+    g: str = "cell",
+    standard_deviation: str = None,
+    group: str = "group",
+    subgroup: str = "sub_group",
+    x_label: str = "Capacity",
+    x_unit: str = "mAh/g",
+    y_label: str = "Voltage",
+    y_unit: str = "V",
+    z_label: str = "Cycle",
+    z_unit: str = "n.",
+    y_label_mapper: dict = None,
+    nbinsx: int = 100,
+    histfunc: str = "avg",
+    histscale: str = "abs-log",
+    direction: str = "charge",
+    direction_col: str = "direction",
+    method: str = "fig_pr_cell",
+    markers: bool = False,
+    group_cells: bool = True,
+    group_legend_muting: bool = True,
+    backend: str = "plotly",
+    cycles: list = None,
+    facetplot: bool = False,
+    cols: int = 3,
+    palette_discrete: str = None,
+    palette_continuous: str = "Viridis",
+    palette_range: tuple = None,
+    height: float = None,
+    width: float = None,
+    **kwargs,
+) -> Any:
+    """create a plot made up of sequences of data (voltage curves, dQ/dV, etc).
+
+    This method contains the "common" operations done for all the sequence plots,
+    currently supporting filtering out the specific cycles, selecting either
+    dividing into subplots by cell or by cycle, and creating the (most basic) figure object.
+
+    Args:
+        collected_curves (pd.DataFrame): collected data in long format.
+        x: column name for x-values.
+        y: column name for y-values.
+        z: if method is 'fig_pr_cell', column name for color (legend), else for subplot.
+        g: if method is 'fig_pr_cell', column name for subplot, else for color.
+        standard_deviation: str = standard deviation column (skipped if None).
+        group: str = "group",
+        subgroup: str = "sub_group",
+        x_label: str = "",
+        x_unit:
+        y_label:
+        y_unit:
+        z_label: str = "Cycle"
+        z_unit: str = "n."
+        y_label_mapper: dict
+        nbinsx: int = 100
+        histfunc: str = "avg"
+        histscale (str) = "abs-log" used for scaling the z-values for 2D array plots (heatmaps and similar).
+        direction (str) = "charge", "discharge", or "both".
+        direction_col (str) = "direction",
+        method: 'fig_pr_cell' or 'fig_pr_cycle'.
+        markers: set to True if you want markers.
+        group_cells (bool):
+        group_legend_muting (bool):
+        backend: what backend to use.
+        cycles: what cycles to include in the plot.
+        palette_discrete:
+        palette_continuous:
+        palette_range (tuple):
+        facetplot (bool): square layout with group horizontally and subgroup vertically.
+        cols: number of columns for layout.
+        height:
+        width:
+
+        **kwargs: sent to backend (if `backend == "plotly"`, it will be
+            sent to `plotly.express` etc.)
+
+    Returns:
+        figure object
+    """
+    logging.debug("running sequence plotter")
+
+    for k in kwargs:
+        logging.debug(f"keyword argument sent to the backend: {k}")
+
+    curves = None
+    seaborn_arguments = dict()
+
+    if method == "film":
+        labels = {
+            f"{x}": f"{x_label} ({x_unit})",
+            f"{z}": f"{z_label} ({z_unit})",
+        }
+        plotly_arguments = dict(
+            x=x,
+            y=z,
+            z=y,
+            labels=labels,
+            facet_col_wrap=cols,
+            nbinsx=nbinsx,
+            histfunc=histfunc,
+        )
+
+    elif method == "summary":
+        labels = {
+            f"{x}": f"{x_label} ({x_unit})",
+        }
+        plotly_arguments = dict(x=x, y=y, labels=labels, markers=markers)
+        if g == "variable" and len(collected_curves[g].unique()) > 1:
+            plotly_arguments["facet_row"] = g
+        if standard_deviation:
+            plotly_arguments["error_y"] = standard_deviation
+
+    else:
+        labels = {
+            f"{x}": f"{x_label} ({x_unit})",
+            f"{y}": f"{y_label} ({y_unit})",
+        }
+        plotly_arguments = dict(x=x, y=y, labels=labels, facet_col_wrap=cols)
+
+    if method in ["fig_pr_cell", "film"]:
+        group_cells = False
+        if method == "fig_pr_cell":
+            plotly_arguments["markers"] = markers
+            plotly_arguments["color"] = z
+        if facetplot:
+            plotly_arguments["facet_col"] = group
+            plotly_arguments["facet_row"] = subgroup
+            plotly_arguments["hover_name"] = g
+        else:
+            plotly_arguments["facet_col"] = g
+
+        if cycles is not None:
+            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
+        else:
+            curves = collected_curves
+        logging.debug(f"filtered_curves:\n{curves}")
+
+        if method == "film":
+            # selecting direction
+            if direction == "charge":
+                curves = curves.query(f"{direction_col} < 0")
+            elif direction == "discharge":
+                curves = curves.query(f"{direction_col} > 0")
+            # scaling (assuming 'y' is the "value" axis):
+            if histscale == "abs-log":
+                curves[y] = curves[y].apply(np.abs).apply(np.log)
+            elif histscale == "abs":
+                curves[y] = curves[y].apply(np.abs)
+            elif histscale == "norm":
+                curves[y] = curves[y].apply(np.abs)
+
+    elif method == "fig_pr_cycle":
+        z, g = g, z
+        plotly_arguments["facet_col"] = g
+
+        if cycles is not None:
+            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
+        else:
+            curves = collected_curves
+
+        if group_cells:
+            plotly_arguments["color"] = group
+            plotly_arguments["symbol"] = subgroup
+        else:
+            plotly_arguments["markers"] = markers
+            plotly_arguments["color"] = z
+
+    elif method == "summary":
+        logging.info("sequence-plotter - summary")
+        if cycles is not None:
+            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
+        else:
+            curves = collected_curves
+
+        if group_cells:
+            plotly_arguments["color"] = group
+            plotly_arguments["symbol"] = subgroup
+        else:
+            plotly_arguments["color"] = z
+
+    if backend == "plotly":
+        if method == "fig_pr_cell":
+            start, end = 0.0, 1.0
+            if palette_range is not None:
+                start, end = palette_range
+            unique_cycle_numbers = curves[z].unique()
+            number_of_colors = len(unique_cycle_numbers)
+
+            selected_colors = px.colors.sample_colorscale(
+                palette_continuous, number_of_colors, low=start, high=end
+            )
+            plotly_arguments["color_discrete_sequence"] = selected_colors
+        elif method == "fig_pr_cycle":
+            if palette_discrete is not None:
+                # plotly_arguments["color_discrete_sequence"] = getattr(px.colors.sequential, palette_discrete)
+                logging.debug(
+                    f"palette_discrete is not implemented yet ({palette_discrete})"
+                )
+
+        elif method == "film":
+            number_of_colors = 10
+            start, end = 0.0, 1.0
+            if palette_range is not None:
+                start, end = palette_range
+            plotly_arguments["color_continuous_scale"] = px.colors.sample_colorscale(
+                palette_continuous, number_of_colors, low=start, high=end
+            )
+
+        elif method == "summary":
+            logging.info("sequence-plotter - summary plotly")
+
+        abs_facet_row_spacing = kwargs.pop("abs_facet_row_spacing", 20)
+        abs_facet_col_spacing = kwargs.pop("abs_facet_col_spacing", 20)
+        facet_row_spacing = kwargs.pop(
+            "facet_row_spacing", abs_facet_row_spacing / height if height else 0.1
+        )
+        facet_col_spacing = kwargs.pop(
+            "facet_col_spacing", abs_facet_col_spacing / (width or 1000)
+        )
+
+        plotly_arguments["facet_row_spacing"] = facet_row_spacing
+        plotly_arguments["facet_col_spacing"] = facet_col_spacing
+
+        logging.debug(f"{plotly_arguments=}")
+        logging.debug(f"{kwargs=}")
+
+        fig = None
+        if method in ["fig_pr_cycle", "fig_pr_cell"]:
+            fig = px.line(
+                curves,
+                **plotly_arguments,
+                **kwargs,
+            )
+
+            if method == "fig_pr_cycle" and group_cells:
+                try:
+                    fig.for_each_trace(
+                        functools.partial(
+                            legend_replacer,
+                            df=curves,
+                            group_legends=group_legend_muting,
+                        )
+                    )
+                    if markers is not True:
+                        fig.for_each_trace(remove_markers)
+                except Exception as e:
+                    print("failed")
+                    print(e)
+
+        elif method == "film":
+            fig = px.density_heatmap(curves, **plotly_arguments, **kwargs)
+            if histscale is None:
+                color_bar_txt = f"{y_label} ({y_unit})"
+            else:
+                color_bar_txt = f"{y_label} ({histscale})"
+
+            if histscale == "hist-eq":
+                fig = fig.for_each_trace(lambda _x: _hist_eq(_x))
+
+            fig.update_layout(coloraxis_colorbar_title_text=color_bar_txt)
+
+        elif method == "summary":
+            # print("TRYING...")
+            #
+            # print(f"{plotly_arguments=}")
+            # print(f"{kwargs=}")
+            # print(f"{curves.columns}")
+            fig = px.line(
+                curves,
+                **plotly_arguments,
+                **kwargs,
+            )
+            if group_cells:
+                try:
+                    fig.for_each_trace(
+                        functools.partial(
+                            legend_replacer,
+                            df=curves,
+                            group_legends=group_legend_muting,
+                        )
+                    )
+                    if markers is not True:
+                        fig.for_each_trace(remove_markers)
+                except Exception as e:
+                    print("failed")
+                    print(e)
+
+            if y_label_mapper:
+                annotations = fig.layout.annotations
+                if annotations:
+                    try:
+                        # might consider a more robust method here - currently
+                        # it assumes that the mapper is a list with same order
+                        # and length as number of rows
+                        for i, (a, la) in enumerate(zip(annotations, y_label_mapper)):
+                            row = i + 1
+                            fig.for_each_yaxis(
+                                functools.partial(y_axis_replacer, label=la),
+                                row=row,
+                            )
+                        fig.update_annotations(text="")
+                    except Exception as e:
+                        print("failed")
+                        print(e)
+                else:
+                    try:
+                        fig.for_each_yaxis(
+                            functools.partial(y_axis_replacer, label=y_label_mapper[0]),
+                        )
+                    except Exception as e:
+                        print("failed")
+                        print(e)
+
+        else:
+            print(f"method '{method}' is not supported by plotly")
+
+        return fig
+
+    elif backend == "matplotlib":
+        print(f"{backend} not implemented yet")
+
+    elif backend == "bokeh":
+        print(f"{backend} not implemented yet")
+
+
+def _cycles_plotter(
+    collected_curves,
+    cycles=None,
+    x="capacity",
+    y="voltage",
+    z="cycle",
+    g="cell",
+    standard_deviation=None,
+    default_title="Charge-Discharge Curves",
+    backend="plotly",
+    method="fig_pr_cell",
+    **kwargs,
+):
+    """Plot charge-discharge curves.
+
+    Args:
+        collected_curves(pd.DataFrame): collected data in long format.
+        backend (str): what backend to use.
+        method (str): 'fig_pr_cell' or 'fig_pr_cycle'.
+
+        **kwargs: consumed first in current function, rest sent to backend in sequence_plotter.
+
+    Returns:
+        styled figure object
+    """
+
+    # --- pre-processing ---
+    logging.debug("picking kwargs for current level - rest goes to sequence_plotter")
+    title = kwargs.pop("title", default_title)
+    width = kwargs.pop("width", None)
+    height = kwargs.pop("height", None)
+    palette = kwargs.pop("palette", None)
+    legend_position = kwargs.pop("legend_position", None)
+    legend_title = kwargs.pop("legend_title", None)
+    show_legend = kwargs.pop("show_legend", None)
+    cols = kwargs.pop("cols", 3)
+    sub_fig_min_height = kwargs.pop("sub_fig_min_height", 200)
+
+    # kwargs from default `BatchCollector.render` method not used by `sequence_plotter`:
+    journal = kwargs.pop("journal", None)
+    units = kwargs.pop("units", None)
+
+    if palette is not None:
+        kwargs["palette_continuous"] = palette
+        kwargs["palette_discrete"] = palette
+
+    if legend_title is None:
+        if method == "fig_pr_cell":
+            legend_title = "Cycle"
+        else:
+            legend_title = "Cell"
+
+    no_cols = cols
+
+    if method in ["fig_pr_cell", "film"]:
+        number_of_figs = len(collected_curves["cell"].unique())
+
+    elif method == "fig_pr_cycle":
+        if cycles is not None:
+            number_of_figs = len(cycles)
+        else:
+            number_of_figs = len(collected_curves["cycle"].unique())
+    elif method == "summary":
+        number_of_figs = len(collected_curves["variable"].unique())
+        sub_fig_min_height = 300
+    else:
+        number_of_figs = 1
+
+    no_rows = math.ceil(number_of_figs / no_cols)
+
+    if not height:
+        height = no_rows * sub_fig_min_height
+
+    fig = sequence_plotter(
+        collected_curves,
+        x=x,
+        y=y,
+        z=z,
+        g=g,
+        standard_deviation=standard_deviation,
+        backend=backend,
+        method=method,
+        cols=cols,
+        cycles=cycles,
+        width=width,
+        height=height,
+        **kwargs,
+    )
+    if fig is None:
+        print("Could not create figure")
+        return
+
+    # Rendering:
+    if backend == "plotly":
+        template = f"{PLOTLY_BASE_TEMPLATE}+{method}"
+
+        legend_orientation = "v"
+        if legend_position == "bottom":
+            legend_orientation = "h"
+
+        legend_dict = {
+            "title": legend_title,
+            "orientation": legend_orientation,
+        }
+        title_dict = {
+            "text": title,
+        }
+
+        fig.update_layout(
+            template=template,
+            title=title_dict,
+            legend=legend_dict,
+            showlegend=show_legend,
+            height=height,
+            width=width,
+        )
+
+    return fig
+
+
+def summary_plotter(collected_curves, cycles_to_plot=None, backend="plotly", **kwargs):
+    """Plot summaries (value vs cycle number).
+
+    Assuming data as pandas.DataFrame with either
+    1) long format (where variables, for example charge capacity, are in the column "variable") or
+    2) mixed long and wide format where the variables are own columns.
+    """
+    col_headers = collected_curves.columns.to_list()
+    possible_id_vars = [
+        "cell",
+        "cycle",
+        "equivalent_cycle",
+        "value",
+        "mean",
+        "std",
+        "group",
+        "sub_group",
+    ]
+
+    id_vars = []
+    for n in possible_id_vars:
+        if n in col_headers:
+            col_headers.remove(n)
+            id_vars.append(n)
+
+    if "variable" not in col_headers:
+        collected_curves = collected_curves.melt(
+            id_vars=id_vars, value_vars=col_headers
+        )
+
+    normalize_cycles = True if "equivalent_cycle" in id_vars else False
+    group_it = False if "group" in id_vars else True
+
+    cols = kwargs.pop("cols", 1)
+
+    z = "cell"
+    g = "variable"
+
+    if normalize_cycles:
+        x = "equivalent_cycle"
+        x_label = "Equivalent Cycle"
+        x_unit = "cum/nom.cap."
+    else:
+        x = "cycle"
+        x_label = "Cycle"
+        x_unit = "n."
+
+    if group_it:
+        group_cells = False
+        y = "mean"
+        standard_deviation = "std"
+
+    else:
+        y = "value"
+        standard_deviation = None
+        group_cells = kwargs.pop("group_cells", True)
+
+    units = kwargs.pop("units", None)
+    label_mapper = {
+        f"{y}": None,
+    }
+    if units:
+        label_mapper[y] = []
+        variables = list(collected_curves[g].unique())
+        for v in variables:
+            # extract units:
+            u_sub = None
+            if v.endswith("_areal"):
+                u_sub = units["cellpy_units"].specific_areal
+            elif v.endswith("_gravimetric"):
+                u_sub = units["cellpy_units"].specific_gravimetric
+            elif v.endswith("_volumetric"):
+                u_sub = units["cellpy_units"].specific_volumetric
+            u_top = None
+            if "_capacity" in v:
+                u_top = units["cellpy_units"].charge
+
+            # creating label:
+            u = u_top or "Value"
+            v = v.split("_")
+            if u_sub:
+                u_sub = u_sub.replace("**", "")
+                u = f"{u}/{u_sub}"
+                v = v[:-1]
+            v = " ".join(v).title()
+            label_mapper[y].append(f"{v} ({u})")
+
+    fig = _cycles_plotter(
+        collected_curves,
+        x=x,
+        y=y,
+        z=z,
+        g=g,
+        standard_deviation=standard_deviation,
+        x_label=x_label,
+        x_unit=x_unit,
+        y_label_mapper=label_mapper[y],
+        group_cells=group_cells,
+        default_title="Summary Plot",
+        backend=backend,
+        method="summary",
+        cycles=cycles_to_plot,
+        cols=cols,
+        **kwargs,
+    )
+
+    fig.update_yaxes(matches=None, showticklabels=True)
+    return fig
+
+
+def cycles_plotter(
+    collected_curves,
+    cycles_to_plot=None,
+    backend="plotly",
+    method="fig_pr_cell",
+    **kwargs,
+):
+    """Plot charge-discharge curves.
+
+    Args:
+        collected_curves(pd.DataFrame): collected data in long format.
+        cycles_to_plot (list): cycles to plot
+        backend (str): what backend to use.
+        method (str): 'fig_pr_cell' or 'fig_pr_cycle'.
+
+        **kwargs: consumed first in current function, rest sent to backend in sequence_plotter.
+
+    Returns:
+        styled figure object
+    """
+
+    if cycles_to_plot is not None:
+        unique_cycles = list(collected_curves.cycle.unique())
+        if len(unique_cycles) > 50:
+            cycles_to_plot = DEFAULT_CYCLES
+
+    return _cycles_plotter(
+        collected_curves,
+        x="capacity",
+        y="voltage",
+        z="cycle",
+        g="cell",
+        default_title="Charge-Discharge Curves",
+        backend=backend,
+        method=method,
+        cycles=cycles_to_plot,
+        **kwargs,
+    )
+
+
+def ica_plotter(
+    collected_curves,
+    cycles_to_plot=None,
+    backend="plotly",
+    method="fig_pr_cell",
+    direction="charge",
+    **kwargs,
+):
+    """Plot charge-discharge curves.
+
+    Args:
+        collected_curves(pd.DataFrame): collected data in long format.
+        cycles_to_plot (list): cycles to plot
+        backend (str): what backend to use.
+        method (str): 'fig_pr_cell' or 'fig_pr_cycle' or 'film'.
+        direction (str): 'charge' or 'discharge'.
+
+        **kwargs: consumed first in current function, rest sent to backend in sequence_plotter.
+
+    Returns:
+        styled figure object
+    """
+
+    if cycles_to_plot is None:
+        unique_cycles = list(collected_curves.cycle.unique())
+        max_cycle = max(unique_cycles)
+        if len(unique_cycles) > 50:
+            cycles_to_plot = DEFAULT_CYCLES
+            max_cycle = max(cycles_to_plot)
+    else:
+        max_cycle = max(cycles_to_plot)
+
+    if direction not in ["charge", "discharge"]:
+        print(f"direction='{direction}' not allowed - setting it to 'charge'")
+        direction = "charge"
+    if method in ["fig_pr_cell", "film"]:
+        kwargs["range_y"] = kwargs.pop("range_y", None) or (1, max_cycle)
+
+    return _cycles_plotter(
+        collected_curves,
+        x="voltage",
+        y="dq",
+        z="cycle",
+        g="cell",
+        x_label="Voltage",
+        x_unit="V",
+        y_label="dQ/dV",
+        y_unit="mAh/g/V.",
+        default_title=f"Incremental Analysis Plots ({direction.capitalize()})",
+        direction=direction,
+        backend=backend,
+        method=method,
+        cycles=cycles_to_plot,
+        **kwargs,
+    )
+
+
+def histogram_equalization(image: np.array) -> np.array:
+    """Perform histogram equalization on a numpy array.
+
+    # from http://www.janeriksolem.net/histogram-equalization-with-python-and.html
+    """
+    number_bins = 256
+    scale = 100
+    image[np.isnan(image)] = 0.0
+    image_histogram, bins = np.histogram(image.flatten(), number_bins, density=True)
+    cdf = image_histogram.cumsum()  # cumulative distribution function
+    cdf = (scale - 1) * cdf / cdf[-1]  # normalize
+    # use linear interpolation of cdf to find new pixel values
+    image_equalized = np.interp(image.flatten(), bins[:-1], cdf)
+
+    return image_equalized.reshape(image.shape)
+
+
+if __name__ == "__main__":
+    from pathlib import Path
+    import os
+
+    import matplotlib.pyplot as plt
+    import pandas as pd
+    import numpy as np
+    import seaborn as sns
+    import plotly.express as px
+
+    import cellpy
+    from cellpy.utils import batch, helpers, plotutils
+
+    project_dir = Path("../../testdata/batch_project")
+    journal = project_dir / "test_project.json"
+    assert project_dir.is_dir()
+    assert journal.is_file()
+    os.chdir(project_dir)
+    print(f"cellpy version: {cellpy.__version__}")
+    cellpy.log.setup_logging("INFO")
+
+    b = batch.from_journal(journal)
+    b.link()
+    c = b.cells.first()
+
+    summaries = BatchSummaryCollector(
+        b,
+        normalize_cycles=False,
+        group_it=False,
+        autorun=False,
+        columns=["charge_capacity_areal", "charge_capacity_gravimetric"],
+    )
+    summaries.update(update_data=True, update_plot=True)
+
+    # must use .figure.show() when not running in notebook:
+    summaries.figure.show()
+    summaries.save()
+
+    dqdvs = BatchICACollector(b, plot_type="film")
+    dqdvs.figure.show()
+
+    print("Ended OK")
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/collectors_old.py` & `cellpy-1.0.0b1/cellpy/utils/collectors_old.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,1309 +1,1309 @@
-"""Collectors are used for simplifying plotting and exporting batch objects."""
-
-import textwrap
-from pprint import pprint
-from pathlib import Path
-from typing import Any
-import inspect
-import logging
-
-import pandas as pd
-
-import cellpy
-from cellpy.readers.core import group_by_interpolate
-from cellpy.utils.batch import Batch
-from cellpy.utils.helpers import concatenate_summaries
-from cellpy.utils.plotutils import plot_concatenated
-from cellpy.utils import ica
-
-try:
-    import holoviews as hv
-    from holoviews.core.io import Pickler
-    from holoviews import opts
-
-    HOLOVIEWS_AVAILABLE = True
-except ImportError:
-    print("Could not import Holoviews. Plotting will be disabled.")
-    HOLOVIEWS_AVAILABLE = False
-
-CELLPY_MINIMUM_VERSION = "0.4.3"
-
-
-def _setup():
-    _welcome_message()
-    _register_holoviews_renderers()
-
-
-def _welcome_message():
-    cellpy_version = cellpy.__version__
-    logging.info(f"cellpy version: {cellpy_version}")
-    logging.info(f"collectors need at least: {CELLPY_MINIMUM_VERSION}")
-
-
-def _register_holoviews_renderers(extensions=None):
-    if HOLOVIEWS_AVAILABLE:
-        if extensions is None:
-            extensions = "bokeh", "matplotlib"
-        logging.info(
-            f"Registering Holoviews extensions {extensions} for the cellpy collectors."
-        )
-        hv.extension(*extensions)
-    else:
-        logging.info(
-            "Could not import Holoviews. Your collectors will not be able to make figures."
-        )
-
-
-def _set_holoviews_renderer(extension=None):
-    if HOLOVIEWS_AVAILABLE:
-        extension = extension.lower()
-        current_backend = hv.Store.current_backend
-        if not extension == current_backend:
-            logging.info(f"switching backend to {extension}")
-            hv.Store.set_current_backend(extension)
-
-
-def _get_current_holoviews_renderer():
-    return hv.Store.current_backend
-
-
-_setup()
-
-
-class BatchCollector:
-    collector_name: str = None
-    data: pd.DataFrame = None
-    figure: Any = None
-    name: str = None
-    nick: str = None
-    autorun: bool = True
-    figure_directory: Path = Path("out")
-    data_directory: Path = Path("data/processed/")
-    renderer: Any = None
-
-    # override default arguments:
-    elevated_data_collector_arguments: dict = None
-    elevated_plotter_arguments: dict = None
-
-    # defaults (and used also when resetting):
-    _default_data_collector_arguments = {}
-    _default_plotter_arguments = {}
-
-    # templates override everything when using autorun:
-    _templates = {
-        "bokeh": [],
-        "matplotlib": [],
-        "plotly": [],
-    }
-
-    def __init__(
-        self,
-        b,
-        data_collector,
-        plotter,
-        collector_name=None,
-        name=None,
-        nick=None,
-        autorun=True,
-        use_templates=True,
-        elevated_data_collector_arguments=None,
-        elevated_plotter_arguments=None,
-        data_collector_arguments: dict = None,
-        plotter_arguments: dict = None,
-        **kwargs,
-    ):
-        """Update both the collected data and the plot(s).
-        Args:
-            b (cellpy.utils.Batch): the batch object.
-            data_collector (callable): method that collects the data.
-            plotter (callable): method that crates the plots.
-            collector_name (str): name of collector.
-            name (str or bool): name used for auto-generating filenames etc.
-            autorun (bool): run collector and plotter immediately if True.
-            use_templates (bool): also apply template(s) in autorun mode if True.
-            elevated_data_collector_arguments (dict): arguments picked up by the child class' initializer.
-            elevated_plotter_arguments (dict): arguments picked up by the child class' initializer.
-            data_collector_arguments (dict): keyword arguments sent to the data collector.
-            plotter_arguments (dict): keyword arguments sent to the plotter.
-            update_name (bool): update the name (using automatic name generation) based on new settings.
-            **kwargs: set Collector attributes.
-        """
-        self.b = b
-        self.data_collector = data_collector
-        self.plotter = plotter
-        self.nick = nick
-        self.collector_name = collector_name or "base"
-
-        # Arguments given as default arguments in the subclass have "low" priority (below elevated arguments at least):
-        self._data_collector_arguments = self._default_data_collector_arguments.copy()
-        self._plotter_arguments = self._default_plotter_arguments.copy()
-        self._update_arguments(data_collector_arguments, plotter_arguments)
-
-        # Elevated arguments have preference above the data_collector and plotter argument dicts:
-        self._parse_elevated_arguments(
-            elevated_data_collector_arguments, elevated_plotter_arguments
-        )
-
-        self._set_attributes(**kwargs)
-
-        if nick is None:
-            self.nick = b.name
-
-        if name is None:
-            name = self.generate_name()
-        self.name = name
-
-        if autorun:
-            self.update(update_name=False)
-            if use_templates:
-                self.apply_templates()
-
-    @property
-    def data_collector_arguments(self):
-        return self._data_collector_arguments
-
-    @data_collector_arguments.setter
-    def data_collector_arguments(self, argument_dict: dict):
-        if argument_dict is not None:
-            self._data_collector_arguments = {
-                **self._data_collector_arguments,
-                **argument_dict,
-            }
-
-    @property
-    def plotter_arguments(self):
-        return self._plotter_arguments
-
-    @plotter_arguments.setter
-    def plotter_arguments(self, argument_dict: dict):
-        if argument_dict is not None:
-            self._plotter_arguments = {**self._plotter_arguments, **argument_dict}
-
-    def __str__(self):
-        class_name = self.__class__.__name__
-
-        txt = f"{class_name}\n{len(class_name) * '='}\n\n"
-        txt += "Attributes:\n"
-        txt += "-----------\n"
-        txt += f" -collector_name: {self.collector_name}\n"
-        txt += f" -autorun: {self.autorun}\n"
-        txt += f" -name: {self.name}\n"
-        txt += f" -nick: {self.nick}\n"
-        txt += f" -csv_include_index: {self.csv_include_index}\n"
-        txt += f" -csv_layout: {self.csv_layout}\n"
-        txt += f" -sep: {self.sep}\n"
-        txt += f" -toolbar: {self.toolbar}\n"
-        txt += f" -figure_directory: {self.figure_directory}\n"
-        txt += f" -data_directory: {self.data_directory}\n"
-        txt += f" -batch-instance: {self.b.name}\n"
-        txt += f" -data_collector_arguments: {self.data_collector_arguments}\n"
-        txt += f" -plotter_arguments: {self.plotter_arguments}\n"
-
-        txt += "\nfigure:\n"
-        txt += ".......\n"
-        txt += f"{self.figure}\n"
-
-        txt += "\ndata:\n"
-        txt += ".....\n"
-        txt += f"{self.data}\n"
-
-        txt += "\nData collector:\n"
-        txt += "---------------\n"
-        data_name = self.data_collector.__name__
-        data_sig = inspect.signature(self.data_collector)
-        data_doc = inspect.getdoc(self.data_collector)
-        txt = f"{txt}{data_name}"
-        txt = f"{txt}{data_sig}\n"
-        txt = f"{txt}\n{data_doc}\n"
-
-        txt += "\nPlotter:\n"
-        txt += "--------\n"
-        plotter_name = self.plotter.__name__
-        plotter_sig = inspect.signature(self.plotter)
-        plotter_doc = inspect.getdoc(self.plotter)
-        txt = f"{txt}{plotter_name}"
-        txt = f"{txt}{plotter_sig}\n"
-        txt = f"{txt}\n{plotter_doc}\n"
-
-        return txt
-
-    def _repr_html_(self):
-        class_name = self.__class__.__name__
-        txt = f"<h2>{class_name}</h2> id={hex(id(self))}"
-        _txt = self.__str__().replace("\n", "<br>")
-        txt += f"<blockquote><code>{_txt}</></blockquote>"
-
-        return txt
-
-    def _set_attributes(self, **kwargs):
-        self.sep = kwargs.get("sep", ";")
-        self.csv_include_index = kwargs.get("csv_include_index", True)
-        self.csv_layout = kwargs.get("csv_layout", "long")
-        self.dpi = kwargs.get("dpi", 200)
-        self.toolbar = kwargs.get("toolbar", True)
-
-    def generate_name(self):
-        names = ["collector", self.collector_name]
-        if self.nick:
-            names.insert(0, self.nick)
-        name = "_".join(names)
-        return name
-
-    def _parse_elevated_arguments(
-        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
-    ):
-        if data_collector_arguments is not None:
-            logging.info(f"Updating elevated arguments")
-            elevated_data_collector_arguments = {}
-            for k, v in data_collector_arguments.items():
-                if v is not None:
-                    elevated_data_collector_arguments[k] = v
-            self._update_arguments(
-                elevated_data_collector_arguments, None, set_as_defaults=True
-            )
-
-        if plotter_arguments is not None:
-            logging.info(f"Updating elevated arguments")
-            elevated_plotter_arguments = {}
-            for k, v in plotter_arguments.items():
-                if v is not None:
-                    elevated_plotter_arguments[k] = v
-
-            self._update_arguments(
-                None, elevated_plotter_arguments, set_as_defaults=True
-            )
-
-    def _update_arguments(
-        self,
-        data_collector_arguments: dict = None,
-        plotter_arguments: dict = None,
-        set_as_defaults=False,
-    ):
-        self.data_collector_arguments = data_collector_arguments
-        self.plotter_arguments = plotter_arguments
-        logging.info(f"**data_collector_arguments: {self.data_collector_arguments}")
-        logging.info(f"**plotter_arguments: {self.plotter_arguments}")
-
-        # setting defaults also (py3.6 compatible):
-        if set_as_defaults:
-            logging.info("updating defaults for current instance")
-            if data_collector_arguments is not None:
-                self._default_data_collector_arguments = {
-                    **self._default_data_collector_arguments,
-                    **data_collector_arguments,
-                }
-            if plotter_arguments is not None:
-                self._default_plotter_arguments = {
-                    **self._default_plotter_arguments,
-                    **plotter_arguments,
-                }
-
-    def reset_arguments(
-        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
-    ):
-        """Reset the arguments to the defaults.
-        Args:
-            data_collector_arguments (dict): optional additional keyword arguments for the data collector.
-            plotter_arguments (dict): optional additional keyword arguments for the plotter.
-        """
-        self._data_collector_arguments = self._default_data_collector_arguments.copy()
-        self._plotter_arguments = self._default_plotter_arguments.copy()
-        self._update_arguments(data_collector_arguments, plotter_arguments)
-
-    def update(
-        self,
-        data_collector_arguments: dict = None,
-        plotter_arguments: dict = None,
-        reset: bool = False,
-        update_data: bool = False,
-        update_name: bool = False,
-        update_plot: bool = True,
-    ):
-        """Update both the collected data and the plot(s).
-        Args:
-            data_collector_arguments (dict): keyword arguments sent to the data collector.
-            plotter_arguments (dict): keyword arguments sent to the plotter.
-            reset (bool): reset the arguments first.
-            update_data (bool): update the data before updating the plot even if data has been collected before.
-            update_name (bool): update the name (using automatic name generation) based on new settings.
-            update_plot (bool): update the plot.
-        """
-        if reset:
-            self.reset_arguments(data_collector_arguments, plotter_arguments)
-        else:
-            self._update_arguments(data_collector_arguments, plotter_arguments)
-        if update_data or self.data is None:
-            try:
-                self.data = self.data_collector(self.b, **self.data_collector_arguments)
-            except TypeError as e:
-                print("Type error:", e)
-                print("Registered data_collector_arguments:")
-                pprint(self.data_collector_arguments)
-                print("Hint: fix it and then re-run using reset=True")
-                return
-        if update_plot:
-            if HOLOVIEWS_AVAILABLE:
-                _set_holoviews_renderer(self.plotter_arguments.get("extension"))
-                try:
-                    self.figure = self.plotter(
-                        self.data, journal=self.b.journal, **self.plotter_arguments
-                    )
-                except TypeError as e:
-                    print("Type error:", e)
-                    print("Registered plotter_arguments:")
-                    pprint(self.plotter_arguments)
-                    print("Hint: fix it and then re-run using reset=True")
-                    return
-
-        if update_name:
-            self.name = self.generate_name()
-
-    def _dynamic_update_template_parameter(self, hv_opt, extension, *args, **kwargs):
-        return hv_opt
-
-    def _register_template(self, hv_opts, extension="bokeh", *args, **kwargs):
-        """Register template for given extension.
-
-        It is also possible to set the options directly in the constructor of the
-        class. But it is recommended to use this method instead to allow for
-        sanitation of the options in the templates
-
-        Args:
-            hv_opts: list of holoviews.core.options.Options- instances
-                e.g. [hv.opts.Curve(xlim=(0,2)), hv.opts.NdLayout(title="Super plot")]
-            extension: Holoviews backend ("matplotlib", "bokeh", or "plotly")
-
-        Returns:
-            None
-        """
-        if extension not in ["bokeh", "matplotlib", "plotly"]:
-            print(f"extension='{extension}' is not supported.")
-        if not isinstance(hv_opts, (list, tuple)):
-            hv_opts = [hv_opts]
-
-        cleaned_hv_opts = []
-        for o in hv_opts:
-            logging.debug(f"Setting prm: {o}")
-            o = self._dynamic_update_template_parameter(o, extension, *args, **kwargs)
-            # ensure all options are registered with correct backend:
-            o.kwargs["backend"] = extension
-            cleaned_hv_opts.append(o)
-
-        self._templates[extension] = cleaned_hv_opts
-
-    def apply_templates(self):
-        if not self._figure_valid():
-            return
-
-        for backend, hv_opt in self._templates.items():
-            try:
-                if len(hv_opt):
-                    print(f"Applying template for {backend}:{hv_opt}")
-                    self.figure = self._set_hv_opts(hv_opt)
-            except TypeError:
-                print("possible bug in apply_template experienced")
-                print(self._templates)
-
-    def _figure_valid(self):
-        # TODO: create a decorator
-        if self.figure is None:
-            print("No figure to show!")
-            return False
-        if not HOLOVIEWS_AVAILABLE:
-            print("Requires Holoviews - please install it first!")
-            return False
-        return True
-
-    def _set_hv_opts(self, hv_opts):
-        if hv_opts is None:
-            return self.figure
-        if isinstance(hv_opts, (tuple, list)):
-            return self.figure.options(*hv_opts)
-        else:
-            return self.figure.options(hv_opts)
-
-    def show(self, hv_opts=None):
-        if not self._figure_valid():
-            return
-
-        print(f"figure name: {self.name}")
-        return self._set_hv_opts(hv_opts)
-
-    def redraw(self, hv_opts=None, extension=None):
-        print("EXPERIMENTAL FEATURE! THIS MIGHT NOT WORK PROPERLY YET")
-        if not self._figure_valid():
-            return
-
-        if extension is not None:
-            _set_holoviews_renderer(extension)
-
-        print(f"figure name: {self.name}")
-        self.figure = self._set_hv_opts(hv_opts)
-        return self.figure
-
-    def render(self):
-        print("Not implemented yet!")
-
-    def preprocess_data_for_csv(self):
-        print(f"the data layout {self.csv_layout} is not supported yet!")
-        return self.data
-
-    def to_csv(self, serial_number=None):
-        filename = self._output_path(serial_number)
-        filename = filename.with_suffix(".csv")
-        if self.csv_layout != "long":
-            data = self.preprocess_data_for_csv()
-        else:
-            data = self.data
-
-        data.to_csv(
-            filename,
-            sep=self.sep,
-            index=self.csv_include_index,
-        )
-        print(f"saved csv file: {filename}")
-
-    def to_image_files(self, serial_number=None):
-        if not self._figure_valid():
-            return
-        filename_pre = self._output_path(serial_number)
-
-        filename_hv = filename_pre.with_suffix(".html")
-        hv.save(
-            self.figure,
-            filename_hv,
-            toolbar=self.toolbar,
-        )
-        print(f"saved file: {filename_hv}")
-
-        filename_png = filename_pre.with_suffix(".png")
-        try:
-            current_renderer = _get_current_holoviews_renderer()
-
-            _set_holoviews_renderer("matplotlib")
-            self.figure.opts(hv.opts.NdOverlay(legend_position="right"))
-            hv.save(
-                self.figure,
-                filename_png,
-                dpi=300,
-            )
-            print(f"saved file: {filename_png}")
-        except Exception as e:
-            print("Could not save png-file.")
-            print(e)
-        finally:
-            _set_holoviews_renderer(current_renderer)
-
-    def save(self, serial_number=None):
-        self.to_csv(serial_number=serial_number)
-
-        if self._figure_valid():
-            filename = self._output_path(serial_number)
-            filename = filename.with_suffix(".hvz")
-            try:
-                Pickler.save(
-                    self.figure,
-                    filename,
-                )
-                print(f"pickled holoviews file: {filename}")
-            except TypeError as e:
-                print("could not save as hvz file")
-            self.to_image_files(serial_number=serial_number)
-
-    def _output_path(self, serial_number=None):
-        d = Path(self.figure_directory)
-        n = self.name
-        if serial_number is not None:
-            n = f"{n}_{serial_number:03}"
-        f = d / n
-        return f
-
-
-# TODO: allow for storing more than one figure setup pr collector
-#    It is time-consuming and memory demanding to re-collect the data
-#    for each time we need a new figure for the collector. We should
-#    allow for creating multiple figures within one collector or for
-#    sharing (passing) collected data. One solution might be to extend
-#    the capabilities of the base class. Another solution might be to
-#    add another sub-class in the chain from the base class to the actual one:
-class BatchMultiFigureCollector(BatchCollector):
-    pass
-
-
-def pick_named_cell(b, label_mapper=None):
-    """generator that picks a cell from the batch object, yields its label and the cell itself.
-
-    Args:
-        b (cellpy.batch object): your batch object
-        label_mapper (callable or dict): function (or dict) that changes the cell names.
-            The dictionary must have the cell labels as given in the `journal.pages` index and new label as values.
-            Similarly, if it is a function it takes the cell label as input and returns the new label.
-            Remark! No check are performed to ensure that the new cell labels are unique.
-
-    Yields:
-        label, group, subgroup, cell
-
-    Example:
-        def my_mapper(n):
-            return "_".join(n.split("_")[1:-1])
-
-        # outputs "nnn_x" etc., if cell-names are of the form "date_nnn_x_y":
-        for label, group, subgroup, cell in pick_named_cell(b, label_mapper=my_mapper):
-            print(label)
-    """
-
-    cell_names = b.cell_names
-    for n in cell_names:
-        group = b.pages.loc[n, "group"]
-        sub_group = b.pages.loc[n, "sub_group"]
-
-        if label_mapper is not None:
-            try:
-                if isinstance(label_mapper, dict):
-                    label = label_mapper[n]
-                else:
-                    label = label_mapper(n)
-            except Exception as e:
-                logging.info(f"label_mapper-error: could not rename cell {n}")
-                logging.debug(f"caught exception: {e}")
-                label = n
-        else:
-            try:
-                label = b.pages.loc[n, "label"]
-            except Exception as e:
-                logging.info(f"lookup in pages failed: could not rename cell {n}")
-                logging.debug(f"caught exception: {e}")
-                label = n
-
-        logging.info(f"renaming {n} -> {label} (group={group}, subgroup={sub_group})")
-        yield label, group, sub_group, b.experiment.data[n]
-
-
-def cycles_collector(
-    b,
-    cycles=None,
-    interpolated=True,
-    number_of_points=100,
-    max_cycle=50,
-    abort_on_missing=False,
-    method="back-and-forth",
-    label_mapper=None,
-):
-    if cycles is None:
-        cycles = list(range(1, max_cycle + 1))
-    all_curves = []
-    keys = []
-    for n, g, sg, c in pick_named_cell(b, label_mapper):
-        curves = c.get_cap(
-            cycle=cycles,
-            label_cycle_number=True,
-            interpolated=interpolated,
-            number_of_points=number_of_points,
-            method=method,
-        )
-        logging.debug(f"processing {n} (cell name: {c.cell_name})")
-        if not curves.empty:
-            curves = curves.assign(group=g, sub_group=sg)
-            all_curves.append(curves)
-            keys.append(n)
-        else:
-            if abort_on_missing:
-                raise ValueError(f"{n} is empty - aborting!")
-            logging.critical(f"[{n} (cell name: {c.cell_name}) empty]")
-    collected_curves = pd.concat(
-        all_curves, keys=keys, axis=0, names=["cell", "point"]
-    ).reset_index(level="cell")
-    return collected_curves
-
-
-def cycles_plotter_simple_holo_map(collected_curves, journal=None, **kwargs):
-    p = hv.Curve(
-        collected_curves, kdims="capacity", vdims=["voltage", "cycle", "cell"]
-    ).groupby("cell")
-    return p
-
-
-def ica_plotter(
-    collected_curves,
-    journal=None,
-    palette="Blues",
-    palette_range=(0.2, 1.0),
-    method="fig_pr_cell",
-    extension="bokeh",
-    cycles_to_plot=None,
-    cols=1,
-    width=None,
-    height=None,
-    xlim_charge=(None, None),
-    xlim_discharge=(None, None),
-    **kwargs,
-):
-    if method == "film":
-        if extension == "matplotlib":
-            print("SORRY, PLOTTING FILM WITH MATPLOTLIB IS NOT IMPLEMENTED YET")
-            return
-
-        return ica_plotter_film_bokeh(
-            collected_curves,
-            journal=journal,
-            palette=palette,
-            extension="bokeh",
-            cycles=cycles_to_plot,
-            xlim_charge=xlim_charge,
-            xlim_discharge=xlim_discharge,
-            width=width,
-            height=height,
-            **kwargs,
-        )
-    else:
-        return sequence_plotter(
-            collected_curves,
-            x="voltage",
-            y="dq",
-            z="cycle",
-            g="cell",
-            journal=journal,
-            palette=palette,
-            palette_range=palette_range,
-            method=method,
-            extension=extension,
-            cycles=cycles_to_plot,
-            cols=cols,
-            width=width,
-        )
-
-
-def ica_plotter_film_bokeh(
-    collected_curves,
-    palette="Blues",
-    cycles=None,
-    xlim_charge=(None, None),
-    xlim_discharge=(None, None),
-    ylim=(None, None),
-    shared_axes=True,
-    width=400,
-    height=500,
-    cformatter="%02.0e",
-    **kwargs,
-):
-    if cycles is not None:
-        filtered_curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
-    else:
-        filtered_curves = collected_curves
-
-    options = {
-        "xlabel": "Voltage (V)",
-        "ylabel": "Cycle",
-        "ylim": ylim,
-        "tools": ["hover"],
-        "width": width,
-        "height": height,
-        "cmap": palette,
-        "cformatter": cformatter,
-        "cnorm": "eq_hist",
-        "shared_axes": shared_axes,
-        "colorbar_opts": {
-            "title": "dQ/dV",
-        },
-    }
-
-    all_charge_plots = {}
-    all_discharge_plots = {}
-    for label, df in filtered_curves.groupby("cell"):
-        _charge = df.query("direction==1")
-        _discharge = df.query("direction==-1")
-        _dq_charge = group_by_interpolate(
-            _charge, x="voltage", y="dq", group_by="cycle", number_of_points=400
-        )
-        _dq_discharge = group_by_interpolate(
-            _discharge, x="voltage", y="dq", group_by="cycle", number_of_points=400
-        )
-
-        _v_charge = _dq_charge.index.values.ravel()
-        _v_discharge = _dq_discharge.index.values.ravel()
-
-        _cycles_charge = _charge.cycle.unique().ravel()
-        _cycles_discharge = _discharge.cycle.unique().ravel()
-
-        _dq_charge = -_dq_charge.values.T
-        _dq_discharge = _dq_discharge.values.T
-
-        charge_plot = hv.Image(
-            (_v_charge, _cycles_charge, _dq_charge), group="ica", label="charge"
-        ).opts(title=f"{label}", xlim=xlim_charge, colorbar=True, **options)
-
-        discharge_plot = hv.Image(
-            (_v_discharge, _cycles_discharge, _dq_discharge),
-            group="ica",
-            label="discharge",
-        ).opts(title=f"{label}", xlim=xlim_discharge, colorbar=True, **options)
-
-        all_charge_plots[f"{label}_charge"] = charge_plot
-        all_discharge_plots[f"{label}_discharge"] = discharge_plot
-
-    all_plots = {**all_charge_plots, **all_discharge_plots}
-    return (
-        hv.NdLayout(all_plots)
-        .opts(title="Incremental Capacity Analysis Film-plots")
-        .cols(2)
-    )
-
-
-def cycles_plotter(
-    collected_curves,
-    method="fig_pr_cell",
-    extension="bokeh",
-    cycles_to_plot=None,
-    width=None,
-    palette=None,
-    palette_range=(0.1, 1.0),
-    legend_position=None,
-    show_legend=None,
-    fig_title="",
-    cols=None,
-    **kwargs,
-):
-    if cols is None:
-        if extension == "matplotlib":
-            cols = 3
-        else:
-            cols = 3 if method == "fig_pr_cell" else 1
-
-    if width is None:
-        width = 400 if method == "fig_pr_cell" else int(800 / cols)
-
-    if palette is None:
-        palette = "Blues" if method == "fig_pr_cell" else "Category10"
-
-    if palette_range is None:
-        palette_range = (0.2, 1.0) if method == "fig_pr_cell" else (0, 1)
-
-    if legend_position is None:
-        legend_position = None if method == "fig_pr_cell" else "right"
-
-    if show_legend is None:
-        show_legend = True
-
-    reverse_palette = True if method == "fig_pr_cell" else False
-
-    backend_specific_kwargs = {
-        "NdLayout": {},
-        "NdOverlay": {},
-        "Curve": {},
-    }
-
-    if extension != "matplotlib":
-        logging.debug(f"setting width for bokeh and plotly: {width}")
-        backend_specific_kwargs["Curve"]["width"] = width
-
-    p = sequence_plotter(
-        collected_curves,
-        x="capacity",
-        y="voltage",
-        z="cycle",
-        g="cell",
-        method=method,
-        cycles=cycles_to_plot,
-        **kwargs,
-    ).cols(cols)
-
-    p.opts(
-        hv.opts.NdLayout(
-            title=fig_title,
-            **backend_specific_kwargs["NdLayout"],
-            backend=extension,
-        ),
-        hv.opts.NdOverlay(
-            **backend_specific_kwargs["NdOverlay"],
-            backend=extension,
-        ),
-        hv.opts.Curve(
-            # TODO: should replace this with custom mapping (see how it is done in plotutils):
-            color=hv.Palette(palette, reverse=reverse_palette, range=palette_range),
-            show_legend=show_legend,
-            **backend_specific_kwargs["Curve"],
-            backend=extension,
-        ),
-    )
-
-    if legend_position is not None:
-        p.opts(hv.opts.NdOverlay(legend_position=legend_position))
-
-    return p
-
-
-def sequence_plotter(
-    collected_curves,
-    x,
-    y,
-    z,
-    g,
-    method="fig_pr_cell",
-    group_label="group",
-    group_txt="cell-group",
-    z_lim=10,
-    cycles=None,
-    **kwargs,
-):
-    for k in kwargs:
-        logging.debug(f"keyword argument {k} given, but not used")
-
-    x_label = "Capacity"
-    x_unit = "mAh"
-
-    y_label = "Voltage"
-    y_unit = "V"
-
-    g_label = "Cell"
-    g_unit = ""
-
-    z_label = "Cycle"
-    z_unit = ""
-
-    x_dim = hv.Dimension(f"{x}", label=x_label, unit=x_unit)
-    y_dim = hv.Dimension(f"{y}", label=y_label, unit=y_unit)
-    g_dim = hv.Dimension(f"{g}", label=g_label, unit=g_unit)
-    z_dim = hv.Dimension(f"{z}", label=z_label, unit=z_unit)
-
-    family = {}
-    curves = None
-
-    if method == "fig_pr_cell":
-        if cycles is not None:
-            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
-        else:
-            curves = collected_curves
-        logging.debug(f"filtered_curves:\n{curves}")
-
-    elif method == "fig_pr_cycle":
-        if cycles is None:
-            unique_cycles = list(collected_curves.cycle.unique())
-            if len(unique_cycles) > 10:
-                cycles = [1, 10, 20]
-        if cycles is not None:
-            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
-        else:
-            curves = collected_curves
-        # g (what we split the figures by) : cycle
-        # z (the "dimension" of the individual curves in one figure): cell
-        z, g = g, z
-        z_dim, g_dim = g_dim, z_dim
-
-        # dirty (?) fix to make plots with a lot of cells look a bit better:
-        unique_z_values = collected_curves[z].unique()
-        no_unique_z_values = len(unique_z_values)
-        if no_unique_z_values > z_lim:
-            logging.critical(
-                f"number of cells ({no_unique_z_values}) larger than z_lim ({z_lim}): grouping"
-            )
-            logging.critical(
-                f"prevent this by modifying z_lim to your plotter_arguments"
-            )
-            z = group_label
-            z_dim = hv.Dimension(f"{z}", label=group_txt, unit="")
-
-    kdims = x_dim
-    vdims = [y_dim, z_dim]
-    for cyc, df in curves.groupby(g):
-        family[cyc] = hv.Curve(df, kdims=kdims, vdims=vdims).groupby(z).overlay()
-
-    return hv.NdLayout(family, kdims=g_dim)
-
-
-def ica_collector(
-    b,
-    cycles=None,
-    voltage_resolution=0.005,
-    max_cycle=50,
-    abort_on_missing=False,
-    label_direction=True,
-    number_of_points=None,
-    label_mapper=None,
-    **kwargs,
-):
-    if cycles is None:
-        cycles = list(range(1, max_cycle + 1))
-    all_curves = []
-    keys = []
-    for n, g, sg, c in pick_named_cell(b, label_mapper):
-        curves = ica.dqdv_frames(
-            c,
-            cycle=cycles,
-            voltage_resolution=voltage_resolution,
-            label_direction=label_direction,
-            number_of_points=number_of_points,
-            **kwargs,
-        )
-        logging.debug(f"processing {n} (session name: {c.cell_name})")
-        if not curves.empty:
-            curves = curves.assign(group=g, sub_group=sg)
-            all_curves.append(curves)
-            keys.append(n)
-        else:
-            if abort_on_missing:
-                raise ValueError(f"{n} is empty - aborting!")
-            logging.critical(f"[{n} (session name: {c.cell_name}) empty]")
-    collected_curves = pd.concat(
-        all_curves, keys=keys, axis=0, names=["cell", "point"]
-    ).reset_index(level="cell")
-    return collected_curves
-
-
-class BatchSummaryCollector(BatchCollector):
-    # Three main levels of arguments to the plotter and collector funcs is available:
-    #  - through dictionaries (`data_collector_arguments`, `plotter_arguments`) to init
-    #  - given as defaults in the subclass (`_default_data_collector_arguments`, `_default_plotter_arguments`)
-    #  - as elevated arguments (i.e. arguments normally given in the dictionaries elevated
-    #    to their own keyword parameters)
-
-    _default_data_collector_arguments = {
-        "columns": ["charge_capacity_gravimetric"],
-    }
-    _default_plotter_arguments = {
-        "extension": "bokeh",
-    }
-
-    _bokeh_template = [
-        hv.opts.Curve(fontsize={"title": "medium"}, width=800, backend="bokeh"),
-        hv.opts.NdOverlay(legend_position="right", backend="bokeh"),
-    ]
-
-    def __init__(
-        self,
-        b,
-        max_cycle: int = None,
-        rate=None,
-        on=None,
-        columns=None,
-        column_names=None,
-        normalize_capacity_on=None,
-        scale_by=None,
-        nom_cap=None,
-        normalize_cycles=None,
-        group_it=None,
-        rate_std=None,
-        rate_column=None,
-        inverse=None,
-        inverted: bool = None,
-        key_index_bounds=None,
-        extension: str = None,
-        title: str = None,
-        points: bool = None,
-        line: bool = None,
-        width: int = None,
-        height: int = None,
-        legend_title: str = None,
-        marker_size: int = None,
-        cmap=None,
-        spread: bool = None,
-        *args,
-        **kwargs,
-    ):
-        """Collects and shows summaries.
-
-        Elevated data collector args:
-            max_cycle (int): drop all cycles above this value.
-            rate (float): filter on rate (C-rate)
-            on (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
-            columns (list): selected column(s) (using cellpy attribute name)
-                [defaults to "charge_capacity_gravimetric"]
-            column_names (list): selected column(s) (using exact column name)
-            normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the
-                normalization (typically the first few cycles after formation)
-            scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap",
-                or given value (defaults to one).
-            nom_cap (float): nominal capacity of the cell
-            normalize_cycles (bool): perform a normalization of the cycle numbers (also called equivalent cycle index)
-            group_it (bool): if True, average pr group.
-            rate_std (float): allow for this inaccuracy when selecting cycles based on rate
-            rate_column (str): name of the column containing the C-rates.
-            inverse (bool): select steps that do not have the given C-rate.
-            inverted (bool): select cycles that do not have the steps filtered by given C-rate.
-            key_index_bounds (list): used when creating a common label for the cells by splitting and combining from
-                key_index_bound[0] to key_index_bound[1].
-
-        Elevated plotter args:
-            extension (str): extension used (defaults to Bokeh)
-            points (bool): plot points if True
-            line (bool): plot line if True
-            width: width of plot
-            height: height of plot
-            legend_title: title to put over the legend
-            marker_size: size of the markers used
-            cmap: color-map to use
-            spread (bool): plot error-bands instead of error-bars if True
-        """
-
-        elevated_data_collector_arguments = dict(
-            max_cycle=max_cycle,
-            rate=rate,
-            on=on,
-            columns=columns,
-            column_names=column_names,
-            normalize_capacity_on=normalize_capacity_on,
-            scale_by=scale_by,
-            nom_cap=nom_cap,
-            normalize_cycles=normalize_cycles,
-            group_it=group_it,
-            rate_std=rate_std,
-            rate_column=rate_column,
-            inverse=inverse,
-            inverted=inverted,
-            key_index_bounds=key_index_bounds,
-        )
-
-        elevated_plotter_arguments = {
-            "extension": extension,
-            "title": title,
-            "points": points,
-            "line": line,
-            "width": width,
-            "height": height,
-            "legend_title": legend_title,
-            "marker_size": marker_size,
-            "cmap": cmap,
-            "spread": spread,
-        }
-
-        self._register_template(self._bokeh_template, extension="bokeh")
-
-        super().__init__(
-            b,
-            plotter=plot_concatenated,
-            data_collector=concatenate_summaries,
-            collector_name="summary",
-            elevated_data_collector_arguments=elevated_data_collector_arguments,
-            elevated_plotter_arguments=elevated_plotter_arguments,
-            *args,
-            **kwargs,
-        )
-
-    def generate_name(self):
-        names = ["collected_summaries"]
-        cols = self.data_collector_arguments.get("columns")
-        grouped = self.data_collector_arguments.get("group_it")
-        equivalent_cycles = self.data_collector_arguments.get("normalize_cycles")
-        normalized_cap = self.data_collector_arguments.get("normalize_capacity_on", [])
-        if self.nick:
-            names.insert(0, self.nick)
-        if cols:
-            names.extend(cols)
-        if grouped:
-            names.append("average")
-        if equivalent_cycles:
-            names.append("equivalents")
-        if len(normalized_cap):
-            names.append("norm")
-
-        name = "_".join(names)
-        return name
-
-
-class BatchICACollector(BatchCollector):
-    _default_data_collector_arguments = {}
-    _default_plotter_arguments = {
-        "extension": "bokeh",
-    }
-
-    def __init__(self, b, plot_type="fig_pr_cell", *args, **kwargs):
-        """Create a collection of ica (dQ/dV) plots."""
-
-        self.plot_type = plot_type
-
-        if plot_type == "fig_pr_cell":
-            _tight = True
-            _fig_inches = 3.5
-        else:
-            _tight = False
-            _fig_inches = 5.5
-
-        matplotlib_template = [
-            hv.opts.Curve(
-                show_frame=True,
-                fontsize={"title": "medium"},
-                backend="matplotlib",
-            ),
-            hv.opts.NdLayout(
-                fig_inches=_fig_inches, tight=_tight, backend="matplotlib"
-            ),
-        ]
-
-        bokeh_template = [
-            hv.opts.Curve(xlabel="Voltage (V)", backend="bokeh"),
-        ]
-        self._default_plotter_arguments["method"] = plot_type
-        self._register_template(matplotlib_template, extension="matplotlib")
-        self._register_template(bokeh_template, extension="bokeh")
-        super().__init__(
-            b,
-            plotter=ica_plotter,
-            data_collector=ica_collector,
-            collector_name="ica",
-            *args,
-            **kwargs,
-        )
-
-    def generate_name(self):
-        names = ["collected_ica"]
-
-        pm = self.plotter_arguments.get("method")
-        if pm == "fig_pr_cell":
-            names.append("pr_cell")
-        elif pm == "fig_pr_cycle":
-            names.append("pr_cyc")
-        elif pm == "film":
-            names.append("film")
-
-        if self.nick:
-            names.insert(0, self.nick)
-
-        name = "_".join(names)
-        return name
-
-
-class BatchCyclesCollector(BatchCollector):
-    _default_data_collector_arguments = {
-        "interpolated": True,
-        "number_of_points": 100,
-        "max_cycle": 50,
-        "abort_on_missing": False,
-        "method": "back-and-forth",
-    }
-    _default_plotter_arguments = {
-        "extension": "bokeh",
-    }
-
-    def __init__(
-        self,
-        b,
-        plot_type="fig_pr_cell",
-        collector_type="back-and-forth",
-        cycles=None,
-        max_cycle=None,
-        label_mapper=None,
-        extension=None,
-        cycles_to_plot=None,
-        width=None,
-        palette=None,
-        show_legend=None,
-        legend_position=None,
-        fig_title=None,
-        cols=None,
-        *args,
-        **kwargs,
-    ):
-        """Create a collection of capacity plots.
-
-        Args:
-            b:
-            plot_type (str): either 'fig_pr_cell' or 'fig_pr_cycle'
-            collector_type (str): how the curves are given
-                "back-and-forth" - standard back and forth; discharge
-                    (or charge) reversed from where charge (or discharge) ends.
-                "forth" - discharge (or charge) continues along x-axis.
-                "forth-and-forth" - discharge (or charge) also starts at 0
-            data_collector_arguments (dict) - arguments transferred to the plotter
-            plotter_arguments (dict) - arguments transferred to the plotter
-
-        Elevated data collector args:
-            cycles (int): drop all cycles above this value.
-            max_cycle (float): filter on rate (C-rate)
-            label_mapper (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
-
-        Elevated plotter args:
-            extension (str): extension used (defaults to Bokeh)
-            cycles_to_plot (int): plot points if True
-            width (float): width of plot
-            legend_position (str): position of the legend
-            show_legend (bool): set to False if you don't want to show legend
-            fig_title (str): title (will be put above the figure)
-            palette (str): color-map to use
-            cols (int): number of columns
-        """
-
-        elevated_data_collector_arguments = dict(
-            cycles=cycles,
-            max_cycle=max_cycle,
-            label_mapper=label_mapper,
-        )
-        elevated_plotter_arguments = dict(
-            extension=extension,
-            cycles_to_plot=cycles_to_plot,
-            width=width,
-            palette=palette,
-            legend_position=legend_position,
-            show_legend=show_legend,
-            fig_title=fig_title,
-            cols=cols,
-        )
-
-        # internal attribute to keep track of plot type:
-        self.plot_type = plot_type
-
-        # moving it to after init to allow for using prms set in init
-        if plot_type == "fig_pr_cell":
-            _tight = True
-            _fig_inches = 3.5
-        else:
-            _tight = False
-            _fig_inches = 5.5
-
-        matplotlib_template = [
-            hv.opts.Curve(
-                show_frame=True,
-                fontsize={"title": "medium"},
-                ylim=(0, 1),
-                backend="matplotlib",
-            ),
-            hv.opts.NdLayout(
-                fig_inches=_fig_inches, tight=_tight, backend="matplotlib"
-            ),
-        ]
-
-        self._max_letters_in_cell_names = max(len(x) for x in b.cell_names)
-        self._register_template(matplotlib_template, extension="matplotlib")
-        self._default_data_collector_arguments["method"] = collector_type
-        self._default_plotter_arguments["method"] = plot_type
-
-        super().__init__(
-            b,
-            plotter=cycles_plotter,
-            data_collector=cycles_collector,
-            collector_name="cycles",
-            elevated_data_collector_arguments=elevated_data_collector_arguments,
-            elevated_plotter_arguments=elevated_plotter_arguments,
-            *args,
-            **kwargs,
-        )
-
-    def _dynamic_update_template_parameter(self, hv_opt, extension, *args, **kwargs):
-        k = hv_opt.key
-        if k == "NdLayout" and extension == "matplotlib":
-            if self.plot_type != "fig_pr_cycle":
-                hv_opt.kwargs["fig_inches"] = self._max_letters_in_cell_names * 0.14
-        return hv_opt
-
-    def generate_name(self):
-        names = ["collected_cycles"]
-
-        if self.data_collector_arguments.get("interpolated"):
-            names.append("intp")
-            if n := self.data_collector_arguments.get("number_of_points"):
-                names.append(f"p{n}")
-        cm = self.data_collector_arguments.get("method")
-        if cm.startswith("b"):
-            names.append("bf")
-        else:
-            names.append("ff")
-
-        pm = self.plotter_arguments.get("method")
-        if pm == "fig_pr_cell":
-            names.append("pr_cell")
-        elif pm == "fig_pr_cycle":
-            names.append("pr_cyc")
-
-        if self.nick:
-            names.insert(0, self.nick)
-
-        name = "_".join(names)
-        return name
+"""Collectors are used for simplifying plotting and exporting batch objects."""
+
+import textwrap
+from pprint import pprint
+from pathlib import Path
+from typing import Any
+import inspect
+import logging
+
+import pandas as pd
+
+import cellpy
+from cellpy.readers.core import group_by_interpolate
+from cellpy.utils.batch import Batch
+from cellpy.utils.helpers import concatenate_summaries
+from cellpy.utils.plotutils import plot_concatenated
+from cellpy.utils import ica
+
+try:
+    import holoviews as hv
+    from holoviews.core.io import Pickler
+    from holoviews import opts
+
+    HOLOVIEWS_AVAILABLE = True
+except ImportError:
+    print("Could not import Holoviews. Plotting will be disabled.")
+    HOLOVIEWS_AVAILABLE = False
+
+CELLPY_MINIMUM_VERSION = "0.4.3"
+
+
+def _setup():
+    _welcome_message()
+    _register_holoviews_renderers()
+
+
+def _welcome_message():
+    cellpy_version = cellpy.__version__
+    logging.info(f"cellpy version: {cellpy_version}")
+    logging.info(f"collectors need at least: {CELLPY_MINIMUM_VERSION}")
+
+
+def _register_holoviews_renderers(extensions=None):
+    if HOLOVIEWS_AVAILABLE:
+        if extensions is None:
+            extensions = "bokeh", "matplotlib"
+        logging.info(
+            f"Registering Holoviews extensions {extensions} for the cellpy collectors."
+        )
+        hv.extension(*extensions)
+    else:
+        logging.info(
+            "Could not import Holoviews. Your collectors will not be able to make figures."
+        )
+
+
+def _set_holoviews_renderer(extension=None):
+    if HOLOVIEWS_AVAILABLE:
+        extension = extension.lower()
+        current_backend = hv.Store.current_backend
+        if not extension == current_backend:
+            logging.info(f"switching backend to {extension}")
+            hv.Store.set_current_backend(extension)
+
+
+def _get_current_holoviews_renderer():
+    return hv.Store.current_backend
+
+
+_setup()
+
+
+class BatchCollector:
+    collector_name: str = None
+    data: pd.DataFrame = None
+    figure: Any = None
+    name: str = None
+    nick: str = None
+    autorun: bool = True
+    figure_directory: Path = Path("out")
+    data_directory: Path = Path("data/processed/")
+    renderer: Any = None
+
+    # override default arguments:
+    elevated_data_collector_arguments: dict = None
+    elevated_plotter_arguments: dict = None
+
+    # defaults (and used also when resetting):
+    _default_data_collector_arguments = {}
+    _default_plotter_arguments = {}
+
+    # templates override everything when using autorun:
+    _templates = {
+        "bokeh": [],
+        "matplotlib": [],
+        "plotly": [],
+    }
+
+    def __init__(
+        self,
+        b,
+        data_collector,
+        plotter,
+        collector_name=None,
+        name=None,
+        nick=None,
+        autorun=True,
+        use_templates=True,
+        elevated_data_collector_arguments=None,
+        elevated_plotter_arguments=None,
+        data_collector_arguments: dict = None,
+        plotter_arguments: dict = None,
+        **kwargs,
+    ):
+        """Update both the collected data and the plot(s).
+        Args:
+            b (cellpy.utils.Batch): the batch object.
+            data_collector (callable): method that collects the data.
+            plotter (callable): method that crates the plots.
+            collector_name (str): name of collector.
+            name (str or bool): name used for auto-generating filenames etc.
+            autorun (bool): run collector and plotter immediately if True.
+            use_templates (bool): also apply template(s) in autorun mode if True.
+            elevated_data_collector_arguments (dict): arguments picked up by the child class' initializer.
+            elevated_plotter_arguments (dict): arguments picked up by the child class' initializer.
+            data_collector_arguments (dict): keyword arguments sent to the data collector.
+            plotter_arguments (dict): keyword arguments sent to the plotter.
+            update_name (bool): update the name (using automatic name generation) based on new settings.
+            **kwargs: set Collector attributes.
+        """
+        self.b = b
+        self.data_collector = data_collector
+        self.plotter = plotter
+        self.nick = nick
+        self.collector_name = collector_name or "base"
+
+        # Arguments given as default arguments in the subclass have "low" priority (below elevated arguments at least):
+        self._data_collector_arguments = self._default_data_collector_arguments.copy()
+        self._plotter_arguments = self._default_plotter_arguments.copy()
+        self._update_arguments(data_collector_arguments, plotter_arguments)
+
+        # Elevated arguments have preference above the data_collector and plotter argument dicts:
+        self._parse_elevated_arguments(
+            elevated_data_collector_arguments, elevated_plotter_arguments
+        )
+
+        self._set_attributes(**kwargs)
+
+        if nick is None:
+            self.nick = b.name
+
+        if name is None:
+            name = self.generate_name()
+        self.name = name
+
+        if autorun:
+            self.update(update_name=False)
+            if use_templates:
+                self.apply_templates()
+
+    @property
+    def data_collector_arguments(self):
+        return self._data_collector_arguments
+
+    @data_collector_arguments.setter
+    def data_collector_arguments(self, argument_dict: dict):
+        if argument_dict is not None:
+            self._data_collector_arguments = {
+                **self._data_collector_arguments,
+                **argument_dict,
+            }
+
+    @property
+    def plotter_arguments(self):
+        return self._plotter_arguments
+
+    @plotter_arguments.setter
+    def plotter_arguments(self, argument_dict: dict):
+        if argument_dict is not None:
+            self._plotter_arguments = {**self._plotter_arguments, **argument_dict}
+
+    def __str__(self):
+        class_name = self.__class__.__name__
+
+        txt = f"{class_name}\n{len(class_name) * '='}\n\n"
+        txt += "Attributes:\n"
+        txt += "-----------\n"
+        txt += f" -collector_name: {self.collector_name}\n"
+        txt += f" -autorun: {self.autorun}\n"
+        txt += f" -name: {self.name}\n"
+        txt += f" -nick: {self.nick}\n"
+        txt += f" -csv_include_index: {self.csv_include_index}\n"
+        txt += f" -csv_layout: {self.csv_layout}\n"
+        txt += f" -sep: {self.sep}\n"
+        txt += f" -toolbar: {self.toolbar}\n"
+        txt += f" -figure_directory: {self.figure_directory}\n"
+        txt += f" -data_directory: {self.data_directory}\n"
+        txt += f" -batch-instance: {self.b.name}\n"
+        txt += f" -data_collector_arguments: {self.data_collector_arguments}\n"
+        txt += f" -plotter_arguments: {self.plotter_arguments}\n"
+
+        txt += "\nfigure:\n"
+        txt += ".......\n"
+        txt += f"{self.figure}\n"
+
+        txt += "\ndata:\n"
+        txt += ".....\n"
+        txt += f"{self.data}\n"
+
+        txt += "\nData collector:\n"
+        txt += "---------------\n"
+        data_name = self.data_collector.__name__
+        data_sig = inspect.signature(self.data_collector)
+        data_doc = inspect.getdoc(self.data_collector)
+        txt = f"{txt}{data_name}"
+        txt = f"{txt}{data_sig}\n"
+        txt = f"{txt}\n{data_doc}\n"
+
+        txt += "\nPlotter:\n"
+        txt += "--------\n"
+        plotter_name = self.plotter.__name__
+        plotter_sig = inspect.signature(self.plotter)
+        plotter_doc = inspect.getdoc(self.plotter)
+        txt = f"{txt}{plotter_name}"
+        txt = f"{txt}{plotter_sig}\n"
+        txt = f"{txt}\n{plotter_doc}\n"
+
+        return txt
+
+    def _repr_html_(self):
+        class_name = self.__class__.__name__
+        txt = f"<h2>{class_name}</h2> id={hex(id(self))}"
+        _txt = self.__str__().replace("\n", "<br>")
+        txt += f"<blockquote><code>{_txt}</></blockquote>"
+
+        return txt
+
+    def _set_attributes(self, **kwargs):
+        self.sep = kwargs.get("sep", ";")
+        self.csv_include_index = kwargs.get("csv_include_index", True)
+        self.csv_layout = kwargs.get("csv_layout", "long")
+        self.dpi = kwargs.get("dpi", 200)
+        self.toolbar = kwargs.get("toolbar", True)
+
+    def generate_name(self):
+        names = ["collector", self.collector_name]
+        if self.nick:
+            names.insert(0, self.nick)
+        name = "_".join(names)
+        return name
+
+    def _parse_elevated_arguments(
+        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
+    ):
+        if data_collector_arguments is not None:
+            logging.info(f"Updating elevated arguments")
+            elevated_data_collector_arguments = {}
+            for k, v in data_collector_arguments.items():
+                if v is not None:
+                    elevated_data_collector_arguments[k] = v
+            self._update_arguments(
+                elevated_data_collector_arguments, None, set_as_defaults=True
+            )
+
+        if plotter_arguments is not None:
+            logging.info(f"Updating elevated arguments")
+            elevated_plotter_arguments = {}
+            for k, v in plotter_arguments.items():
+                if v is not None:
+                    elevated_plotter_arguments[k] = v
+
+            self._update_arguments(
+                None, elevated_plotter_arguments, set_as_defaults=True
+            )
+
+    def _update_arguments(
+        self,
+        data_collector_arguments: dict = None,
+        plotter_arguments: dict = None,
+        set_as_defaults=False,
+    ):
+        self.data_collector_arguments = data_collector_arguments
+        self.plotter_arguments = plotter_arguments
+        logging.info(f"**data_collector_arguments: {self.data_collector_arguments}")
+        logging.info(f"**plotter_arguments: {self.plotter_arguments}")
+
+        # setting defaults also (py3.6 compatible):
+        if set_as_defaults:
+            logging.info("updating defaults for current instance")
+            if data_collector_arguments is not None:
+                self._default_data_collector_arguments = {
+                    **self._default_data_collector_arguments,
+                    **data_collector_arguments,
+                }
+            if plotter_arguments is not None:
+                self._default_plotter_arguments = {
+                    **self._default_plotter_arguments,
+                    **plotter_arguments,
+                }
+
+    def reset_arguments(
+        self, data_collector_arguments: dict = None, plotter_arguments: dict = None
+    ):
+        """Reset the arguments to the defaults.
+        Args:
+            data_collector_arguments (dict): optional additional keyword arguments for the data collector.
+            plotter_arguments (dict): optional additional keyword arguments for the plotter.
+        """
+        self._data_collector_arguments = self._default_data_collector_arguments.copy()
+        self._plotter_arguments = self._default_plotter_arguments.copy()
+        self._update_arguments(data_collector_arguments, plotter_arguments)
+
+    def update(
+        self,
+        data_collector_arguments: dict = None,
+        plotter_arguments: dict = None,
+        reset: bool = False,
+        update_data: bool = False,
+        update_name: bool = False,
+        update_plot: bool = True,
+    ):
+        """Update both the collected data and the plot(s).
+        Args:
+            data_collector_arguments (dict): keyword arguments sent to the data collector.
+            plotter_arguments (dict): keyword arguments sent to the plotter.
+            reset (bool): reset the arguments first.
+            update_data (bool): update the data before updating the plot even if data has been collected before.
+            update_name (bool): update the name (using automatic name generation) based on new settings.
+            update_plot (bool): update the plot.
+        """
+        if reset:
+            self.reset_arguments(data_collector_arguments, plotter_arguments)
+        else:
+            self._update_arguments(data_collector_arguments, plotter_arguments)
+        if update_data or self.data is None:
+            try:
+                self.data = self.data_collector(self.b, **self.data_collector_arguments)
+            except TypeError as e:
+                print("Type error:", e)
+                print("Registered data_collector_arguments:")
+                pprint(self.data_collector_arguments)
+                print("Hint: fix it and then re-run using reset=True")
+                return
+        if update_plot:
+            if HOLOVIEWS_AVAILABLE:
+                _set_holoviews_renderer(self.plotter_arguments.get("extension"))
+                try:
+                    self.figure = self.plotter(
+                        self.data, journal=self.b.journal, **self.plotter_arguments
+                    )
+                except TypeError as e:
+                    print("Type error:", e)
+                    print("Registered plotter_arguments:")
+                    pprint(self.plotter_arguments)
+                    print("Hint: fix it and then re-run using reset=True")
+                    return
+
+        if update_name:
+            self.name = self.generate_name()
+
+    def _dynamic_update_template_parameter(self, hv_opt, extension, *args, **kwargs):
+        return hv_opt
+
+    def _register_template(self, hv_opts, extension="bokeh", *args, **kwargs):
+        """Register template for given extension.
+
+        It is also possible to set the options directly in the constructor of the
+        class. But it is recommended to use this method instead to allow for
+        sanitation of the options in the templates
+
+        Args:
+            hv_opts: list of holoviews.core.options.Options- instances
+                e.g. [hv.opts.Curve(xlim=(0,2)), hv.opts.NdLayout(title="Super plot")]
+            extension: Holoviews backend ("matplotlib", "bokeh", or "plotly")
+
+        Returns:
+            None
+        """
+        if extension not in ["bokeh", "matplotlib", "plotly"]:
+            print(f"extension='{extension}' is not supported.")
+        if not isinstance(hv_opts, (list, tuple)):
+            hv_opts = [hv_opts]
+
+        cleaned_hv_opts = []
+        for o in hv_opts:
+            logging.debug(f"Setting prm: {o}")
+            o = self._dynamic_update_template_parameter(o, extension, *args, **kwargs)
+            # ensure all options are registered with correct backend:
+            o.kwargs["backend"] = extension
+            cleaned_hv_opts.append(o)
+
+        self._templates[extension] = cleaned_hv_opts
+
+    def apply_templates(self):
+        if not self._figure_valid():
+            return
+
+        for backend, hv_opt in self._templates.items():
+            try:
+                if len(hv_opt):
+                    print(f"Applying template for {backend}:{hv_opt}")
+                    self.figure = self._set_hv_opts(hv_opt)
+            except TypeError:
+                print("possible bug in apply_template experienced")
+                print(self._templates)
+
+    def _figure_valid(self):
+        # TODO: create a decorator
+        if self.figure is None:
+            print("No figure to show!")
+            return False
+        if not HOLOVIEWS_AVAILABLE:
+            print("Requires Holoviews - please install it first!")
+            return False
+        return True
+
+    def _set_hv_opts(self, hv_opts):
+        if hv_opts is None:
+            return self.figure
+        if isinstance(hv_opts, (tuple, list)):
+            return self.figure.options(*hv_opts)
+        else:
+            return self.figure.options(hv_opts)
+
+    def show(self, hv_opts=None):
+        if not self._figure_valid():
+            return
+
+        print(f"figure name: {self.name}")
+        return self._set_hv_opts(hv_opts)
+
+    def redraw(self, hv_opts=None, extension=None):
+        print("EXPERIMENTAL FEATURE! THIS MIGHT NOT WORK PROPERLY YET")
+        if not self._figure_valid():
+            return
+
+        if extension is not None:
+            _set_holoviews_renderer(extension)
+
+        print(f"figure name: {self.name}")
+        self.figure = self._set_hv_opts(hv_opts)
+        return self.figure
+
+    def render(self):
+        print("Not implemented yet!")
+
+    def preprocess_data_for_csv(self):
+        print(f"the data layout {self.csv_layout} is not supported yet!")
+        return self.data
+
+    def to_csv(self, serial_number=None):
+        filename = self._output_path(serial_number)
+        filename = filename.with_suffix(".csv")
+        if self.csv_layout != "long":
+            data = self.preprocess_data_for_csv()
+        else:
+            data = self.data
+
+        data.to_csv(
+            filename,
+            sep=self.sep,
+            index=self.csv_include_index,
+        )
+        print(f"saved csv file: {filename}")
+
+    def to_image_files(self, serial_number=None):
+        if not self._figure_valid():
+            return
+        filename_pre = self._output_path(serial_number)
+
+        filename_hv = filename_pre.with_suffix(".html")
+        hv.save(
+            self.figure,
+            filename_hv,
+            toolbar=self.toolbar,
+        )
+        print(f"saved file: {filename_hv}")
+
+        filename_png = filename_pre.with_suffix(".png")
+        try:
+            current_renderer = _get_current_holoviews_renderer()
+
+            _set_holoviews_renderer("matplotlib")
+            self.figure.opts(hv.opts.NdOverlay(legend_position="right"))
+            hv.save(
+                self.figure,
+                filename_png,
+                dpi=300,
+            )
+            print(f"saved file: {filename_png}")
+        except Exception as e:
+            print("Could not save png-file.")
+            print(e)
+        finally:
+            _set_holoviews_renderer(current_renderer)
+
+    def save(self, serial_number=None):
+        self.to_csv(serial_number=serial_number)
+
+        if self._figure_valid():
+            filename = self._output_path(serial_number)
+            filename = filename.with_suffix(".hvz")
+            try:
+                Pickler.save(
+                    self.figure,
+                    filename,
+                )
+                print(f"pickled holoviews file: {filename}")
+            except TypeError as e:
+                print("could not save as hvz file")
+            self.to_image_files(serial_number=serial_number)
+
+    def _output_path(self, serial_number=None):
+        d = Path(self.figure_directory)
+        n = self.name
+        if serial_number is not None:
+            n = f"{n}_{serial_number:03}"
+        f = d / n
+        return f
+
+
+# TODO: allow for storing more than one figure setup pr collector
+#    It is time-consuming and memory demanding to re-collect the data
+#    for each time we need a new figure for the collector. We should
+#    allow for creating multiple figures within one collector or for
+#    sharing (passing) collected data. One solution might be to extend
+#    the capabilities of the base class. Another solution might be to
+#    add another sub-class in the chain from the base class to the actual one:
+class BatchMultiFigureCollector(BatchCollector):
+    pass
+
+
+def pick_named_cell(b, label_mapper=None):
+    """generator that picks a cell from the batch object, yields its label and the cell itself.
+
+    Args:
+        b (cellpy.batch object): your batch object
+        label_mapper (callable or dict): function (or dict) that changes the cell names.
+            The dictionary must have the cell labels as given in the `journal.pages` index and new label as values.
+            Similarly, if it is a function it takes the cell label as input and returns the new label.
+            Remark! No check are performed to ensure that the new cell labels are unique.
+
+    Yields:
+        label, group, subgroup, cell
+
+    Example:
+        def my_mapper(n):
+            return "_".join(n.split("_")[1:-1])
+
+        # outputs "nnn_x" etc., if cell-names are of the form "date_nnn_x_y":
+        for label, group, subgroup, cell in pick_named_cell(b, label_mapper=my_mapper):
+            print(label)
+    """
+
+    cell_names = b.cell_names
+    for n in cell_names:
+        group = b.pages.loc[n, "group"]
+        sub_group = b.pages.loc[n, "sub_group"]
+
+        if label_mapper is not None:
+            try:
+                if isinstance(label_mapper, dict):
+                    label = label_mapper[n]
+                else:
+                    label = label_mapper(n)
+            except Exception as e:
+                logging.info(f"label_mapper-error: could not rename cell {n}")
+                logging.debug(f"caught exception: {e}")
+                label = n
+        else:
+            try:
+                label = b.pages.loc[n, "label"]
+            except Exception as e:
+                logging.info(f"lookup in pages failed: could not rename cell {n}")
+                logging.debug(f"caught exception: {e}")
+                label = n
+
+        logging.info(f"renaming {n} -> {label} (group={group}, subgroup={sub_group})")
+        yield label, group, sub_group, b.experiment.data[n]
+
+
+def cycles_collector(
+    b,
+    cycles=None,
+    interpolated=True,
+    number_of_points=100,
+    max_cycle=50,
+    abort_on_missing=False,
+    method="back-and-forth",
+    label_mapper=None,
+):
+    if cycles is None:
+        cycles = list(range(1, max_cycle + 1))
+    all_curves = []
+    keys = []
+    for n, g, sg, c in pick_named_cell(b, label_mapper):
+        curves = c.get_cap(
+            cycle=cycles,
+            label_cycle_number=True,
+            interpolated=interpolated,
+            number_of_points=number_of_points,
+            method=method,
+        )
+        logging.debug(f"processing {n} (cell name: {c.cell_name})")
+        if not curves.empty:
+            curves = curves.assign(group=g, sub_group=sg)
+            all_curves.append(curves)
+            keys.append(n)
+        else:
+            if abort_on_missing:
+                raise ValueError(f"{n} is empty - aborting!")
+            logging.critical(f"[{n} (cell name: {c.cell_name}) empty]")
+    collected_curves = pd.concat(
+        all_curves, keys=keys, axis=0, names=["cell", "point"]
+    ).reset_index(level="cell")
+    return collected_curves
+
+
+def cycles_plotter_simple_holo_map(collected_curves, journal=None, **kwargs):
+    p = hv.Curve(
+        collected_curves, kdims="capacity", vdims=["voltage", "cycle", "cell"]
+    ).groupby("cell")
+    return p
+
+
+def ica_plotter(
+    collected_curves,
+    journal=None,
+    palette="Blues",
+    palette_range=(0.2, 1.0),
+    method="fig_pr_cell",
+    extension="bokeh",
+    cycles_to_plot=None,
+    cols=1,
+    width=None,
+    height=None,
+    xlim_charge=(None, None),
+    xlim_discharge=(None, None),
+    **kwargs,
+):
+    if method == "film":
+        if extension == "matplotlib":
+            print("SORRY, PLOTTING FILM WITH MATPLOTLIB IS NOT IMPLEMENTED YET")
+            return
+
+        return ica_plotter_film_bokeh(
+            collected_curves,
+            journal=journal,
+            palette=palette,
+            extension="bokeh",
+            cycles=cycles_to_plot,
+            xlim_charge=xlim_charge,
+            xlim_discharge=xlim_discharge,
+            width=width,
+            height=height,
+            **kwargs,
+        )
+    else:
+        return sequence_plotter(
+            collected_curves,
+            x="voltage",
+            y="dq",
+            z="cycle",
+            g="cell",
+            journal=journal,
+            palette=palette,
+            palette_range=palette_range,
+            method=method,
+            extension=extension,
+            cycles=cycles_to_plot,
+            cols=cols,
+            width=width,
+        )
+
+
+def ica_plotter_film_bokeh(
+    collected_curves,
+    palette="Blues",
+    cycles=None,
+    xlim_charge=(None, None),
+    xlim_discharge=(None, None),
+    ylim=(None, None),
+    shared_axes=True,
+    width=400,
+    height=500,
+    cformatter="%02.0e",
+    **kwargs,
+):
+    if cycles is not None:
+        filtered_curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
+    else:
+        filtered_curves = collected_curves
+
+    options = {
+        "xlabel": "Voltage (V)",
+        "ylabel": "Cycle",
+        "ylim": ylim,
+        "tools": ["hover"],
+        "width": width,
+        "height": height,
+        "cmap": palette,
+        "cformatter": cformatter,
+        "cnorm": "eq_hist",
+        "shared_axes": shared_axes,
+        "colorbar_opts": {
+            "title": "dQ/dV",
+        },
+    }
+
+    all_charge_plots = {}
+    all_discharge_plots = {}
+    for label, df in filtered_curves.groupby("cell"):
+        _charge = df.query("direction==1")
+        _discharge = df.query("direction==-1")
+        _dq_charge = group_by_interpolate(
+            _charge, x="voltage", y="dq", group_by="cycle", number_of_points=400
+        )
+        _dq_discharge = group_by_interpolate(
+            _discharge, x="voltage", y="dq", group_by="cycle", number_of_points=400
+        )
+
+        _v_charge = _dq_charge.index.values.ravel()
+        _v_discharge = _dq_discharge.index.values.ravel()
+
+        _cycles_charge = _charge.cycle.unique().ravel()
+        _cycles_discharge = _discharge.cycle.unique().ravel()
+
+        _dq_charge = -_dq_charge.values.T
+        _dq_discharge = _dq_discharge.values.T
+
+        charge_plot = hv.Image(
+            (_v_charge, _cycles_charge, _dq_charge), group="ica", label="charge"
+        ).opts(title=f"{label}", xlim=xlim_charge, colorbar=True, **options)
+
+        discharge_plot = hv.Image(
+            (_v_discharge, _cycles_discharge, _dq_discharge),
+            group="ica",
+            label="discharge",
+        ).opts(title=f"{label}", xlim=xlim_discharge, colorbar=True, **options)
+
+        all_charge_plots[f"{label}_charge"] = charge_plot
+        all_discharge_plots[f"{label}_discharge"] = discharge_plot
+
+    all_plots = {**all_charge_plots, **all_discharge_plots}
+    return (
+        hv.NdLayout(all_plots)
+        .opts(title="Incremental Capacity Analysis Film-plots")
+        .cols(2)
+    )
+
+
+def cycles_plotter(
+    collected_curves,
+    method="fig_pr_cell",
+    extension="bokeh",
+    cycles_to_plot=None,
+    width=None,
+    palette=None,
+    palette_range=(0.1, 1.0),
+    legend_position=None,
+    show_legend=None,
+    fig_title="",
+    cols=None,
+    **kwargs,
+):
+    if cols is None:
+        if extension == "matplotlib":
+            cols = 3
+        else:
+            cols = 3 if method == "fig_pr_cell" else 1
+
+    if width is None:
+        width = 400 if method == "fig_pr_cell" else int(800 / cols)
+
+    if palette is None:
+        palette = "Blues" if method == "fig_pr_cell" else "Category10"
+
+    if palette_range is None:
+        palette_range = (0.2, 1.0) if method == "fig_pr_cell" else (0, 1)
+
+    if legend_position is None:
+        legend_position = None if method == "fig_pr_cell" else "right"
+
+    if show_legend is None:
+        show_legend = True
+
+    reverse_palette = True if method == "fig_pr_cell" else False
+
+    backend_specific_kwargs = {
+        "NdLayout": {},
+        "NdOverlay": {},
+        "Curve": {},
+    }
+
+    if extension != "matplotlib":
+        logging.debug(f"setting width for bokeh and plotly: {width}")
+        backend_specific_kwargs["Curve"]["width"] = width
+
+    p = sequence_plotter(
+        collected_curves,
+        x="capacity",
+        y="voltage",
+        z="cycle",
+        g="cell",
+        method=method,
+        cycles=cycles_to_plot,
+        **kwargs,
+    ).cols(cols)
+
+    p.opts(
+        hv.opts.NdLayout(
+            title=fig_title,
+            **backend_specific_kwargs["NdLayout"],
+            backend=extension,
+        ),
+        hv.opts.NdOverlay(
+            **backend_specific_kwargs["NdOverlay"],
+            backend=extension,
+        ),
+        hv.opts.Curve(
+            # TODO: should replace this with custom mapping (see how it is done in plotutils):
+            color=hv.Palette(palette, reverse=reverse_palette, range=palette_range),
+            show_legend=show_legend,
+            **backend_specific_kwargs["Curve"],
+            backend=extension,
+        ),
+    )
+
+    if legend_position is not None:
+        p.opts(hv.opts.NdOverlay(legend_position=legend_position))
+
+    return p
+
+
+def sequence_plotter(
+    collected_curves,
+    x,
+    y,
+    z,
+    g,
+    method="fig_pr_cell",
+    group_label="group",
+    group_txt="cell-group",
+    z_lim=10,
+    cycles=None,
+    **kwargs,
+):
+    for k in kwargs:
+        logging.debug(f"keyword argument {k} given, but not used")
+
+    x_label = "Capacity"
+    x_unit = "mAh"
+
+    y_label = "Voltage"
+    y_unit = "V"
+
+    g_label = "Cell"
+    g_unit = ""
+
+    z_label = "Cycle"
+    z_unit = ""
+
+    x_dim = hv.Dimension(f"{x}", label=x_label, unit=x_unit)
+    y_dim = hv.Dimension(f"{y}", label=y_label, unit=y_unit)
+    g_dim = hv.Dimension(f"{g}", label=g_label, unit=g_unit)
+    z_dim = hv.Dimension(f"{z}", label=z_label, unit=z_unit)
+
+    family = {}
+    curves = None
+
+    if method == "fig_pr_cell":
+        if cycles is not None:
+            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
+        else:
+            curves = collected_curves
+        logging.debug(f"filtered_curves:\n{curves}")
+
+    elif method == "fig_pr_cycle":
+        if cycles is None:
+            unique_cycles = list(collected_curves.cycle.unique())
+            if len(unique_cycles) > 10:
+                cycles = [1, 10, 20]
+        if cycles is not None:
+            curves = collected_curves.loc[collected_curves.cycle.isin(cycles), :]
+        else:
+            curves = collected_curves
+        # g (what we split the figures by) : cycle
+        # z (the "dimension" of the individual curves in one figure): cell
+        z, g = g, z
+        z_dim, g_dim = g_dim, z_dim
+
+        # dirty (?) fix to make plots with a lot of cells look a bit better:
+        unique_z_values = collected_curves[z].unique()
+        no_unique_z_values = len(unique_z_values)
+        if no_unique_z_values > z_lim:
+            logging.critical(
+                f"number of cells ({no_unique_z_values}) larger than z_lim ({z_lim}): grouping"
+            )
+            logging.critical(
+                f"prevent this by modifying z_lim to your plotter_arguments"
+            )
+            z = group_label
+            z_dim = hv.Dimension(f"{z}", label=group_txt, unit="")
+
+    kdims = x_dim
+    vdims = [y_dim, z_dim]
+    for cyc, df in curves.groupby(g):
+        family[cyc] = hv.Curve(df, kdims=kdims, vdims=vdims).groupby(z).overlay()
+
+    return hv.NdLayout(family, kdims=g_dim)
+
+
+def ica_collector(
+    b,
+    cycles=None,
+    voltage_resolution=0.005,
+    max_cycle=50,
+    abort_on_missing=False,
+    label_direction=True,
+    number_of_points=None,
+    label_mapper=None,
+    **kwargs,
+):
+    if cycles is None:
+        cycles = list(range(1, max_cycle + 1))
+    all_curves = []
+    keys = []
+    for n, g, sg, c in pick_named_cell(b, label_mapper):
+        curves = ica.dqdv_frames(
+            c,
+            cycle=cycles,
+            voltage_resolution=voltage_resolution,
+            label_direction=label_direction,
+            number_of_points=number_of_points,
+            **kwargs,
+        )
+        logging.debug(f"processing {n} (session name: {c.cell_name})")
+        if not curves.empty:
+            curves = curves.assign(group=g, sub_group=sg)
+            all_curves.append(curves)
+            keys.append(n)
+        else:
+            if abort_on_missing:
+                raise ValueError(f"{n} is empty - aborting!")
+            logging.critical(f"[{n} (session name: {c.cell_name}) empty]")
+    collected_curves = pd.concat(
+        all_curves, keys=keys, axis=0, names=["cell", "point"]
+    ).reset_index(level="cell")
+    return collected_curves
+
+
+class BatchSummaryCollector(BatchCollector):
+    # Three main levels of arguments to the plotter and collector funcs is available:
+    #  - through dictionaries (`data_collector_arguments`, `plotter_arguments`) to init
+    #  - given as defaults in the subclass (`_default_data_collector_arguments`, `_default_plotter_arguments`)
+    #  - as elevated arguments (i.e. arguments normally given in the dictionaries elevated
+    #    to their own keyword parameters)
+
+    _default_data_collector_arguments = {
+        "columns": ["charge_capacity_gravimetric"],
+    }
+    _default_plotter_arguments = {
+        "extension": "bokeh",
+    }
+
+    _bokeh_template = [
+        hv.opts.Curve(fontsize={"title": "medium"}, width=800, backend="bokeh"),
+        hv.opts.NdOverlay(legend_position="right", backend="bokeh"),
+    ]
+
+    def __init__(
+        self,
+        b,
+        max_cycle: int = None,
+        rate=None,
+        on=None,
+        columns=None,
+        column_names=None,
+        normalize_capacity_on=None,
+        scale_by=None,
+        nom_cap=None,
+        normalize_cycles=None,
+        group_it=None,
+        rate_std=None,
+        rate_column=None,
+        inverse=None,
+        inverted: bool = None,
+        key_index_bounds=None,
+        extension: str = None,
+        title: str = None,
+        points: bool = None,
+        line: bool = None,
+        width: int = None,
+        height: int = None,
+        legend_title: str = None,
+        marker_size: int = None,
+        cmap=None,
+        spread: bool = None,
+        *args,
+        **kwargs,
+    ):
+        """Collects and shows summaries.
+
+        Elevated data collector args:
+            max_cycle (int): drop all cycles above this value.
+            rate (float): filter on rate (C-rate)
+            on (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
+            columns (list): selected column(s) (using cellpy attribute name)
+                [defaults to "charge_capacity_gravimetric"]
+            column_names (list): selected column(s) (using exact column name)
+            normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the
+                normalization (typically the first few cycles after formation)
+            scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap",
+                or given value (defaults to one).
+            nom_cap (float): nominal capacity of the cell
+            normalize_cycles (bool): perform a normalization of the cycle numbers (also called equivalent cycle index)
+            group_it (bool): if True, average pr group.
+            rate_std (float): allow for this inaccuracy when selecting cycles based on rate
+            rate_column (str): name of the column containing the C-rates.
+            inverse (bool): select steps that do not have the given C-rate.
+            inverted (bool): select cycles that do not have the steps filtered by given C-rate.
+            key_index_bounds (list): used when creating a common label for the cells by splitting and combining from
+                key_index_bound[0] to key_index_bound[1].
+
+        Elevated plotter args:
+            extension (str): extension used (defaults to Bokeh)
+            points (bool): plot points if True
+            line (bool): plot line if True
+            width: width of plot
+            height: height of plot
+            legend_title: title to put over the legend
+            marker_size: size of the markers used
+            cmap: color-map to use
+            spread (bool): plot error-bands instead of error-bars if True
+        """
+
+        elevated_data_collector_arguments = dict(
+            max_cycle=max_cycle,
+            rate=rate,
+            on=on,
+            columns=columns,
+            column_names=column_names,
+            normalize_capacity_on=normalize_capacity_on,
+            scale_by=scale_by,
+            nom_cap=nom_cap,
+            normalize_cycles=normalize_cycles,
+            group_it=group_it,
+            rate_std=rate_std,
+            rate_column=rate_column,
+            inverse=inverse,
+            inverted=inverted,
+            key_index_bounds=key_index_bounds,
+        )
+
+        elevated_plotter_arguments = {
+            "extension": extension,
+            "title": title,
+            "points": points,
+            "line": line,
+            "width": width,
+            "height": height,
+            "legend_title": legend_title,
+            "marker_size": marker_size,
+            "cmap": cmap,
+            "spread": spread,
+        }
+
+        self._register_template(self._bokeh_template, extension="bokeh")
+
+        super().__init__(
+            b,
+            plotter=plot_concatenated,
+            data_collector=concatenate_summaries,
+            collector_name="summary",
+            elevated_data_collector_arguments=elevated_data_collector_arguments,
+            elevated_plotter_arguments=elevated_plotter_arguments,
+            *args,
+            **kwargs,
+        )
+
+    def generate_name(self):
+        names = ["collected_summaries"]
+        cols = self.data_collector_arguments.get("columns")
+        grouped = self.data_collector_arguments.get("group_it")
+        equivalent_cycles = self.data_collector_arguments.get("normalize_cycles")
+        normalized_cap = self.data_collector_arguments.get("normalize_capacity_on", [])
+        if self.nick:
+            names.insert(0, self.nick)
+        if cols:
+            names.extend(cols)
+        if grouped:
+            names.append("average")
+        if equivalent_cycles:
+            names.append("equivalents")
+        if len(normalized_cap):
+            names.append("norm")
+
+        name = "_".join(names)
+        return name
+
+
+class BatchICACollector(BatchCollector):
+    _default_data_collector_arguments = {}
+    _default_plotter_arguments = {
+        "extension": "bokeh",
+    }
+
+    def __init__(self, b, plot_type="fig_pr_cell", *args, **kwargs):
+        """Create a collection of ica (dQ/dV) plots."""
+
+        self.plot_type = plot_type
+
+        if plot_type == "fig_pr_cell":
+            _tight = True
+            _fig_inches = 3.5
+        else:
+            _tight = False
+            _fig_inches = 5.5
+
+        matplotlib_template = [
+            hv.opts.Curve(
+                show_frame=True,
+                fontsize={"title": "medium"},
+                backend="matplotlib",
+            ),
+            hv.opts.NdLayout(
+                fig_inches=_fig_inches, tight=_tight, backend="matplotlib"
+            ),
+        ]
+
+        bokeh_template = [
+            hv.opts.Curve(xlabel="Voltage (V)", backend="bokeh"),
+        ]
+        self._default_plotter_arguments["method"] = plot_type
+        self._register_template(matplotlib_template, extension="matplotlib")
+        self._register_template(bokeh_template, extension="bokeh")
+        super().__init__(
+            b,
+            plotter=ica_plotter,
+            data_collector=ica_collector,
+            collector_name="ica",
+            *args,
+            **kwargs,
+        )
+
+    def generate_name(self):
+        names = ["collected_ica"]
+
+        pm = self.plotter_arguments.get("method")
+        if pm == "fig_pr_cell":
+            names.append("pr_cell")
+        elif pm == "fig_pr_cycle":
+            names.append("pr_cyc")
+        elif pm == "film":
+            names.append("film")
+
+        if self.nick:
+            names.insert(0, self.nick)
+
+        name = "_".join(names)
+        return name
+
+
+class BatchCyclesCollector(BatchCollector):
+    _default_data_collector_arguments = {
+        "interpolated": True,
+        "number_of_points": 100,
+        "max_cycle": 50,
+        "abort_on_missing": False,
+        "method": "back-and-forth",
+    }
+    _default_plotter_arguments = {
+        "extension": "bokeh",
+    }
+
+    def __init__(
+        self,
+        b,
+        plot_type="fig_pr_cell",
+        collector_type="back-and-forth",
+        cycles=None,
+        max_cycle=None,
+        label_mapper=None,
+        extension=None,
+        cycles_to_plot=None,
+        width=None,
+        palette=None,
+        show_legend=None,
+        legend_position=None,
+        fig_title=None,
+        cols=None,
+        *args,
+        **kwargs,
+    ):
+        """Create a collection of capacity plots.
+
+        Args:
+            b:
+            plot_type (str): either 'fig_pr_cell' or 'fig_pr_cycle'
+            collector_type (str): how the curves are given
+                "back-and-forth" - standard back and forth; discharge
+                    (or charge) reversed from where charge (or discharge) ends.
+                "forth" - discharge (or charge) continues along x-axis.
+                "forth-and-forth" - discharge (or charge) also starts at 0
+            data_collector_arguments (dict) - arguments transferred to the plotter
+            plotter_arguments (dict) - arguments transferred to the plotter
+
+        Elevated data collector args:
+            cycles (int): drop all cycles above this value.
+            max_cycle (float): filter on rate (C-rate)
+            label_mapper (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
+
+        Elevated plotter args:
+            extension (str): extension used (defaults to Bokeh)
+            cycles_to_plot (int): plot points if True
+            width (float): width of plot
+            legend_position (str): position of the legend
+            show_legend (bool): set to False if you don't want to show legend
+            fig_title (str): title (will be put above the figure)
+            palette (str): color-map to use
+            cols (int): number of columns
+        """
+
+        elevated_data_collector_arguments = dict(
+            cycles=cycles,
+            max_cycle=max_cycle,
+            label_mapper=label_mapper,
+        )
+        elevated_plotter_arguments = dict(
+            extension=extension,
+            cycles_to_plot=cycles_to_plot,
+            width=width,
+            palette=palette,
+            legend_position=legend_position,
+            show_legend=show_legend,
+            fig_title=fig_title,
+            cols=cols,
+        )
+
+        # internal attribute to keep track of plot type:
+        self.plot_type = plot_type
+
+        # moving it to after init to allow for using prms set in init
+        if plot_type == "fig_pr_cell":
+            _tight = True
+            _fig_inches = 3.5
+        else:
+            _tight = False
+            _fig_inches = 5.5
+
+        matplotlib_template = [
+            hv.opts.Curve(
+                show_frame=True,
+                fontsize={"title": "medium"},
+                ylim=(0, 1),
+                backend="matplotlib",
+            ),
+            hv.opts.NdLayout(
+                fig_inches=_fig_inches, tight=_tight, backend="matplotlib"
+            ),
+        ]
+
+        self._max_letters_in_cell_names = max(len(x) for x in b.cell_names)
+        self._register_template(matplotlib_template, extension="matplotlib")
+        self._default_data_collector_arguments["method"] = collector_type
+        self._default_plotter_arguments["method"] = plot_type
+
+        super().__init__(
+            b,
+            plotter=cycles_plotter,
+            data_collector=cycles_collector,
+            collector_name="cycles",
+            elevated_data_collector_arguments=elevated_data_collector_arguments,
+            elevated_plotter_arguments=elevated_plotter_arguments,
+            *args,
+            **kwargs,
+        )
+
+    def _dynamic_update_template_parameter(self, hv_opt, extension, *args, **kwargs):
+        k = hv_opt.key
+        if k == "NdLayout" and extension == "matplotlib":
+            if self.plot_type != "fig_pr_cycle":
+                hv_opt.kwargs["fig_inches"] = self._max_letters_in_cell_names * 0.14
+        return hv_opt
+
+    def generate_name(self):
+        names = ["collected_cycles"]
+
+        if self.data_collector_arguments.get("interpolated"):
+            names.append("intp")
+            if n := self.data_collector_arguments.get("number_of_points"):
+                names.append(f"p{n}")
+        cm = self.data_collector_arguments.get("method")
+        if cm.startswith("b"):
+            names.append("bf")
+        else:
+            names.append("ff")
+
+        pm = self.plotter_arguments.get("method")
+        if pm == "fig_pr_cell":
+            names.append("pr_cell")
+        elif pm == "fig_pr_cycle":
+            names.append("pr_cyc")
+
+        if self.nick:
+            names.insert(0, self.nick)
+
+        name = "_".join(names)
+        return name
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/data/20160805_test001_45_cc.h5` & `cellpy-1.0.0b1/cellpy/utils/data/20160805_test001_45_cc.h5`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/data/raw/20160805_test001_45_cc_01.res` & `cellpy-1.0.0b1/cellpy/utils/data/raw/20160805_test001_45_cc_01.res`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/easyplot.py` & `cellpy-1.0.0b1/cellpy/utils/easyplot.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/example_data.py` & `cellpy-1.0.0b1/cellpy/utils/example_data.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/ica.py` & `cellpy-1.0.0b1/cellpy/utils/ica.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,1081 +1,1081 @@
-"""ica contains routines for creating and working with
-incremental capacity analysis data"""
-
-import os
-import logging
-import warnings
-
-import numpy as np
-import pandas as pd
-from scipy import stats
-from scipy.interpolate import interp1d
-from scipy.signal import savgol_filter
-from scipy.integrate import simps
-from scipy.ndimage.filters import gaussian_filter1d
-import pandas as pd
-
-from cellpy.exceptions import NullData
-from cellpy.readers.core import collect_capacity_curves
-
-
-# TODO: @jepe - documentation and tests
-# TODO: @jepe - fitting of o-c curves and differentiation
-# TODO: @jepe - modeling and fitting
-# TODO: @jepe - full-cell
-# TODO: @jepe - binning method (assigned to Asbjoern)
-
-
-class Converter:
-    """Class for dq-dv handling.
-
-    Typical usage is to (1) set the data, (2) inspect the data,
-    (3) pre-process the data,
-    (4) perform the dq-dv transform, and finally (5) post-process the data.
-
-    A short note about normalization:
-
-        - If ``normalization`` is set to ``False``, then no normalization will be done.
-        - If ``normalization`` is ``True``, and ``normalization_factor`` is ``None``, the total capacity of
-          the half cycle will be used for normalization, else the ``normalization_factor`` will be used.
-        - If ``normalization`` is ``True``, and ``normalization_roof`` is not ``None``,
-          the capacity divided by ``normalization_roof`` will be used for normalization.
-
-    """
-
-    def __init__(
-        self,
-        capacity=None,
-        voltage=None,
-        points_pr_split=10,
-        max_points=None,
-        voltage_resolution=None,
-        capacity_resolution=None,
-        minimum_splits=3,
-        interpolation_method="linear",
-        increment_method="diff",
-        pre_smoothing=False,
-        smoothing=False,
-        post_smoothing=True,
-        normalize=True,
-        normalizing_factor=None,
-        normalizing_roof=None,
-        savgol_filter_window_divisor_default=50,
-        savgol_filter_window_order=3,
-        voltage_fwhm=0.01,
-        gaussian_order=0,
-        gaussian_mode="reflect",
-        gaussian_cval=0.0,
-        gaussian_truncate=4.0,
-    ):
-        self.capacity = capacity
-        self.voltage = voltage
-
-        self.capacity_preprocessed = None
-        self.voltage_preprocessed = None
-        self.capacity_inverted = None
-        self.voltage_inverted = None
-
-        self.incremental_capacity = None
-        self._incremental_capacity = None  # before smoothing
-        self.voltage_processed = None
-        self._voltage_processed = None  # before shifting / centering
-
-        self.voltage_inverted_step = None
-
-        self.points_pr_split = points_pr_split
-        self.max_points = max_points
-        self.voltage_resolution = voltage_resolution
-        self.capacity_resolution = capacity_resolution
-        self.minimum_splits = minimum_splits
-        self.interpolation_method = interpolation_method
-        self.increment_method = increment_method
-        self.pre_smoothing = pre_smoothing
-        self.smoothing = smoothing
-        self.post_smoothing = post_smoothing
-        self.savgol_filter_window_divisor_default = savgol_filter_window_divisor_default
-        self.savgol_filter_window_order = savgol_filter_window_order
-        self.voltage_fwhm = voltage_fwhm
-        self.gaussian_order = gaussian_order
-        self.gaussian_mode = gaussian_mode
-        self.gaussian_cval = gaussian_cval
-        self.gaussian_truncate = gaussian_truncate
-        self.normalize = normalize
-        self.normalizing_factor = normalizing_factor
-        self.normalizing_roof = normalizing_roof
-
-        self.d_capacity_mean = None
-        self.d_voltage_mean = None
-        self.len_capacity = None
-        self.len_voltage = None
-        self.min_capacity = None
-        self.max_capacity = None
-        self.start_capacity = None
-        self.end_capacity = None
-        self.number_of_points = None
-        self.std_err_median = None
-        self.std_err_mean = None
-
-        self.fixed_voltage_range = False
-
-        self.errors = []
-
-    def __str__(self):
-        txt = f"[ica.converter] {str(type(self))}\n"
-        attrs = vars(self)
-        for name, att in attrs.items():
-            if isinstance(att, (pd.DataFrame, pd.Series, np.ndarray)):
-                str_att = f"<vector> ({str(type(att))})"
-            else:
-                str_att = str(att)
-            txt += f"{name}: {str_att}\n"
-
-        return txt
-
-    def set_data(self, capacity, voltage=None, capacity_label="q", voltage_label="v"):
-        """Set the data."""
-
-        logging.debug("setting data (capacity and voltage)")
-
-        if isinstance(capacity, pd.DataFrame):
-            logging.debug("received a pandas.DataFrame")
-            self.capacity = capacity[capacity_label]
-            self.voltage = capacity[voltage_label]
-        else:
-            assert len(capacity) == len(voltage)
-            self.capacity = capacity
-            self.voltage = voltage
-
-    def inspect_data(self, capacity=None, voltage=None, err_est=False, diff_est=False):
-        """Check and inspect the data."""
-
-        logging.debug("inspecting the data")
-
-        if capacity is None:
-            capacity = self.capacity
-        if voltage is None:
-            voltage = self.voltage
-
-        if capacity is None or voltage is None:
-            raise NullData
-
-        self.len_capacity = len(capacity)
-        self.len_voltage = len(voltage)
-
-        if self.len_capacity <= 1:
-            raise NullData
-        if self.len_voltage <= 1:
-            raise NullData
-
-        self.min_capacity, self.max_capacity = value_bounds(capacity)
-        self.start_capacity, self.end_capacity = index_bounds(capacity)
-
-        self.number_of_points = len(capacity)
-
-        if diff_est:
-            d_capacity = np.diff(capacity)
-            d_voltage = np.diff(voltage)
-
-            self.d_capacity_mean = np.mean(d_capacity)
-            self.d_voltage_mean = np.mean(d_voltage)
-
-        if err_est:
-            splits = int(self.number_of_points / self.points_pr_split)
-            rest = self.number_of_points % self.points_pr_split
-
-            if splits < self.minimum_splits:
-                txt = "no point in splitting, too little data"
-                logging.debug(txt)
-                self.errors.append("splitting: to few points")
-            else:
-                if rest > 0:
-                    _cap = capacity[:-rest]
-                    _vol = voltage[:-rest]
-                else:
-                    _cap = capacity
-                    _vol = voltage
-
-                c_pieces = np.split(_cap, splits)
-                v_pieces = np.split(_vol, splits)
-                # c_middle = int(np.amax(c_pieces) / 2)
-
-                std_err = []
-                c_pieces_avg = []
-                for c, v in zip(c_pieces, v_pieces):
-                    _slope, _intercept, _r_value, _p_value, _std_err = stats.linregress(
-                        c, v
-                    )
-                    std_err.append(_std_err)
-                    c_pieces_avg.append(np.mean(c))
-
-                self.std_err_median = np.median(std_err)
-                self.std_err_mean = np.mean(std_err)
-
-        if not self.start_capacity == self.min_capacity:
-            self.errors.append("capacity: start<>min")
-
-        if not self.end_capacity == self.max_capacity:
-            self.errors.append("capacity: end<>max")
-
-        if self.normalizing_factor is None:
-            self.normalizing_factor = self.end_capacity
-
-        if self.normalizing_roof is not None:
-            self.normalizing_factor = (
-                self.normalizing_factor * self.end_capacity / self.normalizing_roof
-            )
-
-    def pre_process_data(self):
-        """Perform some pre-processing of the data (i.e. interpolation)."""
-
-        logging.debug("pre-processing the data")
-
-        capacity = self.capacity
-        voltage = self.voltage
-
-        # performing an interpolation in v(q) space
-        logging.debug(" - interpolating voltage(capacity)")
-        c1, c2 = index_bounds(capacity)
-        if self.max_points is not None:
-            len_capacity = min(self.max_points, self.len_capacity)
-        elif self.capacity_resolution is not None:
-            len_capacity = int(round(abs(c2 - c1) / self.capacity_resolution, 0))
-        else:
-            len_capacity = self.len_capacity
-
-        f = interp1d(capacity, voltage, kind=self.interpolation_method)
-
-        self.capacity_preprocessed = np.linspace(c1, c2, len_capacity)
-        self.voltage_preprocessed = f(self.capacity_preprocessed)
-
-        if self.pre_smoothing:
-            logging.debug(" - pre-smoothing (savgol filter window)")
-            savgol_filter_window_divisor = np.amin(
-                (self.savgol_filter_window_divisor_default, len_capacity / 5)
-            )
-            savgol_filter_window_length = int(
-                len_capacity / savgol_filter_window_divisor
-            )
-
-            if savgol_filter_window_length % 2 == 0:
-                savgol_filter_window_length -= 1
-            savgol_filter_window_length = np.amax([3, savgol_filter_window_length])
-
-            self.voltage_preprocessed = savgol_filter(
-                self.voltage_preprocessed,
-                savgol_filter_window_length,
-                self.savgol_filter_window_order,
-            )
-
-    def increment_data(self):
-        """Perform the dq-dv transform."""
-
-        # NOTE TO ASBJOERN: Probably insert method for "binning" instead of
-        # TODO: Asbjørn will insert "binning" here
-        # differentiating here
-        # (use self.increment_method as the variable for selecting method for)
-
-        logging.debug("incrementing data")
-
-        # ---- shifting to y-x ----------------------------------------
-        v1, v2 = value_bounds(self.voltage_preprocessed)
-        if self.voltage_resolution is not None:
-            len_voltage = int(round(abs(v2 - v1) / self.voltage_resolution, 0))
-        else:
-            len_voltage = int(len(self.voltage_preprocessed))
-
-        # ---- interpolating ------------------------------------------
-        logging.debug(" - interpolating capacity(voltage)")
-        f = interp1d(
-            self.voltage_preprocessed,
-            self.capacity_preprocessed,
-            kind=self.interpolation_method,
-        )
-
-        self.voltage_inverted = np.linspace(v1, v2, len_voltage)
-        self.voltage_inverted_step = (v2 - v1) / (len_voltage - 1)
-        self.capacity_inverted = f(self.voltage_inverted)
-
-        if self.smoothing:
-            logging.debug(" - smoothing (savgol filter window)")
-            savgol_filter_window_divisor = np.amin(
-                (self.savgol_filter_window_divisor_default, len_voltage / 5)
-            )
-
-            savgol_filter_window_length = int(
-                len(self.voltage_inverted) / savgol_filter_window_divisor
-            )
-
-            if savgol_filter_window_length % 2 == 0:
-                savgol_filter_window_length -= 1
-
-            self.capacity_inverted = savgol_filter(
-                self.capacity_inverted,
-                np.amax([3, savgol_filter_window_length]),
-                self.savgol_filter_window_order,
-            )
-
-        # ---  diff --------------------
-        if self.increment_method == "diff":
-            logging.debug(" - diff using DIFF")
-            self.incremental_capacity = (
-                np.ediff1d(self.capacity_inverted) / self.voltage_inverted_step
-            )
-            self._incremental_capacity = self.incremental_capacity
-            # --- need to adjust voltage ---
-            self._voltage_processed = self.voltage_inverted[1:]
-            self.voltage_processed = (
-                self.voltage_inverted[1:] - 0.5 * self.voltage_inverted_step
-            )
-
-        elif self.increment_method == "hist":
-            logging.debug(" - diff using HIST")
-            logging.warning(
-                "Using the 'hist' method has not been thoroughly tested yet"
-            )
-            # raise NotImplementedError
-
-            df = pd.DataFrame(
-                {"Capacity": self.capacity_inverted, "Voltage": self.voltage_inverted}
-            )
-            df["dQ"] = df.Capacity.diff()
-            df["Voltage"] = df.Voltage.round(decimals=4)
-            df = df.groupby(["Voltage"])["dQ"].sum().to_frame().reset_index()
-            df["dV"] = df.Voltage.diff().rolling(1).sum()
-            df["dQdV"] = df.dQ / df.dV
-            # df = df[df.dQdV.notnull()]  # Might be needed, but could introduce an artefact
-
-            self.incremental_capacity = df.dQdV
-            self.voltage_processed = df.Voltage
-
-            # TODO: Asbjoern, maybe you can put your method here? Yes
-
-    def post_process_data(
-        self, voltage=None, incremental_capacity=None, voltage_step=None
-    ):
-        """Perform post-processing (smoothing, normalisation, interpolation) of the data."""
-
-        logging.debug("post-processing data")
-
-        if voltage is None:
-            voltage = self.voltage_processed
-            incremental_capacity = self.incremental_capacity
-            voltage_step = self.voltage_inverted_step
-
-        if self.post_smoothing:
-            logging.debug(" - post smoothing (gaussian)")
-            logging.debug(f"    * using voltage fwhm: {self.voltage_fwhm}")
-            points_fwhm = int(self.voltage_fwhm / voltage_step)
-
-            sigma = np.amax([1, points_fwhm / 2])
-
-            incremental_capacity = gaussian_filter1d(
-                incremental_capacity,
-                sigma=sigma,
-                order=self.gaussian_order,
-                mode=self.gaussian_mode,
-                cval=self.gaussian_cval,
-                truncate=self.gaussian_truncate,
-            )
-
-        if self.normalize:
-            logging.debug(" - normalizing")
-            area = simps(incremental_capacity, voltage)
-            incremental_capacity = (
-                incremental_capacity * self.normalizing_factor / abs(area)
-            )
-
-        self.incremental_capacity = incremental_capacity
-
-        fixed_range = False
-        if isinstance(self.fixed_voltage_range, np.ndarray):
-            fixed_range = True
-        else:
-            if self.fixed_voltage_range:
-                fixed_range = True
-        if fixed_range:
-            logging.debug(" - using fixed voltage range (interpolating)")
-            v1, v2, number_of_points = self.fixed_voltage_range
-            v = np.linspace(v1, v2, number_of_points)
-            f = interp1d(
-                x=self.voltage_processed,
-                y=incremental_capacity,
-                kind=self.interpolation_method,
-                bounds_error=False,
-                fill_value=np.NaN,
-            )
-            self.incremental_capacity = f(v)
-            self.voltage_processed = v
-
-
-def value_bounds(x):
-    """Returns tuple with min and max in x."""
-    return np.amin(x), np.amax(x)
-
-
-def index_bounds(x):
-    """Returns tuple with first and last item."""
-    if isinstance(x, (pd.DataFrame, pd.Series)):
-        return x.iloc[0], x.iloc[-1]
-    else:
-        return x[0], x[-1]
-
-
-def dqdv_cycle(cycle, splitter=True, label_direction=False, **kwargs):
-    """Convenience functions for creating dq-dv data from given capacity and
-    voltage cycle.
-
-    Returns the DataFrame with a 'voltage' and a 'incremental_capacity'
-    column.
-
-    Args:
-        cycle (pandas.DataFrame): the cycle data ('voltage', 'capacity', 'direction' (1 or -1)).
-        splitter (bool): insert a np.NaN row between charge and discharge.
-        label_direction (bool):
-
-    Returns:
-        List of step numbers corresponding to the selected steptype.
-        Returns a ``pandas.DataFrame`` instead of a list if ``pdtype`` is set to ``True``.
-
-    Additional key-word arguments are sent to Converter:
-
-    Keyword Args:
-        points_pr_split (int): only used when investigating data using splits, defaults to 10.
-        max_points: None
-        voltage_resolution (float): used for interpolating voltage data (e.g. 0.005)
-        capacity_resolution: used for interpolating capacity data
-        minimum_splits (int): defaults to 3.
-        interpolation_method: scipy interpolation method
-        increment_method (str): defaults to "diff"
-        pre_smoothing (bool): set to True for pre-smoothing (window)
-        smoothing (bool): set to True for smoothing during differentiation (window)
-        post_smoothing (bool): set to True for post-smoothing (gaussian)
-        normalize (bool): set to True for normalizing to capacity
-        normalizing_factor (float):
-        normalizing_roof  (float):
-        savgol_filter_window_divisor_default (int): used for window smoothing, defaults to 50
-        savgol_filter_window_order: used for window smoothing
-        voltage_fwhm (float): used for setting the post-processing gaussian sigma, defaults to 0.01
-        gaussian_order (int): defaults to 0
-        gaussian_mode (str): defaults to "reflect"
-        gaussian_cval (float): defaults to 0.0
-        gaussian_truncate (float): defaults to 4.0
-
-    Example:
-        >>> cycle_df = my_data.get_cap(
-        >>> ...   1,
-        >>> ...   categorical_column=True,
-        >>> ...   method = "forth-and-forth"
-        >>> ...   insert_nan=False,
-        >>> ... )
-        >>> voltage, incremental = ica.dqdv_cycle(cycle_df)
-
-    """
-
-    if cycle.empty:
-        raise NullData(f"The cycle (type={type(cycle)}) is empty.")
-
-    c_first = cycle.loc[cycle["direction"] == -1]
-    c_last = cycle.loc[cycle["direction"] == 1]
-
-    converter = Converter(**kwargs)
-
-    converter.set_data(c_first["capacity"], c_first["voltage"])
-    converter.inspect_data()
-    converter.pre_process_data()
-    converter.increment_data()
-    converter.post_process_data()
-    voltage_first = converter.voltage_processed
-    incremental_capacity_first = converter.incremental_capacity
-
-    if splitter:
-        voltage_first = np.append(voltage_first, np.NaN)
-        incremental_capacity_first = np.append(incremental_capacity_first, np.NaN)
-
-    converter = Converter(**kwargs)
-
-    converter.set_data(c_last["capacity"], c_last["voltage"])
-    converter.inspect_data()
-    converter.pre_process_data()
-    converter.increment_data()
-    converter.post_process_data()
-    voltage_last = converter.voltage_processed[::-1]
-    incremental_capacity_last = converter.incremental_capacity[::-1]
-
-    voltage = np.concatenate((voltage_first, voltage_last))
-    incremental_capacity = np.concatenate(
-        (incremental_capacity_first, incremental_capacity_last)
-    )
-
-    if label_direction:
-        direction_first = -np.ones(len(voltage_first))
-        direction_last = np.ones(len(voltage_last))
-        direction = np.concatenate((direction_first, direction_last))
-        return voltage, incremental_capacity, direction
-
-    return voltage, incremental_capacity
-
-
-def dqdv_cycles(cycles, not_merged=False, label_direction=False, **kwargs):
-    """Convenience functions for creating dq-dv data from given capacity and
-    voltage cycles.
-
-    Returns a DataFrame with a 'voltage' and a 'incremental_capacity'
-    column.
-
-    Args:
-        cycles (pandas.DataFrame): the cycle data ('cycle', 'voltage',
-             'capacity', 'direction' (1 or -1)).
-        not_merged (bool): return list of frames instead of concatenating (
-            defaults to False).
-        label_direction (bool): include 'direction' (1 or -1).
-
-    Returns:
-        ``pandas.DataFrame`` with columns 'cycle', 'voltage', 'dq' (and 'direction' if label_direction is True).
-
-    Additional key-word arguments are sent to Converter:
-
-    Keyword Args:
-        points_pr_split (int): only used when investigating data using
-            splits, defaults to 10.
-        max_points: None
-        voltage_resolution (float): used for interpolating voltage data
-            (e.g. 0.005)
-        capacity_resolution: used for interpolating capacity data
-        minimum_splits (int): defaults to 3.
-        interpolation_method: scipy interpolation method
-        increment_method (str): defaults to "diff"
-        pre_smoothing (bool): set to True for pre-smoothing (window)
-        smoothing (bool): set to True for smoothing during
-            differentiation (window)
-        post_smoothing (bool): set to True for post-smoothing (gaussian)
-        normalize (bool): set to True for normalizing to capacity
-        normalizing_factor (float):
-        normalizing_roof  (float):
-        savgol_filter_window_divisor_default (int): used for window
-            smoothing, defaults to 50
-        savgol_filter_window_order: used for window smoothing
-        voltage_fwhm (float): used for setting the post-processing
-            gaussian sigma, defaults to 0.01
-        gaussian_order (int): defaults to 0
-        gaussian_mode (str): defaults to "reflect"
-        gaussian_cval (float): defaults to 0.0
-        gaussian_truncate (float): defaults to 4.0
-
-    Example:
-        >>> cycles_df = my_data.get_cap(
-        >>> ...   categorical_column=True,
-        >>> ...   method = "forth-and-forth",
-        >>> ...   label_cycle_number=True,
-        >>> ...   insert_nan=False,
-        >>> ... )
-        >>> ica_df = ica.dqdv_cycles(cycles_df)
-
-    """
-
-    # TODO: should add option for normalising based on first cycle capacity
-    # this is e.g. done by first finding the first cycle capacity (nom_cap)
-    # (or use nominal capacity given as input) and then propagating this to
-    # Converter using the key-word arguments
-    #   normalize=True, normalization_factor=1.0, normalization_roof=nom_cap
-
-    if len(cycles) < 1:
-        logging.debug("The food was without nutrition")
-        return pd.DataFrame()
-
-    ica_dfs = list()
-    cycle_group = cycles.groupby("cycle")
-    keys = list()
-    for cycle_number, cycle in cycle_group:
-        cycle = cycle.dropna()
-        if label_direction:
-            v, dq, direction = dqdv_cycle(
-                cycle, splitter=True, label_direction=True, **kwargs
-            )
-            _d = {"voltage": v, "dq": dq, "direction": direction}
-            _cols = ["voltage", "dq", "direction"]
-        else:
-            v, dq = dqdv_cycle(cycle, splitter=True, label_direction=False, **kwargs)
-            _d = {"voltage": v, "dq": dq}
-            _cols = ["voltage", "dq"]
-
-        _ica_df = pd.DataFrame(_d)
-        if not not_merged:
-            _cols.insert(0, "cycle")
-            _ica_df["cycle"] = cycle_number
-            _ica_df = _ica_df[_cols]
-        else:
-            keys.append(cycle_number)
-            _ica_df = _ica_df[_cols]
-        ica_dfs.append(_ica_df)
-
-    if not_merged:
-        return keys, ica_dfs
-
-    ica_df = pd.concat(ica_dfs)
-    return ica_df
-
-
-def dqdv(
-    voltage,
-    capacity,
-    voltage_resolution=None,
-    capacity_resolution=None,
-    voltage_fwhm=0.01,
-    pre_smoothing=True,
-    diff_smoothing=False,
-    post_smoothing=True,
-    post_normalization=True,
-    interpolation_method=None,
-    gaussian_order=None,
-    gaussian_mode=None,
-    gaussian_cval=None,
-    gaussian_truncate=None,
-    points_pr_split=None,
-    savgol_filter_window_divisor_default=None,
-    savgol_filter_window_order=None,
-    max_points=None,
-    **kwargs,
-):
-    """Convenience functions for creating dq-dv data from given capacity
-    and voltage data.
-
-    Args:
-        voltage: nd.array or pd.Series
-        capacity: nd.array or pd.Series
-        voltage_resolution: used for interpolating voltage data (e.g. 0.005)
-        capacity_resolution: used for interpolating capacity data
-        voltage_fwhm: used for setting the post-processing gaussian sigma
-        pre_smoothing: set to True for pre-smoothing (window)
-        diff_smoothing: set to True for smoothing during differentiation
-            (window)
-        post_smoothing: set to True for post-smoothing (gaussian)
-        post_normalization: set to True for normalizing to capacity
-        interpolation_method: scipy interpolation method
-        gaussian_order: int
-        gaussian_mode: mode
-        gaussian_cval:
-        gaussian_truncate:
-        points_pr_split: only used when investigating data using splits
-        savgol_filter_window_divisor_default: used for window smoothing
-        savgol_filter_window_order: used for window smoothing
-        max_points: restricting to max points in vector (capacity-selected)
-
-    Returns:
-        (voltage, dqdv)
-
-    """
-    # Notes:
-    #     PEC data
-    #         pre_smoothing = False
-    #         diff_smoothing = False
-    #         pos_smoothing = False
-    #         voltage_resolution = 0.005
-    #     PEC data
-    #         ...
-    #     Arbin data (IFE)
-    #         ...
-
-    converter = Converter(**kwargs)
-    logging.debug("dqdv - starting")
-    logging.debug("dqdv - created Converter obj")
-    converter.pre_smoothing = pre_smoothing
-    converter.post_smoothing = post_smoothing
-    converter.smoothing = diff_smoothing
-    converter.normalize = post_normalization
-    converter.voltage_fwhm = voltage_fwhm
-    logging.debug(f"converter.pre_smoothing: {converter.pre_smoothing}")
-    logging.debug(f"converter.post_smoothing: {converter.post_smoothing}")
-    logging.debug(f"converter.smoothing: {converter.smoothing}")
-    logging.debug(f"converter.normalise: {converter.normalize}")
-    logging.debug(f"converter.voltage_fwhm: {converter.voltage_fwhm}")
-
-    if voltage_resolution is not None:
-        converter.voltage_resolution = voltage_resolution
-
-    if capacity_resolution is not None:
-        converter.capacity_resolution = capacity_resolution
-
-    if savgol_filter_window_divisor_default is not None:
-        converter.savgol_filter_window_divisor_default = (
-            savgol_filter_window_divisor_default
-        )
-
-        logging.debug(
-            f"converter.savgol_filter_window_divisor_default: "
-            f"{converter.savgol_filter_window_divisor_default}"
-        )
-
-    if savgol_filter_window_order is not None:
-        converter.savgol_filter_window_order = savgol_filter_window_order
-
-        logging.debug(
-            f"converter.savgol_filter_window_order: "
-            f"{converter.savgol_filter_window_order}"
-        )
-
-    if gaussian_mode is not None:
-        converter.gaussian_mode = gaussian_mode
-
-    if gaussian_order is not None:
-        converter.gaussian_order = gaussian_order
-
-    if gaussian_truncate is not None:
-        converter.gaussian_truncate = gaussian_truncate
-
-    if gaussian_cval is not None:
-        converter.gaussian_cval = gaussian_cval
-
-    if interpolation_method is not None:
-        converter.interpolation_method = interpolation_method
-
-    if points_pr_split is not None:
-        converter.points_pr_split = points_pr_split
-
-    if max_points is not None:
-        converter.max_points = max_points
-
-    converter.set_data(capacity, voltage)
-    converter.inspect_data()
-    converter.pre_process_data()
-    converter.increment_data()
-    converter.post_process_data()
-
-    return converter.voltage_processed, converter.incremental_capacity
-
-
-def dqdv_frames(cell, split=False, tidy=True, label_direction=False, **kwargs):
-    """Returns dqdv data as pandas.DataFrame(s) for all cycles.
-
-    Args:
-        cell (CellpyCell-object).
-        split (bool): return one frame for charge and one for
-            discharge if True (defaults to False).
-        tidy (bool): returns the split frames in wide format (defaults
-            to True. Remark that this option is currently not available
-            for non-split frames).
-
-    Returns:
-        one or two ``pandas.DataFrame`` with the following columns:
-        cycle: cycle number (if split is set to True).
-        voltage: voltage
-        dq: the incremental capacity
-
-
-    Additional key-word arguments are sent to Converter:
-
-    Keyword Args:
-        cycle (int or list of ints (cycle numbers)): will process all (or up to max_cycle_number)
-            if not given or equal to None.
-        points_pr_split (int): only used when investigating data
-            using splits, defaults to 10.
-        max_points: None
-        voltage_resolution (float): used for interpolating voltage
-            data (e.g. 0.005)
-        capacity_resolution: used for interpolating capacity data
-        minimum_splits (int): defaults to 3.
-        interpolation_method: scipy interpolation method
-        increment_method (str): defaults to "diff"
-        pre_smoothing (bool): set to True for pre-smoothing (window)
-        smoothing (bool): set to True for smoothing during
-            differentiation (window)
-        post_smoothing (bool): set to True for post-smoothing
-            (gaussian)
-        normalize (bool): set to True for normalizing to capacity
-        normalizing_factor (float):
-        normalizing_roof  (float):
-        savgol_filter_window_divisor_default (int): used for window
-            smoothing, defaults to 50
-        savgol_filter_window_order: used for window smoothing
-        voltage_fwhm (float): used for setting the post-processing
-            gaussian sigma, defaults to 0.01
-        gaussian_order (int): defaults to 0
-        gaussian_mode (str): defaults to "reflect"
-        gaussian_cval (float): defaults to 0.0
-        gaussian_truncate (float): defaults to 4.0
-
-    Example:
-        >>> from cellpy.utils import ica
-        >>> charge_df, dcharge_df = ica.ica_frames(my_cell, split=True)
-        >>> charge_df.plot(x=("voltage", "v"))
-
-    """
-    # TODO: should add option for normalizing based on first cycle capacity
-    # this is e.g. done by first finding the first cycle capacity (nom_cap)
-    # (or use nominal capacity given as input) and then propagating this to
-    # Converter using the key-word arguments
-    #   normalize=True, normalization_factor=1.0, normalization_roof=nom_cap
-
-    if split:
-        return _dqdv_split_frames(cell, tidy=tidy, **kwargs)
-    else:
-        return _dqdv_combinded_frame(
-            cell, tidy=tidy, label_direction=label_direction, **kwargs
-        )
-
-
-def _constrained_dq_dv_using_dataframes(capacity, minimum_v, maximum_v, **kwargs):
-    converter = Converter(**kwargs)
-    converter.set_data(capacity)
-    converter.inspect_data()
-    converter.pre_process_data()
-    converter.increment_data()
-    converter.fixed_voltage_range = [minimum_v, maximum_v, 100]
-    converter.post_process_data()
-    return converter.voltage_processed, converter.incremental_capacity
-
-
-def _make_ica_charge_curves(cycles_dfs, cycle_numbers, minimum_v, maximum_v, **kwargs):
-    incremental_charge_list = []
-
-    for c, n in zip(cycles_dfs, cycle_numbers):
-        if c.empty:
-            logging.info(f"{n} is empty")
-            v = [np.nan]
-            dq = [np.nan]
-        else:
-            v, dq = _constrained_dq_dv_using_dataframes(
-                c, minimum_v, maximum_v, **kwargs
-            )
-        if not incremental_charge_list:
-            d = pd.DataFrame({"v": v})
-            d.name = "voltage"
-            incremental_charge_list.append(d)
-
-            d = pd.DataFrame({f"dq": dq})
-            d.name = n
-            incremental_charge_list.append(d)
-
-        else:
-            d = pd.DataFrame({f"dq": dq})
-            # d.name = f"{cycle}"
-            d.name = n
-            incremental_charge_list.append(d)
-
-    return incremental_charge_list
-
-
-def _dqdv_combinded_frame(cell, tidy=True, label_direction=False, **kwargs):
-    """Returns full cycle dqdv data for all cycles as one pd.DataFrame.
-
-    Args:
-        cell: CellpyCell-object
-
-    Returns:
-        pandas.DataFrame with the following columns:
-            cycle: cycle number
-            voltage: voltage
-            dq: the incremental capacity
-    """
-    cycle = kwargs.pop("cycle", None)
-    number_of_points = kwargs.pop("number_of_points", None)
-    cycles = cell.get_cap(
-        cycle=cycle,
-        method="forth-and-forth",
-        categorical_column=True,
-        label_cycle_number=True,
-        insert_nan=False,
-        number_of_points=number_of_points,
-    )
-    ica_df = dqdv_cycles(
-        cycles, not_merged=not tidy, label_direction=label_direction, **kwargs
-    )
-
-    if not tidy:
-        # dqdv_cycles returns a list of cycle numbers and a list of DataFrames
-        # if not_merged is set to True (or not False)
-        keys, ica_df = ica_df
-        ica_df = pd.concat(ica_df, axis=1, keys=keys)
-        return ica_df
-
-    assert isinstance(ica_df, pd.DataFrame)
-    return ica_df
-
-
-def _dqdv_split_frames(
-    cell,
-    tidy=False,
-    trim_taper_steps=None,
-    steps_to_skip=None,
-    steptable=None,
-    max_cycle_number=None,
-    **kwargs,
-):
-    """Returns dqdv data as pandas.DataFrames for all cycles.
-
-    Args:
-        cell (CellpyCell-object).
-        tidy (bool): return in wide format if False (default),
-            long (tidy) format if True.
-
-    Returns:
-        (charge_ica_frame, discharge_ica_frame) where the frames are
-        pandas.DataFrames where the first column is voltage ('v') and
-        the following columns are the incremental capcaity for each
-        cycle (multi-indexed, where cycle number is on the top level).
-
-    Example:
-        >>> from cellpy.utils import ica
-        >>> charge_ica_df, dcharge_ica_df = ica.ica_frames(my_cell)
-        >>> charge_ica_df.plot(x=("voltage", "v"))
-
-    """
-    cycle = kwargs.pop("cycle", None)
-    if cycle and not isinstance(cycle, (list, tuple)):
-        cycle = [cycle]
-
-    charge_dfs, cycles, minimum_v, maximum_v = collect_capacity_curves(
-        cell,
-        direction="charge",
-        trim_taper_steps=trim_taper_steps,
-        steps_to_skip=steps_to_skip,
-        steptable=steptable,
-        max_cycle_number=max_cycle_number,
-        cycle=cycle,
-    )
-    logging.debug(f"retrieved {len(charge_dfs)} charge cycles")
-    # charge_df = pd.concat(
-    # charge_dfs, axis=1, keys=[k.name for k in charge_dfs])
-
-    ica_charge_dfs = _make_ica_charge_curves(
-        charge_dfs, cycles, minimum_v, maximum_v, **kwargs
-    )
-
-    ica_charge_df = pd.concat(
-        ica_charge_dfs, axis=1, keys=[k.name for k in ica_charge_dfs]
-    )
-
-    dcharge_dfs, cycles, minimum_v, maximum_v = collect_capacity_curves(
-        cell,
-        direction="discharge",
-        trim_taper_steps=trim_taper_steps,
-        steps_to_skip=steps_to_skip,
-        steptable=steptable,
-        max_cycle_number=max_cycle_number,
-        cycle=cycle,
-    )
-    logging.debug(f"retrieved {len(dcharge_dfs)} discharge cycles")
-    ica_dcharge_dfs = _make_ica_charge_curves(
-        dcharge_dfs, cycles, minimum_v, maximum_v, **kwargs
-    )
-    ica_discharge_df = pd.concat(
-        ica_dcharge_dfs, axis=1, keys=[k.name for k in ica_dcharge_dfs]
-    )
-    ica_charge_df.columns.names = ["cycle", "value"]
-    ica_discharge_df.columns.names = ["cycle", "value"]
-
-    if tidy:
-        ica_charge_df = ica_charge_df.melt(
-            "voltage", var_name="cycle", value_name="dq", col_level=0
-        )
-        ica_discharge_df = ica_discharge_df.melt(
-            "voltage", var_name="cycle", value_name="dq", col_level=0
-        )
-
-    return ica_charge_df, ica_discharge_df
-
-
-def _check_class_ica():
-    print(40 * "=")
-    print("running check_class_ica")
-    print(40 * "-")
-
-    import matplotlib.pyplot as plt
-
-    cell = _get_a_cell_to_play_with()
-    cycle = 5
-    print("looking at cycle %i" % cycle)
-
-    # ---------- processing and plotting ----------------
-    fig, (ax1, ax2) = plt.subplots(2, 1)
-    capacity, voltage = cell.get_ccap(cycle, as_frame=False)
-    ax1.plot(capacity, voltage, "b.-", label="raw")
-    converter = Converter()
-    converter.set_data(capacity, voltage)
-    converter.inspect_data()
-    converter.pre_process_data()
-    ax1.plot(
-        converter.capacity_preprocessed,
-        converter.voltage_preprocessed,
-        "r.-",
-        alpha=0.3,
-        label="pre-processed",
-    )
-
-    converter.increment_data()
-    ax2.plot(
-        converter.voltage_processed,
-        converter.incremental_capacity,
-        "b.-",
-        label="incremented",
-    )
-
-    converter.fixed_voltage_range = False
-    converter.post_smoothing = True
-    converter.normalize = False
-    converter.post_process_data()
-    ax2.plot(
-        converter.voltage_processed,
-        converter.incremental_capacity,
-        "y-",
-        alpha=0.3,
-        lw=4.0,
-        label="smoothed",
-    )
-
-    converter.fixed_voltage_range = np.array((0.1, 1.2, 100))
-    converter.post_smoothing = False
-    converter.normalize = False
-    converter.post_process_data()
-    ax2.plot(
-        converter.voltage_processed,
-        converter.incremental_capacity,
-        "go",
-        alpha=0.7,
-        label="fixed voltage range",
-    )
-    ax1.legend(numpoints=1)
-    ax2.legend(numpoints=1)
-    ax1.set_ylabel("Voltage (V)")
-    ax1.set_xlabel("Capacity (mAh/g)")
-    ax2.set_xlabel("Voltage (V)")
-    ax2.set_ylabel("dQ/dV (mAh/g/V)")
-    plt.show()
-
-
-def _get_a_cell_to_play_with():
-    from cellpy import cellreader
-
-    # -------- defining overall path-names etc ----------
-    current_file_path = os.path.dirname(os.path.realpath(__file__))
-    print(current_file_path)
-    relative_test_data_dir = "../../testdata/hdf5"
-    test_data_dir = os.path.abspath(
-        os.path.join(current_file_path, relative_test_data_dir)
-    )
-    # test_data_dir_out = os.path.join(test_data_dir, "out")
-    test_cellpy_file = "20160805_test001_45_cc.h5"
-    test_cellpy_file_full = os.path.join(test_data_dir, test_cellpy_file)
-    # mass = 0.078609164
-
-    # ---------- loading test-data ----------------------
-    cell = cellreader.CellpyCell()
-    cell.load(test_cellpy_file_full)
-    list_of_cycles = cell.get_cycle_numbers()
-    number_of_cycles = len(list_of_cycles)
-    print("you have %i cycles" % number_of_cycles)
-    # cell.save(test_cellpy_file_full)
-    return cell
-
-
-def _check_if_works():
-    import pandas as pd
-    from cellpy import cellreader
-
-    cell = _get_a_cell_to_play_with()
-
-    a = dqdv_frames(cell)
-    print("Hei")
-
-
-if __name__ == "__main__":
-    _check_if_works()
+"""ica contains routines for creating and working with
+incremental capacity analysis data"""
+
+import os
+import logging
+import warnings
+
+import numpy as np
+import pandas as pd
+from scipy import stats
+from scipy.interpolate import interp1d
+from scipy.signal import savgol_filter
+from scipy.integrate import simps
+from scipy.ndimage.filters import gaussian_filter1d
+import pandas as pd
+
+from cellpy.exceptions import NullData
+from cellpy.readers.core import collect_capacity_curves
+
+
+# TODO: @jepe - documentation and tests
+# TODO: @jepe - fitting of o-c curves and differentiation
+# TODO: @jepe - modeling and fitting
+# TODO: @jepe - full-cell
+# TODO: @jepe - binning method (assigned to Asbjoern)
+
+
+class Converter:
+    """Class for dq-dv handling.
+
+    Typical usage is to (1) set the data, (2) inspect the data,
+    (3) pre-process the data,
+    (4) perform the dq-dv transform, and finally (5) post-process the data.
+
+    A short note about normalization:
+
+        - If ``normalization`` is set to ``False``, then no normalization will be done.
+        - If ``normalization`` is ``True``, and ``normalization_factor`` is ``None``, the total capacity of
+          the half cycle will be used for normalization, else the ``normalization_factor`` will be used.
+        - If ``normalization`` is ``True``, and ``normalization_roof`` is not ``None``,
+          the capacity divided by ``normalization_roof`` will be used for normalization.
+
+    """
+
+    def __init__(
+        self,
+        capacity=None,
+        voltage=None,
+        points_pr_split=10,
+        max_points=None,
+        voltage_resolution=None,
+        capacity_resolution=None,
+        minimum_splits=3,
+        interpolation_method="linear",
+        increment_method="diff",
+        pre_smoothing=False,
+        smoothing=False,
+        post_smoothing=True,
+        normalize=True,
+        normalizing_factor=None,
+        normalizing_roof=None,
+        savgol_filter_window_divisor_default=50,
+        savgol_filter_window_order=3,
+        voltage_fwhm=0.01,
+        gaussian_order=0,
+        gaussian_mode="reflect",
+        gaussian_cval=0.0,
+        gaussian_truncate=4.0,
+    ):
+        self.capacity = capacity
+        self.voltage = voltage
+
+        self.capacity_preprocessed = None
+        self.voltage_preprocessed = None
+        self.capacity_inverted = None
+        self.voltage_inverted = None
+
+        self.incremental_capacity = None
+        self._incremental_capacity = None  # before smoothing
+        self.voltage_processed = None
+        self._voltage_processed = None  # before shifting / centering
+
+        self.voltage_inverted_step = None
+
+        self.points_pr_split = points_pr_split
+        self.max_points = max_points
+        self.voltage_resolution = voltage_resolution
+        self.capacity_resolution = capacity_resolution
+        self.minimum_splits = minimum_splits
+        self.interpolation_method = interpolation_method
+        self.increment_method = increment_method
+        self.pre_smoothing = pre_smoothing
+        self.smoothing = smoothing
+        self.post_smoothing = post_smoothing
+        self.savgol_filter_window_divisor_default = savgol_filter_window_divisor_default
+        self.savgol_filter_window_order = savgol_filter_window_order
+        self.voltage_fwhm = voltage_fwhm
+        self.gaussian_order = gaussian_order
+        self.gaussian_mode = gaussian_mode
+        self.gaussian_cval = gaussian_cval
+        self.gaussian_truncate = gaussian_truncate
+        self.normalize = normalize
+        self.normalizing_factor = normalizing_factor
+        self.normalizing_roof = normalizing_roof
+
+        self.d_capacity_mean = None
+        self.d_voltage_mean = None
+        self.len_capacity = None
+        self.len_voltage = None
+        self.min_capacity = None
+        self.max_capacity = None
+        self.start_capacity = None
+        self.end_capacity = None
+        self.number_of_points = None
+        self.std_err_median = None
+        self.std_err_mean = None
+
+        self.fixed_voltage_range = False
+
+        self.errors = []
+
+    def __str__(self):
+        txt = f"[ica.converter] {str(type(self))}\n"
+        attrs = vars(self)
+        for name, att in attrs.items():
+            if isinstance(att, (pd.DataFrame, pd.Series, np.ndarray)):
+                str_att = f"<vector> ({str(type(att))})"
+            else:
+                str_att = str(att)
+            txt += f"{name}: {str_att}\n"
+
+        return txt
+
+    def set_data(self, capacity, voltage=None, capacity_label="q", voltage_label="v"):
+        """Set the data."""
+
+        logging.debug("setting data (capacity and voltage)")
+
+        if isinstance(capacity, pd.DataFrame):
+            logging.debug("received a pandas.DataFrame")
+            self.capacity = capacity[capacity_label]
+            self.voltage = capacity[voltage_label]
+        else:
+            assert len(capacity) == len(voltage)
+            self.capacity = capacity
+            self.voltage = voltage
+
+    def inspect_data(self, capacity=None, voltage=None, err_est=False, diff_est=False):
+        """Check and inspect the data."""
+
+        logging.debug("inspecting the data")
+
+        if capacity is None:
+            capacity = self.capacity
+        if voltage is None:
+            voltage = self.voltage
+
+        if capacity is None or voltage is None:
+            raise NullData
+
+        self.len_capacity = len(capacity)
+        self.len_voltage = len(voltage)
+
+        if self.len_capacity <= 1:
+            raise NullData
+        if self.len_voltage <= 1:
+            raise NullData
+
+        self.min_capacity, self.max_capacity = value_bounds(capacity)
+        self.start_capacity, self.end_capacity = index_bounds(capacity)
+
+        self.number_of_points = len(capacity)
+
+        if diff_est:
+            d_capacity = np.diff(capacity)
+            d_voltage = np.diff(voltage)
+
+            self.d_capacity_mean = np.mean(d_capacity)
+            self.d_voltage_mean = np.mean(d_voltage)
+
+        if err_est:
+            splits = int(self.number_of_points / self.points_pr_split)
+            rest = self.number_of_points % self.points_pr_split
+
+            if splits < self.minimum_splits:
+                txt = "no point in splitting, too little data"
+                logging.debug(txt)
+                self.errors.append("splitting: to few points")
+            else:
+                if rest > 0:
+                    _cap = capacity[:-rest]
+                    _vol = voltage[:-rest]
+                else:
+                    _cap = capacity
+                    _vol = voltage
+
+                c_pieces = np.split(_cap, splits)
+                v_pieces = np.split(_vol, splits)
+                # c_middle = int(np.amax(c_pieces) / 2)
+
+                std_err = []
+                c_pieces_avg = []
+                for c, v in zip(c_pieces, v_pieces):
+                    _slope, _intercept, _r_value, _p_value, _std_err = stats.linregress(
+                        c, v
+                    )
+                    std_err.append(_std_err)
+                    c_pieces_avg.append(np.mean(c))
+
+                self.std_err_median = np.median(std_err)
+                self.std_err_mean = np.mean(std_err)
+
+        if not self.start_capacity == self.min_capacity:
+            self.errors.append("capacity: start<>min")
+
+        if not self.end_capacity == self.max_capacity:
+            self.errors.append("capacity: end<>max")
+
+        if self.normalizing_factor is None:
+            self.normalizing_factor = self.end_capacity
+
+        if self.normalizing_roof is not None:
+            self.normalizing_factor = (
+                self.normalizing_factor * self.end_capacity / self.normalizing_roof
+            )
+
+    def pre_process_data(self):
+        """Perform some pre-processing of the data (i.e. interpolation)."""
+
+        logging.debug("pre-processing the data")
+
+        capacity = self.capacity
+        voltage = self.voltage
+
+        # performing an interpolation in v(q) space
+        logging.debug(" - interpolating voltage(capacity)")
+        c1, c2 = index_bounds(capacity)
+        if self.max_points is not None:
+            len_capacity = min(self.max_points, self.len_capacity)
+        elif self.capacity_resolution is not None:
+            len_capacity = int(round(abs(c2 - c1) / self.capacity_resolution, 0))
+        else:
+            len_capacity = self.len_capacity
+
+        f = interp1d(capacity, voltage, kind=self.interpolation_method)
+
+        self.capacity_preprocessed = np.linspace(c1, c2, len_capacity)
+        self.voltage_preprocessed = f(self.capacity_preprocessed)
+
+        if self.pre_smoothing:
+            logging.debug(" - pre-smoothing (savgol filter window)")
+            savgol_filter_window_divisor = np.amin(
+                (self.savgol_filter_window_divisor_default, len_capacity / 5)
+            )
+            savgol_filter_window_length = int(
+                len_capacity / savgol_filter_window_divisor
+            )
+
+            if savgol_filter_window_length % 2 == 0:
+                savgol_filter_window_length -= 1
+            savgol_filter_window_length = np.amax([3, savgol_filter_window_length])
+
+            self.voltage_preprocessed = savgol_filter(
+                self.voltage_preprocessed,
+                savgol_filter_window_length,
+                self.savgol_filter_window_order,
+            )
+
+    def increment_data(self):
+        """Perform the dq-dv transform."""
+
+        # NOTE TO ASBJOERN: Probably insert method for "binning" instead of
+        # TODO: Asbjørn will insert "binning" here
+        # differentiating here
+        # (use self.increment_method as the variable for selecting method for)
+
+        logging.debug("incrementing data")
+
+        # ---- shifting to y-x ----------------------------------------
+        v1, v2 = value_bounds(self.voltage_preprocessed)
+        if self.voltage_resolution is not None:
+            len_voltage = int(round(abs(v2 - v1) / self.voltage_resolution, 0))
+        else:
+            len_voltage = int(len(self.voltage_preprocessed))
+
+        # ---- interpolating ------------------------------------------
+        logging.debug(" - interpolating capacity(voltage)")
+        f = interp1d(
+            self.voltage_preprocessed,
+            self.capacity_preprocessed,
+            kind=self.interpolation_method,
+        )
+
+        self.voltage_inverted = np.linspace(v1, v2, len_voltage)
+        self.voltage_inverted_step = (v2 - v1) / (len_voltage - 1)
+        self.capacity_inverted = f(self.voltage_inverted)
+
+        if self.smoothing:
+            logging.debug(" - smoothing (savgol filter window)")
+            savgol_filter_window_divisor = np.amin(
+                (self.savgol_filter_window_divisor_default, len_voltage / 5)
+            )
+
+            savgol_filter_window_length = int(
+                len(self.voltage_inverted) / savgol_filter_window_divisor
+            )
+
+            if savgol_filter_window_length % 2 == 0:
+                savgol_filter_window_length -= 1
+
+            self.capacity_inverted = savgol_filter(
+                self.capacity_inverted,
+                np.amax([3, savgol_filter_window_length]),
+                self.savgol_filter_window_order,
+            )
+
+        # ---  diff --------------------
+        if self.increment_method == "diff":
+            logging.debug(" - diff using DIFF")
+            self.incremental_capacity = (
+                np.ediff1d(self.capacity_inverted) / self.voltage_inverted_step
+            )
+            self._incremental_capacity = self.incremental_capacity
+            # --- need to adjust voltage ---
+            self._voltage_processed = self.voltage_inverted[1:]
+            self.voltage_processed = (
+                self.voltage_inverted[1:] - 0.5 * self.voltage_inverted_step
+            )
+
+        elif self.increment_method == "hist":
+            logging.debug(" - diff using HIST")
+            logging.warning(
+                "Using the 'hist' method has not been thoroughly tested yet"
+            )
+            # raise NotImplementedError
+
+            df = pd.DataFrame(
+                {"Capacity": self.capacity_inverted, "Voltage": self.voltage_inverted}
+            )
+            df["dQ"] = df.Capacity.diff()
+            df["Voltage"] = df.Voltage.round(decimals=4)
+            df = df.groupby(["Voltage"])["dQ"].sum().to_frame().reset_index()
+            df["dV"] = df.Voltage.diff().rolling(1).sum()
+            df["dQdV"] = df.dQ / df.dV
+            # df = df[df.dQdV.notnull()]  # Might be needed, but could introduce an artefact
+
+            self.incremental_capacity = df.dQdV
+            self.voltage_processed = df.Voltage
+
+            # TODO: Asbjoern, maybe you can put your method here? Yes
+
+    def post_process_data(
+        self, voltage=None, incremental_capacity=None, voltage_step=None
+    ):
+        """Perform post-processing (smoothing, normalisation, interpolation) of the data."""
+
+        logging.debug("post-processing data")
+
+        if voltage is None:
+            voltage = self.voltage_processed
+            incremental_capacity = self.incremental_capacity
+            voltage_step = self.voltage_inverted_step
+
+        if self.post_smoothing:
+            logging.debug(" - post smoothing (gaussian)")
+            logging.debug(f"    * using voltage fwhm: {self.voltage_fwhm}")
+            points_fwhm = int(self.voltage_fwhm / voltage_step)
+
+            sigma = np.amax([1, points_fwhm / 2])
+
+            incremental_capacity = gaussian_filter1d(
+                incremental_capacity,
+                sigma=sigma,
+                order=self.gaussian_order,
+                mode=self.gaussian_mode,
+                cval=self.gaussian_cval,
+                truncate=self.gaussian_truncate,
+            )
+
+        if self.normalize:
+            logging.debug(" - normalizing")
+            area = simps(incremental_capacity, voltage)
+            incremental_capacity = (
+                incremental_capacity * self.normalizing_factor / abs(area)
+            )
+
+        self.incremental_capacity = incremental_capacity
+
+        fixed_range = False
+        if isinstance(self.fixed_voltage_range, np.ndarray):
+            fixed_range = True
+        else:
+            if self.fixed_voltage_range:
+                fixed_range = True
+        if fixed_range:
+            logging.debug(" - using fixed voltage range (interpolating)")
+            v1, v2, number_of_points = self.fixed_voltage_range
+            v = np.linspace(v1, v2, number_of_points)
+            f = interp1d(
+                x=self.voltage_processed,
+                y=incremental_capacity,
+                kind=self.interpolation_method,
+                bounds_error=False,
+                fill_value=np.NaN,
+            )
+            self.incremental_capacity = f(v)
+            self.voltage_processed = v
+
+
+def value_bounds(x):
+    """Returns tuple with min and max in x."""
+    return np.amin(x), np.amax(x)
+
+
+def index_bounds(x):
+    """Returns tuple with first and last item."""
+    if isinstance(x, (pd.DataFrame, pd.Series)):
+        return x.iloc[0], x.iloc[-1]
+    else:
+        return x[0], x[-1]
+
+
+def dqdv_cycle(cycle, splitter=True, label_direction=False, **kwargs):
+    """Convenience functions for creating dq-dv data from given capacity and
+    voltage cycle.
+
+    Returns the DataFrame with a 'voltage' and a 'incremental_capacity'
+    column.
+
+    Args:
+        cycle (pandas.DataFrame): the cycle data ('voltage', 'capacity', 'direction' (1 or -1)).
+        splitter (bool): insert a np.NaN row between charge and discharge.
+        label_direction (bool):
+
+    Returns:
+        List of step numbers corresponding to the selected steptype.
+        Returns a ``pandas.DataFrame`` instead of a list if ``pdtype`` is set to ``True``.
+
+    Additional key-word arguments are sent to Converter:
+
+    Keyword Args:
+        points_pr_split (int): only used when investigating data using splits, defaults to 10.
+        max_points: None
+        voltage_resolution (float): used for interpolating voltage data (e.g. 0.005)
+        capacity_resolution: used for interpolating capacity data
+        minimum_splits (int): defaults to 3.
+        interpolation_method: scipy interpolation method
+        increment_method (str): defaults to "diff"
+        pre_smoothing (bool): set to True for pre-smoothing (window)
+        smoothing (bool): set to True for smoothing during differentiation (window)
+        post_smoothing (bool): set to True for post-smoothing (gaussian)
+        normalize (bool): set to True for normalizing to capacity
+        normalizing_factor (float):
+        normalizing_roof  (float):
+        savgol_filter_window_divisor_default (int): used for window smoothing, defaults to 50
+        savgol_filter_window_order: used for window smoothing
+        voltage_fwhm (float): used for setting the post-processing gaussian sigma, defaults to 0.01
+        gaussian_order (int): defaults to 0
+        gaussian_mode (str): defaults to "reflect"
+        gaussian_cval (float): defaults to 0.0
+        gaussian_truncate (float): defaults to 4.0
+
+    Example:
+        >>> cycle_df = my_data.get_cap(
+        >>> ...   1,
+        >>> ...   categorical_column=True,
+        >>> ...   method = "forth-and-forth"
+        >>> ...   insert_nan=False,
+        >>> ... )
+        >>> voltage, incremental = ica.dqdv_cycle(cycle_df)
+
+    """
+
+    if cycle.empty:
+        raise NullData(f"The cycle (type={type(cycle)}) is empty.")
+
+    c_first = cycle.loc[cycle["direction"] == -1]
+    c_last = cycle.loc[cycle["direction"] == 1]
+
+    converter = Converter(**kwargs)
+
+    converter.set_data(c_first["capacity"], c_first["voltage"])
+    converter.inspect_data()
+    converter.pre_process_data()
+    converter.increment_data()
+    converter.post_process_data()
+    voltage_first = converter.voltage_processed
+    incremental_capacity_first = converter.incremental_capacity
+
+    if splitter:
+        voltage_first = np.append(voltage_first, np.NaN)
+        incremental_capacity_first = np.append(incremental_capacity_first, np.NaN)
+
+    converter = Converter(**kwargs)
+
+    converter.set_data(c_last["capacity"], c_last["voltage"])
+    converter.inspect_data()
+    converter.pre_process_data()
+    converter.increment_data()
+    converter.post_process_data()
+    voltage_last = converter.voltage_processed[::-1]
+    incremental_capacity_last = converter.incremental_capacity[::-1]
+
+    voltage = np.concatenate((voltage_first, voltage_last))
+    incremental_capacity = np.concatenate(
+        (incremental_capacity_first, incremental_capacity_last)
+    )
+
+    if label_direction:
+        direction_first = -np.ones(len(voltage_first))
+        direction_last = np.ones(len(voltage_last))
+        direction = np.concatenate((direction_first, direction_last))
+        return voltage, incremental_capacity, direction
+
+    return voltage, incremental_capacity
+
+
+def dqdv_cycles(cycles, not_merged=False, label_direction=False, **kwargs):
+    """Convenience functions for creating dq-dv data from given capacity and
+    voltage cycles.
+
+    Returns a DataFrame with a 'voltage' and a 'incremental_capacity'
+    column.
+
+    Args:
+        cycles (pandas.DataFrame): the cycle data ('cycle', 'voltage',
+             'capacity', 'direction' (1 or -1)).
+        not_merged (bool): return list of frames instead of concatenating (
+            defaults to False).
+        label_direction (bool): include 'direction' (1 or -1).
+
+    Returns:
+        ``pandas.DataFrame`` with columns 'cycle', 'voltage', 'dq' (and 'direction' if label_direction is True).
+
+    Additional key-word arguments are sent to Converter:
+
+    Keyword Args:
+        points_pr_split (int): only used when investigating data using
+            splits, defaults to 10.
+        max_points: None
+        voltage_resolution (float): used for interpolating voltage data
+            (e.g. 0.005)
+        capacity_resolution: used for interpolating capacity data
+        minimum_splits (int): defaults to 3.
+        interpolation_method: scipy interpolation method
+        increment_method (str): defaults to "diff"
+        pre_smoothing (bool): set to True for pre-smoothing (window)
+        smoothing (bool): set to True for smoothing during
+            differentiation (window)
+        post_smoothing (bool): set to True for post-smoothing (gaussian)
+        normalize (bool): set to True for normalizing to capacity
+        normalizing_factor (float):
+        normalizing_roof  (float):
+        savgol_filter_window_divisor_default (int): used for window
+            smoothing, defaults to 50
+        savgol_filter_window_order: used for window smoothing
+        voltage_fwhm (float): used for setting the post-processing
+            gaussian sigma, defaults to 0.01
+        gaussian_order (int): defaults to 0
+        gaussian_mode (str): defaults to "reflect"
+        gaussian_cval (float): defaults to 0.0
+        gaussian_truncate (float): defaults to 4.0
+
+    Example:
+        >>> cycles_df = my_data.get_cap(
+        >>> ...   categorical_column=True,
+        >>> ...   method = "forth-and-forth",
+        >>> ...   label_cycle_number=True,
+        >>> ...   insert_nan=False,
+        >>> ... )
+        >>> ica_df = ica.dqdv_cycles(cycles_df)
+
+    """
+
+    # TODO: should add option for normalising based on first cycle capacity
+    # this is e.g. done by first finding the first cycle capacity (nom_cap)
+    # (or use nominal capacity given as input) and then propagating this to
+    # Converter using the key-word arguments
+    #   normalize=True, normalization_factor=1.0, normalization_roof=nom_cap
+
+    if len(cycles) < 1:
+        logging.debug("The food was without nutrition")
+        return pd.DataFrame()
+
+    ica_dfs = list()
+    cycle_group = cycles.groupby("cycle")
+    keys = list()
+    for cycle_number, cycle in cycle_group:
+        cycle = cycle.dropna()
+        if label_direction:
+            v, dq, direction = dqdv_cycle(
+                cycle, splitter=True, label_direction=True, **kwargs
+            )
+            _d = {"voltage": v, "dq": dq, "direction": direction}
+            _cols = ["voltage", "dq", "direction"]
+        else:
+            v, dq = dqdv_cycle(cycle, splitter=True, label_direction=False, **kwargs)
+            _d = {"voltage": v, "dq": dq}
+            _cols = ["voltage", "dq"]
+
+        _ica_df = pd.DataFrame(_d)
+        if not not_merged:
+            _cols.insert(0, "cycle")
+            _ica_df["cycle"] = cycle_number
+            _ica_df = _ica_df[_cols]
+        else:
+            keys.append(cycle_number)
+            _ica_df = _ica_df[_cols]
+        ica_dfs.append(_ica_df)
+
+    if not_merged:
+        return keys, ica_dfs
+
+    ica_df = pd.concat(ica_dfs)
+    return ica_df
+
+
+def dqdv(
+    voltage,
+    capacity,
+    voltage_resolution=None,
+    capacity_resolution=None,
+    voltage_fwhm=0.01,
+    pre_smoothing=True,
+    diff_smoothing=False,
+    post_smoothing=True,
+    post_normalization=True,
+    interpolation_method=None,
+    gaussian_order=None,
+    gaussian_mode=None,
+    gaussian_cval=None,
+    gaussian_truncate=None,
+    points_pr_split=None,
+    savgol_filter_window_divisor_default=None,
+    savgol_filter_window_order=None,
+    max_points=None,
+    **kwargs,
+):
+    """Convenience functions for creating dq-dv data from given capacity
+    and voltage data.
+
+    Args:
+        voltage: nd.array or pd.Series
+        capacity: nd.array or pd.Series
+        voltage_resolution: used for interpolating voltage data (e.g. 0.005)
+        capacity_resolution: used for interpolating capacity data
+        voltage_fwhm: used for setting the post-processing gaussian sigma
+        pre_smoothing: set to True for pre-smoothing (window)
+        diff_smoothing: set to True for smoothing during differentiation
+            (window)
+        post_smoothing: set to True for post-smoothing (gaussian)
+        post_normalization: set to True for normalizing to capacity
+        interpolation_method: scipy interpolation method
+        gaussian_order: int
+        gaussian_mode: mode
+        gaussian_cval:
+        gaussian_truncate:
+        points_pr_split: only used when investigating data using splits
+        savgol_filter_window_divisor_default: used for window smoothing
+        savgol_filter_window_order: used for window smoothing
+        max_points: restricting to max points in vector (capacity-selected)
+
+    Returns:
+        (voltage, dqdv)
+
+    """
+    # Notes:
+    #     PEC data
+    #         pre_smoothing = False
+    #         diff_smoothing = False
+    #         pos_smoothing = False
+    #         voltage_resolution = 0.005
+    #     PEC data
+    #         ...
+    #     Arbin data (IFE)
+    #         ...
+
+    converter = Converter(**kwargs)
+    logging.debug("dqdv - starting")
+    logging.debug("dqdv - created Converter obj")
+    converter.pre_smoothing = pre_smoothing
+    converter.post_smoothing = post_smoothing
+    converter.smoothing = diff_smoothing
+    converter.normalize = post_normalization
+    converter.voltage_fwhm = voltage_fwhm
+    logging.debug(f"converter.pre_smoothing: {converter.pre_smoothing}")
+    logging.debug(f"converter.post_smoothing: {converter.post_smoothing}")
+    logging.debug(f"converter.smoothing: {converter.smoothing}")
+    logging.debug(f"converter.normalise: {converter.normalize}")
+    logging.debug(f"converter.voltage_fwhm: {converter.voltage_fwhm}")
+
+    if voltage_resolution is not None:
+        converter.voltage_resolution = voltage_resolution
+
+    if capacity_resolution is not None:
+        converter.capacity_resolution = capacity_resolution
+
+    if savgol_filter_window_divisor_default is not None:
+        converter.savgol_filter_window_divisor_default = (
+            savgol_filter_window_divisor_default
+        )
+
+        logging.debug(
+            f"converter.savgol_filter_window_divisor_default: "
+            f"{converter.savgol_filter_window_divisor_default}"
+        )
+
+    if savgol_filter_window_order is not None:
+        converter.savgol_filter_window_order = savgol_filter_window_order
+
+        logging.debug(
+            f"converter.savgol_filter_window_order: "
+            f"{converter.savgol_filter_window_order}"
+        )
+
+    if gaussian_mode is not None:
+        converter.gaussian_mode = gaussian_mode
+
+    if gaussian_order is not None:
+        converter.gaussian_order = gaussian_order
+
+    if gaussian_truncate is not None:
+        converter.gaussian_truncate = gaussian_truncate
+
+    if gaussian_cval is not None:
+        converter.gaussian_cval = gaussian_cval
+
+    if interpolation_method is not None:
+        converter.interpolation_method = interpolation_method
+
+    if points_pr_split is not None:
+        converter.points_pr_split = points_pr_split
+
+    if max_points is not None:
+        converter.max_points = max_points
+
+    converter.set_data(capacity, voltage)
+    converter.inspect_data()
+    converter.pre_process_data()
+    converter.increment_data()
+    converter.post_process_data()
+
+    return converter.voltage_processed, converter.incremental_capacity
+
+
+def dqdv_frames(cell, split=False, tidy=True, label_direction=False, **kwargs):
+    """Returns dqdv data as pandas.DataFrame(s) for all cycles.
+
+    Args:
+        cell (CellpyCell-object).
+        split (bool): return one frame for charge and one for
+            discharge if True (defaults to False).
+        tidy (bool): returns the split frames in wide format (defaults
+            to True. Remark that this option is currently not available
+            for non-split frames).
+
+    Returns:
+        one or two ``pandas.DataFrame`` with the following columns:
+        cycle: cycle number (if split is set to True).
+        voltage: voltage
+        dq: the incremental capacity
+
+
+    Additional key-word arguments are sent to Converter:
+
+    Keyword Args:
+        cycle (int or list of ints (cycle numbers)): will process all (or up to max_cycle_number)
+            if not given or equal to None.
+        points_pr_split (int): only used when investigating data
+            using splits, defaults to 10.
+        max_points: None
+        voltage_resolution (float): used for interpolating voltage
+            data (e.g. 0.005)
+        capacity_resolution: used for interpolating capacity data
+        minimum_splits (int): defaults to 3.
+        interpolation_method: scipy interpolation method
+        increment_method (str): defaults to "diff"
+        pre_smoothing (bool): set to True for pre-smoothing (window)
+        smoothing (bool): set to True for smoothing during
+            differentiation (window)
+        post_smoothing (bool): set to True for post-smoothing
+            (gaussian)
+        normalize (bool): set to True for normalizing to capacity
+        normalizing_factor (float):
+        normalizing_roof  (float):
+        savgol_filter_window_divisor_default (int): used for window
+            smoothing, defaults to 50
+        savgol_filter_window_order: used for window smoothing
+        voltage_fwhm (float): used for setting the post-processing
+            gaussian sigma, defaults to 0.01
+        gaussian_order (int): defaults to 0
+        gaussian_mode (str): defaults to "reflect"
+        gaussian_cval (float): defaults to 0.0
+        gaussian_truncate (float): defaults to 4.0
+
+    Example:
+        >>> from cellpy.utils import ica
+        >>> charge_df, dcharge_df = ica.ica_frames(my_cell, split=True)
+        >>> charge_df.plot(x=("voltage", "v"))
+
+    """
+    # TODO: should add option for normalizing based on first cycle capacity
+    # this is e.g. done by first finding the first cycle capacity (nom_cap)
+    # (or use nominal capacity given as input) and then propagating this to
+    # Converter using the key-word arguments
+    #   normalize=True, normalization_factor=1.0, normalization_roof=nom_cap
+
+    if split:
+        return _dqdv_split_frames(cell, tidy=tidy, **kwargs)
+    else:
+        return _dqdv_combinded_frame(
+            cell, tidy=tidy, label_direction=label_direction, **kwargs
+        )
+
+
+def _constrained_dq_dv_using_dataframes(capacity, minimum_v, maximum_v, **kwargs):
+    converter = Converter(**kwargs)
+    converter.set_data(capacity)
+    converter.inspect_data()
+    converter.pre_process_data()
+    converter.increment_data()
+    converter.fixed_voltage_range = [minimum_v, maximum_v, 100]
+    converter.post_process_data()
+    return converter.voltage_processed, converter.incremental_capacity
+
+
+def _make_ica_charge_curves(cycles_dfs, cycle_numbers, minimum_v, maximum_v, **kwargs):
+    incremental_charge_list = []
+
+    for c, n in zip(cycles_dfs, cycle_numbers):
+        if c.empty:
+            logging.info(f"{n} is empty")
+            v = [np.nan]
+            dq = [np.nan]
+        else:
+            v, dq = _constrained_dq_dv_using_dataframes(
+                c, minimum_v, maximum_v, **kwargs
+            )
+        if not incremental_charge_list:
+            d = pd.DataFrame({"v": v})
+            d.name = "voltage"
+            incremental_charge_list.append(d)
+
+            d = pd.DataFrame({f"dq": dq})
+            d.name = n
+            incremental_charge_list.append(d)
+
+        else:
+            d = pd.DataFrame({f"dq": dq})
+            # d.name = f"{cycle}"
+            d.name = n
+            incremental_charge_list.append(d)
+
+    return incremental_charge_list
+
+
+def _dqdv_combinded_frame(cell, tidy=True, label_direction=False, **kwargs):
+    """Returns full cycle dqdv data for all cycles as one pd.DataFrame.
+
+    Args:
+        cell: CellpyCell-object
+
+    Returns:
+        pandas.DataFrame with the following columns:
+            cycle: cycle number
+            voltage: voltage
+            dq: the incremental capacity
+    """
+    cycle = kwargs.pop("cycle", None)
+    number_of_points = kwargs.pop("number_of_points", None)
+    cycles = cell.get_cap(
+        cycle=cycle,
+        method="forth-and-forth",
+        categorical_column=True,
+        label_cycle_number=True,
+        insert_nan=False,
+        number_of_points=number_of_points,
+    )
+    ica_df = dqdv_cycles(
+        cycles, not_merged=not tidy, label_direction=label_direction, **kwargs
+    )
+
+    if not tidy:
+        # dqdv_cycles returns a list of cycle numbers and a list of DataFrames
+        # if not_merged is set to True (or not False)
+        keys, ica_df = ica_df
+        ica_df = pd.concat(ica_df, axis=1, keys=keys)
+        return ica_df
+
+    assert isinstance(ica_df, pd.DataFrame)
+    return ica_df
+
+
+def _dqdv_split_frames(
+    cell,
+    tidy=False,
+    trim_taper_steps=None,
+    steps_to_skip=None,
+    steptable=None,
+    max_cycle_number=None,
+    **kwargs,
+):
+    """Returns dqdv data as pandas.DataFrames for all cycles.
+
+    Args:
+        cell (CellpyCell-object).
+        tidy (bool): return in wide format if False (default),
+            long (tidy) format if True.
+
+    Returns:
+        (charge_ica_frame, discharge_ica_frame) where the frames are
+        pandas.DataFrames where the first column is voltage ('v') and
+        the following columns are the incremental capcaity for each
+        cycle (multi-indexed, where cycle number is on the top level).
+
+    Example:
+        >>> from cellpy.utils import ica
+        >>> charge_ica_df, dcharge_ica_df = ica.ica_frames(my_cell)
+        >>> charge_ica_df.plot(x=("voltage", "v"))
+
+    """
+    cycle = kwargs.pop("cycle", None)
+    if cycle and not isinstance(cycle, (list, tuple)):
+        cycle = [cycle]
+
+    charge_dfs, cycles, minimum_v, maximum_v = collect_capacity_curves(
+        cell,
+        direction="charge",
+        trim_taper_steps=trim_taper_steps,
+        steps_to_skip=steps_to_skip,
+        steptable=steptable,
+        max_cycle_number=max_cycle_number,
+        cycle=cycle,
+    )
+    logging.debug(f"retrieved {len(charge_dfs)} charge cycles")
+    # charge_df = pd.concat(
+    # charge_dfs, axis=1, keys=[k.name for k in charge_dfs])
+
+    ica_charge_dfs = _make_ica_charge_curves(
+        charge_dfs, cycles, minimum_v, maximum_v, **kwargs
+    )
+
+    ica_charge_df = pd.concat(
+        ica_charge_dfs, axis=1, keys=[k.name for k in ica_charge_dfs]
+    )
+
+    dcharge_dfs, cycles, minimum_v, maximum_v = collect_capacity_curves(
+        cell,
+        direction="discharge",
+        trim_taper_steps=trim_taper_steps,
+        steps_to_skip=steps_to_skip,
+        steptable=steptable,
+        max_cycle_number=max_cycle_number,
+        cycle=cycle,
+    )
+    logging.debug(f"retrieved {len(dcharge_dfs)} discharge cycles")
+    ica_dcharge_dfs = _make_ica_charge_curves(
+        dcharge_dfs, cycles, minimum_v, maximum_v, **kwargs
+    )
+    ica_discharge_df = pd.concat(
+        ica_dcharge_dfs, axis=1, keys=[k.name for k in ica_dcharge_dfs]
+    )
+    ica_charge_df.columns.names = ["cycle", "value"]
+    ica_discharge_df.columns.names = ["cycle", "value"]
+
+    if tidy:
+        ica_charge_df = ica_charge_df.melt(
+            "voltage", var_name="cycle", value_name="dq", col_level=0
+        )
+        ica_discharge_df = ica_discharge_df.melt(
+            "voltage", var_name="cycle", value_name="dq", col_level=0
+        )
+
+    return ica_charge_df, ica_discharge_df
+
+
+def _check_class_ica():
+    print(40 * "=")
+    print("running check_class_ica")
+    print(40 * "-")
+
+    import matplotlib.pyplot as plt
+
+    cell = _get_a_cell_to_play_with()
+    cycle = 5
+    print("looking at cycle %i" % cycle)
+
+    # ---------- processing and plotting ----------------
+    fig, (ax1, ax2) = plt.subplots(2, 1)
+    capacity, voltage = cell.get_ccap(cycle, as_frame=False)
+    ax1.plot(capacity, voltage, "b.-", label="raw")
+    converter = Converter()
+    converter.set_data(capacity, voltage)
+    converter.inspect_data()
+    converter.pre_process_data()
+    ax1.plot(
+        converter.capacity_preprocessed,
+        converter.voltage_preprocessed,
+        "r.-",
+        alpha=0.3,
+        label="pre-processed",
+    )
+
+    converter.increment_data()
+    ax2.plot(
+        converter.voltage_processed,
+        converter.incremental_capacity,
+        "b.-",
+        label="incremented",
+    )
+
+    converter.fixed_voltage_range = False
+    converter.post_smoothing = True
+    converter.normalize = False
+    converter.post_process_data()
+    ax2.plot(
+        converter.voltage_processed,
+        converter.incremental_capacity,
+        "y-",
+        alpha=0.3,
+        lw=4.0,
+        label="smoothed",
+    )
+
+    converter.fixed_voltage_range = np.array((0.1, 1.2, 100))
+    converter.post_smoothing = False
+    converter.normalize = False
+    converter.post_process_data()
+    ax2.plot(
+        converter.voltage_processed,
+        converter.incremental_capacity,
+        "go",
+        alpha=0.7,
+        label="fixed voltage range",
+    )
+    ax1.legend(numpoints=1)
+    ax2.legend(numpoints=1)
+    ax1.set_ylabel("Voltage (V)")
+    ax1.set_xlabel("Capacity (mAh/g)")
+    ax2.set_xlabel("Voltage (V)")
+    ax2.set_ylabel("dQ/dV (mAh/g/V)")
+    plt.show()
+
+
+def _get_a_cell_to_play_with():
+    from cellpy import cellreader
+
+    # -------- defining overall path-names etc ----------
+    current_file_path = os.path.dirname(os.path.realpath(__file__))
+    print(current_file_path)
+    relative_test_data_dir = "../../testdata/hdf5"
+    test_data_dir = os.path.abspath(
+        os.path.join(current_file_path, relative_test_data_dir)
+    )
+    # test_data_dir_out = os.path.join(test_data_dir, "out")
+    test_cellpy_file = "20160805_test001_45_cc.h5"
+    test_cellpy_file_full = os.path.join(test_data_dir, test_cellpy_file)
+    # mass = 0.078609164
+
+    # ---------- loading test-data ----------------------
+    cell = cellreader.CellpyCell()
+    cell.load(test_cellpy_file_full)
+    list_of_cycles = cell.get_cycle_numbers()
+    number_of_cycles = len(list_of_cycles)
+    print("you have %i cycles" % number_of_cycles)
+    # cell.save(test_cellpy_file_full)
+    return cell
+
+
+def _check_if_works():
+    import pandas as pd
+    from cellpy import cellreader
+
+    cell = _get_a_cell_to_play_with()
+
+    a = dqdv_frames(cell)
+    print("Hei")
+
+
+if __name__ == "__main__":
+    _check_if_works()
```

### Comparing `cellpy-1.0.0b0/cellpy/utils/plotutils.py` & `cellpy-1.0.0b1/cellpy/utils/plotutils.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy/utils/processor.py` & `cellpy-1.0.0b1/cellpy/utils/processor.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/cellpy.egg-info/PKG-INFO` & `cellpy-1.0.0b1/cellpy.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: cellpy
-Version: 1.0.0b0
+Version: 1.0.0b1
 Summary: Extract and manipulate data from battery data testers.
 Home-page: https://github.com/jepegit/cellpy
 Author: Jan Petter Maehlen
 Author-email: jepe@ife.no
 License: MIT license
 Keywords: cellpy
 Classifier: Development Status :: 5 - Production/Stable
@@ -28,17 +28,14 @@
 cellpy - *a library for assisting in analysing batteries and cells*
 ===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
-.. image:: https://img.shields.io/travis/jepegit/cellpy.svg
-        :target: https://travis-ci.org/jepegit/cellpy
-
 .. image:: https://readthedocs.org/projects/cellpy/badge/?version=latest
         :target: https://cellpy.readthedocs.io/en/latest/?badge=latest
         :alt: Documentation Status
 
 .. image:: https://pepy.tech/badge/cellpy
         :target: https://pepy.tech/project/cellpy
```

### Comparing `cellpy-1.0.0b0/cellpy.egg-info/SOURCES.txt` & `cellpy-1.0.0b1/cellpy.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -147,14 +147,15 @@
 docs/examples_and_tutorials/loaders/04_Neware.rst
 docs/examples_and_tutorials/loaders/05_biologics.rst
 docs/examples_and_tutorials/loaders/06_custom.rst
 docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_001.png
 docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_002.png
 docs/examples_and_tutorials/utils/batch.rst
 docs/examples_and_tutorials/utils/collectors.rst
+docs/examples_and_tutorials/utils/custom_file_loaders.rst
 docs/examples_and_tutorials/utils/easyplot.rst
 docs/examples_and_tutorials/utils/ica.rst
 docs/examples_and_tutorials/utils/plotting.rst
 docs/examples_and_tutorials/utils/templates.rst
 docs/examples_and_tutorials/utils/tut_ocv_rlx.rst
 docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png
 docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png
```

### Comparing `cellpy-1.0.0b0/docs/Makefile` & `cellpy-1.0.0b1/docs/Makefile`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png` & `cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png` & `cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png` & `cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png` & `cellpy-1.0.0b1/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png` & `cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png` & `cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png` & `cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png` & `cellpy-1.0.0b1/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-22185705e237fa530df24e4af50cb25833165e25.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-22185705e237fa530df24e4af50cb25833165e25.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-246f2486cd940f2ea40a07f847c86c22b2607ca8.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-246f2486cd940f2ea40a07f847c86c22b2607ca8.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-42e9bc826d476a3d361a7f410e989ed34dc1aa85.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-42e9bc826d476a3d361a7f410e989ed34dc1aa85.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-619216a42370fd49669c083549129b8470c8fae1.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-619216a42370fd49669c083549129b8470c8fae1.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-6412a7c74952b4793798e9032f5bc4e7a1ab70c1.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-6412a7c74952b4793798e9032f5bc4e7a1ab70c1.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-6deb64a460668e8ef9bf0ca653314119adeeae66.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-6deb64a460668e8ef9bf0ca653314119adeeae66.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-83b62e03ef369ff0a30f027892dba95b91ea8b6c.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-83b62e03ef369ff0a30f027892dba95b91ea8b6c.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-8ec82d564b1a6ea5b95a36a4a213f7a78aaedc63.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-8ec82d564b1a6ea5b95a36a4a213f7a78aaedc63.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-ce8a9fe2ba01194aed847e0248d749db4093aca1.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-ce8a9fe2ba01194aed847e0248d749db4093aca1.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/graphviz-e94a5352318e02fcc5ef1f813e02a526c39af791.png` & `cellpy-1.0.0b1/docs/_build/_images/graphviz-e94a5352318e02fcc5ef1f813e02a526c39af791.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/templates_jupyterlab_001.png` & `cellpy-1.0.0b1/docs/_build/_images/templates_jupyterlab_001.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/tutorials_utils_plotting_fig1.png` & `cellpy-1.0.0b1/docs/_build/_images/tutorials_utils_plotting_fig1.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/_build/_images/tutorials_utils_plotting_fig2.png` & `cellpy-1.0.0b1/docs/_build/_images/tutorials_utils_plotting_fig2.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/adapted_readme.rst` & `cellpy-1.0.0b1/docs/adapted_readme.rst`

 * *Files 4% similar despite different names*

```diff
@@ -6,17 +6,14 @@
  - *a library for assisting in analysing batteries and cells*
 ===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
-.. image:: https://img.shields.io/travis/jepegit/cellpy.svg
-        :target: https://travis-ci.org/jepegit/cellpy
-
 .. image:: https://readthedocs.org/projects/cellpy/badge/?version=latest
         :target: https://cellpy.readthedocs.io/en/latest/?badge=latest
         :alt: Documentation Status
 
 .. image:: https://pepy.tech/badge/cellpy
         :target: https://pepy.tech/project/cellpy
```

### Comparing `cellpy-1.0.0b0/docs/conf.py` & `cellpy-1.0.0b1/docs/conf.py`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_data_structure.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_data_structure.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_folder_structure.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_folder_structure.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_packaging_pypi.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_packaging_pypi.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_cellpy_setup.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_cellpy_setup.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_conda_package.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_conda_package.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_docs.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_docs.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_loaders_and_instruments.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_loaders_and_instruments.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/developers_guide/dev_various.rst` & `cellpy-1.0.0b1/docs/developers_guide/dev_various.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/01_getting_started_tutorial.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/01_getting_started_tutorial.rst`

 * *Files 20% similar despite different names*

```diff
@@ -1,49 +1,25 @@
 .. _getting-started:
 
 The getting started with ``cellpy`` tutorial (opinionated version)
 ==================================================================
 
 This tutorial will help you getting started with ``cellpy`` and
-tries to give you a step-by-step recipe.
+tries to give you a step-by-step recipe. It starts with installation, and you
+should select the installation method that best suits your needs (or your level).
 
-How to install ``cellpy`` - the minimalistic explanation
---------------------------------------------------------
+.. _Cellpy_Setup_Standard:
 
-If you know what you are doing, and only need the most basic features
-of ``cellpy``, you should be able to get things up and running by
-issuing a simple
-
-.. code-block:: console
-
-    pip install cellpy
-
-It is recommended that you use a Python environment (or conda
-environment) and give it an easy-to-remember name *e.g.* ``cellpy``.
-
-To make sure your environment contains the correct packages and
-dependencies, you can create the environment based on the available
-`environment.yml <https://github.com/jepegit/cellpy/blob/master/environment.yml>`_
-file. For further information on dependencies and requirements for setting up
-``cellpy`` to read .res (Arbin) files, have a look at the *Install dependencies*
-part of the next section.
-
-For the installation of specific versions and pre-releases, see
-:ref:`check-cellpy`.
-
-How to install and run ``cellpy`` - the tea spoon explanation
--------------------------------------------------------------
+How to install and run ``cellpy`` - the tea spoon explanation for standard users
+--------------------------------------------------------------------------------
 
 If you are used to installing stuff from the command line (or shell),
-then things might very well run smoothly. However, a considerable
-percentage of potential users don’t feel exceedingly comfortable installing
-things by writing commands inside a small black window. Let’s face it; most of us
-belong to the *point-and-click* (or *double-click*) generation, not the
-*write-cryptic-commands* generation. So, hopefully without insulting the
-savvy, here is a “tea-spoon explanation”:
+then things might very well run smoothly. If you are not, then you
+might want to read through the guide for complete beginners first (see below
+:ref:`Cellpy_Setup_Windows`).
 
 1. Install a scientific stack of python 3.x
 ...........................................
 
 If the words “virtual environment” or “miniconda” do not ring any bells,
 you should install the Anaconda scientific Python distribution. Go to
 `www.anaconda.com <https://www.anaconda.com/>`__ and select the
@@ -119,23 +95,23 @@
 stuff and hope for the best. If it prints an older (lower) version
 number than you expect, there is a big chance that you have installed it
 earlier, and what you would like to do is to do an ``upgrade`` instead
 of an ``install``
 
 .. code-block:: console
 
-   pip install --upgrade cellpy
+   python -m pip install --upgrade cellpy
 
 If you want to install a pre-release (a version that is so bleeding edge
 that it ends with a alpha or beta release identification, *e.g.* ends
 with .b2). Then you will need to add the –pre modifier
 
 .. code-block:: console
 
-   pip install --pre cellpy
+   python -m pip install --pre cellpy
 
 To run a more complete check of your installation, there exist a
 ``cellpy`` sub-command than can be helpful
 
 .. code-block:: console
 
    cellpy info --check
@@ -148,55 +124,243 @@
 create an appropriate configuration file and folders for raw data,
 cellpy-files, logs, databases and output data (and inform ``cellpy`` about it).
 
 To do this, run the setup command:
 
 .. code-block:: console
 
-       cellpy setup
+    cellpy setup
 
 To run the setup in interactive mode, use -i:
 
 .. code-block:: console
 
-       cellpy setup -i
+    cellpy setup -i
 
-This creates the cellpy configuration file ``_cellpy_prms_USERNAME.conf``
+This creates the cellpy configuration file ``.cellpy_prms_USERNAME.conf``
 in your home directory (USERNAME = your user name) and creates the standard
 cellpy_data folders (if they do not exist).
 The ``-i`` option makes sure that the setup is done interactively:
 The program will ask you about where specific folders are, *e.g.* where
 you would like to put your outputs and where your cell data files are
 located. If the folders do not exist, ``cellpy`` will try to create them.
 
 If you want to specify a root folder different from the default (your HOME
 folder), you can use the ``-d`` option *e.g.*
 ``cellpy setup -i -d /Users/kingkong/cellpydir``
 
 .. hint:: You can always edit your configurations directly in the cellpy configuration
-   file ``_cellpy_prms_USER.conf``. This file should be located inside your
+   file ``.cellpy_prms_USER.conf``. This file should be located inside your
    home directory, /. in posix and c:\users\USERNAME in not-too-old windows.
 
 
 6. Create a notebook and run ``cellpy``
 .......................................
 
 Inside your Anaconda Prompt window, write:
 
 .. code-block:: console
 
-       jupyter notebook  # or jupyter lab
+    jupyter notebook  # or jupyter lab
 
 Your browser should then open and you are ready to write your first cellpy script.
 
 There are many good tutorials on how to work with jupyter.
 This one by Real Python is good for beginners:
 `Jupyter Notebook: An Introduction <https://realpython.com/jupyter-notebook-introduction/>`_
 
 
+.. _Cellpy_Setup_Windows:
+
+Setting up ``cellpy`` on Windows for complete beginners
+-------------------------------------------------------
+
+This guide provides step-by-step instructions for installing Cellpy on a Windows system,
+especially tailored for beginners.
+
+
+1. Installing Python
+....................
+
+* First, download Python from the `official website <https://www.python.org/downloads/>`_. Choose the latest version for Windows.
+
+* Run the downloaded installer. On the first screen of the setup, ensure to check the box
+   saying "Add Python to PATH" before clicking "Install Now".
+
+* After installation, you can verify it by opening the Command Prompt (see below) and typing::
+
+    python --version
+
+  This command should return the version of Python that you installed.
+
+2. Opening Command Prompt
+.........................
+
+* Press the Windows key, usually located at the bottom row of your keyboard, between the Ctrl and Alt keys.
+
+* Type "Command Prompt" into the search bar that appears at the bottom of the screen when you press the Windows key.
+
+* Click on the "Command Prompt" application to open it.
+
+3. Creating a Virtual Environment
+.................................
+
+A virtual environment is a tool that helps to keep dependencies required by different projects separate by creating isolated
+Python environments for them. Here's how to create one:
+
+* Open Command Prompt.
+
+* Navigate to the directory where you want to create your virtual environment using the `cd` command. For example::
+
+    cd C:\Users\YourUsername\Documents
+
+* Type the following command and press enter to create a new virtual environment (replace `envname` with the name you want to give to your virtual environment)::
+
+    python -m venv envname
+
+* To activate the virtual environment, type the following command and press enter::
+
+    envname\Scripts\activate
+
+  You'll know it worked if you see `(envname)` before the prompt in your Command Prompt window.
+
+4. Installing Jupyter Notebook and matplotlib
+.............................................
+
+Jupyter Notebook is an open-source web application that allows you to create documents containing live code, equations, visualizations,
+and text. It's very useful, especially for beginners. To install Jupyter Notebook:
+
+* Make sure your virtual environment is activated.
+
+* Type the following command and press enter::
+
+    python -m pip install jupyter matplotlib
+
+5. Installing ``cellpy``
+........................
+
+Next, you need to install ``cellpy``. You can install it via pip (Python's package manager).
+To install ``cellpy``:
+
+* Make sure your virtual environment is activated.
+
+* Type the following command and press enter::
+
+    python -m pip install cellpy
+
+6. Launching Jupyter Notebook
+.............................
+
+* Make sure your virtual environment is activated.
+
+* Type the following command and press enter::
+
+    jupyter notebook
+
+* This will open a new tab in your web browser with the Jupyter's interface. From there,
+  create a new Python notebook by clicking on "New" > "Python 3".
+
+7. Trying out ``cellpy``
+........................
+
+Here's a simple example of how to use Cellpy in a Jupyter notebook:
+
+* In the first cell of the notebook, import Cellpy by typing::
+
+        import cellpy
+
+  Press `Shift + Enter` to run the cell.
+
+* In the new cell, load your data file (replace "datafile.res" and "/path/to/your/data" with your actual filename and path)::
+
+        filepath = "/path/to/your/data/datafile.res"
+
+        c = cellpy.get(filepath)  # create a new cellpy object
+
+  Press `Shift + Enter` to run the cell and load the data.
+
+* To see a summary of the loaded data, create a new cell and type::
+
+        print(c.data.summary.head())
+
+  Press `Shift + Enter` to run the cell and print the summary.
+
+Congratulations! You've successfully set up Cellpy in a virtual environment on your Windows PC and loaded your first data file.
+For more information and examples, check out the `official Cellpy documentation <https://cellpy.readthedocs.io/en/latest/>`_.
+
+Cellpy includes convenient functions for accessing the data. Here's a basic example of how to plot voltage vs. capacity.
+
+* In a new cell in your Jupyter notebook, first, import matplotlib, which is a Python plotting library::
+
+        import matplotlib.pyplot as plt
+
+  Press `Shift + Enter` to run the cell.
+
+* Then, iterate through all cycles numbers, extract the capacity curves and plot::
+
+        for cycle in c.get_cycle_numbers():
+            d = c.get_cap(cycle)
+            plt.plot(d["capacity"], d["voltage"])
+        plt.show()
+
+  Press `Shift + Enter` to run the cell.
+
+  This will produce a plot for each cycle in the loaded data.
+
+Once you've loaded your data, you can save it to a hdf5 file for later use::
+
+        c.save("saved_data.h5")
+
+This saves the loaded data to a file named 'saved_data.h5'.
+
+Now, lets try to create some dQ/dV plots. dQ/dV is a plot of the change in capacity (Q) with respect to
+the change in voltage (V). It's often used in battery analysis
+to observe specific electrochemical reactions. Here's how to create one:
+
+* In a new cell in your Jupyter notebook, first, if you have not imported matplotlib::
+
+        import matplotlib.pyplot as plt
+
+  Press `Shift + Enter` to run the cell.
+
+* Then, calculate dQ/dV using Cellpy's ica utility::
+
+        import cellpy.utils.ica as ica
+
+        dqdv = ica.dqdv_frames(c, cycle=[1, 10, 100], voltage_resolution=0.01)
+
+  Press `Shift + Enter` to run the cell.
+
+* Now, you can create a plot of dQ/dV. In a new cell, type::
+
+      plt.figure(figsize=(10, 8))
+      plt.plot(dqdv["v"], dqdv["dq"], label="dQ/dV")
+      plt.xlabel("Voltage (V)")
+      plt.ylabel("dQ/dV (Ah/V)")
+      plt.legend()
+      plt.grid(True)
+      plt.show()
+
+  Press `Shift + Enter` to run the cell.
+
+In the code above, `plt.figure` is used to create a new figure, `plt.plot` plots the data, `plt.xlabel` and `plt.ylabel` set
+the labels for the x and y axes, `plt.legend` adds a legend to the plot, `plt.grid` adds a grid to the plot, and `plt.show` displays the plot.
+
+With this, you should be able to see the dQ/dV plot in your notebook.
+
+Remember that the process of creating a dQ/dV plot can be quite memory-intensive, especially for large datasets,
+so it may take a while for the plot to appear.
+
+For more information and examples, check out the `official Cellpy documentation <https://cellpy.readthedocs.io/en/latest/>`_ and
+the `matplotlib documentation <https://matplotlib.org/stable/contents.html>`_.
+
+This recipe can only take you a certain distance. If you want to become more efficient with Python and Cellpy, you
+might want to try to install it using the method described in the chapter "Installing and setting up cellpy" in the
+`official Cellpy documentation <https://cellpy.readthedocs.io/en/latest/>`_.
+
 More about installing and setting up ``cellpy``
 -----------------------------------------------
 
 Fixing dependencies
 ...................
 
 To make sure your environment contains the correct packages and dependencies
@@ -227,18 +391,14 @@
 
 In general, you need the typical scientific python pack, including
 
 - ``numpy``
 - ``scipy``
 - ``pandas``
 
-It is recommended that you at least install ``scipy`` before you install
-``cellpy`` (the main benefit being that you can use ``conda`` so that you
-do not have to hassle with missing C-compilers if you are on an Windows
-machine).
 Additional dependencies are:
 
 - ``pytables`` is needed for working with the hdf5 files (the cellpy-files):
 
 .. code-block:: console
 
     conda install -c conda-forge pytables
@@ -248,36 +408,24 @@
 .. code-block:: console
 
     conda install -c conda-forge lmfit
 
 - ``holoviz`` and ``plotly``: plotting library used in several of our example notebooks.
 
 - ``jupyter``: used for tutorial notebooks and in general very useful tool
-   for working with and sharing your ``cellpy`` results.
+  for working with and sharing your ``cellpy`` results.
 
 For more details, I recommend that you look at the documentation of these
 packages (google it) and install them. You can most
 likely use the same method as for pytables *etc*.
 
 Additional requirements for .res files
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.res files from Arbin testers are  actually in a Microsoft Access format.
-For loading .res-files (possible also for other *‘to-be-implemented’* file
-formats) you will thus also need a *Python ODBC bridge* (in addition to the
-requirements set in the ``setup.py`` file).
-I recommend `pyodbc <https://github.com/mkleehammer/pyodbc/wiki>`__ that
-can be installed from conda forge or using pip.
-
-.. code-block:: console
-
-    conda install -c conda-forge pyodbc
-
-Additionally, you need a driver or similar to help your ODBC bridge
-accessing it.
+Note! .res files from Arbin testers are actually in a Microsoft Access format.
 
 **For Windows users:** if you do not have one of the
 most recent Office versions, you might not be allowed to install a driver
 of different bit than your office version is using (the installers can be found
 `here <https://www.microsoft.com/en-US/download/details.aspx?id=13255>`__).
 Also remark that the driver needs to be of the same bit as your Python
 (so, if you are using 32 bit Python, you will need the 32 bit driver).
```

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/02_read_cell_data.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/02_read_cell_data.rst`

 * *Files 2% similar despite different names*

```diff
@@ -102,8 +102,9 @@
     # export also the raw data by setting raw=True
 
 
 .. note::
     ``CellpyCell`` objects store the data (including the summary and step-tables)
     in ``pandas DataFrames``. This means that you can easily export the data to
     other formats, such as Excel, by using the ``to_excel`` method of the
-    DataFrame object.
+    DataFrame object. In addition, ``CellpyCell`` objects have a method called
+    ``to_excel`` that exports the data to an Excel file.
```

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/03_more_about_get.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/03_more_about_get.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/04_other_interactions.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/04_other_interactions.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/05_configuring.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/05_configuring.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/06_pandas.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/06_pandas.rst`

 * *Files 9% similar despite different names*

```diff
@@ -5,27 +5,29 @@
     on that would be highly appreciated.
 
 The ``CellpyCell`` object stores the data in several ``pandas.DataFrame`` objects.
 The easies way to get to the DataFrames is by the following procedure:
 
 .. code-block:: python
 
-    # Assumed name of the CellpyCell object: cellpy_data
+    # Assumed name of the CellpyCell object: c
 
     # get the 'test':
-    c = cell_data.data
-    # cellpy_test is now a cellpy Data object (cellpy.readers.cellreader.Data)
+    data = c.data
+    # data is now a cellpy Data object (cellpy.readers.cellreader.Data)
 
     # pandas.DataFrame with data vs cycle number (coulombic efficiency, charge-capacity etc.):
-    summary_data = c.summary
+    summary_data = data.summary
+    # you could also get the summary data by:
+    summary_data = c.data.summary
 
     # pandas.DataFrame with the raw data:
-    raw_data = c.raw
+    raw_data = data.raw
 
     # pandas.DataFrame with statistics on each step and info about step type:
-    step_info = c.steps
+    step_info = data.steps
 
 
 You can then manipulate your data with the standard ``pandas.DataFrame`` methods
 (and ``pandas`` methods in general).
 
 Happy pandas-ing!
```

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/07_data_mining.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/07_data_mining.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/basic_interactions/08_the_cellpy_cmd.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/basic_interactions/08_the_cellpy_cmd.rst`

 * *Files 2% similar despite different names*

```diff
@@ -7,25 +7,26 @@
 
 .. code-block:: shell
 
     $ cellpy
     Usage: cellpy [OPTIONS] COMMAND [ARGS]...
 
     Options:
-     --help  Show this message and exit.
+      --help  Show this message and exit.
 
     Commands:
-      edit   Edit your cellpy config file.
+      edit   Edit your cellpy config or database files.
       info   This will give you some valuable information about your cellpy.
       new    Set up a batch experiment (might need git installed).
       pull   Download examples or tests from the big internet (needs git).
-      run    Run a cellpy process (batch-job, edit db, ...).
+      run    Run a cellpy process (e.g.
       serve  Start a Jupyter server.
       setup  This will help you to set up cellpy.
 
+
 The cli is still under development (cli stands for command-line-interface, by the way).
 Both the ``cellpy new`` and the ``cellpy serve`` command worked the last time I tried them.
 But it might not work on your computer. If you run into problems, let us know.
 
 
 Information
 -----------
@@ -36,15 +37,15 @@
 
 .. code-block:: shell
 
     $ cellpy info --version
     [cellpy] version: 0.4.1
 
     $ cellpy info --configloc
-    [cellpy] ->C:\Users\jepe\_cellpy_prms_jepe.conf
+    [cellpy] -> C:\Users\jepe\.cellpy_prms_jepe.conf
 
 
 Setting up ``cellpy`` from the cli
 ----------------------------------
 
 To get the most out of ``cellpy`` it is to best to set it up properly. To help
 with this, you can use the ``setup`` command. If you include the ``--interactive`` switch,
@@ -52,15 +53,15 @@
 ``cellpy`` uses (it still will work without them, though).
 
 
 .. code-block:: shell
 
     $ cellpy setup --interactive
 
-The command will create a starting ``cellpy`` configuration file (`_cellpy_prms_yourname.conf`)
+The command will create a starting ``cellpy`` configuration file (`,cellpy_prms_USERNAME.conf`)
 or update it if it exists, and create the following directory structure:
 
 .. code-block:: shell
 
     batchfiles/
     cellpyfiles/
     db/
@@ -173,15 +174,14 @@
 .. code-block:: shell
 
     $ cellpy run --help
 
     Usage: cellpy run [OPTIONS] [NAME]
 
       Run a cellpy process (batch-job, edit db, ...).
-
       You can use this to launch specific applications.
 
       Examples:
 
           edit your cellpy database
 
              cellpy run db
```

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_001.png` & `cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_001.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_002.png` & `cellpy-1.0.0b1/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_002.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/tips_and_tricks.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/tips_and_tricks.rst`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 ====================
 Some tips and tricks
 ====================
 
 Overriding settings
--------------------
+===================
 If you would like to override some of the standard settings, then
 the route of less friction is to import the prms class and set
 new values directly.
 
 Below is a simple example where we set an option so that we can load
 arbin-files (.res) using mdbtools (i.e. spawning a subprocess that
 converts the .res-file to .csv-files, then loading and deleting the
```

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/utils/batch.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/utils/batch.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png` & `cellpy-1.0.0b1/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png` & `cellpy-1.0.0b1/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/utils/plotting.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/utils/plotting.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/examples_and_tutorials/utils/tut_ocv_rlx.rst` & `cellpy-1.0.0b1/docs/examples_and_tutorials/utils/tut_ocv_rlx.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/figures/cellpy-icon-bw.png` & `cellpy-1.0.0b1/docs/figures/cellpy-icon-bw.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/figures/cellpy-logo-v1.png` & `cellpy-1.0.0b1/docs/figures/cellpy-logo-v1.png`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/index.rst` & `cellpy-1.0.0b1/docs/index.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/main_description/formats.rst` & `cellpy-1.0.0b1/docs/main_description/formats.rst`

 * *Files 23% similar despite different names*

```diff
@@ -38,28 +38,127 @@
 .. code-block:: python
 
     CellpyCell.tester = "arbin_res"  # TODO - update this
     CellpyCell.auto_dirs = True
     print(CellpyCell.cellpy_datadir)
 
 
-The data for the experiment(s)/runs(s) are stored in the class attribute
-``CellpyCell.cells``
-This attribute is just a list of runs (each run is a
-``cellpy.cellreader.Data`` instance).
-This implies that you can store many runs in one ``CellpyCell`` instance.
-Sometimes this can be necessary, but it is recommended to only store one
-run in one instance. Most of the functions (the class methods) automatically
-selects the 0-th item in ``CellpyCell.cells`` if the ``test_number`` is not
-explicitly given.
 
-You may already have figured it out: in cellpy, data for a given cell
-is usually named a run. And each run is a ``cellpy.cellreader.Data`` instance.
+CellpyCell - methods
+--------------------
+
+The ``CellpyCell`` object contains lots of methods for manipulating, extracting
+and summarising the data from the run(s). Two methods are typically automatically run when
+you create your ``CellpyCell`` object when running ``cellpy.get(filename)``:
+
+    - ``make_step_table``: creates a statistical summary of all the steps in the run(s) and categorizes
+      the step type from that. It is also possible to give the step types directly (step_specifications).
+
+    - ``make_summary``: create a summary based on cycle number.
+
+Other methods worth mentioning are (based on what I typically use):
+
+    - ``load``: load a cellpy file.
+
+    - ``load_raw``: load raw data file(s) (merges automatically if several filenames are given as a list).
+
+    - ``get_cap``: get the capacity-voltage graph from one or more cycles in three different formats as well
+      as optionally interpolated, normalized and/or scaled.
+
+    - ``get_cycle_numbers``: get the cycle numbers for your run.
+
+    - ``get_ocv``: get the rest steps after each charge and discharge step.
+
+
+Take a look at API section (Module index, ``cellpy.readers.cellreader.CellpyCell``) for more info.
+
+Data
+----
+
+.. graphviz::
+
+   digraph {
+    "CellpyCell" -> "Data";
+        "Data" -> "cell metadata (cell)";
+        "Data" -> "cell metadata (test)";
+        "Data" -> "methods";
+        "Data" -> "raw";
+        "Data" -> "steps";
+        "Data" -> "summary";
+   }
+
+The data for the experiment are stored in the class attribute
+``CellpyCell.data`` (a ``cellpy.cellreader.Data`` instance).
+
+The instance contain general information about
+the run-settings (such as mass etc.).
+The measurement data, information, and summary is stored
+in three ``pandas.DataFrames``:
+
+    - ``raw``: raw data from the run.
+    - ``steps``: stats from each step (and step type), created using the
+      ``CellpyCell.make_step_table`` method.
+    - ``summary``: summary data vs. cycle number (e.g. coulombic coulombic efficiency), created using
+      the ``CellpyCell.make_summary`` method.
+
+The headers (columns) for the different DataFrames were given earlier in this chapter.
+As mentioned above, the ``Data`` object also contains metadata for the run.
+
+metadata
+........
+
+.. code-block:: python
+
+    cell_no = None
+    mass = prms.Materials.default_mass  # active material (in mg)
+    tot_mass = prms.Materials.default_mass  # total material (in mg)
+    no_cycles = 0.0
+    charge_steps = None
+    discharge_steps = None
+    ir_steps = None
+    ocv_steps = None
+    nom_cap = prms.DataSet.nom_cap  # mAh/g (for finding c-rates)
+    mass_given = False
+    material = prms.Materials.default_material
+    merged = False
+    file_errors = None  # not in use at the moment
+    loaded_from = None  # loaded from (can be list if merged)
+    channel_index = None
+    channel_number = None
+    creator = None
+    item_ID = None
+    schedule_file_name = None
+    start_datetime = None
+    test_ID = None
+    name = None
+    cycle_mode = prms.Reader.cycle_mode
+    active_electrode_area = None  # [cm2]
+    active_electrode_thickness = None  # [micron]
+    electrolyte_type = None  #
+    electrolyte_volume = None  # [micro-liter]
+    active_electrode_type = None
+    counter_electrode_type = None
+    reference_electrode_type = None
+    experiment_type = None
+    cell_type = None
+    separator_type = None
+    active_electrode_current_collector = None
+    reference_electrode_current_collector = None
+    comment = None
+
+
+The ``Data`` object can also take custom metadata if provided as keyword arguments (for developers).
+
+FileID
+......
+
+The ``FileID`` object contains information about the raw file(s) and is used when comparing the cellpy-file
+with the raw file(s) (for example to check if it has been updated compared to the cellpy-file).
+Notice that ``FileID`` will contain a list of file identification parameters if the run is from several raw files.
 
-Here is a list of other important class attributes in ``CellpyCell``:
 
 Column headings
 ...............
 ``cellpy`` uses ``pandas.DataFrame`` objects internally. The column headers
 of the dataframes are defined in corresponding dataclass objects that can be
 accessed using both dot-notation and through normal dictionary look-up.
 
@@ -74,28 +173,29 @@
 
 column headings - raw data (or "normal" data)
 '''''''''''''''''''''''''''''''''''''''''''''
 
 .. code-block:: python
 
     @dataclass
-    class HeadersNormal(BaseSettings):
+    class HeadersNormal(BaseHeaders):
         aci_phase_angle_txt: str = "aci_phase_angle"
         ref_aci_phase_angle_txt: str = "ref_aci_phase_angle"
         ac_impedance_txt: str = "ac_impedance"
         ref_ac_impedance_txt: str = "ref_ac_impedance"
         charge_capacity_txt: str = "charge_capacity"
         charge_energy_txt: str = "charge_energy"
         current_txt: str = "current"
         cycle_index_txt: str = "cycle_index"
         data_point_txt: str = "data_point"
         datetime_txt: str = "date_time"
         discharge_capacity_txt: str = "discharge_capacity"
         discharge_energy_txt: str = "discharge_energy"
         internal_resistance_txt: str = "internal_resistance"
+        power_txt: str = "power"
         is_fc_data_txt: str = "is_fc_data"
         step_index_txt: str = "step_index"
         sub_step_index_txt: str = "sub_step_index"
         step_time_txt: str = "step_time"
         sub_step_time_txt: str = "sub_step_time"
         test_id_txt: str = "test_id"
         test_time_txt: str = "test_time"
@@ -110,70 +210,82 @@
 
 column headings - summary data
 ''''''''''''''''''''''''''''''
 
 .. code-block:: python
 
     @dataclass
-    class HeadersSummary(BaseSettings):
+    class HeadersSummary(BaseHeaders):
+        """In addition to the headers defined here, the summary might also contain
+        specific headers (ending in _gravimetric or _areal).
+        """
+
+        postfixes = ["gravimetric", "areal"]
+
         cycle_index: str = "cycle_index"
         data_point: str = "data_point"
         test_time: str = "test_time"
         datetime: str = "date_time"
         discharge_capacity_raw: str = "discharge_capacity"
         charge_capacity_raw: str = "charge_capacity"
         test_name: str = "test_name"
         data_flag: str = "data_flag"
         channel_id: str = "channel_id"
-        discharge_capacity: str = "discharge_capacity_u_mAh_g"
-        charge_capacity: str = "charge_capacity_u_mAh_g"
-        cumulated_charge_capacity: str = "cumulated_charge_capacity_u_mAh_g"
-        cumulated_discharge_capacity: str = "cumulated_discharge_capacity_u_mAh_g"
-        coulombic_efficiency: str = "coulombic_efficiency_u_percentage"
-        cumulated_coulombic_efficiency: str = "cumulated_coulombic_efficiency_u_percentage"
-        coulombic_difference: str = "coulombic_difference_u_mAh_g"
-        cumulated_coulombic_difference: str = "cumulated_coulombic_difference_u_mAh_g"
-        discharge_capacity_loss: str = "discharge_capacity_loss_u_mAh_g"
-        charge_capacity_loss: str = "charge_capacity_loss_u_mAh_g"
-        cumulated_discharge_capacity_loss: str = "cumulated_discharge_capacity_loss_u_mAh_g"
-        cumulated_charge_capacity_loss: str = "cumulated_charge_capacity_loss_u_mAh_g"
-        ir_discharge: str = "ir_discharge_u_Ohms"
-        ir_charge: str = "ir_charge_u_Ohms"
-        ocv_first_min: str = "ocv_first_min_u_V"
-        ocv_second_min: str = "ocv_second_min_u_V"
-        ocv_first_max: str = "ocv_first_max_u_V"
-        ocv_second_max: str = "ocv_second_max_u_V"
-        end_voltage_discharge: str = "end_voltage_discharge_u_V"
-        end_voltage_charge: str = "end_voltage_charge_u_V"
-        cumulated_ric_disconnect: str = "cumulated_ric_disconnect_u_none"
-        cumulated_ric_sei: str = "cumulated_ric_sei_u_none"
-        cumulated_ric: str = "cumulated_ric_u_none"
-        normalized_cycle_index: str = "normalized_cycle_index"
+
+        coulombic_efficiency: str = "coulombic_efficiency"
+        cumulated_coulombic_efficiency: str = "cumulated_coulombic_efficiency"
+
+        discharge_capacity: str = "discharge_capacity"
+        charge_capacity: str = "charge_capacity"
+        cumulated_charge_capacity: str = "cumulated_charge_capacity"
+        cumulated_discharge_capacity: str = "cumulated_discharge_capacity"
+
+        coulombic_difference: str = "coulombic_difference"
+        cumulated_coulombic_difference: str = "cumulated_coulombic_difference"
+        discharge_capacity_loss: str = "discharge_capacity_loss"
+        charge_capacity_loss: str = "charge_capacity_loss"
+        cumulated_discharge_capacity_loss: str = "cumulated_discharge_capacity_loss"
+        cumulated_charge_capacity_loss: str = "cumulated_charge_capacity_loss"
+
         normalized_charge_capacity: str = "normalized_charge_capacity"
         normalized_discharge_capacity: str = "normalized_discharge_capacity"
-        low_level: str = "low_level_u_percentage"
-        high_level: str = "high_level_u_percentage"
-        shifted_charge_capacity: str = "shifted_charge_capacity_u_mAh_g"
-        shifted_discharge_capacity: str = "shifted_discharge_capacity_u_mAh_g"
-        temperature_last: str = "temperature_last_u_C"
-        temperature_mean: str = "temperature_mean_u_C"
-        areal_charge_capacity: str = "areal_charge_capacity_u_mAh_cm2"
-        areal_discharge_capacity: str = "areal_discharge_capacity_u_mAh_cm2"
+
+        shifted_charge_capacity: str = "shifted_charge_capacity"
+        shifted_discharge_capacity: str = "shifted_discharge_capacity"
+
+        ir_discharge: str = "ir_discharge"
+        ir_charge: str = "ir_charge"
+        ocv_first_min: str = "ocv_first_min"
+        ocv_second_min: str = "ocv_second_min"
+        ocv_first_max: str = "ocv_first_max"
+        ocv_second_max: str = "ocv_second_max"
+        end_voltage_discharge: str = "end_voltage_discharge"
+        end_voltage_charge: str = "end_voltage_charge"
+        cumulated_ric_disconnect: str = "cumulated_ric_disconnect"
+        cumulated_ric_sei: str = "cumulated_ric_sei"
+        cumulated_ric: str = "cumulated_ric"
+        normalized_cycle_index: str = "normalized_cycle_index"
+        low_level: str = "low_level"
+        high_level: str = "high_level"
+
+        temperature_last: str = "temperature_last"
+        temperature_mean: str = "temperature_mean"
+
         charge_c_rate: str = "charge_c_rate"
         discharge_c_rate: str = "discharge_c_rate"
         pre_aux: str = "aux_"
 
 
 column headings - step table
 ............................
 
 .. code-block:: python
 
     @dataclass
-    class HeadersStepTable(BaseSettings):
+    class HeadersStepTable(BaseHeaders):
         test: str = "test"
         ustep: str = "ustep"
         cycle: str = "cycle"
         step: str = "step"
         test_time: str = "test_time"
         step_time: str = "step_time"
         sub_step: str = "sub_step"
@@ -191,30 +303,32 @@
 
 column headings - journal pages
 ...............................
 
 .. code-block:: python
 
     @dataclass
-    class HeadersJournal(BaseSettings):
+    class HeadersJournal(BaseHeaders):
         filename: str = "filename"
         mass: str = "mass"
         total_mass: str = "total_mass"
         loading: str = "loading"
+        area: str = "area"
         nom_cap: str = "nom_cap"
         experiment: str = "experiment"
         fixed: str = "fixed"
         label: str = "label"
         cell_type: str = "cell_type"
         instrument: str = "instrument"
         raw_file_names: str = "raw_file_names"
         cellpy_file_name: str = "cellpy_file_name"
         group: str = "group"
         sub_group: str = "sub_group"
         comment: str = "comment"
+        argument: str = "argument"
 
 
     CellpyCell.keys_journal_session = ["starred", "bad_cells", "bad_cycles", "notes"]
 
 step types
 ..........
 
@@ -226,236 +340,16 @@
     list_of_step_types = ['charge', 'discharge',
                           'cv_charge', 'cv_discharge',
                           'charge_cv', 'discharge_cv',
                           'ocvrlx_up', 'ocvrlx_down', 'ir',
                           'rest', 'not_known']
 
 
-For each type of testers that are supported by ``cellpy``,
-a set of column headings and
-other different settings/attributes must be provided. These definitions stored in the
-``cellpy.parameters.internal_settings`` module and are also injected into
-the CellpyCell class upon initiation.
-
-Supported testers are:
-
-* arbin (.res type files)
-
-Testers that are partly supported (but not tested very well) are:
-
-* pec (txt files)
-* arbin (ms sql-server and .csv and .xlsx exports)
-* maccor (txt files)
-
-Testers that is planned supported:
-
-* biologic
-* maccor
-
-In addition, ``cellpy`` can load custom csv-ish files by providing a file description (using the
-``ìnstruments.Custom`` object).
-
-
 Tester dependent attributes
 ---------------------------
 
-arbin .res
-..........
-
-Three tables are read from the .res file:
-
-* normal table: contains measurement data.
-* global table: contains overall parametres for the test.
-* stats table: contains statistics (for each cycle).
-
-
-
-table names
-'''''''''''
-
-.. code-block:: python
-
-    tablename_normal = "Channel_Normal_Table"
-    tablename_global = "Global_Table"
-    tablename_statistic = "Channel_Statistic_Table"
-
-column headings - global table
-''''''''''''''''''''''''''''''
-
-.. code-block:: python
-
-    applications_path_txt = 'Applications_Path'
-    channel_index_txt = 'Channel_Index'
-    channel_nuer_txt = 'Channel_Number'
-    channel_type_txt = 'Channel_Type'
-    comments_txt = 'Comments'
-    creator_txt = 'Creator'
-    daq_index_txt = 'DAQ_Index'
-    item_id_txt = 'Item_ID'
-    log_aux_data_flag_txt = 'Log_Aux_Data_Flag'
-    log_chanstat_data_flag_txt = 'Log_ChanStat_Data_Flag'
-    log_event_data_flag_txt = 'Log_Event_Data_Flag'
-    log_smart_battery_data_flag_txt = 'Log_Smart_Battery_Data_Flag'
-    mapped_aux_conc_cnumber_txt = 'Mapped_Aux_Conc_CNumber'
-    mapped_aux_di_cnumber_txt = 'Mapped_Aux_DI_CNumber'
-    mapped_aux_do_cnumber_txt = 'Mapped_Aux_DO_CNumber'
-    mapped_aux_flow_rate_cnumber_txt = 'Mapped_Aux_Flow_Rate_CNumber'
-    mapped_aux_ph_number_txt = 'Mapped_Aux_PH_Number'
-    mapped_aux_pressure_number_txt = 'Mapped_Aux_Pressure_Number'
-    mapped_aux_temperature_number_txt = 'Mapped_Aux_Temperature_Number'
-    mapped_aux_voltage_number_txt = 'Mapped_Aux_Voltage_Number'
-    schedule_file_name_txt = 'Schedule_File_Name'
-    start_datetime_txt = 'Start_DateTime'
-    test_id_txt = 'Test_ID'
-    test_name_txt = 'Test_Name'
-
-column headings - normal table
-''''''''''''''''''''''''''''''
-
-.. code-block:: python
-
-    aci_phase_angle_txt = 'ACI_Phase_Angle'
-    ac_impedance_txt = 'AC_Impedance'
-    charge_capacity_txt = 'Charge_Capacity'
-    charge_energy_txt = 'Charge_Energy'
-    current_txt = 'Current'
-    cycle_index_txt = 'Cycle_Index'
-    data_point_txt = 'Data_Point'
-    datetime_txt = 'DateTime'
-    discharge_capacity_txt = 'Discharge_Capacity'
-    discharge_energy_txt = 'Discharge_Energy'
-    internal_resistance_txt = 'Internal_Resistance'
-    is_fc_data_txt = 'Is_FC_Data'
-    step_index_txt = 'Step_Index'
-    step_time_txt = 'Step_Time'
-    test_id_txt = 'Test_ID'
-    test_time_txt = 'Test_Time'
-    voltage_txt = 'Voltage'
-    dv_dt_txt = 'dV/dt'
-
-arbin MS SQL SERVER
-...................
-
-TODO...
-
-
-PEC .csv
-........
-
-TODO...
-
-
-Maccor .txt
-...........
-
-TODO...
-
-
-CellpyCell - methods
---------------------
-
-The ``CellpyCell`` object contains lots of methods for manipulating, extracting
-and summarising the data from the run(s). Two methods are typically automatically run when
-you create your ``CellpyCell`` object when running ``cellpy.get(filename)``:
-
-    - ``make_step_table``: creates a statistical summary of all the steps in the run(s) and categorizes
-      the step type from that. It is also possible to give the step types directly (step_specifications).
-
-    - ``make_summary``: create a summary based on cycle number.
-
-Other methods worth mentioning are (based on what I typically use):
-
-    - ``load``: load a cellpy file.
-
-    - ``load_raw``: load raw data file(s) (merges automatically if several filenames are given as a list).
-
-    - ``get_cap``: get the capacity-voltage graph from one or more cycles in three different formats as well
-      as optionally interpolated, normalized and/or scaled.
-
-    - ``get_cycle_numbers``: get the cycle numbers for your run.
-
-    - ``get_ocv``: get the rest steps after each charge and discharge step.
-
-Take a look at API section (Module index, ``cellpy.readers.cellreader.CellpyCell``) for more info.
-
-Data
-----
-
-.. graphviz::
-
-   digraph {
-    "CellpyCell" -> "Data";
-        "Data" -> "cell metadata (cell)";
-        "Data" -> "cell metadata (test)";
-        "Data" -> "methods";
-        "Data" -> "raw";
-        "Data" -> "steps";
-        "Data" -> "summary";
-   }
-
-
-Each run is a ``cellpy.cellreader.Data`` instance.
-The instance contain general information about
-the run-settings (such as mass etc.).
-The measurement data, information, and summary is stored
-in three ``pandas.DataFrames``:
-
-    - ``raw``: raw data from the run.
-    - ``steps``: stats from each step (and step type), created using the
-      ``CellpyCell.make_step_table`` method.
-    - ``summary``: summary data vs. cycle number (e.g. coulombic coulombic efficiency), created using
-      the ``CellpyCell.make_summary`` method.
-
-The headers (columns) for the different DataFrames were given earlier in this chapter.
-As mentioned above, the ``Data`` object also contains metadata for the run.
-
-metadata
-........
-
-.. code-block:: python
-
-    cell_no = None
-    mass = prms.Materials.default_mass  # active material (in mg)
-    tot_mass = prms.Materials.default_mass  # total material (in mg)
-    no_cycles = 0.0
-    charge_steps = None
-    discharge_steps = None
-    ir_steps = None
-    ocv_steps = None
-    nom_cap = prms.DataSet.nom_cap  # mAh/g (for finding c-rates)
-    mass_given = False
-    material = prms.Materials.default_material
-    merged = False
-    file_errors = None  # not in use at the moment
-    loaded_from = None  # loaded from (can be list if merged)
-    channel_index = None
-    channel_number = None
-    creator = None
-    item_ID = None
-    schedule_file_name = None
-    start_datetime = None
-    test_ID = None
-    name = None
-    cycle_mode = prms.Reader.cycle_mode
-    active_electrode_area = None  # [cm2]
-    active_electrode_thickness = None  # [micron]
-    electrolyte_type = None  #
-    electrolyte_volume = None  # [micro-liter]
-    active_electrode_type = None
-    counter_electrode_type = None
-    reference_electrode_type = None
-    experiment_type = None
-    cell_type = None
-    separator_type = None
-    active_electrode_current_collector = None
-    reference_electrode_current_collector = None
-    comment = None
-
-
-The ``Data`` object can also take custom metadata if provieded as keyword arguments (for developers).
+For each type of testers that are supported by ``cellpy``,
+a set of column headings and other different settings/attributes might also exist.
+These definitions stored in the ``cellpy.parameters.internal_settings`` module and
+are also injected into the ``CellpyCell`` class upon initiation.
 
-FileID
-------
 
-The ``FileID`` object contains information about the raw file(s) and is used when comparing the cellpy-file
-with the raw file(s) (for example to check if it has been updated compared to the cellpy-file).
-Notice that ``FileID`` will contain a list of file identification parameters if the run is from several raw files.
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `cellpy-1.0.0b0/docs/main_description/installation.rst` & `cellpy-1.0.0b1/docs/main_description/installation.rst`

 * *Files 8% similar despite different names*

```diff
@@ -7,53 +7,51 @@
 If you are (relatively) new to installing python packages, please jump to the
 getting started tutorial (:ref:`getting-started`)
 for an opinionated step-by-step procedure.
 
 Stable release
 ==============
 
-The preferred way to install ``cellpy`` is by using conda:
+Conda
+-----
+
+Usually, the easiest way to install ``cellpy`` is by using conda:
 
 .. code-block:: console
 
     $ conda install cellpy --channel conda-forge
 
 
 This will also install all of the critical dependencies, as well as ``jupyter``
 that comes in handy when working with ``cellpy``.
 
-If you would like to install only ``cellpy``, you should install using pip.
-You also need to take into account that ``cellpy`` uses several packages
-that are a bit cumbersome to install on
-windows. It is therefore recommended to install one of the ``anaconda``
-python packages (python 3.9 or above) before installing ``cellpy``.
-If you chose ``miniconda``, you should install
-``scipy``, ``numpy`` and ``pytables`` using ``conda``:
 
-.. code-block:: console
-
-    $ conda install scipy numpy pytables
+Pip
+---
 
+If you would like to install only ``cellpy``, you should install using pip.
+A small warning if you are on Windows: ``cellpy`` uses several packages
+that are a bit cumbersome to install on windows (e.g. ``scipy``, ``numpy`` and ``pytables`` ).
 
-Then install ``cellpy``, by running this command in your terminal:
+Install ``cellpy`` by running this command in your terminal:
 
 .. code-block:: console
 
-    $ pip install cellpy
+    $ python -m pip install cellpy
 
 
 You can install pre-releases by adding the ``--pre`` flag.
 
 If you are on Windows and plan to work with Arbin files,
 we recommend that you try to install `pyodbc`_ (Python ODBC bridge).
 Either by using pip or from conda-forge:
 
 .. code-block:: console
 
-    $ pip install pyodbc
+    $ python -m pip install pyodbc
 
 or:
 
 .. code-block:: console
 
     $ conda install -c conda-forge pyodbc
 
@@ -76,20 +74,29 @@
 This will install a ``.cellpy_prms_USER.conf`` file in your home directory
 (USER = your user name).
 Feel free to edit this to fit your needs.
 
 If you are OK with letting ``cellpy`` select your settings, you can omit
 the `-i` (interactive mode).
 
-.. hint:: It is recommended to run the command also after
+.. hint:: Since ``cellpy`` uses several packages that are a bit cumbersome to
+    install on windows, you circumvent this by install one of the ``anaconda`` python
+    packages (python 3.9 or above) before installing ``cellpy``.
+    Remark, that if you chose ``miniconda``, you need to manually install
+    ``scipy``, ``numpy`` and ``pytables`` using ``conda``:
+
+    .. code-block:: console
+
+        $ conda install scipy numpy pytables
+
+.. hint:: It is recommended to run the ``cellpy setup`` command also after
     each time you upgrade ``cellpy``. It will keep the settings you already
     have in your prms-file and, if the newer version
     has introduced some new parameters, it will add those too.
 
-
 .. hint:: You can restore your prms-file by running ``cellpy setup -r`` if needed
     (*i.e.* get a copy of the default file copied to your user folder).
 
 .. caution:: Since Arbin (at least some versions) uses access database files, you
     will need to install ``pyodbc``, a python ODBC bridge that can talk to database
     files. On windows, at least if you don´t have a newer version of office 365,
     you  most likely need to use Microsoft's dll for handling access
```

### Comparing `cellpy-1.0.0b0/docs/main_description/usage.rst` & `cellpy-1.0.0b1/docs/main_description/usage.rst`

 * *Files 8% similar despite different names*

```diff
@@ -39,50 +39,55 @@
 the different cycles? No problem. Now you are set to extract data
 for specific cycles and steps::
 
     >>> list_of_cycles = c.get_cycle_numbers()
     >>> number_of_cycles = len(list_of_cycles)
     >>> print(f"you have {number_of_cycles} cycles")
     you have 658 cycles
-    >>> current,voltage = c.get_cap(5) # current and voltage for cycle 5
+    >>> current_voltage_df = c.get_cap(5) # current and voltage for cycle 5 (as pandas.DataFrame)
 
 You can also look for open circuit voltage steps::
 
     >>> cycle = 44
-    >>> time1, voltage1 = c.get_ocv(ocv_type='ocvrlx_up', cycle_number=cycle)
-    >>> time2, voltage2 = c.get_ocv(ocv_type='ocvrlx_down', cycle_number=cycle)
+    >>> time_voltage_df1 = c.get_ocv(ocv_type='ocvrlx_up', cycle_number=cycle)
+    >>> time_voltage_df2 = c.get_ocv(ocv_type='ocvrlx_down', cycle_number=cycle)
 
 There are many more methods available, including methods
 for selecting steps and cycles (``get_current``, ``get_voltage``, *etc.*)
 or tuning the data (*e.g.* ``split`` and ``merge``).
 
 Take a look at the index page (:doc:`modules <source/modules>`), some of
 the tutorials (:doc:`tutorials <basics>`) or notebook examples (:doc:`Example notebooks <notebooks>`).
 
 
 2. Convenience methods and tools
 ================================
 
-The easiest way to load a file is to use the ``cellpy.get`` method. It
-interprets the file-type from the file extension and automatically creates
+The ``cellpy.get`` method interprets the file-type from the file extension and automatically creates
 the step table as well as the summary table::
 
     >>> import cellpy
     >>> c = cellpy.get(r"C:\data\20141030_CELL_6_cc_01.res", mass=0.982)
     >>> # or load the cellpy-file:
     >>> # cellpy.get("cellpyfiles/20141030_CELL_6_cc_0.h5")
 
 
-There also exists a method that takes the raw-file name and the cellpy-file name
-as input and only loads the raw-file if the cellpy-file is older than the
+If you provide the raw-file name and the cellpy-file name
+as input, ``cellpy.get`` only loads the raw-file if the cellpy-file is older than the
 raw-file::
 
-    >>> c = cellreader.CellpyCell()
+    >>> c = cellpy.get(raw_file_name, cellpyfile=cellpy_file_name)
+
+Also, if your cell test consists of several raw files, you can provide a list of filenames::
+
     >>> raw_files = [rawfile_01, rawfile_02]
-    >>> c.loadcell(raw_files, cellpy_file)
+    >>> c.get(raw_files, cellpy_file)
+
+``cellpy`` will merge the two files for you and shift the running numbers (such as data-point) into
+one "continuous" file.
 
 ``cellpy`` contains a logger (the logs are saved in the cellpy logging
 directory as defined in the config file). You can set the log level
 (to the screen) by::
 
     >>> from cellpy import log
     >>> log.setup_logging(default_level="DEBUG")
```

### Comparing `cellpy-1.0.0b0/docs/make.bat` & `cellpy-1.0.0b1/docs/make.bat`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.parameters.rst` & `cellpy-1.0.0b1/docs/source/cellpy.parameters.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.configurations.rst` & `cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.configurations.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.loader_specific_modules.rst` & `cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.loader_specific_modules.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.processors.rst` & `cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.processors.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.readers.instruments.rst` & `cellpy-1.0.0b1/docs/source/cellpy.readers.instruments.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.readers.rst` & `cellpy-1.0.0b1/docs/source/cellpy.readers.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.rst` & `cellpy-1.0.0b1/docs/source/cellpy.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.utils.batch_tools.rst` & `cellpy-1.0.0b1/docs/source/cellpy.utils.batch_tools.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/docs/source/cellpy.utils.rst` & `cellpy-1.0.0b1/docs/source/cellpy.utils.rst`

 * *Files identical despite different names*

### Comparing `cellpy-1.0.0b0/setup.py` & `cellpy-1.0.0b1/setup.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,118 +1,118 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""Setup script for PyPI packaging
-
-This script is used for creating the PyPI package.
-
-$ python setup.py sdist # create gzip distr (source dist)
-$ python setup.py bdist_wheel # create build
-$ twine upload dist/* # upload to PyPI
-"""
-import os
-
-from setuptools import find_packages, setup
-
-with open("README.rst") as readme_file:
-    readme = readme_file.read()
-
-with open("HISTORY.rst") as history_file:
-    history = history_file.read()
-
-included_packages = find_packages(
-    exclude=[
-        "build",
-        "docs",
-        "templates",
-        "tests",
-        "examples",
-        "dev_data",
-        "dev_utils",
-        "testdata",
-        "recipe",
-        ".github",
-        ".pytest_cache",
-    ]
-)
-
-# TODO: update this
-requirements = [
-    "scipy",
-    "numpy>=1.16.4",
-    "pandas>=1.5.0",
-    "python-box",
-    "setuptools",
-    "ruamel.yaml",
-    "matplotlib",
-    "openpyxl",
-    "click",
-    "PyGithub",
-    "tqdm",
-    "pint",
-    'pyodbc;platform_system=="windows"',
-    "sqlalchemy>=2.0.0",
-    'sqlalchemy-access;platform_system=="windows"',
-    "python-dotenv",
-    "fabric",
-    # 'tables', # not available by pip
-]
-
-test_requirements = [
-    "lmfit",
-    "pytest",
-]
-
-extra_req_batch = ["ipython", "jupyter", "plotly", "seaborn", "kaleido==0.1.*"]
-extra_req_fit = ["lmfit"]
-extra_req_all = extra_req_batch + extra_req_fit
-
-extra_requirements = {
-    "batch": extra_req_batch,
-    "fit": extra_req_fit,
-    "all": extra_req_all,
-}
-name = "cellpy"
-
-here = os.path.abspath(os.path.dirname(__file__))
-
-user_dir = os.path.expanduser("~")
-
-version_ns = {}
-with open(os.path.join(here, name, "_version.py")) as f:
-    exec(f.read(), {}, version_ns)
-
-description = "Extract and manipulate data from battery data testers."
-
-setup(
-    name=name,
-    version=version_ns["__version__"],
-    description=description,
-    long_description=readme + "\n\n" + history,
-    author="Jan Petter Maehlen",
-    author_email="jepe@ife.no",
-    url="https://github.com/jepegit/cellpy",
-    packages=included_packages,
-    package_dir={"cellpy": "cellpy"},
-    package_data={
-        "parameters": [".cellpy_prms_default.conf"],
-        "utils/data": ["*.h5"],
-        "utils/data/raw": ["*.res"],
-    },
-    entry_points={"console_scripts": ["cellpy=cellpy.cli:cli"]},
-    include_package_data=True,
-    install_requires=requirements,
-    license="MIT license",
-    zip_safe=False,
-    keywords="cellpy",
-    classifiers=[
-        "Development Status :: 5 - Production/Stable",
-        "Intended Audience :: Science/Research",
-        "License :: OSI Approved :: MIT License",
-        "Natural Language :: English",
-        "Programming Language :: Python :: 3.9",
-        "Programming Language :: Python :: 3.10",
-        "Programming Language :: Python :: 3.11",
-    ],
-    test_suite="tests",
-    tests_require=test_requirements,
-    extras_require=extra_requirements,
-)
+#!/usr/bin/env python
+# -*- coding: utf-8 -*-
+"""Setup script for PyPI packaging
+
+This script is used for creating the PyPI package.
+
+$ python setup.py sdist # create gzip distr (source dist)
+$ python setup.py bdist_wheel # create build
+$ twine upload dist/* # upload to PyPI
+"""
+import os
+
+from setuptools import find_packages, setup
+
+with open("README.rst") as readme_file:
+    readme = readme_file.read()
+
+with open("HISTORY.rst") as history_file:
+    history = history_file.read()
+
+included_packages = find_packages(
+    exclude=[
+        "build",
+        "docs",
+        "templates",
+        "tests",
+        "examples",
+        "dev_data",
+        "dev_utils",
+        "testdata",
+        "recipe",
+        ".github",
+        ".pytest_cache",
+    ]
+)
+
+# TODO: update this
+requirements = [
+    "scipy",
+    "numpy>=1.16.4",
+    "pandas>=1.5.0",
+    "python-box",
+    "setuptools",
+    "ruamel.yaml",
+    "matplotlib",
+    "openpyxl",
+    "click",
+    "PyGithub",
+    "tqdm",
+    "pint",
+    'pyodbc;platform_system=="windows"',
+    "sqlalchemy>=2.0.0",
+    'sqlalchemy-access;platform_system=="windows"',
+    "python-dotenv",
+    "fabric",
+    # 'tables', # not available by pip
+]
+
+test_requirements = [
+    "lmfit",
+    "pytest",
+]
+
+extra_req_batch = ["ipython", "jupyter", "plotly", "seaborn", "kaleido==0.1.*"]
+extra_req_fit = ["lmfit"]
+extra_req_all = extra_req_batch + extra_req_fit
+
+extra_requirements = {
+    "batch": extra_req_batch,
+    "fit": extra_req_fit,
+    "all": extra_req_all,
+}
+name = "cellpy"
+
+here = os.path.abspath(os.path.dirname(__file__))
+
+user_dir = os.path.expanduser("~")
+
+version_ns = {}
+with open(os.path.join(here, name, "_version.py")) as f:
+    exec(f.read(), {}, version_ns)
+
+description = "Extract and manipulate data from battery data testers."
+
+setup(
+    name=name,
+    version=version_ns["__version__"],
+    description=description,
+    long_description=readme + "\n\n" + history,
+    author="Jan Petter Maehlen",
+    author_email="jepe@ife.no",
+    url="https://github.com/jepegit/cellpy",
+    packages=included_packages,
+    package_dir={"cellpy": "cellpy"},
+    package_data={
+        "parameters": [".cellpy_prms_default.conf"],
+        "utils/data": ["*.h5"],
+        "utils/data/raw": ["*.res"],
+    },
+    entry_points={"console_scripts": ["cellpy=cellpy.cli:cli"]},
+    include_package_data=True,
+    install_requires=requirements,
+    license="MIT license",
+    zip_safe=False,
+    keywords="cellpy",
+    classifiers=[
+        "Development Status :: 5 - Production/Stable",
+        "Intended Audience :: Science/Research",
+        "License :: OSI Approved :: MIT License",
+        "Natural Language :: English",
+        "Programming Language :: Python :: 3.9",
+        "Programming Language :: Python :: 3.10",
+        "Programming Language :: Python :: 3.11",
+    ],
+    test_suite="tests",
+    tests_require=test_requirements,
+    extras_require=extra_requirements,
+)
```

